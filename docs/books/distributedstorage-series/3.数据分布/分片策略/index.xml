<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>分片策略 | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/</link>
      <atom:link href="https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/index.xml" rel="self" type="application/rss+xml" />
    <description>分片策略</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>分片策略</title>
      <link>https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/</link>
    </image>
    
    <item>
      <title>次级索引</title>
      <link>https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/%E6%AC%A1%E7%BA%A7%E7%B4%A2%E5%BC%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/%E6%AC%A1%E7%BA%A7%E7%B4%A2%E5%BC%95/</guid>
      <description>&lt;h1 id=&#34;分片与次级索引&#34;&gt;分片与次级索引&lt;/h1&gt;
&lt;p&gt;到目前为止，我们讨论的分区方案依赖于键值数据模型。如果只通过主键访问记录，我们可以从该键确定分区，并使用它来将读写请求路由到负责该键的分区。辅助索引通常并不能唯一地标识记录，而是一种搜索记录中出现特定值的方式：查找用户 123 的所有操作，查找包含词语 hogwash 的所有文章，查找所有颜色为红色的车辆等等。&lt;/p&gt;
&lt;p&gt;次级索引是关系型数据库的基础，并且在文档数据库中也很普遍。许多键值存储（如 HBase 和 Volde-mort）为了减少实现的复杂度而放弃了次级索引，但是一些（如 Riak）已经开始添加它们，因为它们对于数据模型实在是太有用了。并且次级索引也是 Solr 和 Elasticsearch 等搜索服务器的基石。&lt;/p&gt;
&lt;p&gt;次级索引的问题是它们不能整齐地映射到分区。有两种用二级索引对数据库进行分区的方法：基于文档的分区（document-based）和基于关键词（term-based）的分区。&lt;/p&gt;
&lt;h1 id=&#34;按文档的二级索引&#34;&gt;按文档的二级索引&lt;/h1&gt;
&lt;p&gt;假设你正在经营一个销售二手车的网站，每个列表都有一个唯一的 文档 ID，并且用文档 ID 对数据库进行分区（例如，分区 0 中的 ID 0 到 499，分区 1 中的 ID 500 到 999 等）。你想让用户搜索汽车，允许他们通过颜色和厂商过滤，所以需要一个在颜色和厂商上的次级索引（文档数据库中这些是字段（field），关系数据库中这些是列（column））如果您声明了索引，则数据库可以自动执行索引。例如，无论何时将红色汽车添加到数据库，数据库分区都会自动将其添加到索引条目 &lt;code&gt;color：red&lt;/code&gt; 的文档 ID 列表中。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2020/02/09/1hQJMt.md.png&#34; alt=&#34;按文档分区二级索引&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在这种索引方法中，每个分区是完全独立的：每个分区维护自己的二级索引，仅覆盖该分区中的文档。它不关心存储在其他分区的数据。无论何时您需要写入数据库（添加，删除或更新文档），只需处理包含您正在编写的文档 ID 的分区即可。出于这个原因，文档分区索引也被称为本地索引（local index）。&lt;/p&gt;
&lt;p&gt;但是，从文档分区索引中读取需要注意：除非您对文档 ID 做了特别的处理，否则没有理由将所有具有特定颜色或特定品牌的汽车放在同一个分区中。譬如，红色汽车出现在分区 0 和分区 1 中。因此，如果要搜索红色汽车，则需要将查询发送到所有分区，并合并所有返回的结果。&lt;/p&gt;
&lt;p&gt;这种查询分区数据库的方法有时被称为分散/聚集（scatter/gather），并且可能会使二级索引上的读取查询相当昂贵。即使并行查询分区，分散/聚集也容易导致尾部延迟放大。然而，它被广泛使用：MongoDB，Riak，Cassandra，Elasticsearch，SolrCloud 和 VoltDB 都使用文档分区二级索引。大多数数据库供应商建议您构建一个能从单个分区提供二级索引查询的分区方案，但这并不总是可行，尤其是当在单个查询中使用多个二级索引时（例如同时需要按颜色和制造商查询）。&lt;/p&gt;
&lt;h1 id=&#34;根据关键词term的二级索引&#34;&gt;根据关键词(Term)的二级索引&lt;/h1&gt;
&lt;p&gt;我们可以构建一个覆盖所有分区数据的全局索引，而不是给每个分区创建自己的次级索引（本地索引）。但是，我们不能只把这个索引存储在一个节点上，因为它可能会成为瓶颈，违背了分区的目的。全局索引也必须进行分区，但可以采用与主键不同的分区方式。&lt;/p&gt;
&lt;p&gt;下图述了这可能是什么样子：来自所有分区的红色汽车在红色索引中，并且索引是分区的，首字母从 a 到 r 的颜色在分区 0 中，s 到 z 的在分区 1。汽车制造商的索引也与之类似（分区边界在 f 和 h 之间）。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2020/02/09/1hQTQ1.md.png&#34; alt=&#34;按关键词对二级索引进行分区&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;我们将这种索引称为关键词分区（term-partitioned），因为我们寻找的关键词决定了索引的分区方式。例如，一个关键词可能是：颜色：红色。关键词(Term) 来源于来自全文搜索索引（一种特殊的次级索引），指文档中出现的所有单词。和之前一样，我们可以通过关键词本身或者它的哈希进行索引分区。根据它本身分区对于范围扫描非常有用（例如对于数字,像汽车的报价），而对关键词的哈希分区提供了负载均衡的能力。&lt;/p&gt;
&lt;p&gt;关键词分区的全局索引优于文档分区索引的地方点是它可以使读取更有效率：不需要分散/收集所有分区，客户端只需要向包含关键词的分区发出请求。全局索引的缺点在于写入速度较慢且较为复杂，因为写入单个文档现在可能会影响索引的多个分区（文档中的每个关键词可能位于不同的分区或者不同的节点上）。&lt;/p&gt;
&lt;p&gt;理想情况下，索引总是最新的，写入数据库的每个文档都会立即反映在索引中。但是，在关键词分区索引中，这需要跨分区的分布式事务，并不是所有数据库都支持。在实践中，对全局二级索引的更新通常是异步的（也就是说，如果在写入之后不久读取索引，刚才所做的更改可能尚未反映在索引中）。例如，Amazon DynamoDB 声称在正常情况下，其全局次级索引会在不到一秒的时间内更新，但在基础架构出现故障的情况下可能会有延迟。&lt;/p&gt;
&lt;p&gt;全局关键词分区索引的其他用途包括 Riak 的搜索功能和 Oracle 数据仓库，它允许您在本地和全局索引之间进行选择。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>分片再平衡</title>
      <link>https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/%E5%88%86%E7%89%87%E5%86%8D%E5%B9%B3%E8%A1%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/%E5%88%86%E7%89%87%E5%86%8D%E5%B9%B3%E8%A1%A1/</guid>
      <description>&lt;h1 id=&#34;分区再平衡&#34;&gt;分区再平衡&lt;/h1&gt;
&lt;p&gt;随着时间的推移，数据库会有各种变化。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查询吞吐量增加，所以您想要添加更多的 CPU 来处理负载。&lt;/li&gt;
&lt;li&gt;数据集大小增加，所以您想添加更多的磁盘和 RAM 来存储它。&lt;/li&gt;
&lt;li&gt;机器出现故障，其他机器需要接管故障机器的责任。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所有这些更改都需要数据和请求从一个节点移动到另一个节点将负载从集群中的一个节点向另一个节点移动的过程称为再平衡（reblancing）。无论使用哪种分区方案，再平衡通常都要满足一些最低要求：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;再平衡之后，负载（数据存储，读取和写入请求）应该在集群中的节点之间公平地共享。&lt;/li&gt;
&lt;li&gt;再平衡发生时，数据库应该继续接受读取和写入。&lt;/li&gt;
&lt;li&gt;节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘 I/O 负载。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;hash-mod-n&#34;&gt;hash mod N&lt;/h1&gt;
&lt;p&gt;在分片策略中我们讨论过最好将可能的哈希分成不同的范围，并将每个范围分配给一个分区（例如，如果 $0≤hash(key)&amp;lt;b_0$，则将键分配给分区 0，如果 $b_0 ≤ hash(key) &amp;lt;b_1$，则分配给分区 1）。&lt;/p&gt;
&lt;p&gt;也许你想知道为什么我们不使用 mod（许多编程语言中的％运算符）。例如，hash(key) mod 10 会返回一个介于 0 和 9 之间的数字（如果我们将哈希写为十进制数，哈希模 10 将是最后一个数字）。如果我们有 10 个节点，编号为 0 到 9，这似乎是将每个键分配给一个节点的简单方法。&lt;/p&gt;
&lt;p&gt;模 $N$ 方法的问题是，如果节点数量 N 发生变化，大多数密钥将需要从一个节点移动到另一个节点。例如，假设 $hash(key)=123456$。如果最初有 10 个节点，那么这个键一开始放在节点 6 上（因为 $123456\ mod\ 10 = 6$）。当您增长到 11 个节点时，密钥需要移动到节点 3（$123456\ mod\ 11 = 3$），当您增长到 12 个节点时，需要移动到节点 0（$123456\ mod\ 12 = 0$）。这种频繁的举动使得重新平衡过于昂贵。&lt;/p&gt;
&lt;h1 id=&#34;固定数量的分区&#34;&gt;固定数量的分区&lt;/h1&gt;
&lt;p&gt;针对上面的问题，我们可以创建比节点更多的分区，并为每个节点分配多个分区。例如，运行在 10 个节点的集群上的数据库可能会从一开始就被拆分为 1,000 个分区，因此大约有 100 个分区被分配给每个节点。&lt;/p&gt;
&lt;p&gt;现在，如果一个节点被添加到集群中，新节点可以从当前每个节点中窃取一些分区，直到分区再次公平分配。如果从集群中删除一个节点，则会发生相反的情况。只有分区在节点之间的移动。分区的数量不会改变，键所指定的分区也不会改变。唯一改变的是分区所在的节点。这种变更并不是即时的，在网络上传输大量的数据需要一些时间，所以在传输过程中，原有分区仍然会接受读写操作。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2020/02/09/1h1ph4.png&#34; alt=&#34;将新节点添加到每个节点具有多个分区的数据库群集&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;原则上，您甚至可以解决集群中的硬件不匹配问题：通过为更强大的节点分配更多的分区，可以强制这些节点承载更多的负载。在 Riak，Elasticsearch，Couchbase 和 Voldemort 中使用了这种再平衡的方法。在这种配置中，分区的数量通常在数据库第一次建立时确定，之后不会改变。虽然原则上可以分割和合并分区，但固定数量的分区在操作上更简单，因此许多固定分区数据库选择不实施分区分割。因此，一开始配置的分区数就是您可以拥有的最大节点数量，所以您需要选择足够多的分区以适应未来的增长。但是，每个分区也有管理开销，所以选择太大的数字会适得其反。&lt;/p&gt;
&lt;p&gt;如果数据集的总大小难以预估（例如，如果它开始很小，但随着时间的推移可能会变得更大），选择正确的分区数是困难的。由于每个分区包含了总数据量固定比率的数据，因此每个分区的大小与集群中的数据总量成比例增长。如果分区非常大，再平衡和从节点故障恢复变得昂贵。但是，如果分区太小，则会产生太多的开销。当分区大小“恰到好处”的时候才能获得很好的性能，如果分区数量固定，但数据量变动很大，则难以达到最佳性能。&lt;/p&gt;
&lt;h1 id=&#34;动态分区&#34;&gt;动态分区&lt;/h1&gt;
&lt;p&gt;对于使用键范围分区的数据库，具有固定边界的固定数量的分区将非常不便：如果出现边界错误，则可能会导致一个分区中的所有数据或者其他分区中的所有数据为空。手动重新配置分区边界将非常繁琐。出于这个原因，按键的范围进行分区的数据库（如 HBase 和 RethinkDB）会动态创建分区。当分区增长到超过配置的大小时（在 HBase 上，默认值是 10GB），会被分成两个分区，每个分区约占一半的数据。与之相反，如果大量数据被删除并且分区缩小到某个阈值以下，则可以将其与相邻分区合并。此过程与 B 树顶层发生的过程类似。&lt;/p&gt;
&lt;p&gt;每个分区分配给一个节点，每个节点可以处理多个分区，就像固定数量的分区一样。大型分区拆分后，可以将其中的一半转移到另一个节点，以平衡负载。在 HBase 中，分区文件的传输通过 HDFS（底层分布式文件系统）来实现。动态分区的一个优点是分区数量适应总数据量。如果只有少量的数据，少量的分区就足够了，所以开销很小;如果有大量的数据，每个分区的大小被限制在一个可配置的最大值。&lt;/p&gt;
&lt;p&gt;需要注意的是，一个空的数据库从一个分区开始，因为没有关于在哪里绘制分区边界的先验信息。数据集开始时很小，直到达到第一个分区的分割点，所有写入操作都必须由单个节点处理，而其他节点则处于空闲状态。为了解决这个问题，HBase 和 MongoDB 允许在一个空的数据库上配置一组初始分区（这被称为预分割（pre-splitting））。在键范围分区的情况中，预分割需要提前知道键是如何进行分配的。&lt;/p&gt;
&lt;p&gt;动态分区不仅适用于数据的范围分区，而且也适用于哈希分区。从版本 2.4 开始，MongoDB 同时支持范围和哈希分区，并且都是进行动态分割分区。&lt;/p&gt;
&lt;h1 id=&#34;按节点比例分区&#34;&gt;按节点比例分区&lt;/h1&gt;
&lt;p&gt;通过动态分区，分区的数量与数据集的大小成正比，因为拆分和合并过程将每个分区的大小保持在固定的最小值和最大值之间。另一方面，对于固定数量的分区，每个分区的大小与数据集的大小成正比。在这两种情况下，分区的数量都与节点的数量无关。Cassandra 和 Ketama 使用的第三种方法是使分区数与节点数成正比，即每个节点具有固定数量的分区。&lt;/p&gt;
&lt;p&gt;在这种情况下，每个分区的大小与数据集大小成比例地增长，而节点数量保持不变，但是当增加节点数时，分区将再次变小。由于较大的数据量通常需要较大数量的节点进行存储，因此这种方法也使每个分区的大小较为稳定。当一个新节点加入集群时，它随机选择固定数量的现有分区进行拆分，然后占有这些拆分分区中每个分区的一半，同时将每个分区的另一半留在原地。随机化可能会产生不公平的分割，但是平均在更大数量的分区上时（在 Cassandra 中，默认情况下，每个节点有 256 个分区），新节点最终从现有节点获得公平的负载份额 Cassandra 3.0 引入了另一种再分配的算法来避免不公平的分割。&lt;/p&gt;
&lt;p&gt;随机选择分区边界要求使用基于哈希的分区（可以从哈希函数产生的数字范围中挑选边界）。实际上，这种方法最符合一致性哈希的原始定义。最新的哈希函数可以在较低元数据开销的情况下达到类似的效果。&lt;/p&gt;
&lt;h1 id=&#34;运维策略&#34;&gt;运维策略&lt;/h1&gt;
&lt;p&gt;在全自动重新平衡（系统自动决定何时将分区从一个节点移动到另一个节点，无须人工干预）和完全手动（分区指派给节点由管理员明确配置，仅在管理员明确重新配置时才会更改）之间有一个权衡。例如，Couchbase，Riak 和 Voldemort 会自动生成建议的分区分配，但需要管理员提交才能生效。&lt;/p&gt;
&lt;p&gt;全自动重新平衡可以很方便，因为正常维护的操作工作较少。但是，这可能是不可预测的。再平衡是一个昂贵的操作，因为它需要重新路由请求并将大量数据从一个节点移动到另一个节点。如果没有做好，这个过程可能会使网络或节点负载过重，降低其他请求的性能。&lt;/p&gt;
&lt;p&gt;这种自动化与自动故障检测相结合可能十分危险。例如，假设一个节点过载，并且对请求的响应暂时很慢。其他节点得出结论：过载的节点已经死亡，并自动重新平衡集群，使负载离开它。这会对已经超负荷的节点，其他节点和网络造成额外的负载，从而使情况变得更糟，并可能导致级联失败。&lt;/p&gt;
&lt;p&gt;出于这个原因，再平衡的过程中有人参与是一件好事。这比完全自动的过程慢，但可以帮助防止运维意外。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>键的分片</title>
      <link>https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/%E9%94%AE%E7%9A%84%E5%88%86%E7%89%87/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/distributedstorage-series/3.%E6%95%B0%E6%8D%AE%E5%88%86%E5%B8%83/%E5%88%86%E7%89%87%E7%AD%96%E7%95%A5/%E9%94%AE%E7%9A%84%E5%88%86%E7%89%87/</guid>
      <description>&lt;h1 id=&#34;键值数据的分片&#34;&gt;键值数据的分片&lt;/h1&gt;
&lt;p&gt;假设你有大量数据并且想要分区,如何决定在哪些节点上存储哪些记录呢？分区目标是将数据和查询负载均匀分布在各个节点上。如果每个节点公平分享数据和负载，那么理论上 10 个节点应该能够处理 10 倍的数据量和 10 倍的单个节点的读写吞吐量（暂时忽略复制）。&lt;/p&gt;
&lt;p&gt;如果分区是不公平的，一些分区比其他分区有更多的数据或查询，我们称之为偏斜（skew）。数据偏斜的存在使分区效率下降很多。在极端的情况下，所有的负载可能压在一个分区上，其余 9 个节点空闲的，瓶颈落在这一个繁忙的节点上。不均衡导致的高负载的分区被称为热点（hot spot）。&lt;/p&gt;
&lt;p&gt;避免热点最简单的方法是将记录随机分配给节点。这将在所有节点上平均分配数据，但是它有一个很大的缺点：当你试图读取一个特定的值时，你无法知道它在哪个节点上，所以你必须并行地查询所有的节点。&lt;/p&gt;
&lt;h1 id=&#34;range-based-sharding基于范围分片&#34;&gt;Range-based sharding，基于范围分片&lt;/h1&gt;
&lt;p&gt;基于范围的分片假定数据库系统中的所有键都可以排序，并且将键的连续部分作为分片单元。如果知道范围之间的边界，则可以轻松确定哪个分区包含某个值。如果您还知道分区所在的节点，那么可以直接向相应的节点发出请求（对于百科全书而言，就像从书架上选取正确的书籍）。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2020/02/09/1hKU6P.md.png&#34; alt=&#34;印刷版百科全书按照关键字范围进行分区&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;键的范围不一定均匀分布，因为数据也很可能不均匀分布。例如在上图中，第 1 卷包含以 A 和 B 开头的单词，但第 12 卷则包含以 T，U，V，X，Y 和 Z 开头的单词。只是简单的规定每个卷包含两个字母会导致一些卷比其他卷大。为了均匀分配数据，分区边界需要依据数据调整。&lt;/p&gt;
&lt;p&gt;分区边界可以由管理员手动选择，也可以由数据库自动选择，Bigtable，HBase，RethinkDB 以及 2.4 版本之前的 MondoDB 都是使用了这种分片策略。HBase Keys 按字节顺序排序，而 MySQL 密钥按自动增量 ID 顺序排序。对于譬如对于日志结构的合并树（LSM-Tree）和 B-Tree，键自然是有序的。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2020/01/25/1euDN6.png&#34; alt=&#34;Range-based sharding for data partitioning&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;如上图就是 MongoDB 的基于范围的分片策略，键空间（Key Space）被划分为了 &lt;code&gt;(minKey, maxKey)&lt;/code&gt;。每个分片单元（块）都是连续键的一部分。基于范围的分片的优点是相邻数据在一起的可能性很高（例如具有公共前缀的数据），可以很好地支持“范围扫描（Range Scan）”之类的操作。例如，HBase Region 是一种典型的基于范围的分片策略。&lt;/p&gt;
&lt;p&gt;在每个分区中，我们可以按照一定的顺序保存键，好处是进行范围扫描非常简单，您可以将键作为联合索引来处理，以便在一次查询中获取多个相关记录。例如，假设我们有一个程序来存储传感器网络的数据，其中主键是测量的时间戳（年月日时分秒）。范围扫描在这种情况下非常有用，因为我们可以轻松获取某个月份的所有数据。关系数据库通常需要执行“表扫描”或“索引扫描”，因此它们常选择基于范围的分片。&lt;/p&gt;
&lt;p&gt;但是，基于范围的分片对工作量大的顺序写入不友好。还是刚才的物联网例子中，在时间序列类型的写入负载的时候，写入热点始终位于最后一个区域中。发生这种情况是因为日志键通常与时间戳有关，并且时间单调增加。为了避免传感器数据库中的这个问题，需要使用除了时间戳以外的其他东西作为主键的第一个部分例如，可以在每个时间戳前添加传感器名称，这样会首先按传感器名称，然后按时间进行分区假设有多个传感器同时运行，写入负载将最终均匀分布在不同分区上。现在，当想要在一个时间范围内获取多个传感器的值时，您需要为每个传感器名称执行一个单独的范围查询。&lt;/p&gt;
&lt;h1 id=&#34;hash-based-sharding&#34;&gt;Hash-based sharding&lt;/h1&gt;
&lt;p&gt;由于偏斜和热点的风险，许多分布式数据存储使用哈希函数来确定给定键的分区，即基于哈希的分片使用哈希函数处理密钥，然后使用结果获取分片 ID。一个好的哈希函数可以将将偏斜的数据均匀分布。假设你有一个 32 位哈希函数,无论何时给定一个新的字符串输入，它将返回一个 0 到$2^{32}$ -1 之间的&amp;quot;随机&amp;quot;数。即使输入的字符串非常相似，它们的哈希也会均匀分布在这个数字范围内。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2020/02/09/1hMM3n.md.png&#34; alt=&#34;按哈希键分区&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;出于分区的目的，哈希函数不需要多么强壮的加密算法：例如，Cassandra 和 MongoDB 使用 MD5，Voldemort 使用 Fowler-Noll-Vo 函数。许多编程语言都有内置的简单哈希函数（它们用于哈希表），但是它们可能不适合分区：例如，在 Java 的&lt;code&gt;Object.hashCode()&lt;/code&gt;和 Ruby 的&lt;code&gt;Object#hash&lt;/code&gt;，同一个键可能在不同的进程中有不同的哈希值。一旦你有一个合适的键哈希函数，你可以为每个分区分配一个哈希范围（而不是键的范围），每个通过哈希哈希落在分区范围内的键将被存储在该分区中。&lt;/p&gt;
&lt;p&gt;基于哈希的分片的一些典型示例是 Cassandra 一致性哈希，Redis Cluster 和 Codis 的 Presharding 以及 Twemproxy 一致性哈希。如下图所示的就是 MongoDB 中基于 Hash 的分片策略：&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2020/01/25/1eKu8O.md.png&#34; alt=&#34;Hash-based sharding for data partitioning&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;与基于范围的分片相反，基于哈希的分片具有以下优点：密钥几乎是随机分布的，因此分布是均匀的。因此，它对于写入工作量和读取工作量几乎都是随机的系统更为友好。这是因为写入压力可以均匀地分布在群集中，从而使“范围扫描”之类的操作非常困难。&lt;/p&gt;
&lt;p&gt;不幸的是，通过使用 Key 哈希进行分区，我们失去了键范围分区的一个很好的属性：高效执行范围查询的能力。曾经相邻的密钥现在分散在所有分区中，所以它们之间的顺序就丢失了。在 MongoDB 中，如果您使用了基于哈希的分区模式，则任何范围查询都必须发送到所有分区。Riak，Couchbase 或 Voldemort 不支持主键上的范围查询。Cassandra 采取了折衷的策略 Cassandra 中的表可以使用由多个列组成的复合主键来声明。键中只有第一列会作为哈希的依据，而其他列则被用作 Casssandra 的 SSTables 中排序数据的连接索引。尽管查询无法在复合主键的第一列中按范围扫表，但如果第一列已经指定了固定值，则可以对该键的其他列执行有效的范围扫描。&lt;/p&gt;
&lt;p&gt;请注意，基于哈希和基于范围的分片策略不是隔离的。相反，您可以灵活地组合它们。例如，您可以建立一个多级分片策略，该策略在最上层使用哈希，而在每个基于哈希的分片单元中，数据将按顺序存储。譬如在社交媒体网站上，一个用户可能会发布很多更新。如果更新的主键被选择为 &lt;code&gt;(user_id, update_timestamp)&lt;/code&gt;，那么您可以有效地检索特定用户在某个时间间隔内按时间戳排序的所有更新。不同的用户可以存储在不同的分区上，对于每个用户，更新按时间戳顺序存储在单个分区上。&lt;/p&gt;
&lt;h1 id=&#34;策略选择&#34;&gt;策略选择&lt;/h1&gt;
&lt;p&gt;对于弹性可伸缩性，使用基于范围的分片的系统很容易实现：只需拆分 Region。假设您有一个范围区域[1，100），则只需选择一个分割点，例如 50。然后将此区域分为 &lt;code&gt;[1，50)&lt;/code&gt; 和 &lt;code&gt;[50，100)&lt;/code&gt;。之后，将两个区域移动到两台不同的计算机中，并且负载达到平衡。&lt;/p&gt;
&lt;p&gt;基于范围的分片可能会带来读写热点，但是可以通过拆分和移动消除这些热点。热点的拆分和移动落后于基于哈希的分片。但是总的来说，对于关系数据库，基于范围的分片是一个不错的选择。&lt;/p&gt;
&lt;p&gt;相反，为使用基于哈希的分片的系统实现弹性可伸缩性非常昂贵。原因很明显。假定当前系统有三个节点，然后添加一个新的物理节点。在哈希模型中，n 从 3 更改为 4，这会导致较大的系统抖动。尽管您可以使用像 Ketama 这样的一致的哈希算法来尽可能减少系统抖动，但很难完全避免这种情况。这是因为在应用哈希函数后，数据将随机分配，并且调整哈希算法肯定会更改大多数数据的分配规则。&lt;/p&gt;
&lt;h2 id=&#34;负载倾斜与消除热点&#34;&gt;负载倾斜与消除热点&lt;/h2&gt;
&lt;p&gt;如前所述，哈希分区可以帮助减少热点。但是，它不能完全避免它们：在极端情况下，所有的读写操作都是针对同一个键的，所有的请求都会被路由到同一个分区。这种场景也许并不常见，但并非闻所未闻：例如，在社交媒体网站上，一个拥有数百万追随者的名人用户在做某事时可能会引发一场风暴。这个事件可能导致大量写入同一个键（键可能是名人的用户 ID，或者人们正在评论的动作的 ID）。哈希策略不起作用，因为两个相同 ID 的哈希值仍然是相同的。&lt;/p&gt;
&lt;p&gt;如今，大多数数据系统无法自动补偿这种高度偏斜的负载，因此应用程序有责任减少偏斜。例如，如果一个主键被认为是非常火爆的，一个简单的方法是在主键的开始或结尾添加一个随机数。只要一个两位数的十进制随机数就可以将主键分散为 100 种不同的主键,从而存储在不同的分区中。&lt;/p&gt;
&lt;p&gt;然而，将主键进行分割之后，任何读取都必须要做额外的工作，因为他们必须从所有 100 个主键分布中读取数据并将其合并。此技术还需要额外的记录：只需要对少量热点附加随机数;对于写入吞吐量低的绝大多数主键来是不必要的开销。因此，您还需要一些方法来跟踪哪些键需要被分割。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
