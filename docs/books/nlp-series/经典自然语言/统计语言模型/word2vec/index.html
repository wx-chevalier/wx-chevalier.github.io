<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="Word2Vec 词向量最直观的理解就是将每一个单词表征为 深度学习(DeepLearning)在图像、语音、视频等多方应用中大放异彩，从本质而言，深度学习是表征学习(Representation Learning)的一"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/"><meta property="og:title" content="Word2Vec | Next-gen Tech Edu"><meta property="og:description" content="Word2Vec 词向量最直观的理解就是将每一个单词表征为 深度学习(DeepLearning)在图像、语音、视频等多方应用中大放异彩，从本质而言，深度学习是表征学习(Representation Learning)的一"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>Word2Vec | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=7d6fcad0618b187e8b2729105424f0db><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">统计语言模型</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id5e80cd7d6ea0bbeea81f44344f06ffea")' href=#id5e80cd7d6ea0bbeea81f44344f06ffea aria-expanded=false aria-controls=id5e80cd7d6ea0bbeea81f44344f06ffea aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/>经典自然语言</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id5e80cd7d6ea0bbeea81f44344f06ffea aria-expanded=false aria-controls=id5e80cd7d6ea0bbeea81f44344f06ffea><i class="fa-solid fa-angle-down" id=caret-id5e80cd7d6ea0bbeea81f44344f06ffea></i></a></div><ul class="nav docs-sidenav collapse show" id=id5e80cd7d6ea0bbeea81f44344f06ffea><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idf9cb082466e74db79015a924753627b4")' href=#idf9cb082466e74db79015a924753627b4 aria-expanded=false aria-controls=idf9cb082466e74db79015a924753627b4 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/>词嵌入</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idf9cb082466e74db79015a924753627b4 aria-expanded=false aria-controls=idf9cb082466e74db79015a924753627b4><i class="fa-solid fa-angle-right" id=caret-idf9cb082466e74db79015a924753627b4></i></a></div><ul class="nav docs-sidenav collapse" id=idf9cb082466e74db79015a924753627b4><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id192ce96c2ca4a85e961d10a6c57769b7")' href=#id192ce96c2ca4a85e961d10a6c57769b7 aria-expanded=false aria-controls=id192ce96c2ca4a85e961d10a6c57769b7 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/>词向量</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id192ce96c2ca4a85e961d10a6c57769b7 aria-expanded=false aria-controls=id192ce96c2ca4a85e961d10a6c57769b7><i class="fa-solid fa-angle-right" id=caret-id192ce96c2ca4a85e961d10a6c57769b7></i></a></div><ul class="nav docs-sidenav collapse" id=id192ce96c2ca4a85e961d10a6c57769b7><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/>基于 Gensim 的 Word2Vec 实践</a></li></ul></div><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E6%A6%82%E8%BF%B0/>概述</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idd85c6af5640425111159bb9469160c26")' href=#idd85c6af5640425111159bb9469160c26 aria-expanded=false aria-controls=idd85c6af5640425111159bb9469160c26 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>统计语言模型</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idd85c6af5640425111159bb9469160c26 aria-expanded=false aria-controls=idd85c6af5640425111159bb9469160c26><i class="fa-solid fa-angle-down" id=caret-idd85c6af5640425111159bb9469160c26></i></a></div><ul class="nav docs-sidenav collapse show" id=idd85c6af5640425111159bb9469160c26><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6411e45dc7bee277ca6705e0235b3a88")' href=#id6411e45dc7bee277ca6705e0235b3a88 aria-expanded=false aria-controls=id6411e45dc7bee277ca6705e0235b3a88 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/>BERT</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id6411e45dc7bee277ca6705e0235b3a88 aria-expanded=false aria-controls=id6411e45dc7bee277ca6705e0235b3a88><i class="fa-solid fa-angle-right" id=caret-id6411e45dc7bee277ca6705e0235b3a88></i></a></div><ul class="nav docs-sidenav collapse" id=id6411e45dc7bee277ca6705e0235b3a88><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/>目标函数</a></li><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA/>输入表示</a></li></ul></div><li class="child level active"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/>Word2Vec</a></li><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E8%AF%8D%E8%A1%A8%E7%A4%BA/>词表示</a></li><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/>基础文本处理</a></li><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>统计语言模型</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id4a648d9ccd6a02ca8fdc13681fefab7f")' href=#id4a648d9ccd6a02ca8fdc13681fefab7f aria-expanded=false aria-controls=id4a648d9ccd6a02ca8fdc13681fefab7f aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%AD%E6%B3%95%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/>语法语义分析</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id4a648d9ccd6a02ca8fdc13681fefab7f aria-expanded=false aria-controls=id4a648d9ccd6a02ca8fdc13681fefab7f><i class="fa-solid fa-angle-right" id=caret-id4a648d9ccd6a02ca8fdc13681fefab7f></i></a></div><ul class="nav docs-sidenav collapse" id=id4a648d9ccd6a02ca8fdc13681fefab7f><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%AD%E6%B3%95%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/>命名实体识别</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id7905f06a5081e168ca189bc50bfa7e6c")' href=#id7905f06a5081e168ca189bc50bfa7e6c aria-expanded=false aria-controls=id7905f06a5081e168ca189bc50bfa7e6c aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/>主题模型</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id7905f06a5081e168ca189bc50bfa7e6c aria-expanded=false aria-controls=id7905f06a5081e168ca189bc50bfa7e6c><i class="fa-solid fa-angle-right" id=caret-id7905f06a5081e168ca189bc50bfa7e6c></i></a></div><ul class="nav docs-sidenav collapse" id=id7905f06a5081e168ca189bc50bfa7e6c><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/lda/>LDA</a></li></ul></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#python>Python</a><ul><li><a href=#中文实验>中文实验</a></li></ul></li><li><a href=#维基百科实验>维基百科实验</a></li></ul><ul><li><a href=#cbow>CBOW</a></li><li><a href=#skip-gram>Skip-Gram</a></li></ul><ul><li><a href=#learning-phrases>Learning Phrases</a></li></ul><ul><li><a href=#deeplearning4j>Deeplearning4j</a></li><li><a href=#python-1>Python</a></li></ul><ul><li><a href=#training>Training</a></li><li><a href=#predictions>Predictions</a><ul><li><a href=#phrases>Phrases</a></li><li><a href=#analogies>Analogies</a></li><li><a href=#clusters>Clusters</a></li></ul></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>Word2Vec</h1><div class=article-style><h1 id=word2vec>Word2Vec</h1><p>词向量最直观的理解就是将每一个单词表征为</p><p>深度学习(DeepLearning)在图像、语音、视频等多方应用中大放异彩，从本质而言，深度学习是表征学习(Representation Learning)的一种方法，可以看做对事物进行分类的不同过滤器的组成。</p><p>Word2Vec 是 Google 在 2013 年年中开源的一款将词表征为实数值向量的高效 工具，采用的模型有 CBOW (Continuous Bag-Of-Words，即连续的词袋模型)和 Skip-Gram 两种。word2vec 代码链接为：https://code.google.com/p/word2vec/，遵循 Apache License 2.0 开源协议，是一种对商业应用友好的许可，当然需要充分尊重原作者的著作权。Word2Vec 采用了所谓的 Distributed Representation 方式来表示词。Distributed representation 最早是 Hinton 在 1986 年的论文《Learning distributed representations of concepts》中提出的。虽然这篇文章没有说要将词做 Distributed representation，但至少这种先进的思想在那个时候就在人们的心中埋下了火种，到 2000 年之后开始逐渐被人重视。Distributed representation 用来表示词，通常被称为“Word Representation”或“Word Embedding”，中文俗称“词向量”。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=http://deeplearning4j.org/img/word2vec.png alt loading=lazy data-zoomable></div></div></figure></p><p>Word2vec 是一个神经网络，它用来在使用深度学习算法之前预处理文本。它本身并没有实现深度学习，但是 Word2Vec 把文本变成深度学习能够理解的向量形式。</p><p>Word2vec 在不需要人工干预的情况下创建特征，包括词的上下文特征。这些上下文来自于多个词的窗口。如果有足够多的数据，用法和上下文，Word2Vec 能够基于这个词的出现情况高度精确的预测一个词的词义(对于深度学习来说，一个词的词义只是一个简单的信号，这个信号能用来对更大的实体分类；比如把一个文档分类到一个类别中)。</p><p>Word2vec 需要一串句子做为其输入。每个句子，也就是一个词的数组，被转换成 n 维向量空间中的一个向量并且可以和其它句子(词的数组)所转换成向量进行比较。在这个向量空间里，相关的词语和词组会出现在一起。把它们变成向量之后，我们可以一定程度的计算它们的相似度并且对其进行聚类。这些类别可以作为搜索，情感分析和推荐的基础。</p><p>Word2vec 神经网络的输出是一个词表，每个词由一个向量来表示，这个向量可以做为深度神经网络的输入来进行分类。</p><h1 id=quick-start>Quick Start</h1><h2 id=python>Python</h2><p>笔者推荐使用 Anaconda 这个 Python 的机器学习发布包，此处用的测试数据来自于<a href=http://mattmahoney.net/dc/text8.zip target=_blank rel=noopener>这里</a></p><ul><li>Installation</li></ul><p>使用<code>pip install word2vec</code>，然后使用<code>import word2vec</code>引入</p><ul><li>文本文件预处理</li></ul><pre tabindex=0><code>word2vec.word2phrase(&#39;/Users/drodriguez/Downloads/text8&#39;, &#39;/Users/drodriguez/Downloads/text8-phrases&#39;, verbose=True)
</code></pre><pre tabindex=0><code>[u&#39;word2phrase&#39;, u&#39;-train&#39;, u&#39;/Users/drodriguez/Downloads/text8&#39;, u&#39;-output&#39;, u&#39;/Users/drodriguez/Downloads/text8-phrases&#39;, u&#39;-min-count&#39;, u&#39;5&#39;, u&#39;-threshold&#39;, u&#39;100&#39;, u&#39;-debug&#39;, u&#39;2&#39;]
Starting training using file /Users/drodriguez/Downloads/text8
Words processed: 17000K     Vocab size: 4399K
Vocab size (unigrams + bigrams): 2419827
Words in train file: 17005206
</code></pre><h3 id=中文实验>中文实验</h3><ul><li><p>语料</p><p>首先准备数据：采用网上博客上推荐的全网新闻数据(SogouCA)，大小为 2.1G。</p><pre><code>  从ftp上下载数据包SogouCA.tar.gz：
</code></pre></li></ul><pre tabindex=0><code>1 wget ftp://ftp.labs.sogou.com/Data/SogouCA/SogouCA.tar.gz --ftp-user=hebin_hit@foxmail.com --ftp-password=4FqLSYdNcrDXvNDi -r
</code></pre><pre><code>      解压数据包：
</code></pre><pre tabindex=0><code>1 gzip -d SogouCA.tar.gz
2 tar -xvf SogouCA.tar
</code></pre><pre><code>      再将生成的txt文件归并到SogouCA.txt中，取出其中包含content的行并转码，得到语料corpus.txt，大小为2.7G。
</code></pre><pre tabindex=0><code>1 cat *.txt &gt; SogouCA.txt
2 cat SogouCA.txt | iconv -f gbk -t utf-8 -c | grep &#34;&lt;content&gt;&#34; &gt; corpus.txt
</code></pre><ul><li><p>分词</p><p>用 ANSJ 对 corpus.txt 进行分词，得到分词结果 resultbig.txt，大小为 3.1G。在分词工具 seg_tool 目录下先编译再执行得到分词结果 resultbig.txt，内含 426221 个词，次数总计 572308385 个。</p></li><li><p>词向量训练</p></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>nohup ./word2vec -train resultbig.txt -output vectors.bin -cbow <span class=m>0</span> -size <span class=m>200</span> -window <span class=m>5</span> -negative <span class=m>0</span> -hs <span class=m>1</span> -sample 1e-3 -threads <span class=m>12</span> -binary <span class=m>1</span> <span class=p>&amp;</span>
</span></span></code></pre></div><ul><li>分析</li></ul><p>(1)相似词计算</p><pre tabindex=0><code>./distance vectors.bin
</code></pre><pre><code> ./distance可以看成计算词与词之间的距离，把词看成向量空间上的一个点，distance看成向量空间上点与点的距离。
</code></pre><p>(2)潜在的语言学规律</p><pre><code>  在对demo-analogy.sh修改后得到下面几个例子：

  法国的首都是巴黎，英国的首都是伦敦，vector(&quot;法国&quot;) - vector(&quot;巴黎) + vector(&quot;英国&quot;) --&gt; vector(&quot;伦敦&quot;)&quot;
</code></pre><p>(3)聚类</p><pre><code>将经过分词后的语料resultbig.txt中的词聚类并按照类别排序:
</code></pre><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=m>1</span> nohup ./word2vec -train resultbig.txt -output classes.txt -cbow <span class=m>0</span> -size <span class=m>200</span> -window <span class=m>5</span> -negative <span class=m>0</span> -hs <span class=m>1</span> -sample 1e-3 -threads <span class=m>12</span> -classes <span class=m>500</span>  <span class=p>&amp;</span>
</span></span><span class=line><span class=cl><span class=m>2</span> sort classes.txt -k <span class=m>2</span> -n &gt; classes_sorted_sogouca.txt
</span></span></code></pre></div><p>(4)短语分析</p><pre><code>先利用经过分词的语料resultbig.txt中得出包含词和短语的文件sogouca_phrase.txt，再训练该文件中词与短语的向量表示。
</code></pre><pre tabindex=0><code>1 ./word2phrase -train resultbig.txt -output sogouca_phrase.txt -threshold 500 -debug 2
2 ./word2vec -train sogouca_phrase.txt -output vectors_sogouca_phrase.bin -cbow 0 -size 300 -window 10 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1
</code></pre><h2 id=维基百科实验>维基百科实验</h2><h1 id=algorithms>Algorithms</h1><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=http://deeplearning4j.org/img/word2vec_diagrams.png alt loading=lazy data-zoomable></div></div></figure></p><h2 id=cbow>CBOW</h2><p>CBOW 是 Continuous Bag-of-Words Model 的缩写，是一种与前向 NNLM 类似 的模型，不同点在于 CBOW 去掉了最耗时的非线性隐层且所有词共享隐层。如 下图所示。可以看出，CBOW 模型是预测$P(w<em>t|w</em>{t-k},w*{t-(k-1)},\dots,w*{t-1},w*{t+1},\dots,w*{t+k})$。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=http://7xlgth.com1.z0.glb.clouddn.com/1424C789-5B58-43BA-952C-EACDF43E2AEB.png alt loading=lazy data-zoomable></div></div></figure></p><p>从输入层到隐层所进行的操作实际就是上下文向量的加和，具体的代码如下。其中 sentence_position 为当前 word 在句子中的下标。以一个具体的句子 A B C D 为例，第一次进入到下面代码时当前 word 为 A，sentence_position 为 0。b 是一 个随机生成的 0 到$window-1$的词，整个窗口的大小为$2<em>window + 1 - 2</em>b$，相当于左右各看$window-b$个词。可以看出随着窗口的从左往右滑动，其大小也 是随机的$3 (b=window-1)$到$2*window+1(b=0)$之间随机变通，即随机值 b 的大小决定了当前窗口的大小。代码中的 neu1 即为隐层向量，也就是上下文(窗口 内除自己之外的词)对应 vector 之和。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=http://7xlgth.com1.z0.glb.clouddn.com/36F89DA8-F3A0-4C6C-84F8-C31BB19CEEC1.png alt loading=lazy data-zoomable></div></div></figure></p><h2 id=skip-gram>Skip-Gram</h2><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=http://7xlgth.com1.z0.glb.clouddn.com/F0E76FE8-7B78-4E4C-BB6A-8FB47A67645C.png alt loading=lazy data-zoomable></div></div></figure></p><p>Skip-Gram 模型的图与 CBOW 正好方向相反，从图中看应该 Skip-Gram 应该预测概率$p(w_i,|w_t)$，其中$t - c \le i \le t + c$且$i \ne t,c$是决定上下文窗口大小的常数，$c$越大则需要考虑的 pair 就越多，一般能够带来更精确的结果，但是训练时间也 会增加。假设存在一个$w_1,w_2,w_3,…,w_T$的词组序列，Skip-gram 的目标是最大化：</p><p>$$
\frac{1}{T}\sum^{T}<em>{t=1}\sum</em>{-c \le j \le c, j \ne 0}log p(w_{t+j}|w_t)
$$</p><p>基本的 Skip-Gram 模型定义$p(w_o|w_I)$为：</p><p>$$
P(w<em>o | w_I) = \frac{e^{v</em>{w<em>o}^{T</em>{V*{w_I}}}}}{\Sigma*{w=1}^{W}e^{V<em>w^{T</em>{V_{w_I}}}}}
$$</p><p>从公式不难看出，Skip-Gram 是一个对称的模型，如果$w_t$为中心词时$w_k$在其窗口内，则$w_t$也必然在以$w_k$为中心词的同样大小的窗口内，也就是：</p><p>$$
\frac{1}{T}\sum^{T}<em>{t=1}\sum</em>{-c \le j \le c, j \ne 0}log p(w*{t+j}|w_t) = \ \frac{1}{T}\sum^{T}<em>{t=1}\sum</em>{-c \le j \le c, j \ne 0}log p(w*{t}|w_{t+j})
$$</p><p>同时，Skip-Gram 中的每个词向量表征了上下文的分布。Skip-Gram 中的 Skip 是指在一定窗口内的词两两都会计算概率，就算他们之间隔着一些词，这样的好处是“白色汽车”和“白色的汽车”很容易被识别为相同的短语。</p><p>与 CBOW 类似，Skip-Gram 也有两种可选的算法：层次 Softmax 和 Negative Sampling。层次 Sofamax 算法也结合了 Huffman 编码，每个词$w$都可以从树的根节点沿着唯一一条路径被访问到。假设$n(w,j)$为这条路径上的第$j$个结点，且$L(w)$为这条路径的长度，注意$j$从 1 开始编码，即$n(w,1)=root,n(w,L(w))=w$。层次 Softmax 定义的概率$p(w|w_I)$为：</p><p>$$
p(w|w<em>I)=\Pi</em>{j=1}^{L(w)-1}\sigma([n(w,j+1)=ch(n(w,j))]*v&rsquo;^T_{n(w,j)}v_I)
$$</p><p>$ch(n(w,j))$既可以是$n(w,j)$的左子结点也可以是$n(w,j)$的右子结点，word2vec 源代码中采用的是左子节点(Label 为$1-code[j]$)，其实此处改为右子节点也是可以的。</p><h1 id=tricks>Tricks</h1><h2 id=learning-phrases>Learning Phrases</h2><p>对于某些词语，经常出现在一起的，我们就判定他们是短语。那么如何衡量呢？用以下公式。</p><p>$score(w_i,w_j)=\frac{count(w_iw_j) - \delta}{count(w_i) * count(w_j)}$</p><p>输入两个词向量，如果算出的 score 大于某个阈值时，我们就认定他们是“在一起的”。为了考虑到更长的短语，我们拿 2-4 个词语作为训练数据，依次降低阈值。</p><h1 id=implementation>Implementation</h1><p>Word2Vec 高效率的原因可以认为如下：</p><p>1.去掉了费时的非线性隐层；</p><p>2.Huffman Huffman 编码 相当于做了一定聚类，不需要统计所有词对；</p><p>3.Negative Sampling；</p><p>4.随机梯度算法；</p><p>5.只过一遍数据，不需要反复迭代；</p><p>6.编程实现中的一些 trick，比如指数运算的预计，高频词亚采样等。</p><p>word2vec 可调整的超参数有很多：</p><table><thead><tr><th>参数名</th><th>说明</th><th></th></tr></thead><tbody><tr><td>-size</td><td>向量维度</td><td>一般维度越高越好，但并不总是这样</td></tr><tr><td>-window</td><td>上下文窗口大小</td><td>Skip-gram—般 10 左右，CBOW—般 5 左右，</td></tr><tr><td>-sample</td><td>高频词亚采样</td><td>对大数据集合可以同时提高精度和速度，sample 的取值 在 1e-3 到 1e-5 之间效果最佳，</td></tr><tr><td>-hs</td><td>是否采用层次 softmax</td><td>层次 softmax 对低频词效果更好；对应的 negative sampling 对高频词效果更好，向量维度较低时效果更好</td></tr><tr><td>-negative</td><td>负例数目</td><td></td></tr><tr><td>-min-count</td><td>被截断的低频词阈值</td><td></td></tr><tr><td>-alpha</td><td>开始的学习速率</td><td></td></tr><tr><td>-cbow</td><td>使用 CBOW</td><td>Skip-gram 更慢一些，但是对低频词效果更好；对应的 CBOW 则速度更快一些，</td></tr></tbody></table><h2 id=deeplearning4j>Deeplearning4j</h2><ul><li><a href=http://deeplearning4j.org/zh-word2vec.html target=_blank rel=noopener>Word2vec</a><blockquote></blockquote></li><li><a href=http://deeplearning4j.org/word2vec.html#intro target=_blank rel=noopener>DL4J-Word2Vec</a></li></ul><h2 id=python-1>Python</h2><ul><li><a href=http://www.52nlp.cn/%E4%B8%AD%E8%8B%B1%E6%96%87%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E8%AF%AD%E6%96%99%E4%B8%8A%E7%9A%84word2vec%E5%AE%9E%E9%AA%8C target=_blank rel=noopener>中英文维基百科语料上的 Word2Vec 实验</a></li></ul><pre tabindex=0><code>%load_ext autoreload
%autoreload 2
</code></pre><h1 id=word2vec-1>word2vec</h1><p>This notebook is equivalent to <code>demo-word.sh</code>, <code>demo-analogy.sh</code>, <code>demo-phrases.sh</code> and <code>demo-classes.sh</code> from Google.</p><h2 id=training>Training</h2><p>Download some data, for example: <a href=http://mattmahoney.net/dc/text8.zip target=_blank rel=noopener>http://mattmahoney.net/dc/text8.zip</a></p><pre tabindex=0><code>import word2vec
</code></pre><p>Run <code>word2phrase</code> to group up similar words &ldquo;Los Angeles&rdquo; to &ldquo;Los_Angeles&rdquo;</p><pre tabindex=0><code>word2vec.word2phrase(&#39;/Users/drodriguez/Downloads/text8&#39;, &#39;/Users/drodriguez/Downloads/text8-phrases&#39;, verbose=True)
</code></pre><pre tabindex=0><code>[u&#39;word2phrase&#39;, u&#39;-train&#39;, u&#39;/Users/drodriguez/Downloads/text8&#39;, u&#39;-output&#39;, u&#39;/Users/drodriguez/Downloads/text8-phrases&#39;, u&#39;-min-count&#39;, u&#39;5&#39;, u&#39;-threshold&#39;, u&#39;100&#39;, u&#39;-debug&#39;, u&#39;2&#39;]
Starting training using file /Users/drodriguez/Downloads/text8
Words processed: 17000K     Vocab size: 4399K
Vocab size (unigrams + bigrams): 2419827
Words in train file: 17005206
</code></pre><p>This will create a <code>text8-phrases</code> that we can use as a better input for <code>word2vec</code>.Note that you could easily skip this previous step and use the origial data as input for <code>word2vec</code>.</p><p>Train the model using the <code>word2phrase</code> output.</p><pre tabindex=0><code>word2vec.word2vec(&#39;/Users/drodriguez/Downloads/text8-phrases&#39;, &#39;/Users/drodriguez/Downloads/text8.bin&#39;, size=100, verbose=True)
</code></pre><pre tabindex=0><code>Starting training using file /Users/drodriguez/Downloads/text8-phrases
Vocab size: 98331
Words in train file: 15857306
Alpha: 0.000002  Progress: 100.03%  Words/thread/sec: 286.52k
</code></pre><p>That generated a <code>text8.bin</code> file containing the word vectors in a binary format.</p><p>Do the clustering of the vectors based on the trained model.</p><pre tabindex=0><code>word2vec.word2clusters(&#39;/Users/drodriguez/Downloads/text8&#39;, &#39;/Users/drodriguez/Downloads/text8-clusters.txt&#39;, 100, verbose=True)
</code></pre><pre tabindex=0><code>Starting training using file /Users/drodriguez/Downloads/text8
Vocab size: 71291
Words in train file: 16718843
Alpha: 0.000002  Progress: 100.02%  Words/thread/sec: 287.55k
</code></pre><p>That created a <code>text8-clusters.txt</code> with the cluster for every word in the vocabulary</p><h2 id=predictions>Predictions</h2><pre tabindex=0><code>import word2vec
</code></pre><p>Import the <code>word2vec</code> binary file created above</p><pre tabindex=0><code>model = word2vec.load(&#39;/Users/drodriguez/Downloads/text8.bin&#39;)
</code></pre><p>We can take a look at the vocabulaty as a numpy array</p><pre tabindex=0><code>model.vocab
</code></pre><pre tabindex=0><code>array([u&#39;&lt;/s&gt;&#39;, u&#39;the&#39;, u&#39;of&#39;, ..., u&#39;dakotas&#39;, u&#39;nias&#39;, u&#39;burlesques&#39;],
      dtype=&#39;&lt;U78&#39;)
</code></pre><p>Or take a look at the whole matrix</p><pre tabindex=0><code>model.vectors.shape
</code></pre><pre tabindex=0><code>(98331, 100)
</code></pre><pre tabindex=0><code>model.vectors
</code></pre><pre tabindex=0><code>array([[ 0.14333282,  0.15825513, -0.13715845, ...,  0.05456942,
         0.10955409,  0.00693387],
       [ 0.1220774,  0.04939618,  0.09545057, ..., -0.00804222,
        -0.05441621, -0.10076696],
       [ 0.16844609,  0.03734054,  0.22085373, ...,  0.05854521,
         0.04685341,  0.02546694],
       ...,
       [-0.06760896,  0.03737842,  0.09344187, ...,  0.14559349,
        -0.11704484, -0.05246212],
       [ 0.02228479, -0.07340827,  0.15247506, ...,  0.01872172,
        -0.18154132, -0.06813737],
       [ 0.02778879, -0.06457976,  0.07102411, ..., -0.00270281,
        -0.0471223, -0.135444  ]])
</code></pre><p>We can retreive the vector of individual words</p><pre tabindex=0><code>model[&#39;dog&#39;].shape
</code></pre><pre tabindex=0><code>(100,)
</code></pre><pre tabindex=0><code>model[&#39;dog&#39;][:10]
</code></pre><pre tabindex=0><code>array([ 0.05753701,  0.0585594,  0.11341395,  0.02016246,  0.11514406,
        0.01246986,  0.00801256,  0.17529851,  0.02899276,  0.0203866 ])
</code></pre><p>We can do simple queries to retreive words similar to &ldquo;socks&rdquo; based on cosine similarity:</p><pre tabindex=0><code>indexes, metrics = model.cosine(&#39;socks&#39;)
indexes, metrics
</code></pre><pre tabindex=0><code>(array([20002, 28915, 30711, 33874, 27482, 14631, 22992, 24195, 25857, 23705]),
 array([ 0.8375354,  0.83590846,  0.82818749,  0.82533614,  0.82278399,
         0.81476386,  0.8139092,  0.81253798,  0.8105933,  0.80850171]))
</code></pre><p>This returned a tuple with 2 items:</p><ol><li>numpy array with the indexes of the similar words in the vocabulary</li><li>numpy array with cosine similarity to each word</li></ol><p>Its possible to get the words of those indexes</p><pre tabindex=0><code>model.vocab[indexes]
</code></pre><pre tabindex=0><code>array([u&#39;hairy&#39;, u&#39;pumpkin&#39;, u&#39;gravy&#39;, u&#39;nosed&#39;, u&#39;plum&#39;, u&#39;winged&#39;,
       u&#39;bock&#39;, u&#39;petals&#39;, u&#39;biscuits&#39;, u&#39;striped&#39;],
      dtype=&#39;&lt;U78&#39;)
</code></pre><p>There is a helper function to create a combined response: a numpy <a href=http://docs.scipy.org/doc/numpy/user/basics.rec.html target=_blank rel=noopener>record array</a></p><pre tabindex=0><code>model.generate_response(indexes, metrics)
</code></pre><pre tabindex=0><code>rec.array([(u&#39;hairy&#39;, 0.8375353970603848), (u&#39;pumpkin&#39;, 0.8359084628493809),
       (u&#39;gravy&#39;, 0.8281874915608026), (u&#39;nosed&#39;, 0.8253361379785071),
       (u&#39;plum&#39;, 0.8227839904046932), (u&#39;winged&#39;, 0.8147638561412592),
       (u&#39;bock&#39;, 0.8139092031538545), (u&#39;petals&#39;, 0.8125379796045767),
       (u&#39;biscuits&#39;, 0.8105933044655644), (u&#39;striped&#39;, 0.8085017054444408)],
      dtype=[(u&#39;word&#39;, &#39;&lt;U78&#39;), (u&#39;metric&#39;, &#39;&lt;f8&#39;)])
</code></pre><p>Is easy to make that numpy array a pure python response:</p><pre tabindex=0><code>model.generate_response(indexes, metrics).tolist()
</code></pre><pre tabindex=0><code>[(u&#39;hairy&#39;, 0.8375353970603848),
 (u&#39;pumpkin&#39;, 0.8359084628493809),
 (u&#39;gravy&#39;, 0.8281874915608026),
 (u&#39;nosed&#39;, 0.8253361379785071),
 (u&#39;plum&#39;, 0.8227839904046932),
 (u&#39;winged&#39;, 0.8147638561412592),
 (u&#39;bock&#39;, 0.8139092031538545),
 (u&#39;petals&#39;, 0.8125379796045767),
 (u&#39;biscuits&#39;, 0.8105933044655644),
 (u&#39;striped&#39;, 0.8085017054444408)]
</code></pre><h3 id=phrases>Phrases</h3><p>Since we trained the model with the output of <code>word2phrase</code> we can ask for similarity of &ldquo;phrases&rdquo;</p><pre tabindex=0><code>indexes, metrics = model.cosine(&#39;los_angeles&#39;)
model.generate_response(indexes, metrics).tolist()
</code></pre><pre tabindex=0><code>[(u&#39;san_francisco&#39;, 0.886558000570455),
 (u&#39;san_diego&#39;, 0.8731961018831669),
 (u&#39;seattle&#39;, 0.8455603712285231),
 (u&#39;las_vegas&#39;, 0.8407843553947962),
 (u&#39;miami&#39;, 0.8341796009062884),
 (u&#39;detroit&#39;, 0.8235412519780195),
 (u&#39;cincinnati&#39;, 0.8199138493085706),
 (u&#39;st_louis&#39;, 0.8160655356728751),
 (u&#39;chicago&#39;, 0.8156786240847214),
 (u&#39;california&#39;, 0.8154244925085712)]
</code></pre><h3 id=analogies>Analogies</h3><p>Its possible to do more complex queries like analogies such as: <code>king - man + woman = queen</code> This method returns the same as <code>cosine</code> the indexes of the words in the vocab and the metric</p><pre tabindex=0><code>indexes, metrics = model.analogy(pos=[&#39;king&#39;, &#39;woman&#39;], neg=[&#39;man&#39;], n=10)
indexes, metrics
</code></pre><pre tabindex=0><code>(array([1087, 1145, 7523, 3141, 6768, 1335, 8419, 1826,  648, 1426]),
 array([ 0.2917969,  0.27353295,  0.26877692,  0.26596514,  0.26487509,
         0.26428581,  0.26315492,  0.26261258,  0.26136635,  0.26099078]))
</code></pre><pre tabindex=0><code>model.generate_response(indexes, metrics).tolist()
</code></pre><pre tabindex=0><code>[(u&#39;queen&#39;, 0.2917968955611075),
 (u&#39;prince&#39;, 0.27353295205311695),
 (u&#39;empress&#39;, 0.2687769174818083),
 (u&#39;monarch&#39;, 0.2659651399832089),
 (u&#39;regent&#39;, 0.26487508713026797),
 (u&#39;wife&#39;, 0.2642858109968327),
 (u&#39;aragon&#39;, 0.2631549214361766),
 (u&#39;throne&#39;, 0.26261257728511833),
 (u&#39;emperor&#39;, 0.2613663460665488),
 (u&#39;bishop&#39;, 0.26099078142148696)]
</code></pre><h3 id=clusters>Clusters</h3><pre tabindex=0><code>clusters = word2vec.load_clusters(&#39;/Users/drodriguez/Downloads/text8-clusters.txt&#39;)
</code></pre><p>We can see get the cluster number for individual words</p><pre tabindex=0><code>clusters[&#39;dog&#39;]
</code></pre><pre tabindex=0><code>11
</code></pre><p>We can see get all the words grouped on an specific cluster</p><pre tabindex=0><code>clusters.get_words_on_cluster(90).shape
</code></pre><pre tabindex=0><code>(221,)
</code></pre><pre tabindex=0><code>clusters.get_words_on_cluster(90)[:10]
</code></pre><pre tabindex=0><code>array([&#39;along&#39;, &#39;together&#39;, &#39;associated&#39;, &#39;relationship&#39;, &#39;deal&#39;,
       &#39;combined&#39;, &#39;contact&#39;, &#39;connection&#39;, &#39;bond&#39;, &#39;respect&#39;], dtype=object)
</code></pre><p>We can add the clusters to the word2vec model and generate a response that includes the clusters</p><pre tabindex=0><code>model.clusters = clusters
</code></pre><pre tabindex=0><code>indexes, metrics = model.analogy(pos=[&#39;paris&#39;, &#39;germany&#39;], neg=[&#39;france&#39;], n=10)
</code></pre><pre tabindex=0><code>model.generate_response(indexes, metrics).tolist()
</code></pre><pre tabindex=0><code>[(u&#39;berlin&#39;, 0.32333651414395953, 20),
 (u&#39;munich&#39;, 0.28851564633559, 20),
 (u&#39;vienna&#39;, 0.2768927258877336, 12),
 (u&#39;leipzig&#39;, 0.2690537010929304, 91),
 (u&#39;moscow&#39;, 0.26531859560322785, 74),
 (u&#39;st_petersburg&#39;, 0.259534503067277, 61),
 (u&#39;prague&#39;, 0.25000637367753303, 72),
 (u&#39;dresden&#39;, 0.2495974800117785, 71),
 (u&#39;bonn&#39;, 0.24403155303236473, 8),
 (u&#39;frankfurt&#39;, 0.24199720792200027, 31)]
</code></pre><pre tabindex=0><code></code></pre></div><div class=article-widget><div class="container-xl row post-nav"><div class="col-6 post-nav-item"><div class=meta-nav>下一页</div><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E8%AF%8D%E8%A1%A8%E7%A4%BA/ rel=prev>词表示</a></div></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>