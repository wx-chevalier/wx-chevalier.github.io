<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>BERT | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/</link><atom:link href="https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/index.xml" rel="self" type="application/rss+xml"/><description>BERT</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>BERT</title><link>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/</link></image><item><title>目标函数</title><link>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/</guid><description>&lt;h1 id="目标函数">目标函数&lt;/h1>
&lt;h1 id="masked-language-model-mlm">Masked Language Model, MLM&lt;/h1>
&lt;p>MLM 是为了训练深度双向语言表示向量，BERT 用了一个非常直接的方式，遮住句子里某些单词，让编码器预测这个单词是什么。具体操作流程如下图示例：先按一个较小概率 mask 掉一些字，再对这些字利用语言模型由上下文做预测。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://i.postimg.cc/85DMhMtm/image.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>BERT 具体训练方法为：随机遮住 15% 的单词作为训练样本，其中 80%用 masked token 来代替；10% 用随机的一个词来替换，10% 保持这个词不变。&lt;/p>
&lt;p>直观上来说，只有 15%的词被遮盖的原因是性能开销，双向编码器比单向编码器训练要慢；选 80%mask，20%具体单词的原因是在 pretrain 的时候做了 mask，在特定任务微调如分类任务的时候，并不对输入序列做 mask，会产生 gap，任务不一致；10%用随机的一个词来替换，10%保持这个词不变的原因是让编码器不知道哪些词需要预测的，哪些词是错误的，因此被迫需要学习每一个 token 的表示向量，做了一个折中。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://i.postimg.cc/rmJWgsHX/image.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>预训练一个二分类的模型，来学习句子之间的关系。预测下一个句子的方法对学习句子之间关系很有帮助。&lt;/p>
&lt;p>训练方法：正样本和负样本比例是 1：1，50% 的句子是正样本，即给定句子 A 和 B，B 是 A 的实际语境下一句；负样本：在语料库中随机选择的句子作为 B。通过两个特定的 token[CLS] 和 [SEP] 来串接两个句子，该任务在[CLS]位置输出预测。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://i.postimg.cc/90Q9kV34/image.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p></description></item><item><title>输入表示</title><link>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA/</guid><description>&lt;h1 id="输入表示">输入表示&lt;/h1>
&lt;p>Input: 每个输入序列的第一个 token [CLS]专门用来分类, 直接利用此位置的最后输出作为分类任务的输入 embedding。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://i.postimg.cc/5yKzdG1s/image.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>从直观上来说，在预训练时，[CLS]不参与 mask，因而该位置面向整个序列的所有 position 做 attention，[CLS]位置的输出足够表达整个句子的信息，类似于一个 global feature；而单词 token 对应的 embedding 更关注该 token 的语义语法及上下文信息表达，类似于一个 local feature。&lt;/p>
&lt;p>Position Embeddings: transformer 的 Position Encoding 是通过 sin, cos 直接构造出来的，Position Embeddings 是通过模型学习到的 embedding 向量，最高支持 512 维。&lt;/p>
&lt;p>Segment Embeddings：在预训练的句对预测任务及问答、相似匹配等任务中，需要对前后句子做区分，将句对输入同一序列，以特殊标记[SEP]分割，同时对第一个句子的每个 token 添加 Sentence A Embedding, 第二个句子添加 Sentence B Embedding,实验中让 $E_A$ =1, $E_B$ =0。&lt;/p>
&lt;h1 id="fine-tune">Fine-tune&lt;/h1>
&lt;p>针对不同任务，BERT 采用不同部分的输出做预测,分类任务利用 [CLS] 位置的 embedding，NER 任务利用每个 token 的输出 embedding。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://i.postimg.cc/kXD8FnFz/image.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p></description></item></channel></rss>