<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="基于 Python 的简单自然语言处理 从属于笔者的 程序猿的数据科学与机器学习实战手册。 基于 Python 的简单自然语言处理 本文是对于基于 Python 进行简单自然语言处理任务的介绍，本文的所有代码放置在这里。建议前置阅读 Python 语法速览与机器学"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/"><meta property="og:title" content="基础文本处理 | Next-gen Tech Edu"><meta property="og:description" content="基于 Python 的简单自然语言处理 从属于笔者的 程序猿的数据科学与机器学习实战手册。 基于 Python 的简单自然语言处理 本文是对于基于 Python 进行简单自然语言处理任务的介绍，本文的所有代码放置在这里。建议前置阅读 Python 语法速览与机器学"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>基础文本处理 | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=f3c9f1082e0a4bd0ad2d93f9c8a1936a><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">统计语言模型</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id5e80cd7d6ea0bbeea81f44344f06ffea")' href=#id5e80cd7d6ea0bbeea81f44344f06ffea aria-expanded=false aria-controls=id5e80cd7d6ea0bbeea81f44344f06ffea aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/>经典自然语言</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id5e80cd7d6ea0bbeea81f44344f06ffea aria-expanded=false aria-controls=id5e80cd7d6ea0bbeea81f44344f06ffea><i class="fa-solid fa-angle-down" id=caret-id5e80cd7d6ea0bbeea81f44344f06ffea></i></a></div><ul class="nav docs-sidenav collapse show" id=id5e80cd7d6ea0bbeea81f44344f06ffea><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idf9cb082466e74db79015a924753627b4")' href=#idf9cb082466e74db79015a924753627b4 aria-expanded=false aria-controls=idf9cb082466e74db79015a924753627b4 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/>词嵌入</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idf9cb082466e74db79015a924753627b4 aria-expanded=false aria-controls=idf9cb082466e74db79015a924753627b4><i class="fa-solid fa-angle-right" id=caret-idf9cb082466e74db79015a924753627b4></i></a></div><ul class="nav docs-sidenav collapse" id=idf9cb082466e74db79015a924753627b4><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id192ce96c2ca4a85e961d10a6c57769b7")' href=#id192ce96c2ca4a85e961d10a6c57769b7 aria-expanded=false aria-controls=id192ce96c2ca4a85e961d10a6c57769b7 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/>词向量</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id192ce96c2ca4a85e961d10a6c57769b7 aria-expanded=false aria-controls=id192ce96c2ca4a85e961d10a6c57769b7><i class="fa-solid fa-angle-right" id=caret-id192ce96c2ca4a85e961d10a6c57769b7></i></a></div><ul class="nav docs-sidenav collapse" id=id192ce96c2ca4a85e961d10a6c57769b7><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/>基于 Gensim 的 Word2Vec 实践</a></li></ul></div><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E6%A6%82%E8%BF%B0/>概述</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idd85c6af5640425111159bb9469160c26")' href=#idd85c6af5640425111159bb9469160c26 aria-expanded=false aria-controls=idd85c6af5640425111159bb9469160c26 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>统计语言模型</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idd85c6af5640425111159bb9469160c26 aria-expanded=false aria-controls=idd85c6af5640425111159bb9469160c26><i class="fa-solid fa-angle-down" id=caret-idd85c6af5640425111159bb9469160c26></i></a></div><ul class="nav docs-sidenav collapse show" id=idd85c6af5640425111159bb9469160c26><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6411e45dc7bee277ca6705e0235b3a88")' href=#id6411e45dc7bee277ca6705e0235b3a88 aria-expanded=false aria-controls=id6411e45dc7bee277ca6705e0235b3a88 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/>BERT</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id6411e45dc7bee277ca6705e0235b3a88 aria-expanded=false aria-controls=id6411e45dc7bee277ca6705e0235b3a88><i class="fa-solid fa-angle-right" id=caret-id6411e45dc7bee277ca6705e0235b3a88></i></a></div><ul class="nav docs-sidenav collapse" id=id6411e45dc7bee277ca6705e0235b3a88><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/>目标函数</a></li><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA/>输入表示</a></li></ul></div><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/>Word2Vec</a></li><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E8%AF%8D%E8%A1%A8%E7%A4%BA/>词表示</a></li><li class="child level active"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/>基础文本处理</a></li><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>统计语言模型</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id4a648d9ccd6a02ca8fdc13681fefab7f")' href=#id4a648d9ccd6a02ca8fdc13681fefab7f aria-expanded=false aria-controls=id4a648d9ccd6a02ca8fdc13681fefab7f aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%AD%E6%B3%95%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/>语法语义分析</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id4a648d9ccd6a02ca8fdc13681fefab7f aria-expanded=false aria-controls=id4a648d9ccd6a02ca8fdc13681fefab7f><i class="fa-solid fa-angle-right" id=caret-id4a648d9ccd6a02ca8fdc13681fefab7f></i></a></div><ul class="nav docs-sidenav collapse" id=id4a648d9ccd6a02ca8fdc13681fefab7f><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%AD%E6%B3%95%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/>命名实体识别</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id7905f06a5081e168ca189bc50bfa7e6c")' href=#id7905f06a5081e168ca189bc50bfa7e6c aria-expanded=false aria-controls=id7905f06a5081e168ca189bc50bfa7e6c aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/>主题模型</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id7905f06a5081e168ca189bc50bfa7e6c aria-expanded=false aria-controls=id7905f06a5081e168ca189bc50bfa7e6c><i class="fa-solid fa-angle-right" id=caret-id7905f06a5081e168ca189bc50bfa7e6c></i></a></div><ul class="nav docs-sidenav collapse" id=id7905f06a5081e168ca189bc50bfa7e6c><li class="child level"><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/lda/>LDA</a></li></ul></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>基础文本处理</h1><div class=article-style><ul><li><a href=https://zhuanlan.zhihu.com/p/26249110 target=_blank rel=noopener>基于 Python 的简单自然语言处理</a> 从属于笔者的 <a href=https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders target=_blank rel=noopener>程序猿的数据科学与机器学习实战手册</a>。</li></ul><h1 id=基于-python-的简单自然语言处理>基于 Python 的简单自然语言处理</h1><p>本文是对于基于 Python 进行简单自然语言处理任务的介绍，本文的所有代码放置在<a href=https://parg.co/b4h target=_blank rel=noopener>这里</a>。建议前置阅读 <a href=https://zhuanlan.zhihu.com/p/24536868 target=_blank rel=noopener>Python 语法速览与机器学习开发环境搭建</a>，更多机器学习资料参考<a href=https://zhuanlan.zhihu.com/p/25612011 target=_blank rel=noopener>机器学习、深度学习与自然语言处理领域推荐的书籍列表</a>以及<a href=https://parg.co/b4C target=_blank rel=noopener>面向程序猿的数据科学与机器学习知识体系及资料合集</a>。</p><h1 id=twenty-news-group-语料集处理>Twenty News Group 语料集处理</h1><p>20 Newsgroup 数据集包含了约 20000 篇来自于不同的新闻组的文档，最早由 Ken Lang 搜集整理。本部分包含了对于数据集的抓取、特征提取、简单分类器训练、主题模型训练等。本部分代码包括主要的处理代码<a href=https://parg.co/b4M target=_blank rel=noopener>封装库</a>与<a href=https://parg.co/b4t target=_blank rel=noopener>基于 Notebook 的交互示范</a>。我们首先需要进行数据抓取：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>fetch_data</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>subset</span><span class=o>=</span><span class=s1>&#39;train&#39;</span><span class=p>,</span> <span class=n>categories</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;return data
</span></span></span><span class=line><span class=cl><span class=s2>    执行数据抓取操作
</span></span></span><span class=line><span class=cl><span class=s2>    Arguments:
</span></span></span><span class=line><span class=cl><span class=s2>    subset -&gt; string -- 抓取的目标集合 train / test / all
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>rand</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>mtrand</span><span class=o>.</span><span class=n>RandomState</span><span class=p>(</span><span class=mi>8675309</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>data</span> <span class=o>=</span> <span class=n>fetch_20newsgroups</span><span class=p>(</span><span class=n>subset</span><span class=o>=</span><span class=n>subset</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>categories</span><span class=o>=</span><span class=n>categories</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>shuffle</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                <span class=n>random_state</span><span class=o>=</span><span class=n>rand</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=bp>self</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=n>subset</span><span class=p>]</span> <span class=o>=</span> <span class=n>data</span>
</span></span></code></pre></div><p>然后在 Notebook 中交互查看数据格式：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=c1># 实例化对象</span>
</span></span><span class=line><span class=cl><span class=n>twp</span> <span class=o>=</span> <span class=n>TwentyNewsGroup</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=c1># 抓取数据</span>
</span></span><span class=line><span class=cl><span class=n>twp</span><span class=o>.</span><span class=n>fetch_data</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>twenty_train</span> <span class=o>=</span> <span class=n>twp</span><span class=o>.</span><span class=n>data</span><span class=p>[</span><span class=s1>&#39;train&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;数据集结构&#34;</span><span class=p>,</span> <span class=s2>&#34;-&gt;&#34;</span><span class=p>,</span> <span class=n>twenty_train</span><span class=o>.</span><span class=n>keys</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;文档数目&#34;</span><span class=p>,</span> <span class=s2>&#34;-&gt;&#34;</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>twenty_train</span><span class=o>.</span><span class=n>data</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;目标分类&#34;</span><span class=p>,</span> <span class=s2>&#34;-&gt;&#34;</span><span class=p>,[</span> <span class=n>twenty_train</span><span class=o>.</span><span class=n>target_names</span><span class=p>[</span><span class=n>t</span><span class=p>]</span> <span class=k>for</span> <span class=n>t</span> <span class=ow>in</span> <span class=n>twenty_train</span><span class=o>.</span><span class=n>target</span><span class=p>[:</span><span class=mi>10</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>数据集结构</span> <span class=o>-&gt;</span> <span class=n>dict_keys</span><span class=p>([</span><span class=s1>&#39;data&#39;</span><span class=p>,</span> <span class=s1>&#39;filenames&#39;</span><span class=p>,</span> <span class=s1>&#39;target_names&#39;</span><span class=p>,</span> <span class=s1>&#39;target&#39;</span><span class=p>,</span> <span class=s1>&#39;DESCR&#39;</span><span class=p>,</span> <span class=s1>&#39;description&#39;</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>文档数目</span> <span class=o>-&gt;</span> <span class=mi>11314</span>
</span></span><span class=line><span class=cl><span class=n>目标分类</span> <span class=o>-&gt;</span> <span class=p>[</span><span class=s1>&#39;sci.space&#39;</span><span class=p>,</span> <span class=s1>&#39;comp.sys.mac.hardware&#39;</span><span class=p>,</span> <span class=s1>&#39;sci.electronics&#39;</span><span class=p>,</span> <span class=s1>&#39;comp.sys.mac.hardware&#39;</span><span class=p>,</span> <span class=s1>&#39;sci.space&#39;</span><span class=p>,</span> <span class=s1>&#39;rec.sport.hockey&#39;</span><span class=p>,</span> <span class=s1>&#39;talk.religion.misc&#39;</span><span class=p>,</span> <span class=s1>&#39;sci.med&#39;</span><span class=p>,</span> <span class=s1>&#39;talk.religion.misc&#39;</span><span class=p>,</span> <span class=s1>&#39;talk.politics.guns&#39;</span><span class=p>]</span>
</span></span></code></pre></div><p>接下来我们可以对语料集中的特征进行提取：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=c1># 进行特征提取</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 构建文档-词矩阵(Document-Term Matrix)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.feature_extraction.text</span> <span class=kn>import</span> <span class=n>CountVectorizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>count_vect</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>X_train_counts</span> <span class=o>=</span> <span class=n>count_vect</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>twenty_train</span><span class=o>.</span><span class=n>data</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;DTM 结构&#34;</span><span class=p>,</span><span class=s2>&#34;-&gt;&#34;</span><span class=p>,</span><span class=n>X_train_counts</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 查看某个词在词表中的下标</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;词对应下标&#34;</span><span class=p>,</span><span class=s2>&#34;-&gt;&#34;</span><span class=p>,</span> <span class=n>count_vect</span><span class=o>.</span><span class=n>vocabulary_</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=sa>u</span><span class=s1>&#39;algorithm&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>DTM</span> <span class=n>结构</span> <span class=o>-&gt;</span> <span class=p>(</span><span class=mi>11314</span><span class=p>,</span> <span class=mi>130107</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>词对应下标</span> <span class=o>-&gt;</span> <span class=mi>27366</span>
</span></span></code></pre></div><p>为了将文档用于进行分类任务，还需要使用 TF-IDF 等常见方法将其转化为特征向量：</p><pre tabindex=0><code># 构建文档的 TF 特征向量
from sklearn.feature_extraction.text import TfidfTransformer

tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
X_train_tf = tf_transformer.transform(X_train_counts)

print(&#34;某文档 TF 特征向量&#34;,&#34;-&gt;&#34;,X_train_tf)

# 构建文档的 TF-IDF 特征向量
from sklearn.feature_extraction.text import TfidfTransformer

tf_transformer = TfidfTransformer().fit(X_train_counts)
X_train_tfidf = tf_transformer.transform(X_train_counts)

print(&#34;某文档 TF-IDF 特征向量&#34;,&#34;-&gt;&#34;,X_train_tfidf)

某文档 TF 特征向量 -&gt;   (0, 6447)	0.0380693493813
  (0, 37842)	0.0380693493813
</code></pre><p>我们可以将特征提取、分类器训练与预测封装为单独函数：</p><pre tabindex=0><code>    def extract_feature(self):
        &#34;&#34;&#34;
        从语料集中抽取文档特征
        &#34;&#34;&#34;

        # 获取训练数据的文档-词矩阵
        self.train_dtm = self.count_vect.fit_transform(self.data[&#39;train&#39;].data)

        # 获取文档的 TF 特征

        tf_transformer = TfidfTransformer(use_idf=False)

        self.train_tf = tf_transformer.transform(self.train_dtm)

        # 获取文档的 TF-IDF 特征

        tfidf_transformer = TfidfTransformer().fit(self.train_dtm)

        self.train_tfidf = tf_transformer.transform(self.train_dtm)

    def train_classifier(self):
        &#34;&#34;&#34;
        从训练集中训练出分类器
        &#34;&#34;&#34;

        self.extract_feature();

        self.clf = MultinomialNB().fit(
            self.train_tfidf, self.data[&#39;train&#39;].target)

    def predict(self, docs):
        &#34;&#34;&#34;
        从训练集中训练出分类器
        &#34;&#34;&#34;

        X_new_counts = self.count_vect.transform(docs)

        tfidf_transformer = TfidfTransformer().fit(X_new_counts)

        X_new_tfidf = tfidf_transformer.transform(X_new_counts)

        return self.clf.predict(X_new_tfidf)
</code></pre><p>然后执行训练并且进行预测与评价：</p><pre tabindex=0><code># 训练分类器
twp.train_classifier()

# 执行预测
docs_new = [&#39;God is love&#39;, &#39;OpenGL on the GPU is fast&#39;]
predicted = twp.predict(docs_new)

for doc, category in zip(docs_new, predicted):
    print(&#39;%r =&gt; %s&#39; % (doc, twenty_train.target_names[category]))

# 执行模型评测
twp.fetch_data(subset=&#39;test&#39;)

predicted = twp.predict(twp.data[&#39;test&#39;].data)

import numpy as np

# 误差计算

# 简单误差均值
np.mean(predicted == twp.data[&#39;test&#39;].target)

# Metrics

from sklearn import metrics

print(metrics.classification_report(
    twp.data[&#39;test&#39;].target, predicted,
    target_names=twp.data[&#39;test&#39;].target_names))

# Confusion Matrix
metrics.confusion_matrix(twp.data[&#39;test&#39;].target, predicted)

&#39;God is love&#39; =&gt; soc.religion.christian
&#39;OpenGL on the GPU is fast&#39; =&gt; rec.autos
                          precision    recall  f1-score   support

             alt.atheism       0.79      0.50      0.61       319
           ...
      talk.religion.misc       1.00      0.08      0.15       251

             avg / total       0.82      0.79      0.77      7532

Out[16]:
array([[158,   0,   1,   1,   0,   1,   0,   3,   7,   1,   2,   6,   1,
          8,   3, 114,   6,   7,   0,   0],
       ...
       [ 35,   3,   1,   0,   0,   0,   1,   4,   1,   1,   6,   3,   0,
          6,   5, 127,  30,   5,   2,  21]])
</code></pre><p>我们也可以对文档集进行主题提取：</p><pre tabindex=0><code># 进行主题提取

twp.topics_by_lda()

Topic 0 : stream s1 astronaut zoo laurentian maynard s2 gtoal pem fpu
Topic 1 : 145 cx 0d bh sl 75u 6um m6 sy gld
Topic 2 : apartment wpi mars nazis monash palestine ottoman sas winner gerard
Topic 3 : livesey contest satellite tamu mathew orbital wpd marriage solntze pope
Topic 4 : x11 contest lib font string contrib visual xterm ahl brake
Topic 5 : ax g9v b8f a86 1d9 pl 0t wm 34u giz
Topic 6 : printf null char manes behanna senate handgun civilians homicides magpie
Topic 7 : buf jpeg chi tor bos det que uwo pit blah
Topic 8 : oracle di t4 risc nist instruction msg postscript dma convex
Topic 9 : candida cray yeast viking dog venus bloom symptoms observatory roby
Topic 10 : cx ck hz lk mv cramer adl optilink k8 uw
Topic 11 : ripem rsa sandvik w0 bosnia psuvm hudson utk defensive veal
Topic 12 : db espn sabbath br widgets liar davidian urartu sdpa cooling
Topic 13 : ripem dyer ucsu carleton adaptec tires chem alchemy lockheed rsa
Topic 14 : ingr sv alomar jupiter borland het intergraph factory paradox captain
Topic 15 : militia palestinian cpr pts handheld sharks igc apc jake lehigh
Topic 16 : alaska duke col russia uoknor aurora princeton nsmca gene stereo
Topic 17 : uuencode msg helmet eos satan dseg homosexual ics gear pyron
Topic 18 : entries myers x11r4 radar remark cipher maine hamburg senior bontchev
Topic 19 : cubs ufl vitamin temple gsfc mccall astro bellcore uranium wesleyan
</code></pre><h1 id=常见自然语言处理工具封装>常见自然语言处理工具封装</h1><p>经过上面对于 20NewsGroup 语料集处理的介绍我们可以发现常见自然语言处理任务包括，数据获取、数据预处理、数据特征提取、分类模型训练、主题模型或者词向量等高级特征提取等等。笔者还习惯用 <a href=https://github.com/google/python-fire target=_blank rel=noopener>python-fire</a> 将类快速封装为可通过命令行调用的工具，同时也支持外部模块调用使用。本部分我们主要以中文语料集为例，譬如我们需要对中文维基百科数据进行分析，可以使用 gensim 中的<a href=https://parg.co/b44 target=_blank rel=noopener>维基百科处理类</a>：</p><pre tabindex=0><code>class Wiki(object):
    &#34;&#34;&#34;
    维基百科语料集处理
    &#34;&#34;&#34;

    def wiki2texts(self, wiki_data_path, wiki_texts_path=&#39;./wiki_texts.txt&#39;):
        &#34;&#34;&#34;
        将维基百科数据转化为文本数据
        Arguments:
        wiki_data_path -- 维基压缩文件地址
        &#34;&#34;&#34;
        if not wiki_data_path:
            print(&#34;请输入 Wiki 压缩文件路径或者前往 https://dumps.wikimedia.org/zhwiki/ 下载&#34;)
            exit()

        # 构建维基语料集
        wiki_corpus = WikiCorpus(wiki_data_path, dictionary={})
        texts_num = 0

        with open(wiki_text_path, &#39;w&#39;, encoding=&#39;utf-8&#39;) as output:
            for text in wiki_corpus.get_texts():
                output.write(b&#39; &#39;.join(text).decode(&#39;utf-8&#39;) + &#39;\n&#39;)
                texts_num += 1
                if texts_num % 10000 == 0:
                    logging.info(&#34;已处理 %d 篇文章&#34; % texts_num)

        print(&#34;处理完毕，请使用 OpenCC 转化为简体字&#34;)
</code></pre><p>抓取完毕后，我们还需要用 OpenCC 转化为简体字。抓取完毕后我们可以使用结巴分词对生成的文本文件进行分词，代码参考<a href=https://parg.co/b4R target=_blank rel=noopener>这里</a>，我们直接使用 <code>python chinese_text_processor.py tokenize_file /output.txt</code> 直接执行该任务并且生成输出文件。获取分词之后的文件，我们可以将其转化为简单的词袋表示或者文档-词向量，详细代码参考<a href=https://parg.co/b4f target=_blank rel=noopener>这里</a>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>class</span> <span class=nc>CorpusProcessor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    语料集处理
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>corpus2bow</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tokenized_corpus</span><span class=o>=</span><span class=n>default_documents</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;returns (vocab,corpus_in_bow)
</span></span></span><span class=line><span class=cl><span class=s2>        将语料集转化为 BOW 形式
</span></span></span><span class=line><span class=cl><span class=s2>        Arguments:
</span></span></span><span class=line><span class=cl><span class=s2>        tokenized_corpus -- 经过分词的文档列表
</span></span></span><span class=line><span class=cl><span class=s2>        Return:
</span></span></span><span class=line><span class=cl><span class=s2>        vocab -- {&#39;human&#39;: 0, ... &#39;minors&#39;: 11}
</span></span></span><span class=line><span class=cl><span class=s2>        corpus_in_bow -- [[(0, 1), (1, 1), (2, 1)]...]
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>dictionary</span> <span class=o>=</span> <span class=n>corpora</span><span class=o>.</span><span class=n>Dictionary</span><span class=p>(</span><span class=n>tokenized_corpus</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 获取词表</span>
</span></span><span class=line><span class=cl>        <span class=n>vocab</span> <span class=o>=</span> <span class=n>dictionary</span><span class=o>.</span><span class=n>token2id</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 获取文档的词袋表示</span>
</span></span><span class=line><span class=cl>        <span class=n>corpus_in_bow</span> <span class=o>=</span> <span class=p>[</span><span class=n>dictionary</span><span class=o>.</span><span class=n>doc2bow</span><span class=p>(</span><span class=n>text</span><span class=p>)</span> <span class=k>for</span> <span class=n>text</span> <span class=ow>in</span> <span class=n>tokenized_corpus</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=n>vocab</span><span class=p>,</span> <span class=n>corpus_in_bow</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>corpus2dtm</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tokenized_corpus</span><span class=o>=</span><span class=n>default_documents</span><span class=p>,</span> <span class=n>min_df</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>max_df</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;&#34;&#34;returns (vocab, DTM)
</span></span></span><span class=line><span class=cl><span class=s2>        将语料集转化为文档-词矩阵
</span></span></span><span class=line><span class=cl><span class=s2>        - dtm -&gt; matrix: 文档-词矩阵
</span></span></span><span class=line><span class=cl><span class=s2>                I	like	hate	databases
</span></span></span><span class=line><span class=cl><span class=s2>        D1	1	  1	      0	        1
</span></span></span><span class=line><span class=cl><span class=s2>        D2	1	  0	      1	        1
</span></span></span><span class=line><span class=cl><span class=s2>        &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>type</span><span class=p>(</span><span class=n>tokenized_corpus</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span> <span class=ow>is</span> <span class=nb>list</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>documents</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>document</span><span class=p>)</span> <span class=k>for</span> <span class=n>document</span> <span class=ow>in</span> <span class=n>tokenized_corpus</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>documents</span> <span class=o>=</span> <span class=n>tokenized_corpus</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>max_df</span> <span class=o>==</span> <span class=o>-</span><span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>max_df</span> <span class=o>=</span> <span class=nb>round</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span> <span class=o>/</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 构建语料集统计向量</span>
</span></span><span class=line><span class=cl>        <span class=n>vec</span> <span class=o>=</span> <span class=n>CountVectorizer</span><span class=p>(</span><span class=n>min_df</span><span class=o>=</span><span class=n>min_df</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=n>max_df</span><span class=o>=</span><span class=n>max_df</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=n>analyzer</span><span class=o>=</span><span class=s2>&#34;word&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=n>token_pattern</span><span class=o>=</span><span class=s2>&#34;[\S]+&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=n>tokenizer</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=n>preprocessor</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                              <span class=n>stop_words</span><span class=o>=</span><span class=kc>None</span>
</span></span><span class=line><span class=cl>                              <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 对于数据进行分析</span>
</span></span><span class=line><span class=cl>        <span class=n>DTM</span> <span class=o>=</span> <span class=n>vec</span><span class=o>.</span><span class=n>fit_transform</span><span class=p>(</span><span class=n>documents</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># 获取词表</span>
</span></span><span class=line><span class=cl>        <span class=n>vocab</span> <span class=o>=</span> <span class=n>vec</span><span class=o>.</span><span class=n>get_feature_names</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=p>(</span><span class=n>vocab</span><span class=p>,</span> <span class=n>DTM</span><span class=p>)</span>
</span></span></code></pre></div><p>我们也可以对分词之后的文档进行主题模型或者词向量提取，这里使用分词之后的文件就可以忽略中英文的差异：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>topics_by_lda</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tokenized_corpus_path</span><span class=p>,</span> <span class=n>num_topics</span><span class=o>=</span><span class=mi>20</span><span class=p>,</span> <span class=n>num_words</span><span class=o>=</span><span class=mi>10</span><span class=p>,</span> <span class=n>max_lines</span><span class=o>=</span><span class=mi>10000</span><span class=p>,</span> <span class=n>split</span><span class=o>=</span><span class=s2>&#34;\s+&#34;</span><span class=p>,</span> <span class=n>max_df</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    读入经过分词的文件并且对其进行 LDA 训练
</span></span></span><span class=line><span class=cl><span class=s2>    Arguments:
</span></span></span><span class=line><span class=cl><span class=s2>    tokenized_corpus_path -&gt; string -- 经过分词的语料集地址
</span></span></span><span class=line><span class=cl><span class=s2>    num_topics -&gt; integer -- 主题数目
</span></span></span><span class=line><span class=cl><span class=s2>    num_words -&gt; integer -- 主题词数目
</span></span></span><span class=line><span class=cl><span class=s2>    max_lines -&gt; integer -- 每次读入的最大行数
</span></span></span><span class=line><span class=cl><span class=s2>    split -&gt; string -- 文档的词之间的分隔符
</span></span></span><span class=line><span class=cl><span class=s2>    max_df -&gt; integer -- 避免常用词，过滤超过该阈值的词
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 存放所有语料集信息</span>
</span></span><span class=line><span class=cl>    <span class=n>corpus</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>tokenized_corpus_path</span><span class=p>,</span> <span class=s1>&#39;r&#39;</span><span class=p>,</span> <span class=n>encoding</span><span class=o>=</span><span class=s1>&#39;utf-8&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>tokenized_corpus</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>flag</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>document</span> <span class=ow>in</span> <span class=n>tokenized_corpus</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 判断是否读取了足够的行数</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span><span class=p>(</span><span class=n>flag</span> <span class=o>&gt;</span> <span class=n>max_lines</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=c1># 将读取到的内容添加到语料集中</span>
</span></span><span class=line><span class=cl>            <span class=n>corpus</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>re</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=n>split</span><span class=p>,</span> <span class=n>document</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>flag</span> <span class=o>=</span> <span class=n>flag</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 构建语料集的 BOW 表示</span>
</span></span><span class=line><span class=cl>    <span class=p>(</span><span class=n>vocab</span><span class=p>,</span> <span class=n>DTM</span><span class=p>)</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>corpus2dtm</span><span class=p>(</span><span class=n>corpus</span><span class=p>,</span> <span class=n>max_df</span><span class=o>=</span><span class=n>max_df</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 训练 LDA 模型</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>lda</span> <span class=o>=</span> <span class=n>LdaMulticore</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>matutils</span><span class=o>.</span><span class=n>Sparse2Corpus</span><span class=p>(</span><span class=n>DTM</span><span class=p>,</span> <span class=n>documents_columns</span><span class=o>=</span><span class=kc>False</span><span class=p>),</span>
</span></span><span class=line><span class=cl>        <span class=n>num_topics</span><span class=o>=</span><span class=n>num_topics</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>id2word</span><span class=o>=</span><span class=nb>dict</span><span class=p>([(</span><span class=n>i</span><span class=p>,</span> <span class=n>s</span><span class=p>)</span> <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>s</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>vocab</span><span class=p>)]),</span>
</span></span><span class=line><span class=cl>        <span class=n>workers</span><span class=o>=</span><span class=mi>4</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 打印并且返回主题数据</span>
</span></span><span class=line><span class=cl>    <span class=n>topics</span> <span class=o>=</span> <span class=n>lda</span><span class=o>.</span><span class=n>show_topics</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>num_topics</span><span class=o>=</span><span class=n>num_topics</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>num_words</span><span class=o>=</span><span class=n>num_words</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>formatted</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>log</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>ti</span><span class=p>,</span> <span class=n>topic</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>topics</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Topic&#34;</span><span class=p>,</span> <span class=n>ti</span><span class=p>,</span> <span class=s2>&#34;:&#34;</span><span class=p>,</span> <span class=s2>&#34; &#34;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>word</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>topic</span><span class=p>[</span><span class=mi>1</span><span class=p>]))</span>
</span></span></code></pre></div><p>该函数同样可以使用命令行直接调用，传入分词之后的文件。我们也可以对其语料集建立词向量，代码参考<a href=https://parg.co/b4N target=_blank rel=noopener>这里</a>；如果对于词向量基本使用尚不熟悉的同学可以参考<a href=https://zhuanlan.zhihu.com/p/24961011 target=_blank rel=noopener>基于 Gensim 的 Word2Vec 实践</a>：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>wv_train</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tokenized_text_path</span><span class=p>,</span> <span class=n>output_model_path</span><span class=o>=</span><span class=s1>&#39;./wv_model.bin&#39;</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    对于文本进行词向量训练，并将输出的词向量保存
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>sentences</span> <span class=o>=</span> <span class=n>word2vec</span><span class=o>.</span><span class=n>Text8Corpus</span><span class=p>(</span><span class=n>tokenized_text_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 进行模型训练</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>word2vec</span><span class=o>.</span><span class=n>Word2Vec</span><span class=p>(</span><span class=n>sentences</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=mi>250</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 保存模型</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=n>output_model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>wv_visualize</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>model_path</span><span class=p>,</span> <span class=n>word</span><span class=o>=</span><span class=p>[</span><span class=s2>&#34;中国&#34;</span><span class=p>,</span> <span class=s2>&#34;航空&#34;</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    根据输入的词搜索邻近词然后可视化展示
</span></span></span><span class=line><span class=cl><span class=s2>    参数：
</span></span></span><span class=line><span class=cl><span class=s2>        model_path: Word2Vec 模型地址
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 加载模型</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>word2vec</span><span class=o>.</span><span class=n>Word2Vec</span><span class=o>.</span><span class=n>load</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 寻找出最相似的多个词</span>
</span></span><span class=line><span class=cl>    <span class=n>words</span> <span class=o>=</span> <span class=p>[</span><span class=n>wp</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=k>for</span> <span class=n>wp</span> <span class=ow>in</span> <span class=n>model</span><span class=o>.</span><span class=n>most_similar</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=n>topn</span><span class=o>=</span><span class=mi>20</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 提取出词对应的词向量</span>
</span></span><span class=line><span class=cl>    <span class=n>wordsInVector</span> <span class=o>=</span> <span class=p>[</span><span class=n>model</span><span class=p>[</span><span class=n>word</span><span class=p>]</span> <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=n>words</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 进行 PCA 降维</span>
</span></span><span class=line><span class=cl>    <span class=n>pca</span> <span class=o>=</span> <span class=n>PCA</span><span class=p>(</span><span class=n>n_components</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>pca</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>wordsInVector</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>pca</span><span class=o>.</span><span class=n>transform</span><span class=p>(</span><span class=n>wordsInVector</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 绘制图形</span>
</span></span><span class=line><span class=cl>    <span class=n>xs</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>ys</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>xs</span><span class=p>,</span> <span class=n>ys</span><span class=p>,</span> <span class=n>marker</span><span class=o>=</span><span class=s1>&#39;o&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 遍历所有的词添加点注释</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span><span class=p>,</span> <span class=n>w</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>words</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>plt</span><span class=o>.</span><span class=n>annotate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>w</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>xy</span><span class=o>=</span><span class=p>(</span><span class=n>xs</span><span class=p>[</span><span class=n>i</span><span class=p>],</span> <span class=n>ys</span><span class=p>[</span><span class=n>i</span><span class=p>]),</span> <span class=n>xytext</span><span class=o>=</span><span class=p>(</span><span class=mi>6</span><span class=p>,</span> <span class=mi>6</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>textcoords</span><span class=o>=</span><span class=s1>&#39;offset points&#39;</span><span class=p>,</span> <span class=n>ha</span><span class=o>=</span><span class=s1>&#39;left&#39;</span><span class=p>,</span> <span class=n>va</span><span class=o>=</span><span class=s1>&#39;top&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=o>**</span><span class=nb>dict</span><span class=p>(</span><span class=n>fontsize</span><span class=o>=</span><span class=mi>10</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></div></div><div class=article-widget><div class="container-xl row post-nav"><div class="col-6 post-nav-item"><div class=meta-nav>上一页</div><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E8%AF%8D%E8%A1%A8%E7%A4%BA/ rel=next>词表示</a></div><div class="col-6 post-nav-item"><div class=meta-nav>下一页</div><a href=/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/ rel=prev>统计语言模型</a></div></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>