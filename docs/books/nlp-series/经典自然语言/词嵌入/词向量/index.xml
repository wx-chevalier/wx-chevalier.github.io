<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>词向量 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/</link><atom:link href="https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/index.xml" rel="self" type="application/rss+xml"/><description>词向量</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>词向量</title><link>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/</link></image><item><title>基于 Gensim 的 Word2Vec 实践</title><link>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/nlp-series/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/</guid><description>&lt;h1 id="word2vec">Word2Vec&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/24961011" target="_blank" rel="noopener">基于 Gensim 的 Word2Vec 实践&lt;/a>，从属于笔者的&lt;a href="https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders" target="_blank" rel="noopener">程序猿的数据科学与机器学习实战手册&lt;/a>，代码参考&lt;a href="https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders/blob/master/code/python/nlp/genism/gensim.ipynb" target="_blank" rel="noopener">gensim.ipynb&lt;/a>。推荐前置阅读&lt;a href="https://zhuanlan.zhihu.com/p/24536868" target="_blank" rel="noopener">Python 语法速览与机器学习开发环境搭建&lt;/a>，&lt;a href="https://zhuanlan.zhihu.com/p/24770526" target="_blank" rel="noopener">Scikit-Learn 备忘录&lt;/a>。&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://i.ytimg.com/vi/xMwx2A_o5r4/maxresdefault.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;blockquote>
&lt;ul>
&lt;li>&lt;a href="https://rare-technologies.com/word2vec-tutorial/" target="_blank" rel="noopener">Word2Vec Tutorial&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python" target="_blank" rel="noopener">Getting Started with Word2Vec and GloVe in Python&lt;/a>&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;h2 id="模型创建">模型创建&lt;/h2>
&lt;p>&lt;a href="http://radimrehurek.com/gensim/models/word2vec.html" target="_blank" rel="noopener">Gensim&lt;/a>中 Word2Vec 模型的期望输入是进过分词的句子列表，即是某个二维数组。这里我们暂时使用 Python 内置的数组，不过其在输入数据集较大的情况下会占用大量的 RAM。Gensim 本身只是要求能够迭代的有序句子列表，因此在工程实践中我们可以使用自定义的生成器，只在内存中保存单条语句。&lt;/p>
&lt;pre tabindex="0">&lt;code># 引入 word2vec
from gensim.models import word2vec
# 引入日志配置
import logging
logging.basicConfig(format=&amp;#39;%(asctime)s : %(levelname)s : %(message)s&amp;#39;, level=logging.INFO)
# 引入数据集
raw_sentences = [&amp;#34;the quick brown fox jumps over the lazy dogs&amp;#34;,&amp;#34;yoyoyo you go home now to sleep&amp;#34;]
# 切分词汇
sentences= [s.encode(&amp;#39;utf-8&amp;#39;).split() for s in sentences]
# 构建模型
model = word2vec.Word2Vec(sentences, min_count=1)
# 进行相关性比较
model.similarity(&amp;#39;dogs&amp;#39;,&amp;#39;you&amp;#39;)
&lt;/code>&lt;/pre>&lt;p>这里我们调用&lt;code>Word2Vec&lt;/code>创建模型实际上会对数据执行两次迭代操作，第一轮操作会统计词频来构建内部的词典数结构，第二轮操作会进行神经网络训练，而这两个步骤是可以分步进行的，这样对于某些不可重复的流(譬如 Kafka 等流式数据中)可以手动控制：&lt;/p>
&lt;pre tabindex="0">&lt;code>model = gensim.models.Word2Vec(iter=1) # an empty model, no training yet
model.build_vocab(some_sentences) # can be a non-repeatable, 1-pass generator
model.train(other_sentences) # can be a non-repeatable, 1-pass generator
&lt;/code>&lt;/pre>&lt;h3 id="word2vec-参数">Word2Vec 参数&lt;/h3>
&lt;ul>
&lt;li>min_count&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>model = Word2Vec(sentences, min_count=10) # default value is 5
&lt;/code>&lt;/pre>&lt;p>在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置&lt;code>min_count&lt;/code>参数进行控制。一般而言，合理的参数值会设置在 0~100 之间。&lt;/p>
&lt;ul>
&lt;li>size&lt;/li>
&lt;/ul>
&lt;p>&lt;code>size&lt;/code>参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为 100 层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。&lt;/p>
&lt;pre tabindex="0">&lt;code>model = Word2Vec(sentences, size=200) # default value is 100
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>workers&lt;/li>
&lt;/ul>
&lt;p>&lt;code>workers&lt;/code>参数用于设置并发训练时候的线程数，不过仅当&lt;code>Cython&lt;/code>安装的情况下才会起作用：&lt;/p>
&lt;pre tabindex="0">&lt;code>model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization
&lt;/code>&lt;/pre>&lt;h2 id="外部语料集">外部语料集&lt;/h2>
&lt;p>在真实的训练场景中我们往往会使用较大的语料集进行训练，譬如这里以 Word2Vec 官方的&lt;a href="http://mattmahoney.net/dc/text8.zip" target="_blank" rel="noopener">text8&lt;/a>为例，只要改变模型中的语料集开源即可：&lt;/p>
&lt;pre tabindex="0">&lt;code>sentences = word2vec.Text8Corpus(&amp;#39;text8&amp;#39;)
model = word2vec.Word2Vec(sentences, size=200)
&lt;/code>&lt;/pre>&lt;p>这里语料集中的语句是经过分词的，因此可以直接使用。笔者在第一次使用该类时报错了，因此把 Gensim 中的源代码贴一下，也方便以后自定义处理其他语料集：&lt;/p>
&lt;pre tabindex="0">&lt;code>class Text8Corpus(object):
&amp;#34;&amp;#34;&amp;#34;Iterate over sentences from the &amp;#34;text8&amp;#34; corpus, unzipped from http://mattmahoney.net/dc/text8.zip .&amp;#34;&amp;#34;&amp;#34;
def __init__(self, fname, max_sentence_length=MAX_WORDS_IN_BATCH):
self.fname = fname
self.max_sentence_length = max_sentence_length
def __iter__(self):
# the entire corpus is one gigantic line -- there are no sentence marks at all
# so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens
sentence, rest = [], b&amp;#39;&amp;#39;
with utils.smart_open(self.fname) as fin:
while True:
text = rest + fin.read(8192) # avoid loading the entire file (=1 line) into RAM
if text == rest: # EOF
words = utils.to_unicode(text).split()
sentence.extend(words) # return the last chunk of words, too (may be shorter/longer)
if sentence:
yield sentence
break
last_token = text.rfind(b&amp;#39; &amp;#39;) # last token may have been split in two... keep for next iteration
words, rest = (utils.to_unicode(text[:last_token]).split(),
text[last_token:].strip()) if last_token &amp;gt;= 0 else ([], text)
sentence.extend(words)
while len(sentence) &amp;gt;= self.max_sentence_length:
yield sentence[:self.max_sentence_length]
sentence = sentence[self.max_sentence_length:]
&lt;/code>&lt;/pre>&lt;p>我们在上文中也提及，如果是对于大量的输入语料集或者需要整合磁盘上多个文件夹下的数据，我们可以以迭代器的方式而不是一次性将全部内容读取到内存中来节省 RAM 空间：&lt;/p>
&lt;pre tabindex="0">&lt;code>class MySentences(object):
def __init__(self, dirname):
self.dirname = dirname
def __iter__(self):
for fname in os.listdir(self.dirname):
for line in open(os.path.join(self.dirname, fname)):
yield line.split()
sentences = MySentences(&amp;#39;/some/directory&amp;#39;) # a memory-friendly iterator
model = gensim.models.Word2Vec(sentences)
&lt;/code>&lt;/pre>&lt;h2 id="模型保存与读取">模型保存与读取&lt;/h2>
&lt;pre tabindex="0">&lt;code>model.save(&amp;#39;text8.model&amp;#39;)
2015-02-24 11:19:26,059 : INFO : saving Word2Vec object under text8.model, separately None
2015-02-24 11:19:26,060 : INFO : not storing attribute syn0norm
2015-02-24 11:19:26,060 : INFO : storing numpy array &amp;#39;syn0&amp;#39; to text8.model.syn0.npy
2015-02-24 11:19:26,742 : INFO : storing numpy array &amp;#39;syn1&amp;#39; to text8.model.syn1.npy
model1 = Word2Vec.load(&amp;#39;text8.model&amp;#39;)
model.save_word2vec_format(&amp;#39;text.model.bin&amp;#39;, binary=True)
2015-02-24 11:19:52,341 : INFO : storing 71290x200 projection weights into text.model.bin
model1 = word2vec.Word2Vec.load_word2vec_format(&amp;#39;text.model.bin&amp;#39;, binary=True)
2015-02-24 11:22:08,185 : INFO : loading projection weights from text.model.bin
2015-02-24 11:22:10,322 : INFO : loaded (71290, 200) matrix from text.model.bin
2015-02-24 11:22:10,322 : INFO : precomputing L2-norms of word weight vectors
&lt;/code>&lt;/pre>&lt;h2 id="模型预测">模型预测&lt;/h2>
&lt;p>Word2Vec 最著名的效果即是以语义化的方式推断出相似词汇：&lt;/p>
&lt;pre tabindex="0">&lt;code>model.most_similar(positive=[&amp;#39;woman&amp;#39;, &amp;#39;king&amp;#39;], negative=[&amp;#39;man&amp;#39;], topn=1)
[(&amp;#39;queen&amp;#39;, 0.50882536)]
model.doesnt_match(&amp;#34;breakfast cereal dinner lunch&amp;#34;;.split())
&amp;#39;cereal&amp;#39;
model.similarity(&amp;#39;woman&amp;#39;, &amp;#39;man&amp;#39;)
0.73723527
model.most_similar([&amp;#39;man&amp;#39;])
[(u&amp;#39;woman&amp;#39;, 0.5686948895454407),
(u&amp;#39;girl&amp;#39;, 0.4957364797592163),
(u&amp;#39;young&amp;#39;, 0.4457539916038513),
(u&amp;#39;luckiest&amp;#39;, 0.4420626759529114),
(u&amp;#39;serpent&amp;#39;, 0.42716869711875916),
(u&amp;#39;girls&amp;#39;, 0.42680859565734863),
(u&amp;#39;smokes&amp;#39;, 0.4265017509460449),
(u&amp;#39;creature&amp;#39;, 0.4227582812309265),
(u&amp;#39;robot&amp;#39;, 0.417464017868042),
(u&amp;#39;mortal&amp;#39;, 0.41728296875953674)]
&lt;/code>&lt;/pre>&lt;p>如果我们希望直接获取某个单词的向量表示，直接以下标方式访问即可：&lt;/p>
&lt;pre tabindex="0">&lt;code>model[&amp;#39;computer&amp;#39;] # raw NumPy vector of a word
array([-0.00449447, -0.00310097, 0.02421786, ...], dtype=float32)
&lt;/code>&lt;/pre>&lt;h3 id="模型评估">模型评估&lt;/h3>
&lt;p>Word2Vec 的训练属于无监督模型，并没有太多的类似于监督学习里面的客观评判方式，更多的依赖于端应用。Google 之前公开了 20000 条左右的语法与语义化训练样本，每一条遵循&lt;code>A is to B as C is to D&lt;/code>这个格式，地址在&lt;a href="https://word2vec.googlecode.com/svn/trunk/questions-words.txt" target="_blank" rel="noopener">这里&lt;/a>:&lt;/p>
&lt;pre tabindex="0">&lt;code>model.accuracy(&amp;#39;/tmp/questions-words.txt&amp;#39;)
2014-02-01 22:14:28,387 : INFO : family: 88.9% (304/342)
2014-02-01 22:29:24,006 : INFO : gram1-adjective-to-adverb: 32.4% (263/812)
2014-02-01 22:36:26,528 : INFO : gram2-opposite: 50.3% (191/380)
2014-02-01 23:00:52,406 : INFO : gram3-comparative: 91.7% (1222/1332)
2014-02-01 23:13:48,243 : INFO : gram4-superlative: 87.9% (617/702)
2014-02-01 23:29:52,268 : INFO : gram5-present-participle: 79.4% (691/870)
2014-02-01 23:57:04,965 : INFO : gram7-past-tense: 67.1% (995/1482)
2014-02-02 00:15:18,525 : INFO : gram8-plural: 89.6% (889/992)
2014-02-02 00:28:18,140 : INFO : gram9-plural-verbs: 68.7% (482/702)
2014-02-02 00:28:18,140 : INFO : total: 74.3% (5654/7614)
&lt;/code>&lt;/pre>&lt;p>还是需要强调下，训练集上表现的好也不意味着 Word2Vec 在真实应用中就会表现的很好，还是需要因地制宜。&lt;/p>
&lt;h1 id="模型训练">模型训练&lt;/h1>
&lt;h2 id="简单语料集">简单语料集&lt;/h2>
&lt;h2 id="外部语料集-1">外部语料集&lt;/h2>
&lt;h2 id="中文语料集">中文语料集&lt;/h2>
&lt;p>约三十余万篇文章&lt;/p>
&lt;h1 id="模型应用">模型应用&lt;/h1>
&lt;h2 id="可视化预览">可视化预览&lt;/h2>
&lt;pre tabindex="0">&lt;code>from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
def wv_visualizer(model, word = [&amp;#34;man&amp;#34;]):
# 寻找出最相似的十个词
words = [wp[0] for wp in model.most_similar(word,20)]
# 提取出词对应的词向量
wordsInVector = [model[word] for word in words]
# 进行 PCA 降维
pca = PCA(n_components=2)
pca.fit(wordsInVector)
X = pca.transform(wordsInVector)
# 绘制图形
xs = X[:, 0]
ys = X[:, 1]
# draw
plt.figure(figsize=(12,8))
plt.scatter(xs, ys, marker = &amp;#39;o&amp;#39;)
for i, w in enumerate(words):
plt.annotate(
w,
xy = (xs[i], ys[i]), xytext = (6, 6),
textcoords = &amp;#39;offset points&amp;#39;, ha = &amp;#39;left&amp;#39;, va = &amp;#39;top&amp;#39;,
**dict(fontsize=10)
)
plt.show()
# 调用时传入目标词组即可
wv_visualizer(model,[&amp;#34;China&amp;#34;,&amp;#34;Airline&amp;#34;])
&lt;/code>&lt;/pre></description></item></channel></rss>