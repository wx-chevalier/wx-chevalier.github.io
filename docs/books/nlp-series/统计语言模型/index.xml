<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>统计语言模型 | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link>
      <atom:link href="https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/index.xml" rel="self" type="application/rss+xml" />
    <description>统计语言模型</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>统计语言模型</title>
      <link>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link>
    </image>
    
    <item>
      <title>Word2Vec</title>
      <link>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/</guid>
      <description>&lt;h1 id=&#34;word2vec&#34;&gt;Word2Vec&lt;/h1&gt;
&lt;p&gt;词向量最直观的理解就是将每一个单词表征为&lt;/p&gt;
&lt;p&gt;深度学习(DeepLearning)在图像、语音、视频等多方应用中大放异彩，从本质而言，深度学习是表征学习(Representation Learning)的一种方法，可以看做对事物进行分类的不同过滤器的组成。&lt;/p&gt;
&lt;p&gt;Word2Vec 是 Google 在 2013 年年中开源的一款将词表征为实数值向量的高效 工具，采用的模型有 CBOW (Continuous Bag-Of-Words，即连续的词袋模型)和 Skip-Gram 两种。word2vec 代码链接为：https://code.google.com/p/word2vec/，遵循 Apache License 2.0 开源协议，是一种对商业应用友好的许可，当然需要充分尊重原作者的著作权。Word2Vec 采用了所谓的 Distributed Representation 方式来表示词。Distributed representation 最早是 Hinton 在 1986 年的论文《Learning distributed representations of concepts》中提出的。虽然这篇文章没有说要将词做 Distributed representation，但至少这种先进的思想在那个时候就在人们的心中埋下了火种，到 2000 年之后开始逐渐被人重视。Distributed representation 用来表示词，通常被称为“Word Representation”或“Word Embedding”，中文俗称“词向量”。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://deeplearning4j.org/img/word2vec.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Word2vec 是一个神经网络，它用来在使用深度学习算法之前预处理文本。它本身并没有实现深度学习，但是 Word2Vec 把文本变成深度学习能够理解的向量形式。&lt;/p&gt;
&lt;p&gt;Word2vec 在不需要人工干预的情况下创建特征，包括词的上下文特征。这些上下文来自于多个词的窗口。如果有足够多的数据，用法和上下文，Word2Vec 能够基于这个词的出现情况高度精确的预测一个词的词义(对于深度学习来说，一个词的词义只是一个简单的信号，这个信号能用来对更大的实体分类；比如把一个文档分类到一个类别中)。&lt;/p&gt;
&lt;p&gt;Word2vec 需要一串句子做为其输入。每个句子，也就是一个词的数组，被转换成 n 维向量空间中的一个向量并且可以和其它句子(词的数组)所转换成向量进行比较。在这个向量空间里，相关的词语和词组会出现在一起。把它们变成向量之后，我们可以一定程度的计算它们的相似度并且对其进行聚类。这些类别可以作为搜索，情感分析和推荐的基础。&lt;/p&gt;
&lt;p&gt;Word2vec 神经网络的输出是一个词表，每个词由一个向量来表示，这个向量可以做为深度神经网络的输入来进行分类。&lt;/p&gt;
&lt;h1 id=&#34;quick-start&#34;&gt;Quick Start&lt;/h1&gt;
&lt;h2 id=&#34;python&#34;&gt;Python&lt;/h2&gt;
&lt;p&gt;笔者推荐使用 Anaconda 这个 Python 的机器学习发布包，此处用的测试数据来自于&lt;a href=&#34;http://mattmahoney.net/dc/text8.zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Installation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用&lt;code&gt;pip install word2vec&lt;/code&gt;，然后使用&lt;code&gt;import word2vec&lt;/code&gt;引入&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本文件预处理&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;word2vec.word2phrase(&amp;#39;/Users/drodriguez/Downloads/text8&amp;#39;, &amp;#39;/Users/drodriguez/Downloads/text8-phrases&amp;#39;, verbose=True)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[u&amp;#39;word2phrase&amp;#39;, u&amp;#39;-train&amp;#39;, u&amp;#39;/Users/drodriguez/Downloads/text8&amp;#39;, u&amp;#39;-output&amp;#39;, u&amp;#39;/Users/drodriguez/Downloads/text8-phrases&amp;#39;, u&amp;#39;-min-count&amp;#39;, u&amp;#39;5&amp;#39;, u&amp;#39;-threshold&amp;#39;, u&amp;#39;100&amp;#39;, u&amp;#39;-debug&amp;#39;, u&amp;#39;2&amp;#39;]
Starting training using file /Users/drodriguez/Downloads/text8
Words processed: 17000K     Vocab size: 4399K
Vocab size (unigrams + bigrams): 2419827
Words in train file: 17005206
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;中文实验&#34;&gt;中文实验&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;语料&lt;/p&gt;
&lt;p&gt;首先准备数据：采用网上博客上推荐的全网新闻数据(SogouCA)，大小为 2.1G。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  从ftp上下载数据包SogouCA.tar.gz：
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1 wget ftp://ftp.labs.sogou.com/Data/SogouCA/SogouCA.tar.gz --ftp-user=hebin_hit@foxmail.com --ftp-password=4FqLSYdNcrDXvNDi -r
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;      解压数据包：
&lt;/code&gt;&lt;/pre&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1 gzip -d SogouCA.tar.gz
2 tar -xvf SogouCA.tar
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;      再将生成的txt文件归并到SogouCA.txt中，取出其中包含content的行并转码，得到语料corpus.txt，大小为2.7G。
&lt;/code&gt;&lt;/pre&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1 cat *.txt &amp;gt; SogouCA.txt
2 cat SogouCA.txt | iconv -f gbk -t utf-8 -c | grep &amp;#34;&amp;lt;content&amp;gt;&amp;#34; &amp;gt; corpus.txt
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;分词&lt;/p&gt;
&lt;p&gt;用 ANSJ 对 corpus.txt 进行分词，得到分词结果 resultbig.txt，大小为 3.1G。在分词工具 seg_tool 目录下先编译再执行得到分词结果 resultbig.txt，内含 426221 个词，次数总计 572308385 个。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;词向量训练&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;nohup ./word2vec -train resultbig.txt -output vectors.bin -cbow &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; -size &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; -window &lt;span class=&#34;m&#34;&gt;5&lt;/span&gt; -negative &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; -hs &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; -sample 1e-3 -threads &lt;span class=&#34;m&#34;&gt;12&lt;/span&gt; -binary &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;分析&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;(1)相似词计算&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;./distance vectors.bin
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt; ./distance可以看成计算词与词之间的距离，把词看成向量空间上的一个点，distance看成向量空间上点与点的距离。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(2)潜在的语言学规律&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  在对demo-analogy.sh修改后得到下面几个例子：

  法国的首都是巴黎，英国的首都是伦敦，vector(&amp;quot;法国&amp;quot;) - vector(&amp;quot;巴黎) + vector(&amp;quot;英国&amp;quot;) --&amp;gt; vector(&amp;quot;伦敦&amp;quot;)&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;(3)聚类&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;将经过分词后的语料resultbig.txt中的词聚类并按照类别排序:
&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; nohup ./word2vec -train resultbig.txt -output classes.txt -cbow &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; -size &lt;span class=&#34;m&#34;&gt;200&lt;/span&gt; -window &lt;span class=&#34;m&#34;&gt;5&lt;/span&gt; -negative &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt; -hs &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; -sample 1e-3 -threads &lt;span class=&#34;m&#34;&gt;12&lt;/span&gt; -classes &lt;span class=&#34;m&#34;&gt;500&lt;/span&gt;  &lt;span class=&#34;p&#34;&gt;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; sort classes.txt -k &lt;span class=&#34;m&#34;&gt;2&lt;/span&gt; -n &amp;gt; classes_sorted_sogouca.txt
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;(4)短语分析&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;先利用经过分词的语料resultbig.txt中得出包含词和短语的文件sogouca_phrase.txt，再训练该文件中词与短语的向量表示。
&lt;/code&gt;&lt;/pre&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;1 ./word2phrase -train resultbig.txt -output sogouca_phrase.txt -threshold 500 -debug 2
2 ./word2vec -train sogouca_phrase.txt -output vectors_sogouca_phrase.bin -cbow 0 -size 300 -window 10 -negative 0 -hs 1 -sample 1e-3 -threads 12 -binary 1
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;维基百科实验&#34;&gt;维基百科实验&lt;/h2&gt;
&lt;h1 id=&#34;algorithms&#34;&gt;Algorithms&lt;/h1&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://deeplearning4j.org/img/word2vec_diagrams.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;cbow&#34;&gt;CBOW&lt;/h2&gt;
&lt;p&gt;CBOW 是 Continuous Bag-of-Words Model 的缩写，是一种与前向 NNLM 类似 的模型，不同点在于 CBOW 去掉了最耗时的非线性隐层且所有词共享隐层。如 下图所示。可以看出，CBOW 模型是预测$P(w&lt;em&gt;t|w&lt;/em&gt;{t-k},w*{t-(k-1)},\dots,w*{t-1},w*{t+1},\dots,w*{t+k})$。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://7xlgth.com1.z0.glb.clouddn.com/1424C789-5B58-43BA-952C-EACDF43E2AEB.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;从输入层到隐层所进行的操作实际就是上下文向量的加和，具体的代码如下。其中 sentence_position 为当前 word 在句子中的下标。以一个具体的句子 A B C D 为例，第一次进入到下面代码时当前 word 为 A，sentence_position 为 0。b 是一 个随机生成的 0 到$window-1$的词，整个窗口的大小为$2&lt;em&gt;window + 1 - 2&lt;/em&gt;b$，相当于左右各看$window-b$个词。可以看出随着窗口的从左往右滑动，其大小也 是随机的$3 (b=window-1)$到$2*window+1(b=0)$之间随机变通，即随机值 b 的大小决定了当前窗口的大小。代码中的 neu1 即为隐层向量，也就是上下文(窗口 内除自己之外的词)对应 vector 之和。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://7xlgth.com1.z0.glb.clouddn.com/36F89DA8-F3A0-4C6C-84F8-C31BB19CEEC1.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;skip-gram&#34;&gt;Skip-Gram&lt;/h2&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://7xlgth.com1.z0.glb.clouddn.com/F0E76FE8-7B78-4E4C-BB6A-8FB47A67645C.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;Skip-Gram 模型的图与 CBOW 正好方向相反，从图中看应该 Skip-Gram 应该预测概率$p(w_i,|w_t)$，其中$t - c \le i \le t + c$且$i \ne t,c$是决定上下文窗口大小的常数，$c$越大则需要考虑的 pair 就越多，一般能够带来更精确的结果，但是训练时间也 会增加。假设存在一个$w_1,w_2,w_3,…,w_T$的词组序列，Skip-gram 的目标是最大化：&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{T}\sum^{T}&lt;em&gt;{t=1}\sum&lt;/em&gt;{-c \le j \le c, j \ne 0}log p(w_{t+j}|w_t)
$$&lt;/p&gt;
&lt;p&gt;基本的 Skip-Gram 模型定义$p(w_o|w_I)$为：&lt;/p&gt;
&lt;p&gt;$$
P(w&lt;em&gt;o | w_I) = \frac{e^{v&lt;/em&gt;{w&lt;em&gt;o}^{T&lt;/em&gt;{V*{w_I}}}}}{\Sigma*{w=1}^{W}e^{V&lt;em&gt;w^{T&lt;/em&gt;{V_{w_I}}}}}
$$&lt;/p&gt;
&lt;p&gt;从公式不难看出，Skip-Gram 是一个对称的模型，如果$w_t$为中心词时$w_k$在其窗口内，则$w_t$也必然在以$w_k$为中心词的同样大小的窗口内，也就是：&lt;/p&gt;
&lt;p&gt;$$
\frac{1}{T}\sum^{T}&lt;em&gt;{t=1}\sum&lt;/em&gt;{-c \le j \le c, j \ne 0}log p(w*{t+j}|w_t) = \ \frac{1}{T}\sum^{T}&lt;em&gt;{t=1}\sum&lt;/em&gt;{-c \le j \le c, j \ne 0}log p(w*{t}|w_{t+j})
$$&lt;/p&gt;
&lt;p&gt;同时，Skip-Gram 中的每个词向量表征了上下文的分布。Skip-Gram 中的 Skip 是指在一定窗口内的词两两都会计算概率，就算他们之间隔着一些词，这样的好处是“白色汽车”和“白色的汽车”很容易被识别为相同的短语。&lt;/p&gt;
&lt;p&gt;与 CBOW 类似，Skip-Gram 也有两种可选的算法：层次 Softmax 和 Negative Sampling。层次 Sofamax 算法也结合了 Huffman 编码，每个词$w$都可以从树的根节点沿着唯一一条路径被访问到。假设$n(w,j)$为这条路径上的第$j$个结点，且$L(w)$为这条路径的长度，注意$j$从 1 开始编码，即$n(w,1)=root,n(w,L(w))=w$。层次 Softmax 定义的概率$p(w|w_I)$为：&lt;/p&gt;
&lt;p&gt;$$
p(w|w&lt;em&gt;I)=\Pi&lt;/em&gt;{j=1}^{L(w)-1}\sigma([n(w,j+1)=ch(n(w,j))]*v&amp;rsquo;^T_{n(w,j)}v_I)
$$&lt;/p&gt;
&lt;p&gt;$ch(n(w,j))$既可以是$n(w,j)$的左子结点也可以是$n(w,j)$的右子结点，word2vec 源代码中采用的是左子节点(Label 为$1-code[j]$)，其实此处改为右子节点也是可以的。&lt;/p&gt;
&lt;h1 id=&#34;tricks&#34;&gt;Tricks&lt;/h1&gt;
&lt;h2 id=&#34;learning-phrases&#34;&gt;Learning Phrases&lt;/h2&gt;
&lt;p&gt;对于某些词语，经常出现在一起的，我们就判定他们是短语。那么如何衡量呢？用以下公式。&lt;/p&gt;
&lt;p&gt;$score(w_i,w_j)=\frac{count(w_iw_j) - \delta}{count(w_i) * count(w_j)}$&lt;/p&gt;
&lt;p&gt;输入两个词向量，如果算出的 score 大于某个阈值时，我们就认定他们是“在一起的”。为了考虑到更长的短语，我们拿 2-4 个词语作为训练数据，依次降低阈值。&lt;/p&gt;
&lt;h1 id=&#34;implementation&#34;&gt;Implementation&lt;/h1&gt;
&lt;p&gt;Word2Vec 高效率的原因可以认为如下：&lt;/p&gt;
&lt;p&gt;1.去掉了费时的非线性隐层；&lt;/p&gt;
&lt;p&gt;2.Huffman Huffman 编码 相当于做了一定聚类，不需要统计所有词对；&lt;/p&gt;
&lt;p&gt;3.Negative Sampling；&lt;/p&gt;
&lt;p&gt;4.随机梯度算法；&lt;/p&gt;
&lt;p&gt;5.只过一遍数据，不需要反复迭代；&lt;/p&gt;
&lt;p&gt;6.编程实现中的一些 trick，比如指数运算的预计，高频词亚采样等。&lt;/p&gt;
&lt;p&gt;word2vec 可调整的超参数有很多：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;参数名&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;-size&lt;/td&gt;
&lt;td&gt;向量维度&lt;/td&gt;
&lt;td&gt;一般维度越高越好，但并不总是这样&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-window&lt;/td&gt;
&lt;td&gt;上下文窗口大小&lt;/td&gt;
&lt;td&gt;Skip-gram—般 10 左右，CBOW—般 5 左右，&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-sample&lt;/td&gt;
&lt;td&gt;高频词亚采样&lt;/td&gt;
&lt;td&gt;对大数据集合可以同时提高精度和速度，sample 的取值 在 1e-3 到 1e-5 之间效果最佳，&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-hs&lt;/td&gt;
&lt;td&gt;是否采用层次 softmax&lt;/td&gt;
&lt;td&gt;层次 softmax 对低频词效果更好；对应的 negative sampling 对高频词效果更好，向量维度较低时效果更好&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-negative&lt;/td&gt;
&lt;td&gt;负例数目&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-min-count&lt;/td&gt;
&lt;td&gt;被截断的低频词阈值&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-alpha&lt;/td&gt;
&lt;td&gt;开始的学习速率&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;-cbow&lt;/td&gt;
&lt;td&gt;使用 CBOW&lt;/td&gt;
&lt;td&gt;Skip-gram 更慢一些，但是对低频词效果更好；对应的 CBOW 则速度更快一些，&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;deeplearning4j&#34;&gt;Deeplearning4j&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://deeplearning4j.org/zh-word2vec.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Word2vec&lt;/a&gt;
&lt;blockquote&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://deeplearning4j.org/word2vec.html#intro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DL4J-Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;python-1&#34;&gt;Python&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.52nlp.cn/%E4%B8%AD%E8%8B%B1%E6%96%87%E7%BB%B4%E5%9F%BA%E7%99%BE%E7%A7%91%E8%AF%AD%E6%96%99%E4%B8%8A%E7%9A%84word2vec%E5%AE%9E%E9%AA%8C&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;中英文维基百科语料上的 Word2Vec 实验&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;%load_ext autoreload
%autoreload 2
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;word2vec-1&#34;&gt;word2vec&lt;/h1&gt;
&lt;p&gt;This notebook is equivalent to &lt;code&gt;demo-word.sh&lt;/code&gt;, &lt;code&gt;demo-analogy.sh&lt;/code&gt;, &lt;code&gt;demo-phrases.sh&lt;/code&gt; and &lt;code&gt;demo-classes.sh&lt;/code&gt; from Google.&lt;/p&gt;
&lt;h2 id=&#34;training&#34;&gt;Training&lt;/h2&gt;
&lt;p&gt;Download some data, for example: &lt;a href=&#34;http://mattmahoney.net/dc/text8.zip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://mattmahoney.net/dc/text8.zip&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import word2vec
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Run &lt;code&gt;word2phrase&lt;/code&gt; to group up similar words &amp;ldquo;Los Angeles&amp;rdquo; to &amp;ldquo;Los_Angeles&amp;rdquo;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;word2vec.word2phrase(&amp;#39;/Users/drodriguez/Downloads/text8&amp;#39;, &amp;#39;/Users/drodriguez/Downloads/text8-phrases&amp;#39;, verbose=True)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[u&amp;#39;word2phrase&amp;#39;, u&amp;#39;-train&amp;#39;, u&amp;#39;/Users/drodriguez/Downloads/text8&amp;#39;, u&amp;#39;-output&amp;#39;, u&amp;#39;/Users/drodriguez/Downloads/text8-phrases&amp;#39;, u&amp;#39;-min-count&amp;#39;, u&amp;#39;5&amp;#39;, u&amp;#39;-threshold&amp;#39;, u&amp;#39;100&amp;#39;, u&amp;#39;-debug&amp;#39;, u&amp;#39;2&amp;#39;]
Starting training using file /Users/drodriguez/Downloads/text8
Words processed: 17000K     Vocab size: 4399K
Vocab size (unigrams + bigrams): 2419827
Words in train file: 17005206
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will create a &lt;code&gt;text8-phrases&lt;/code&gt; that we can use as a better input for &lt;code&gt;word2vec&lt;/code&gt;.Note that you could easily skip this previous step and use the origial data as input for &lt;code&gt;word2vec&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Train the model using the &lt;code&gt;word2phrase&lt;/code&gt; output.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;word2vec.word2vec(&amp;#39;/Users/drodriguez/Downloads/text8-phrases&amp;#39;, &amp;#39;/Users/drodriguez/Downloads/text8.bin&amp;#39;, size=100, verbose=True)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Starting training using file /Users/drodriguez/Downloads/text8-phrases
Vocab size: 98331
Words in train file: 15857306
Alpha: 0.000002  Progress: 100.03%  Words/thread/sec: 286.52k
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That generated a &lt;code&gt;text8.bin&lt;/code&gt; file containing the word vectors in a binary format.&lt;/p&gt;
&lt;p&gt;Do the clustering of the vectors based on the trained model.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;word2vec.word2clusters(&amp;#39;/Users/drodriguez/Downloads/text8&amp;#39;, &amp;#39;/Users/drodriguez/Downloads/text8-clusters.txt&amp;#39;, 100, verbose=True)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Starting training using file /Users/drodriguez/Downloads/text8
Vocab size: 71291
Words in train file: 16718843
Alpha: 0.000002  Progress: 100.02%  Words/thread/sec: 287.55k
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That created a &lt;code&gt;text8-clusters.txt&lt;/code&gt; with the cluster for every word in the vocabulary&lt;/p&gt;
&lt;h2 id=&#34;predictions&#34;&gt;Predictions&lt;/h2&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;import word2vec
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Import the &lt;code&gt;word2vec&lt;/code&gt; binary file created above&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model = word2vec.load(&amp;#39;/Users/drodriguez/Downloads/text8.bin&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can take a look at the vocabulaty as a numpy array&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.vocab
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;array([u&amp;#39;&amp;lt;/s&amp;gt;&amp;#39;, u&amp;#39;the&amp;#39;, u&amp;#39;of&amp;#39;, ..., u&amp;#39;dakotas&amp;#39;, u&amp;#39;nias&amp;#39;, u&amp;#39;burlesques&amp;#39;],
      dtype=&amp;#39;&amp;lt;U78&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Or take a look at the whole matrix&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.vectors.shape
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(98331, 100)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.vectors
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;array([[ 0.14333282,  0.15825513, -0.13715845, ...,  0.05456942,
         0.10955409,  0.00693387],
       [ 0.1220774,  0.04939618,  0.09545057, ..., -0.00804222,
        -0.05441621, -0.10076696],
       [ 0.16844609,  0.03734054,  0.22085373, ...,  0.05854521,
         0.04685341,  0.02546694],
       ...,
       [-0.06760896,  0.03737842,  0.09344187, ...,  0.14559349,
        -0.11704484, -0.05246212],
       [ 0.02228479, -0.07340827,  0.15247506, ...,  0.01872172,
        -0.18154132, -0.06813737],
       [ 0.02778879, -0.06457976,  0.07102411, ..., -0.00270281,
        -0.0471223, -0.135444  ]])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can retreive the vector of individual words&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model[&amp;#39;dog&amp;#39;].shape
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(100,)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model[&amp;#39;dog&amp;#39;][:10]
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;array([ 0.05753701,  0.0585594,  0.11341395,  0.02016246,  0.11514406,
        0.01246986,  0.00801256,  0.17529851,  0.02899276,  0.0203866 ])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can do simple queries to retreive words similar to &amp;ldquo;socks&amp;rdquo; based on cosine similarity:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;indexes, metrics = model.cosine(&amp;#39;socks&amp;#39;)
indexes, metrics
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(array([20002, 28915, 30711, 33874, 27482, 14631, 22992, 24195, 25857, 23705]),
 array([ 0.8375354,  0.83590846,  0.82818749,  0.82533614,  0.82278399,
         0.81476386,  0.8139092,  0.81253798,  0.8105933,  0.80850171]))
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This returned a tuple with 2 items:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;numpy array with the indexes of the similar words in the vocabulary&lt;/li&gt;
&lt;li&gt;numpy array with cosine similarity to each word&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Its possible to get the words of those indexes&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.vocab[indexes]
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;array([u&amp;#39;hairy&amp;#39;, u&amp;#39;pumpkin&amp;#39;, u&amp;#39;gravy&amp;#39;, u&amp;#39;nosed&amp;#39;, u&amp;#39;plum&amp;#39;, u&amp;#39;winged&amp;#39;,
       u&amp;#39;bock&amp;#39;, u&amp;#39;petals&amp;#39;, u&amp;#39;biscuits&amp;#39;, u&amp;#39;striped&amp;#39;],
      dtype=&amp;#39;&amp;lt;U78&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There is a helper function to create a combined response: a numpy &lt;a href=&#34;http://docs.scipy.org/doc/numpy/user/basics.rec.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;record array&lt;/a&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.generate_response(indexes, metrics)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;rec.array([(u&amp;#39;hairy&amp;#39;, 0.8375353970603848), (u&amp;#39;pumpkin&amp;#39;, 0.8359084628493809),
       (u&amp;#39;gravy&amp;#39;, 0.8281874915608026), (u&amp;#39;nosed&amp;#39;, 0.8253361379785071),
       (u&amp;#39;plum&amp;#39;, 0.8227839904046932), (u&amp;#39;winged&amp;#39;, 0.8147638561412592),
       (u&amp;#39;bock&amp;#39;, 0.8139092031538545), (u&amp;#39;petals&amp;#39;, 0.8125379796045767),
       (u&amp;#39;biscuits&amp;#39;, 0.8105933044655644), (u&amp;#39;striped&amp;#39;, 0.8085017054444408)],
      dtype=[(u&amp;#39;word&amp;#39;, &amp;#39;&amp;lt;U78&amp;#39;), (u&amp;#39;metric&amp;#39;, &amp;#39;&amp;lt;f8&amp;#39;)])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Is easy to make that numpy array a pure python response:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.generate_response(indexes, metrics).tolist()
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[(u&amp;#39;hairy&amp;#39;, 0.8375353970603848),
 (u&amp;#39;pumpkin&amp;#39;, 0.8359084628493809),
 (u&amp;#39;gravy&amp;#39;, 0.8281874915608026),
 (u&amp;#39;nosed&amp;#39;, 0.8253361379785071),
 (u&amp;#39;plum&amp;#39;, 0.8227839904046932),
 (u&amp;#39;winged&amp;#39;, 0.8147638561412592),
 (u&amp;#39;bock&amp;#39;, 0.8139092031538545),
 (u&amp;#39;petals&amp;#39;, 0.8125379796045767),
 (u&amp;#39;biscuits&amp;#39;, 0.8105933044655644),
 (u&amp;#39;striped&amp;#39;, 0.8085017054444408)]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;phrases&#34;&gt;Phrases&lt;/h3&gt;
&lt;p&gt;Since we trained the model with the output of &lt;code&gt;word2phrase&lt;/code&gt; we can ask for similarity of &amp;ldquo;phrases&amp;rdquo;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;indexes, metrics = model.cosine(&amp;#39;los_angeles&amp;#39;)
model.generate_response(indexes, metrics).tolist()
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[(u&amp;#39;san_francisco&amp;#39;, 0.886558000570455),
 (u&amp;#39;san_diego&amp;#39;, 0.8731961018831669),
 (u&amp;#39;seattle&amp;#39;, 0.8455603712285231),
 (u&amp;#39;las_vegas&amp;#39;, 0.8407843553947962),
 (u&amp;#39;miami&amp;#39;, 0.8341796009062884),
 (u&amp;#39;detroit&amp;#39;, 0.8235412519780195),
 (u&amp;#39;cincinnati&amp;#39;, 0.8199138493085706),
 (u&amp;#39;st_louis&amp;#39;, 0.8160655356728751),
 (u&amp;#39;chicago&amp;#39;, 0.8156786240847214),
 (u&amp;#39;california&amp;#39;, 0.8154244925085712)]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;analogies&#34;&gt;Analogies&lt;/h3&gt;
&lt;p&gt;Its possible to do more complex queries like analogies such as: &lt;code&gt;king - man + woman = queen&lt;/code&gt; This method returns the same as &lt;code&gt;cosine&lt;/code&gt; the indexes of the words in the vocab and the metric&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;indexes, metrics = model.analogy(pos=[&amp;#39;king&amp;#39;, &amp;#39;woman&amp;#39;], neg=[&amp;#39;man&amp;#39;], n=10)
indexes, metrics
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(array([1087, 1145, 7523, 3141, 6768, 1335, 8419, 1826,  648, 1426]),
 array([ 0.2917969,  0.27353295,  0.26877692,  0.26596514,  0.26487509,
         0.26428581,  0.26315492,  0.26261258,  0.26136635,  0.26099078]))
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.generate_response(indexes, metrics).tolist()
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[(u&amp;#39;queen&amp;#39;, 0.2917968955611075),
 (u&amp;#39;prince&amp;#39;, 0.27353295205311695),
 (u&amp;#39;empress&amp;#39;, 0.2687769174818083),
 (u&amp;#39;monarch&amp;#39;, 0.2659651399832089),
 (u&amp;#39;regent&amp;#39;, 0.26487508713026797),
 (u&amp;#39;wife&amp;#39;, 0.2642858109968327),
 (u&amp;#39;aragon&amp;#39;, 0.2631549214361766),
 (u&amp;#39;throne&amp;#39;, 0.26261257728511833),
 (u&amp;#39;emperor&amp;#39;, 0.2613663460665488),
 (u&amp;#39;bishop&amp;#39;, 0.26099078142148696)]
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;clusters&#34;&gt;Clusters&lt;/h3&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;clusters = word2vec.load_clusters(&amp;#39;/Users/drodriguez/Downloads/text8-clusters.txt&amp;#39;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can see get the cluster number for individual words&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;clusters[&amp;#39;dog&amp;#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;11
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can see get all the words grouped on an specific cluster&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;clusters.get_words_on_cluster(90).shape
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(221,)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;clusters.get_words_on_cluster(90)[:10]
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;array([&amp;#39;along&amp;#39;, &amp;#39;together&amp;#39;, &amp;#39;associated&amp;#39;, &amp;#39;relationship&amp;#39;, &amp;#39;deal&amp;#39;,
       &amp;#39;combined&amp;#39;, &amp;#39;contact&amp;#39;, &amp;#39;connection&amp;#39;, &amp;#39;bond&amp;#39;, &amp;#39;respect&amp;#39;], dtype=object)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can add the clusters to the word2vec model and generate a response that includes the clusters&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.clusters = clusters
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;indexes, metrics = model.analogy(pos=[&amp;#39;paris&amp;#39;, &amp;#39;germany&amp;#39;], neg=[&amp;#39;france&amp;#39;], n=10)
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;model.generate_response(indexes, metrics).tolist()
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[(u&amp;#39;berlin&amp;#39;, 0.32333651414395953, 20),
 (u&amp;#39;munich&amp;#39;, 0.28851564633559, 20),
 (u&amp;#39;vienna&amp;#39;, 0.2768927258877336, 12),
 (u&amp;#39;leipzig&amp;#39;, 0.2690537010929304, 91),
 (u&amp;#39;moscow&amp;#39;, 0.26531859560322785, 74),
 (u&amp;#39;st_petersburg&amp;#39;, 0.259534503067277, 61),
 (u&amp;#39;prague&amp;#39;, 0.25000637367753303, 72),
 (u&amp;#39;dresden&amp;#39;, 0.2495974800117785, 71),
 (u&amp;#39;bonn&amp;#39;, 0.24403155303236473, 8),
 (u&amp;#39;frankfurt&amp;#39;, 0.24199720792200027, 31)]
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&lt;/code&gt;&lt;/pre&gt;</description>
    </item>
    
    <item>
      <title>词表示</title>
      <link>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E8%AF%8D%E8%A1%A8%E7%A4%BA/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E8%AF%8D%E8%A1%A8%E7%A4%BA/</guid>
      <description>&lt;h1 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h1&gt;
&lt;p&gt;———这里是正式回答的分割线————&lt;/p&gt;
&lt;p&gt;自然语言处理(简称 NLP)，是研究计算机处理人类语言的一门技术，包括：&lt;/p&gt;
&lt;p&gt;1.句法语义分析：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。&lt;/p&gt;
&lt;p&gt;2.信息抽取：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。&lt;/p&gt;
&lt;p&gt;3.文本挖掘(或者文本数据挖掘)：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。&lt;/p&gt;
&lt;p&gt;4.机器翻译：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络(编码-解码)的方法，逐渐形成了一套比较严谨的方法体系。&lt;/p&gt;
&lt;p&gt;5.信息检索：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用 1，2，3 的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。&lt;/p&gt;
&lt;p&gt;6.问答系统: 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。&lt;/p&gt;
&lt;p&gt;7.对话系统：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。自然语言处理经历了从规则的方法到基于统计的方法。基于统计的自然语言处理方法，在数学模型上和通信就是相同的，甚至相同的。但是科学家们也是用了几十年才认识到这个问题。统计语言模型的初衷是为了解决语音识别问题，在语音识别中，计算机需要知道一个文字序列能否构成一个有意义的句子。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简单&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拼写检查&lt;/li&gt;
&lt;li&gt;关键字搜索&lt;/li&gt;
&lt;li&gt;查找同义词&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;中等难度&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从网络或文档中提取信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;难&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;机器翻译(号称自然语言领域的圣杯)&lt;/li&gt;
&lt;li&gt;语义分析(一句话是什么意思)&lt;/li&gt;
&lt;li&gt;交叉引用(一句话中，他，这个等代词所对应的主体是哪个)&lt;/li&gt;
&lt;li&gt;问答系统(Siri, Google Now, 小娜等)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;word-representation词表示&#34;&gt;Word Representation:词表示&lt;/h1&gt;
&lt;p&gt;自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。&lt;/p&gt;
&lt;h2 id=&#34;词典&#34;&gt;词典&lt;/h2&gt;
&lt;p&gt;现实生活中，我们通过查词典来知道一个词的意思，这实际上是用另外的词或短语来表达一个词。这一方法在计算机领域也有，比如 &lt;a href=&#34;http://wordnet.princeton.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WordNet&lt;/a&gt; 实际上就是个电子化的英语词典。&lt;/p&gt;
&lt;p&gt;然而，这一方式有以下几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有大量的同义词，不利于计算&lt;/li&gt;
&lt;li&gt;更新缓慢，没有办法自动地添加新词&lt;/li&gt;
&lt;li&gt;一个词释义含有比较明显的主观色彩&lt;/li&gt;
&lt;li&gt;需要人工来创建和维护&lt;/li&gt;
&lt;li&gt;很难计算词的相似性&lt;/li&gt;
&lt;li&gt;很难进行计算，因为计算机本质上只认识 0 和 1&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;one-hot-representation基于统计的词语向量表达&#34;&gt;One-hot Representation:基于统计的词语向量表达&lt;/h2&gt;
&lt;p&gt;NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。
比如，在一个精灵国里，他们的语言非常简单，总共只有三句话：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I like NLP.&lt;/li&gt;
&lt;li&gt;I like deep learning.&lt;/li&gt;
&lt;li&gt;I enjoy flying.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这样，我们可以看到这个精灵国的词典是 [I, like, NLP, deep, learning, enjoy, flying, .]。没错，我们把标点也认为是一个词。用向量来表达词时，我们创建一个向量，向量的维度与词典的个数相同，然后让向量的某个位置为 1，其他位置全为 0。这样就创建了一个向量词 (one-hot)。&lt;/p&gt;
&lt;p&gt;比如，在我们的精灵国里，I 这个词的向量是：[1 0 0 0 0 0 0 0], deep 这个词的向量表达是 [0 0 0 1 0 0 0 0]。&lt;/p&gt;
&lt;p&gt;看起来挺好，我们终于把词转换为 0 和 1 这种计算机能理解的格式了。然而，这种表达也有个问题，很多同义词没办法表达出来，因为他们是不同的向量。怎么解决这个问题呢？我们可以通过词的上下文来表达一个词。通过上下文表达一个词的另外一个好处是，一个词往往有多个意思，具体在某个句子里是什么意思往往由它的上下文决定。&lt;/p&gt;
&lt;p&gt;这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字&lt;/p&gt;
&lt;p&gt;ID。比如刚才的例子中，话筒记为 3，麦克记为 8(假设从 0 开始记)。如果要编程实现的话，用 Hash&lt;/p&gt;
&lt;p&gt;表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。&lt;/p&gt;
&lt;p&gt;当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。&lt;/p&gt;
&lt;h2 id=&#34;基于上下文的表达&#34;&gt;基于上下文的表达&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;You shall know a word by the company it keeps. &amp;mdash; (J. R. Firth 1957: 11)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;词向量(Distributed Representation)&lt;/p&gt;
&lt;p&gt;而是用 &lt;strong&gt;Distributed Representation&lt;/strong&gt;(不知道这个应该怎么翻译，因为还存在一种叫“Distributional Representation”的表示方法，又是另一个不同的概念)表示的一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177,−0.107, 0.109, −0.542, …]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的，后文会提到目前计算出这种向量的主流方法。(个人认为)Distributed representation&lt;/p&gt;
&lt;p&gt;最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。&lt;/p&gt;
&lt;h1 id=&#34;document-representation文档表示&#34;&gt;Document Representation(文档表示)&lt;/h1&gt;
&lt;h2 id=&#34;bag-of-words&#34;&gt;Bag-of-Words&lt;/h2&gt;
&lt;p&gt;BOW (bag of words) 模型简介 Bag of words 模型最初被用在文本分类中，将文档表示成特征矢量。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是讲每篇文档都看成一个袋子(因为里面装的都是词汇，所以称为词袋，Bag of words 即因此而来)，然后看这个袋子里装的都是些什么词汇，将其分类。如果文档中猪、马、牛、羊、山谷、土地、拖拉机这样的词汇多些，而银行、大厦、汽车、公园这样的词汇少些，我们就倾向于判断它是一篇描绘乡村的文档，而不是描述城镇的。举个例子，有如下两个文档：&lt;/p&gt;
&lt;p&gt;文档一：Bob likes to play basketball, Jim likes too.&lt;/p&gt;
&lt;p&gt;文档二：Bob also likes to play football games.&lt;/p&gt;
&lt;p&gt;BOW 不仅是直观的感受，其数学原理还依托于离散数学中的&lt;a href=&#34;https://zh.m.wikipedia.org/wiki/%E5%A4%9A%E9%87%8D%E9%9B%86&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;多重集&lt;/a&gt;这个概念，多重集或多重集合是数学中的一个概念，是集合概念的推广。在一个集合中，相同的元素只能出现一次，因此只能显示出有或无的属性。在多重集之中，同一个元素可以出现多次。正式的多重集的概念大约出现在 1970 年代。多重集的势的计算和一般集合的计算方法一样，出现多次的元素则需要按出现的次数计算，不能只算一次。一个元素在多重集里出现的次数称为这个元素在多重集里面的重数(或重次、重复度)。举例来说，{1,2,3} 是一个集合，而 {\displaystyle \left{1,1,1,2,2,3\right}} 不是一个集合，而是一个多重集。其中元素 1 的重数是 3，2 的重数是 2，3 的重数是 1。{\displaystyle \left{1,1,1,2,2,3\right}} 的元素个数是 6。有时为了和一般的集合相区别，多重集合会用方括号而不是花括号标记，比如 {\displaystyle \left{1,1,1,2,2,3\right}} 会被记为 {\displaystyle \left[1,1,1,2,2,3\right]}。和多元组或数组的概念不同，多重集中的元素是没有顺序分别的，也就是说 {\displaystyle \left[1,1,1,2,2,3\right]} 和 {\displaystyle \left[1,1,2,1,2,3\right]} 是同一个多重集。&lt;/p&gt;
&lt;p&gt;基于这两个文本文档，构造一个词典：&lt;/p&gt;
&lt;p&gt;Dictionary = {1:”Bob”, 2. “like”, 3. “to”, 4. “play”, 5. “basketball”, 6. “also”, 7. “football”，8. “games”, 9. “Jim”, 10. “too”}。&lt;/p&gt;
&lt;p&gt;这个词典一共包含 10 个不同的单词，利用词典的索引号，上面两个文档每一个都可以用一个 10 维向量表示(用整数数字 0~n(n 为正整数)表示某个单词在文档中出现的次数)：&lt;/p&gt;
&lt;p&gt;1：[1, 2, 1, 1, 1, 0, 0, 0, 1, 1]&lt;/p&gt;
&lt;p&gt;2：[1, 1, 1, 1 ,0, 1, 1, 1, 0, 0]&lt;/p&gt;
&lt;p&gt;向量中每个元素表示词典中相关元素在文档中出现的次数(下文中，将用单词的直方图表示)。不过，在构造文档向量的过程中可以看到，我们并没有表达单词在原来句子中出现的次序(这是本 Bag-of-words 模型的缺点之一，不过瑕不掩瑜甚至在此处无关紧要)。&lt;/p&gt;
&lt;h1 id=&#34;distributed-representation分布式表示&#34;&gt;Distributed Representation:分布式表示&lt;/h1&gt;
&lt;p&gt;分布式表示的&lt;/p&gt;
&lt;p&gt;某个词的含义可以由其所处上下文中的其他词推导而来，譬如在 &lt;code&gt;saying that Europe needs unified banking regulation to replace the hodgepodge&lt;/code&gt; 与 &lt;code&gt;government debt problems turning into banking cries as has happened in&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>基础文本处理</title>
      <link>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/26249110&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;基于 Python 的简单自然语言处理&lt;/a&gt; 从属于笔者的 &lt;a href=&#34;https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;程序猿的数据科学与机器学习实战手册&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;基于-python-的简单自然语言处理&#34;&gt;基于 Python 的简单自然语言处理&lt;/h1&gt;
&lt;p&gt;本文是对于基于 Python 进行简单自然语言处理任务的介绍，本文的所有代码放置在&lt;a href=&#34;https://parg.co/b4h&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;。建议前置阅读 &lt;a href=&#34;https://zhuanlan.zhihu.com/p/24536868&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python 语法速览与机器学习开发环境搭建&lt;/a&gt;，更多机器学习资料参考&lt;a href=&#34;https://zhuanlan.zhihu.com/p/25612011&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;机器学习、深度学习与自然语言处理领域推荐的书籍列表&lt;/a&gt;以及&lt;a href=&#34;https://parg.co/b4C&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;面向程序猿的数据科学与机器学习知识体系及资料合集&lt;/a&gt;。&lt;/p&gt;
&lt;h1 id=&#34;twenty-news-group-语料集处理&#34;&gt;Twenty News Group 语料集处理&lt;/h1&gt;
&lt;p&gt;20 Newsgroup 数据集包含了约 20000 篇来自于不同的新闻组的文档，最早由 Ken Lang 搜集整理。本部分包含了对于数据集的抓取、特征提取、简单分类器训练、主题模型训练等。本部分代码包括主要的处理代码&lt;a href=&#34;https://parg.co/b4M&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;封装库&lt;/a&gt;与&lt;a href=&#34;https://parg.co/b4t&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;基于 Notebook 的交互示范&lt;/a&gt;。我们首先需要进行数据抓取：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;fetch_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;return data
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    执行数据抓取操作
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Arguments:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    subset -&amp;gt; string -- 抓取的目标集合 train / test / all
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;rand&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mtrand&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RandomState&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8675309&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;fetch_20newsgroups&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                &lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                &lt;span class=&#34;n&#34;&gt;shuffle&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rand&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subset&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后在 Notebook 中交互查看数据格式：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 实例化对象&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;twp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;TwentyNewsGroup&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 抓取数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;twp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fetch_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;twenty_train&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;twp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;数据集结构&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;twenty_train&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;文档数目&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;twenty_train&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;目标分类&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,[&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;twenty_train&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;t&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;t&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;twenty_train&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;target&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;数据集结构&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dict_keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;data&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;filenames&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;target_names&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;target&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;DESCR&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;description&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;文档数目&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;11314&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;目标分类&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;sci.space&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;comp.sys.mac.hardware&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sci.electronics&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;comp.sys.mac.hardware&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sci.space&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;rec.sport.hockey&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;talk.religion.misc&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;sci.med&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;talk.religion.misc&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;talk.politics.guns&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;接下来我们可以对语料集中的特征进行提取：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 进行特征提取&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 构建文档-词矩阵(Document-Term Matrix)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CountVectorizer&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;count_vect&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;X_train_counts&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count_vect&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;twenty_train&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;DTM 结构&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X_train_counts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 查看某个词在词表中的下标&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;词对应下标&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;-&amp;gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;count_vect&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vocabulary_&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;sa&#34;&gt;u&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;algorithm&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;DTM&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;结构&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;11314&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;130107&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;词对应下标&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;27366&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;为了将文档用于进行分类任务，还需要使用 TF-IDF 等常见方法将其转化为特征向量：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 构建文档的 TF 特征向量
from sklearn.feature_extraction.text import TfidfTransformer

tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)
X_train_tf = tf_transformer.transform(X_train_counts)

print(&amp;#34;某文档 TF 特征向量&amp;#34;,&amp;#34;-&amp;gt;&amp;#34;,X_train_tf)

# 构建文档的 TF-IDF 特征向量
from sklearn.feature_extraction.text import TfidfTransformer

tf_transformer = TfidfTransformer().fit(X_train_counts)
X_train_tfidf = tf_transformer.transform(X_train_counts)

print(&amp;#34;某文档 TF-IDF 特征向量&amp;#34;,&amp;#34;-&amp;gt;&amp;#34;,X_train_tfidf)

某文档 TF 特征向量 -&amp;gt;   (0, 6447)	0.0380693493813
  (0, 37842)	0.0380693493813
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们可以将特征提取、分类器训练与预测封装为单独函数：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    def extract_feature(self):
        &amp;#34;&amp;#34;&amp;#34;
        从语料集中抽取文档特征
        &amp;#34;&amp;#34;&amp;#34;

        # 获取训练数据的文档-词矩阵
        self.train_dtm = self.count_vect.fit_transform(self.data[&amp;#39;train&amp;#39;].data)

        # 获取文档的 TF 特征

        tf_transformer = TfidfTransformer(use_idf=False)

        self.train_tf = tf_transformer.transform(self.train_dtm)

        # 获取文档的 TF-IDF 特征

        tfidf_transformer = TfidfTransformer().fit(self.train_dtm)

        self.train_tfidf = tf_transformer.transform(self.train_dtm)

    def train_classifier(self):
        &amp;#34;&amp;#34;&amp;#34;
        从训练集中训练出分类器
        &amp;#34;&amp;#34;&amp;#34;

        self.extract_feature();

        self.clf = MultinomialNB().fit(
            self.train_tfidf, self.data[&amp;#39;train&amp;#39;].target)

    def predict(self, docs):
        &amp;#34;&amp;#34;&amp;#34;
        从训练集中训练出分类器
        &amp;#34;&amp;#34;&amp;#34;

        X_new_counts = self.count_vect.transform(docs)

        tfidf_transformer = TfidfTransformer().fit(X_new_counts)

        X_new_tfidf = tfidf_transformer.transform(X_new_counts)

        return self.clf.predict(X_new_tfidf)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;然后执行训练并且进行预测与评价：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 训练分类器
twp.train_classifier()

# 执行预测
docs_new = [&amp;#39;God is love&amp;#39;, &amp;#39;OpenGL on the GPU is fast&amp;#39;]
predicted = twp.predict(docs_new)

for doc, category in zip(docs_new, predicted):
    print(&amp;#39;%r =&amp;gt; %s&amp;#39; % (doc, twenty_train.target_names[category]))

# 执行模型评测
twp.fetch_data(subset=&amp;#39;test&amp;#39;)

predicted = twp.predict(twp.data[&amp;#39;test&amp;#39;].data)

import numpy as np

# 误差计算

# 简单误差均值
np.mean(predicted == twp.data[&amp;#39;test&amp;#39;].target)

# Metrics

from sklearn import metrics

print(metrics.classification_report(
    twp.data[&amp;#39;test&amp;#39;].target, predicted,
    target_names=twp.data[&amp;#39;test&amp;#39;].target_names))

# Confusion Matrix
metrics.confusion_matrix(twp.data[&amp;#39;test&amp;#39;].target, predicted)

&amp;#39;God is love&amp;#39; =&amp;gt; soc.religion.christian
&amp;#39;OpenGL on the GPU is fast&amp;#39; =&amp;gt; rec.autos
                          precision    recall  f1-score   support

             alt.atheism       0.79      0.50      0.61       319
           ...
      talk.religion.misc       1.00      0.08      0.15       251

             avg / total       0.82      0.79      0.77      7532

Out[16]:
array([[158,   0,   1,   1,   0,   1,   0,   3,   7,   1,   2,   6,   1,
          8,   3, 114,   6,   7,   0,   0],
       ...
       [ 35,   3,   1,   0,   0,   0,   1,   4,   1,   1,   6,   3,   0,
          6,   5, 127,  30,   5,   2,  21]])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;我们也可以对文档集进行主题提取：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 进行主题提取

twp.topics_by_lda()

Topic 0 : stream s1 astronaut zoo laurentian maynard s2 gtoal pem fpu
Topic 1 : 145 cx 0d bh sl 75u 6um m6 sy gld
Topic 2 : apartment wpi mars nazis monash palestine ottoman sas winner gerard
Topic 3 : livesey contest satellite tamu mathew orbital wpd marriage solntze pope
Topic 4 : x11 contest lib font string contrib visual xterm ahl brake
Topic 5 : ax g9v b8f a86 1d9 pl 0t wm 34u giz
Topic 6 : printf null char manes behanna senate handgun civilians homicides magpie
Topic 7 : buf jpeg chi tor bos det que uwo pit blah
Topic 8 : oracle di t4 risc nist instruction msg postscript dma convex
Topic 9 : candida cray yeast viking dog venus bloom symptoms observatory roby
Topic 10 : cx ck hz lk mv cramer adl optilink k8 uw
Topic 11 : ripem rsa sandvik w0 bosnia psuvm hudson utk defensive veal
Topic 12 : db espn sabbath br widgets liar davidian urartu sdpa cooling
Topic 13 : ripem dyer ucsu carleton adaptec tires chem alchemy lockheed rsa
Topic 14 : ingr sv alomar jupiter borland het intergraph factory paradox captain
Topic 15 : militia palestinian cpr pts handheld sharks igc apc jake lehigh
Topic 16 : alaska duke col russia uoknor aurora princeton nsmca gene stereo
Topic 17 : uuencode msg helmet eos satan dseg homosexual ics gear pyron
Topic 18 : entries myers x11r4 radar remark cipher maine hamburg senior bontchev
Topic 19 : cubs ufl vitamin temple gsfc mccall astro bellcore uranium wesleyan
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;常见自然语言处理工具封装&#34;&gt;常见自然语言处理工具封装&lt;/h1&gt;
&lt;p&gt;经过上面对于 20NewsGroup 语料集处理的介绍我们可以发现常见自然语言处理任务包括，数据获取、数据预处理、数据特征提取、分类模型训练、主题模型或者词向量等高级特征提取等等。笔者还习惯用 &lt;a href=&#34;https://github.com/google/python-fire&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;python-fire&lt;/a&gt; 将类快速封装为可通过命令行调用的工具，同时也支持外部模块调用使用。本部分我们主要以中文语料集为例，譬如我们需要对中文维基百科数据进行分析，可以使用 gensim 中的&lt;a href=&#34;https://parg.co/b44&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;维基百科处理类&lt;/a&gt;：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;class Wiki(object):
    &amp;#34;&amp;#34;&amp;#34;
    维基百科语料集处理
    &amp;#34;&amp;#34;&amp;#34;

    def wiki2texts(self, wiki_data_path, wiki_texts_path=&amp;#39;./wiki_texts.txt&amp;#39;):
        &amp;#34;&amp;#34;&amp;#34;
        将维基百科数据转化为文本数据
        Arguments:
        wiki_data_path -- 维基压缩文件地址
        &amp;#34;&amp;#34;&amp;#34;
        if not wiki_data_path:
            print(&amp;#34;请输入 Wiki 压缩文件路径或者前往 https://dumps.wikimedia.org/zhwiki/ 下载&amp;#34;)
            exit()

        # 构建维基语料集
        wiki_corpus = WikiCorpus(wiki_data_path, dictionary={})
        texts_num = 0

        with open(wiki_text_path, &amp;#39;w&amp;#39;, encoding=&amp;#39;utf-8&amp;#39;) as output:
            for text in wiki_corpus.get_texts():
                output.write(b&amp;#39; &amp;#39;.join(text).decode(&amp;#39;utf-8&amp;#39;) + &amp;#39;\n&amp;#39;)
                texts_num += 1
                if texts_num % 10000 == 0:
                    logging.info(&amp;#34;已处理 %d 篇文章&amp;#34; % texts_num)

        print(&amp;#34;处理完毕，请使用 OpenCC 转化为简体字&amp;#34;)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;抓取完毕后，我们还需要用 OpenCC 转化为简体字。抓取完毕后我们可以使用结巴分词对生成的文本文件进行分词，代码参考&lt;a href=&#34;https://parg.co/b4R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;，我们直接使用 &lt;code&gt;python chinese_text_processor.py tokenize_file /output.txt&lt;/code&gt; 直接执行该任务并且生成输出文件。获取分词之后的文件，我们可以将其转化为简单的词袋表示或者文档-词向量，详细代码参考&lt;a href=&#34;https://parg.co/b4f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;CorpusProcessor&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    语料集处理
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;corpus2bow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_documents&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;returns (vocab,corpus_in_bow)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        将语料集转化为 BOW 形式
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        Arguments:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        tokenized_corpus -- 经过分词的文档列表
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        Return:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        vocab -- {&amp;#39;human&amp;#39;: 0, ... &amp;#39;minors&amp;#39;: 11}
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        corpus_in_bow -- [[(0, 1), (1, 1), (2, 1)]...]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;corpora&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Dictionary&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 获取词表&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;vocab&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;token2id&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 获取文档的词袋表示&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;corpus_in_bow&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;dictionary&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;doc2bow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vocab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;corpus_in_bow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;corpus2dtm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;default_documents&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;min_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;returns (vocab, DTM)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        将语料集转化为文档-词矩阵
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        - dtm -&amp;gt; matrix: 文档-词矩阵
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;                I	like	hate	databases
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        D1	1	  1	      0	        1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        D2	1	  0	      1	        1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;documents&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;document&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;document&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;else&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;documents&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;==&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;round&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;len&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;documents&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;/&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 构建语料集统计向量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;vec&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;min_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;n&#34;&gt;analyzer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;word&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;n&#34;&gt;token_pattern&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;[\S]+&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;n&#34;&gt;tokenizer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;n&#34;&gt;preprocessor&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;n&#34;&gt;stop_words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                              &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 对于数据进行分析&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;DTM&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit_transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;documents&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;c1&#34;&gt;# 获取词表&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;vocab&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_feature_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vocab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DTM&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;我们也可以对分词之后的文档进行主题模型或者词向量提取，这里使用分词之后的文件就可以忽略中英文的差异：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;topics_by_lda&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_corpus_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_topics&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_lines&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10000&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;\s+&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    读入经过分词的文件并且对其进行 LDA 训练
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Arguments:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    tokenized_corpus_path -&amp;gt; string -- 经过分词的语料集地址
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    num_topics -&amp;gt; integer -- 主题数目
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    num_words -&amp;gt; integer -- 主题词数目
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    max_lines -&amp;gt; integer -- 每次读入的最大行数
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    split -&amp;gt; string -- 文档的词之间的分隔符
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    max_df -&amp;gt; integer -- 避免常用词，过滤超过该阈值的词
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 存放所有语料集信息&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;corpus&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokenized_corpus_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;r&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;flag&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;document&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 判断是否读取了足够的行数&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;flag&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_lines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                &lt;span class=&#34;k&#34;&gt;break&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;c1&#34;&gt;# 将读取到的内容添加到语料集中&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;corpus&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;re&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;document&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;flag&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;flag&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 构建语料集的 BOW 表示&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vocab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;DTM&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus2dtm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;max_df&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 训练 LDA 模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;lda&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;LdaMulticore&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;matutils&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Sparse2Corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;DTM&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;documents_columns&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;num_topics&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_topics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;id2word&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;vocab&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;workers&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 打印并且返回主题数据&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;topics&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;lda&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show_topics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;num_topics&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_topics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;num_words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;formatted&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;log&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ti&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topic&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;topics&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nb&#34;&gt;print&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Topic&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ti&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;:&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;该函数同样可以使用命令行直接调用，传入分词之后的文件。我们也可以对其语料集建立词向量，代码参考&lt;a href=&#34;https://parg.co/b4N&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;；如果对于词向量基本使用尚不熟悉的同学可以参考&lt;a href=&#34;https://zhuanlan.zhihu.com/p/24961011&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;基于 Gensim 的 Word2Vec 实践&lt;/a&gt;：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;wv_train&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenized_text_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output_model_path&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;./wv_model.bin&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    对于文本进行词向量训练，并将输出的词向量保存
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;sentences&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word2vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Text8Corpus&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokenized_text_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 进行模型训练&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word2vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sentences&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;250&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 保存模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;save&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;output_model_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;wv_visualize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;中国&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;航空&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    根据输入的词搜索邻近词然后可视化展示
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    参数：
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;        model_path: Word2Vec 模型地址
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 加载模型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word2vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Word2Vec&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;load&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model_path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 寻找出最相似的多个词&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wp&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;most_similar&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 提取出词对应的词向量&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;wordsInVector&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;word&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;word&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 进行 PCA 降维&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pca&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;PCA&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_components&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;pca&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordsInVector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pca&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;transform&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;wordsInVector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 绘制图形&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;xs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figure&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;figsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;marker&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;o&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 遍历所有的词添加点注释&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;enumerate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;annotate&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;w&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;xy&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;xs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]),&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;xytext&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;),&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;n&#34;&gt;textcoords&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;offset points&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;left&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;va&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;top&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;            &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;dict&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fontsize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;show&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    
    <item>
      <title>统计语言模型</title>
      <link>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-series/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/28323093&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;统计语言模型浅谈&lt;/a&gt;从属于笔者的&lt;a href=&#34;https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;程序猿的数据科学与机器学习实战手册&lt;/a&gt;，其他相关阅读&lt;a href=&#34;https://zhuanlan.zhihu.com/p/24536868&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python 语法速览与机器学习开发环境搭建&lt;/a&gt;，&lt;a href=&#34;https://zhuanlan.zhihu.com/p/24770526&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scikit-Learn 备忘录&lt;/a&gt;，&lt;a href=&#34;https://zhuanlan.zhihu.com/p/24961011&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;基于 Gensim 的 Word2Vec 实践&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;统计语言模型&#34;&gt;统计语言模型&lt;/h2&gt;
&lt;p&gt;统计语言模型(Statistical Language Model)即是用来描述词、语句乃至于整个文档这些不同的语法单元的概率分布的模型，能够用于衡量某句话或者词序列是否符合所处语言环境下人们日常的行文说话方式。统计语言模型对于复杂的大规模自然语言处理应用有着非常重要的价值，它能够有助于提取出自然语言中的内在规律从而提高语音识别、机器翻译、文档分类、光学字符识别等自然语言应用的表现。好的统计语言模型需要依赖大量的训练数据，在上世纪七八十年代，基本上模型的表现优劣往往会取决于该领域数据的丰富程度。IBM 曾进行过一次信息检索评测，发现二元语法模型(Bi-gram)需要数以亿计的词汇才能达到最优表现，而三元语法模型(TriGram)则需要数十亿级别的词汇才能达成饱和。本世纪初，最流行的统计语言模型当属 N-gram，其属于典型的基于稀疏表示(Sparse Representation)的语言模型；近年来随着深度学习的爆发与崛起，以词向量(WordEmbedding)为代表的分布式表示(Distributed Representation)的语言模型取得了更好的效果，并且深刻地影响了自然语言处理领域的其他模型与应用的变革。除此之外，Ronald Rosenfeld[7] 还提到了基于决策树的语言模型(Decision Tree Models)、最大熵模型以及自适应语言模型(Adaptive Models)等。
统计语言模型可以用来表述词汇序列的统计特性，譬如学习序列中单词的联合分布概率函数。如果我们用$w_1$ 到 $w_t$ 依次表示这句话中的各个词，那么该句式的出现概率可以简单表示为：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
P(w_1,&amp;hellip;,w_t) = \prod_{i=1}^{t}P(w_i|w_1,&amp;hellip;,w_{i-1}) = \prod_{i=1}^{t}P(w_i|Context) \
P(w_1, w_2, …, w_t) = P(w_1) \times P(w_2 | w_1) \times P(w_3 | w_1, w_2) \times … \times P(w_t | w_1, w_2, …, w_{t-1})
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;统计语言模型训练目标也可以是采用极大似然估计来求取最大化的对数似然，公式为$\frac{1}{T}\sum^T_{t=1}\sum_{-c \le j\le c,j \ne0}log p(w_{t+j}|w_t)$。其中$c$是训练上下文的大小。譬如$c$取值为 5 的情况下，一次就拿 5 个连续的词语进行训练。一般来说$c$越大，效果越好，但是花费的时间也会越多。$p(w_{t+j}|w_t)$表示$w_t$条件下出现$w_{t+j}$的概率。常见的对于某个语言模型度量的标准即是其困惑度(Perplexity)，需要注意的是这里的困惑度与信息论中的困惑度并不是相同的含义。这里的困惑度定义公式参考 Stolcke[11]，为$exp(-logP(w_t)/|\vec{w}|)$，即是$1/P(w_t|w_1^{t-1})$的几何平均数。最小化困惑度的值即是最大化每个单词的概率，不过困惑度的值严重依赖于词表以及具体使用的单词，因此其常常被用作评判其他因素相同的两个系统而不是通用的绝对性的度量参考。&lt;/p&gt;
&lt;h3 id=&#34;n-gram-语言模型&#34;&gt;N-gram 语言模型&lt;/h3&gt;
&lt;p&gt;参照上文的描述，在统计学语言模型中我们致力于计算某个词序列$E = w_1^T$的出现概率，可以形式化表示为：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
P(E) = P(|E| = T,w_1^T)
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;上式中我们求取概率的目标词序列$E$的长度为$T$，序列中第一个词为$w_1$，第二个词为$w_2$，等等，直到最后一个词为$w_T$。上式非常直观易懂，不过在真实环境下却是不可行的，因为序列的长度$T$是未知的，并且词表中词的组合方式也是非常庞大的数目，无法直接求得。为了寻找实际可行的简化模型，我们可以将整个词序列的联合概率复写为单个词或者单个词对的概率连乘。即上述公式可以复写为$P(w_1,w_2,w_3)=P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)$，推导到通用词序列，我们可以得到如下形式化表示：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
P(E) = \prod_{t=1}^{T+1}P(w_t|w_1^{t-1})
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;此时我们已经将整个词序列的联合概率分解为近似地求 $P(w_t | w_1, w_2, …, w_{t-1})$。而这里要讨论的 N-gram 模型就是用 $P(w_t | w_{t-n+1}, …, w_{t-1})$ 近似表示前者。根据$N$的取值不同我们又可以分为一元语言模型(Uni-gram)、二元语言模型(Bi-gram)、三元语言模型(Tri-gram)等等类推。该模型在中文中被称为汉语语言模型(CLM, Chinese Language Model)，即在需要把代表字母或笔画的数字，或连续无空格的拼音、笔画，转换成汉字串(即句子)时，利用上下文中相邻词间的搭配信息，计算出最大概率的句子；而不需要用户手动选择，避开了许多汉字对应一个相同的拼音(或笔画串、数字串)的重码问题。
一元语言模型又称为上下文无关语言模型，是一种简单易实现但实际应用价值有限的统计语言模型。该模型不考虑该词所对应的上下文环境，仅考虑当前词本身的概率，即是 N-gram 模型中当$N=1$的特殊情形。&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
p(w_t|Context)=p(w_t)=\frac{N_{w_t}}{N}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;N-gram 语言模型也存在一些问题，这种模型无法建模出词之间的相似度，有时候两个具有某种相似性的词，如果一个词经常出现在某段词之后，那么也许另一个词出现在这段词后面的概率也比较大。比如“白色的汽车”经常出现，那完全可以认为“白色的轿车”也可能经常出现。N-gram 语言模型无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型。大部分研究或工作都是使用 Tri-gram，就算使用高阶的模型，其统计 到的概率可信度就大打折扣，还有一些比较小的问题采用 Bi-gram。训练语料里面有些 n 元组没有出现过，其对应的条件概率就是 0,导致计算一整句话的概率为 0。最简单的计算词出现概率的方法就是在准备好的训练集中计算固定长度的词序列的出现次数，然后除以其所在上下文的次数；譬如以 Bi-gram 为例，我们有下面三条训练数据：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;i am from jiangsu.&lt;/li&gt;
&lt;li&gt;i study at nanjing university.&lt;/li&gt;
&lt;li&gt;my mother is from yancheng.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以推导出词 am, study 分别相对于 i 的后验概率：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
p(w_2={am} | w_1 = i) = \frac{w_1=i,w_2=am}{c(w_1 = 1)} = \frac{1}{2} = 0.5 \
p(w_2={study} | w_1 = i) = \frac{w_1=i,w_2=study}{c(w_1 = 1)} = \frac{1}{2} = 0.5
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;上述的计算过程可以推导为如下的泛化公式：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
P_{ML}(w_t|w_1^{t-1}) = \frac{c_{prefix} (w_1^t) }{c_{prefix} (w_1^{t-1}) }
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;这里$c_{prefix}(\cdot)$表示指定字符串在训练集中出现的次数，这种方法也就是所谓的最大似然估计(Maximum Likelihood Estimation)；该方法十分简单易用，同时还能保证较好地利用训练集中的统计特性。根据这个方法我们同样可以得出 Tri-gram 模型似然计算公式如下：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
P(w_t | w_{t-2}, w_{t-1}) =
\frac
{count(w_{t-2}w_{t-1}w_t)}
{count(w_{t-2}w_{t-1})}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;我们将 N-gram 模型中的参数记作$\theta$，其包含了给定前$n-1$个词时第$n$个词出现的概率，形式化表示为：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\theta_{w_{t-n+1}^t} = P_{ML}(w_t|w_{t-n+1}^{t-1})=\frac{c(w_{t-n+1}^t)}{c(w_{t-n+1}^{t-1})}
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;朴素的 N-gram 模型中对于训练集中尚未出现过的词序列会默认其概率为零，因为我们的模型是多个词概率的连乘，最终会导致整个句式的概率为零。我们可以通过所谓的平滑技巧来解决这个问题，即组合对于不同的$N$取值来计算平均概率。譬如我们可以组合 Uni-gram 模型与 Bi-gram 模型：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
P(w_t|w_{t-1}) = (1-\alpha)P_{ML}(w_t|w_{t-1}) + \alpha P_{ML}(w_t)
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;其中$\alpha$表示分配给 Uni-gram 求得的概率的比重，如果我们设置了$\alpha &amp;gt; 0$，那么词表中的任何词都会被赋予一定的概率。这种方法即是所谓的插入平滑(Interpolation)，被应用在了很多低频稀疏的模型中以保证其鲁棒性。当然，我们也可以引入更多的$N$的不同的取值，整个组合概率递归定义如下：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
P(w_t|w_{t-m+1}^{t-1}) = (1 - \alpha_m)P_{ML}(w_t|w_{t-m+1}^{t-1}) + \alpha_mP(w_t|w_{t-m+2}^{t-1})
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;[Stanley et al., 1996] 中还介绍了很多其他复杂但精致的平滑方法，譬如基于上下文的平滑因子计算(Context-dependent Smoothing Coefficients)，其并没有设置固定的$\alpha$值，而是动态地设置为$\alpha_{w_{t-m+1}^{t-1}}$。这就保证了模型能够在有较多的训练样例时将更多的比重分配给高阶的 N-gram 模型，而在训练样例较少时将更多的比重分配给低阶的 N-gram 模型。目前公认的使用最为广泛也最有效的平滑方式也是 [Stanley et al., 1996] 中提出的 Modified Kneser-Ney smoothing( MKN ) 模型，其综合使用了上下文平滑因子计算、打折以及低阶分布修正等手段来保证较准确地概率估计。&lt;/p&gt;
&lt;h3 id=&#34;神经网络语言模型&#34;&gt;神经网络语言模型&lt;/h3&gt;
&lt;p&gt;顾名思义，神经网络语言模型(Neural Network Language Model)即是基于神经网络的语言模型，其能够利用神经网络在非线性拟合方面的能力推导出词汇或者文本的分布式表示。在神经网络语言模型中某个单词的分布式表示会被看做激活神经元的向量空间，其区别于所谓的局部表示，即每次仅有一个神经元被激活。标准的神经网络语言模型架构如下图所示：&lt;/p&gt;
&lt;p&gt;神经网络语言模型中最著名的当属 Bengio[10] 中提出的概率前馈神经网络语言模型(Probabilistic Feedforward Neural Network Language Model)，它包含了输入(Input)、投影(Projection)、隐藏(Hidden)以及输出(Output)这四层。在输入层中，会从$V$个单词中挑选出$N$个单词以下标进行编码，其中$V$是整个词表的大小。然后输入层会通过$N \times D$这个共享的投影矩阵投射到投影层$P$；由于同一时刻仅有$N$个输入值处于激活状态，因此这个计算压力还不是很大。NNLM 模型真正的计算压力在于投影层与隐层之间的转换，譬如我们选定$N = 10$，那么投影层$P$的维度在 500 到 2000 之间，而隐层$H$的维度在于$500$到$1000$之间。同时，隐层$H$还负责计算词表中所有单词的概率分布，因此输出层的维度也是$V$。综上所述，整个模型的训练复杂度为：&lt;/p&gt;
&lt;p&gt;$$
Q = N \times D + N \times D \times H + H \times V
$$&lt;/p&gt;
&lt;p&gt;其训练集为某个巨大但固定的词汇集合$V$ 中的单词序列$w_1&amp;hellip;w_t$；其目标函数为学习到一个好的模型$f(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1})=p(w_t|w_1^{t-1})$，约束为$f(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1}) &amp;gt; 0$并且$\Sigma_{i=1}^{|V|} f(i,w_{t-1},\dots,w_{t-n+2},w_{t-n+1}) = 1$。每个输入词都被映射为一个向量，该映射用$C$表示，所以$C(w_{t-1})$即为$w_{t-1}$的词向量。定义$g$为一个前馈或者递归神经网络，其输出是一个向量，向量中的第$i$个元素表示概率$p(w_t=i|w_1^{t-1})$。训练的目标依然是最大似然加正则项，即：&lt;/p&gt;
&lt;p&gt;$$
Max Likelihood = max \frac{1}{T}\sum_tlogf(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1};\theta) + R(\theta)
$$&lt;/p&gt;
&lt;p&gt;其中$\theta$为参数，$R(\theta)$为正则项，输出层采用 sofamax 函数：&lt;/p&gt;
&lt;p&gt;$$
p(w_t|w_{t-1},\dots,w_{t-n+2},w_{t-n+1})=\frac{e^{y_{w_t}}}{\sum_ie^{y_i}}
$$&lt;/p&gt;
&lt;p&gt;其中$y_i$是每个输出词$i$的未归一化$log$概率，计算公式为$y=b+Wx+Utanh(d+Hx)$。其中$b,W,U,d,H$都是参数，$x$为输入，需要注意的是，一般的神经网络输入是不需要优化，而在这里，$x=(C(w_{t-1}),C(w_{t-2}),\dots,C(w_{t-n+1}))$，也是需要优化的参数。在图中，如果下层原始输入$x$不直接连到输出的话，可以令$b=0$，$W=0$。如果采用随机梯度算法的话，梯度的更新规则为：&lt;/p&gt;
&lt;p&gt;$$
\theta + \epsilon \frac{\partial log p(w_t | w_{t-1},\dots,w_{t-n+2},w_{t-n+1})}{\partial \theta} \to \theta
$$&lt;/p&gt;
&lt;p&gt;其中$\epsilon$为学习速率，需要注意的是，一般神经网络的输入层只是一个输入值，而在这里，输入层$x$也是参数(存在$C$中)，也是需要优化的。优化结束之后，词向量有了，语言模型也有了。这个 Softmax 模型使得概率取值为(0,1)，因此不会出现概率为 0 的情况，也就是自带平滑，无需传统 N-gram 模型中那些复杂的平滑算法。Bengio 在 APNews 数据集上做的对比实验也表明他的模型效果比精心设计平滑算法的普通 N-gram 算法要好 10%到 20%。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://7xlgth.com1.z0.glb.clouddn.com/2288BF90-FD22-493A-B703-C5AB32726FF2.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h3 id=&#34;循环神经网络语言模型&#34;&gt;循环神经网络语言模型&lt;/h3&gt;
&lt;p&gt;好的语言模型应当至少捕获自然语言的两个特征：语法特性与语义特性。为了保证语法的正确性，我们往往只需要考虑生成词的前置上下文；这也就意味着语法特性往往是属于局部特性。而语义的一致性则复杂了许多，我们需要考虑大量的乃至于整个文档语料集的上下文信息来获取正确的全局语义。神经网络语言模型相较于经典的 N-gram 模型具有更强大的表现力与更好的泛化能力，不过传统的 N-gram 语言模型与 [Bengio et al., 2003] 中提出的神经网络语言模型都不能有效地捕获全局语义信息。为了解决这个问题，[Mikolov et al., 2010; 2011] 中提出的基于循环神经网络(Recurrent Neural Network, RNN)的语言模型使用了隐状态来记录词序的历史信息，其能够捕获语言中的长程依赖。在自然语言中，往往在句式中相隔较远的两个词却具备一定的语法与语义关联，譬如&lt;code&gt;He doesn&#39;t have very much confidence in himself&lt;/code&gt; 与 &lt;code&gt;She doesn&#39;t have very much confidence in herself&lt;/code&gt; 这两句话中的&lt;code&gt;&amp;lt;He, himself&amp;gt;&lt;/code&gt;与&lt;code&gt;&amp;lt;She, herself&amp;gt;&lt;/code&gt;这两个词对，尽管句子中间的词可能会发生变化，但是这两种词对中两个词之间的关联却是固定的。这种依赖也不仅仅出现在英语中，在汉语、俄罗斯语中也都存在有大量此类型的词对组合。而另一种长期依赖(Long-term Dependencies)的典型就是所谓的选择限制(Selectional Preferences);简而言之，选择限制主要基于已知的某人会去做某事这样的信息。譬如&lt;code&gt;我要用叉子吃沙拉&lt;/code&gt;与&lt;code&gt;我要和我的朋友一起吃沙拉&lt;/code&gt;这两句话中，&lt;code&gt;叉子&lt;/code&gt;指代的是某种工具，而&lt;code&gt;我的朋友&lt;/code&gt;则是伴侣的意思。如果有人说&lt;code&gt;我要用双肩背包来吃沙拉&lt;/code&gt;就觉得很奇怪了，&lt;code&gt;双肩背包&lt;/code&gt;并不是工具也不是伴侣；如果我们破坏了这种选择限制就会生成大量的无意义句子。最后，某个句式或者文档往往都会归属于某个主题下，如果我们在某个技术主题的文档中突然发现了某个关于体育的句子，肯定会觉得很奇怪，这也就是所谓的破坏了主题一致性。&lt;/p&gt;
&lt;p&gt;[Eriguchi et al., 2016] 中介绍的循环神经网络在机器翻译上的应用就很值得借鉴，它能够有效地处理这种所谓长期依赖的问题。它的思想精髓在于计算新的隐状态$\vec{h}$时引入某个之前的隐状态$\vec{h_{t-1}}$，形式化表述如下：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\vec{h}&lt;em&gt;t =
\begin{cases}
tanh(W&lt;/em&gt;{xh}\vec{x}&lt;em&gt;t + W&lt;/em&gt;{hh}\vec{h}_{t-1} + \vec{b}_h), &amp;amp; \text{t $\geq1,$} \
0, &amp;amp; \text{otherwises}
\end{cases}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;我们可以看出，在$t \geq 1$时其与标准神经网络中隐层计算公式的区别在于多了一个连接$W_{hh}\vec{h}_{t-1}$，该连接源于前一个时间点的隐状态。在对于 RNN 有了基本的了解之后，我们就可以将其直接引入语言模型的构建中，即对于上文讨论的神经网络语言模型添加新的循环连接：&lt;/p&gt;
&lt;p&gt;$$
\begin{equation}
\begin{split}
\vec{m}&lt;em&gt;t  = M&lt;/em&gt;{\cdot,w_{t-1}} \
\vec{h}&lt;em&gt;t =
\begin{cases}
tanh(W&lt;/em&gt;{xh}\vec{x}&lt;em&gt;t + W&lt;/em&gt;{hh}\vec{h}_{t-1} + \vec{b}_h), &amp;amp; \text{t $\geq1,$} \
0, &amp;amp; \text{otherwises}
\end{cases} \
\vec{p}&lt;em&gt;t = softmax(W&lt;/em&gt;{hs}\vec{h}_t + b_s)
\end{split}
\end{equation}
$$&lt;/p&gt;
&lt;p&gt;注意，与上文介绍的前馈神经网络语言模型相对，循环神经网络语言模型中只是将前一个词而不是前两个词作为输入；这是因为我们假设$w_{t-2}$的信息已经包含在了隐状态$\vec{h_{t-1}}$中，因此不需要重复代入。&lt;/p&gt;
&lt;h4 id=&#34;平滑法&#34;&gt;平滑法&lt;/h4&gt;
&lt;p&gt;方法一为平滑法。最简单的方法是把每个 n 元组的出现次数加 1，那么原来出现 k 次的某个 n 元组就会记为 k+1 次，原来出现 0 次的 n 元组就会记为出现 1 次。这种也称为 Laplace 平滑。当然还有很多更复杂的其他平滑方法，其本质都 是将模型变为贝叶斯模型，通过引入先验分布打破似然一统天下的局面。而引入 先验方法的不同也就产生了很多不同的平滑方法。&lt;/p&gt;
&lt;h4 id=&#34;回退法&#34;&gt;回退法&lt;/h4&gt;
&lt;p&gt;方法二是回退法。有点像决策树中的后剪枝方法，即如果 n 元的概率不到，那就往上回退一步，用 n-1 元的概率乘上一个权重来模拟。&lt;/p&gt;
&lt;h2 id=&#34;n-pos-模型context--cw_t-n1cw_t-n2dotscw_t-1&#34;&gt;N-Pos 模型(Context = $c(w_{t-n+1}),c(w_{t-n+2}),\dots,c(w_{t-1})$)&lt;/h2&gt;
&lt;p&gt;严格来说 N-Pos 只是 N-Gram 的一种衍生模型。N-Gram 模型假定第 t 个词出现概率条件依赖它前 N-1 个词，而现实中很多词出现的概率是条件依赖于它前面词的语法功能的。N-Pos 模型就是基于这种假设的模型，它将词按照其语法功能进行分类，由这些词类决定下一个词出现的概率。这样的词类称为词性 (Part-of-Speech，简称为 POS)。N-Pos 模型中的每个词的条件概率表示为：&lt;/p&gt;
&lt;p&gt;$p(s)=p(w^T_1)=p(w_1,w_2,\dots,w_T)= \ \Pi^T_{t=1}p(w_t|c(w_{t-n+1}),c(w_{t-n+2}),\dots,c(w_{t-1}))$&lt;/p&gt;
&lt;p&gt;$c$为类别映射函数，即把$T$个词映射到$K$个类别($1 \le K \le T$)，实际上 N-Pos 使用了一种聚类的思想，使得 N-Gram 中$w_{t-n+1},w_{t-n+2},\dots,w_{t-1}$中的可能为$T^{n-1}$减少为$c(w_{t-n+1}),c(w_{t-n+2}),\dots,c(w_{t-1})$中的$K^{N-1}$，同时这种减少还采用了语义有意义的类别。&lt;/p&gt;
&lt;h2 id=&#34;基于决策树的语言模型&#34;&gt;基于决策树的语言模型&lt;/h2&gt;
&lt;p&gt;上面提到的上下文无关语言模型、n-gram 语言模型、n-pos 语言模型等等，都可以以统计决策树的形式表示出来。而统计决策树中每个结点的决策规则是一 个上下文相关的问题。这些问题可以是“前一个词时 w 吗？”“前一个词属于类别 C,吗？”。当然基于决策树的语言模型还可以更灵活一些，可以是一些“前一个词是动词?”，“后面有介词吗?”之类的复杂语法语义问题。基于决策树的语言模型优点是：分布数不是预先固定好的，而是根据训练预 料库中的实际情况确定，更为灵活。缺点是：构造统计决策树的问题很困难，且时空开销很大。&lt;/p&gt;
&lt;h2 id=&#34;最大熵模型&#34;&gt;最大熵模型&lt;/h2&gt;
&lt;p&gt;最大熵原理是 E.T. Jayness 于上世纪 50 年代提出的，其基本思想是：对一个 随机事件的概率分布进行预测时，在满足全部已知的条件下对未知的情况不做任何主观假设。从信息论的角度来说就是：在只掌握关于未知分布的部分知识时,应当选取符合这些知识但又能使得熵最大的概率分布。&lt;/p&gt;
&lt;p&gt;$p(w|Context)=\frac{e^{\Sigma_i \lambda_i f_i(context,w)}}{Z(Context)}$&lt;/p&gt;
&lt;h2 id=&#34;自适应语言模型&#34;&gt;自适应语言模型&lt;/h2&gt;
&lt;p&gt;前面的模型概率分布都是预先从训练语料库中估算好的，属于静态语言模型。而自适应语言模型类似是 Online Learning 的过程，即根据少量新数据动态调整模型，属于动态模型。在自然语言中，经常出现这样现象：某些在文本中通常很少出现的词，在某一局部文本中突然大量地出现。能够根据词在局部文本中出现的 情况动态地调整语言模型中的概率分布数据的语言模型成为动态、自适应或者基于缓存的语言模型。通常的做法是将静态模型与动态模型通过参数融合到一起，这种混合模型可以有效地避免数据稀疏的问题。还有一种主题相关的自适应语言模型，直观的例子为：专门针对体育相关内 容训练一个语言模型，同时保留所有语料训练的整体语言模型，当新来的数据属 于体育类别时，其应该使用的模型就是体育相关主题模型和整体语言模型相融合 的混合模型。&lt;/p&gt;
&lt;h2 id=&#34;skip-gram&#34;&gt;Skip-Gram&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A CloserLook at Skip-gram Modelling&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;根据论文中的定义可知道，常说的&lt;code&gt;k-skip-n-grams&lt;/code&gt;在句子$w_1 \dots w_m$可以表示为：&lt;/p&gt;
&lt;p&gt;${ w_{i_1},w_{i_2}, \dots w_{i_n} | \sum_{j=1}^{n}i_j - i_{j-1} &amp;lt; k }$&lt;/p&gt;
&lt;p&gt;Skip-gram 實際上的定義很簡單，就是允许跳几个字的意思… 依照原論文裡的定義，這個句子：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Insurgents killed in ongoing fighting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;在 bi-grams 的時候是拆成：{    `insurgents killed, killed in, in ongoing, ongoing fighting`    }。

在 2-skip-bi-grams 的時候拆成：{    `insurgents killed, insurgents in, insurgents ongoing, killed in, killed ongoing, killed fighting, in ongoing, in fighting, ongoing fighting`    }。

在 tri-grams 的時候是：{    `insurgents killed in, killed in ongoing, in ongoing fighting`    }。

在 2-skip-tri-grams 的時候是：{    `insurgents killed in, insurgents killed ongoing, insurgents killed fighting, insurgentsin ongoing, insurgents in fighting, insurgents ongoing fighting, killed in ongoing, killed in fighting, killed ongoing fighting, in ongoing fighting`    }。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;对于上文的语言模型的目标公式而言，Skip-Gram 模型中的$p(w_{t+j} | w_t)$公式采用的是 Softmax 函数：&lt;/p&gt;
&lt;p&gt;$p(w_o | w_I) = \frac{exp(v&amp;rsquo;^T_{w_o}v_{w_I})}{\sum^W_{w=1}exp(v&amp;rsquo;^T_wv_{w_I})}$&lt;/p&gt;
&lt;p&gt;其中$p(w_o | w_I)$表示在词语$w_I$条件下出现$w_o$的概率，$v_{w_o}$表示$w_o$代表的词向量，而$v_w$代表词汇表中所有词语的向量。$W$是词汇表的长度。不过该公式不太切实际，因为$W$太大了，通常是$10^5–10^7$。&lt;/p&gt;
&lt;h3 id=&#34;hierarchical-softmax&#34;&gt;Hierarchical Softmax&lt;/h3&gt;
&lt;p&gt;这种是原始 skip-gram 模型的变形。我们假设有这么一棵二叉树，每个叶子节点对应词汇表的词语，一一对应。所以我们可以通过这棵树来找到一条路径来找到某个词语。比如我们可以对词汇表，根据词频，建立一棵 huffman 树。每个词语都会对应一个 huffman 编码，huffman 编码就反映了这个词语在 huffman 树的路径。对于每个节点，都会定义孩子节点概率，左节点跟右节点的概率不同的，具体跟输入有关。譬如，待训练的词组中存在一句：“我爱中国”。&lt;/p&gt;
&lt;p&gt;输入：爱&lt;/p&gt;
&lt;p&gt;预测：我&lt;/p&gt;
&lt;p&gt;假设，“我”的 Huffman 编码是 1101，那么就在 Huffman 树上从根节点沿着往下走，每次走的时候，我们会根据当前节点和“爱”的向量算出(具体怎么算先不管)，走到下一个节点的概率是多少。于是，我们得到一连串的概率，我们的目标就是使得这些概率的连乘值(联合概率)最大。&lt;/p&gt;
&lt;p&gt;$p(w|w_I)=\Pi_{j=1}^{L(w)-1}\sigma([n(w,j+1)=ch(n(w,j))]*v&amp;rsquo;^T_{n(w,j)}v_{w_I})$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$L(w)$为词语$w$在二叉树路径中的长度&lt;/li&gt;
&lt;li&gt;$\sigma(*)$即为 Sigmoid 函数&lt;/li&gt;
&lt;li&gt;$n(w,j+1)$即指$w$在二叉树的第$j+1$个节点&lt;/li&gt;
&lt;li&gt;$ch(n(w,j))$表示定义了任意一个固定的节点，要么是左，要么是右。合起来的意思是左右节点的正负号是不一致的，可以是左负右正，可以是左正右负。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而对于单一的选择左右节点的概率：&lt;/p&gt;
&lt;p&gt;$\sigma(x)=\frac{1}{1+e^{-x}} \ \sigma(-x)=\frac{1}{1+e^{x}} \ \sigma(x) + \sigma(-x) = 1 $&lt;/p&gt;
&lt;p&gt;显然，我们计算这个联合概率的复杂度取决了词语在 huffman 树的路径长度，显然她比 W 小得多了。另外，由于按词频建立的 huffman 树，词频高的，huffman 编码短，计算起来就比较快。词频高的需要计算概率的次数肯定多，而 huffman 让高频词计算概率的速度比低频词的快。这也是很犀利的一个设计。&lt;/p&gt;
&lt;h2 id=&#34;nnlm&#34;&gt;NNLM&lt;/h2&gt;
&lt;p&gt;NNLM 是 Neural Network Language Model 的缩写，即神经网络语言模型。神经网络语言模型方面最值得阅读的文章是 Deep Learning 二号任人物 Bengio 的《A Neural Probabilistic Language Model》，JMLR 2003。NNLM 米用的是 Distributed Representation，即每个词被表示为一个浮点向量。其模型图如下：&lt;/p&gt;
&lt;p&gt;目标是要学到一个好的模型：&lt;/p&gt;
&lt;p&gt;$f(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1})=p(w_t|w_1^{t-1})$&lt;/p&gt;
&lt;p&gt;需要满足的约束为：&lt;/p&gt;
&lt;p&gt;上图中，&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
