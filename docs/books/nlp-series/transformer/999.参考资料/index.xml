<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>999.参考资料 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</link><atom:link href="https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/index.xml" rel="self" type="application/rss+xml"/><description>999.参考资料</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>999.参考资料</title><link>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</link></image><item><title>2019-NLP 中的 RNN、Seq2Seq 与 Attention 注意力机制</title><link>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-nlp-%E4%B8%AD%E7%9A%84-rnnseq2seq-%E4%B8%8E-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-nlp-%E4%B8%AD%E7%9A%84-rnnseq2seq-%E4%B8%8E-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid><description>&lt;blockquote>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/52119092" target="_blank" rel="noopener">原文地址&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>2020-完全解析 RNN, Seq2Seq, Attention 注意力机制</title><link>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2020-%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90-rnn-seq2seq-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2020-%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90-rnn-seq2seq-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid><description>&lt;blockquote>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/51383402" target="_blank" rel="noopener">原文地址&lt;/a>&lt;/p>
&lt;/blockquote></description></item><item><title>2021-Transformer模型详解（图解最完整版）</title><link>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%AE%8C%E6%95%B4%E7%89%88/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%AE%8C%E6%95%B4%E7%89%88/</guid><description>&lt;blockquote>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/338817680" target="_blank" rel="noopener">原文地址&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h1 id="transformer-模型详解图解最完整版">Transformer 模型详解（图解最完整版）&lt;/h1></description></item><item><title>2021-超详细图解 Self-Attention</title><link>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E8%B6%85%E8%AF%A6%E7%BB%86%E5%9B%BE%E8%A7%A3-self-attention/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/nlp-series/transformer/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E8%B6%85%E8%AF%A6%E7%BB%86%E5%9B%BE%E8%A7%A3-self-attention/</guid><description>&lt;blockquote>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/410776234" target="_blank" rel="noopener">原文地址&lt;/a>&lt;/p>
&lt;/blockquote></description></item></channel></rss>