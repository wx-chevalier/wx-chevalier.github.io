<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kube-proxy | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/k8s-series/3.%E6%9C%8D%E5%8A%A1%E8%B4%9F%E8%BD%BD/kube-proxy/</link>
      <atom:link href="https://ng-tech.icu/books/k8s-series/3.%E6%9C%8D%E5%8A%A1%E8%B4%9F%E8%BD%BD/kube-proxy/index.xml" rel="self" type="application/rss+xml" />
    <description>kube-proxy</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>kube-proxy</title>
      <link>https://ng-tech.icu/books/k8s-series/3.%E6%9C%8D%E5%8A%A1%E8%B4%9F%E8%BD%BD/kube-proxy/</link>
    </image>
    
    <item>
      <title>kube-proxy</title>
      <link>https://ng-tech.icu/books/k8s-series/3.%E6%9C%8D%E5%8A%A1%E8%B4%9F%E8%BD%BD/kube-proxy/kube-proxy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/k8s-series/3.%E6%9C%8D%E5%8A%A1%E8%B4%9F%E8%BD%BD/kube-proxy/kube-proxy/</guid>
      <description>&lt;h1 id=&#34;kube-proxy&#34;&gt;kube-proxy&lt;/h1&gt;
&lt;p&gt;当 service 有了 port 和 nodePort 之后，就可以对内/外提供服务。那么其具体是通过什么原理来实现的呢？奥妙就在 kube-proxy 在本地 node 上创建的 iptables 规则。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.postimg.cc/QtpTC39C/image.png&#34; alt=&#34;kube-proxy 路由规则&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;kube-proxy 通过配置 DNAT 规则（从容器出来的访问，从本地主机出来的访问两方面），将到这个服务地址的访问映射到本地的 kube-proxy 端口（随机端口）。然后 kube-proxy 会监听在本地的对应端口，将到这个端口的访问给代理到远端真实的 Pod 地址上去。&lt;/p&gt;
&lt;h1 id=&#34;chains&#34;&gt;Chains&lt;/h1&gt;
&lt;p&gt;创建 Service 以后，kube-proxy 会自动在集群里的 Node 上创建以下两条规则：KUBE-PORTALS-CONTAINER、KUBE-PORTALS-HOST。如果是 NodePort 方式，还会额外生成两条：KUBE-NODEPORT-CONTAINER、KUBE-NODEPORT-HOST&lt;/p&gt;
&lt;h2 id=&#34;kube-portals-container&#34;&gt;KUBE-PORTALS-CONTAINER&lt;/h2&gt;
&lt;p&gt;主要将由网络接口到来的通过服务集群入口 &lt;code&gt;&amp;lt;cluster ip&amp;gt;:port&lt;/code&gt; 的请求重定向到本地 kube-proxy 端口（随机端口）的映射，即来自本地容器的服务访问请求。这种情况的网络包不可能来自外部网络，因为 cluster ip 是个 virtual ip，外部网络中不存在这样的路由将该数据包发送到本机；所以该请求只能来自本地容器，从本地容器出来的访问，服务访问请求是通过本地容器虚拟网卡输入到本地网络接口的。&lt;/p&gt;
&lt;h2 id=&#34;kube-nodeport-container&#34;&gt;KUBE-NODEPORT-CONTAINER&lt;/h2&gt;
&lt;p&gt;主要将由网络接口到来的通过服务集群外部入口 &lt;code&gt;&amp;lt;node ip&amp;gt;:nodePort&lt;/code&gt; 的请求重定向到本地 kube-proxy 端口（随机端口）的映射；即来自 K8s 集群外部网络的服务访问请求，可以来自本机容器，也可以来自其他 node 的容器，还可以来自其他 node 的进程。&lt;/p&gt;
&lt;h1 id=&#34;kube-proxy-反向代理&#34;&gt;kube-proxy 反向代理&lt;/h1&gt;
&lt;p&gt;不管是通过集群内部服务入口 &lt;code&gt;&amp;lt;cluster ip&amp;gt;:port&lt;/code&gt; 还是通过集群外部服务入口 &lt;code&gt;&amp;lt;node ip&amp;gt;:nodePort&lt;/code&gt; 的请求都将重定向到本地 kube-proxy 端口（随机端口）的映射，然后将到这个 kube-proxy 端口的访问给代理到远端真实的 Pod 地址上去。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.postimg.cc/y8mx0P6p/image.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://i.postimg.cc/mrTDjHtL/image.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>代理与负载均衡</title>
      <link>https://ng-tech.icu/books/k8s-series/3.%E6%9C%8D%E5%8A%A1%E8%B4%9F%E8%BD%BD/kube-proxy/%E4%BB%A3%E7%90%86%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/k8s-series/3.%E6%9C%8D%E5%8A%A1%E8%B4%9F%E8%BD%BD/kube-proxy/%E4%BB%A3%E7%90%86%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description>&lt;h1 id=&#34;代理与负载均衡&#34;&gt;代理与负载均衡&lt;/h1&gt;
&lt;p&gt;在 Kubernetes 中，我们可能会遇到许多不同的代理的概念，主要包含以下几类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;kubectl 代理：在用户的桌面或 pod 中运行，代理从本地主机地址到 Kubernetes apiserver；客户端到代理将使用 HTTP，代理到 apiserver 使用 HTTPS，定位 apiserver，添加身份验证 header。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;apiserver 代理：内置于 apiserver 中，将集群外部的用户连接到集群 IP，否则这些 IP 可能无法访问。运行在 apiserver 进程中，客户端代理使用 HTTPS（也可配置为 http），代理将根据可用的信息决定使用 HTTP 或者 HTTPS 代理到目标。可用于访问节点、Pod 或服务，在访问服务时进行负载平衡。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;kube proxy：运行在每个节点上，代理 UDP 和 TCP，不能代理 HTTP。提供负载均衡，只能用来访问服务。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;位于 apiserver 之前的 Proxy/Load-balancer：存在和实现因集群而异（例如 nginx），位于所有客户和一个或多个 apiserver 之间，如果有多个 apiserver，则充当负载均衡器。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;外部服务上的云负载均衡器：由一些云提供商提供（例如 AWS ELB，Google Cloud Load Balancer），当 Kubernetes 服务类型为 LoadBalancer 时自动创建，只使用 UDP/TCP，具体实现因云提供商而异。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Kubernetes 在设计之初就充分考虑了针对容器的服务发现与负载均衡机制，提供了 Service 资源，并通过 kube-proxy 配合 Cloud Provider 来适应不同的应用场景。随着 kubernetes 用户的激增，用户场景的不断丰富，又产生了一些新的负载均衡机制。目前，Kubernetes 中的负载均衡大致可以分为以下几种机制，每种机制都有其特定的应用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Service：直接用 Service 提供 Cluster 内部的负载均衡，并借助 Cloud Provider 提供的 LB 提供外部访问。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ingress Controller：还是用 Service 提供 Cluster 内部的负载均衡，但是通过自定义 LB 提供外部访问。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Service Load Balancer：把 LB 直接跑在容器中，实现 Bare Metal 的 Service Load Balancer。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Custom Load Balancer：自定义负载均衡，并替代 kube-proxy，一般在物理部署 Kubernetes 时使用，方便接入公司已有的外部服务。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;vip-和-service-代理&#34;&gt;VIP 和 Service 代理&lt;/h1&gt;
&lt;p&gt;在 Kubernetes 集群中，每个 Node 运行一个 kube-proxy 进程。kube-proxy 负责为 Service 实现了一种 VIP（虚拟 IP）的形式，而不是 ExternalName 的形式。在 Kubernetes v1.0 版本，代理完全在 userspace。在 Kubernetes v1.1 版本，新增了 iptables 代理，但并不是默认的运行模式。从 Kubernetes v1.2 起，默认就是 iptables 代理。&lt;/p&gt;
&lt;p&gt;在 Kubernetes v1.0 版本，Service 是“4 层”（TCP/UDP over IP）概念。在 Kubernetes v1.1 版本，新增了 Ingress API（beta 版），用来表示“7 层”（HTTP）服务。&lt;/p&gt;
&lt;h2 id=&#34;userspace-代理模式&#34;&gt;userspace 代理模式&lt;/h2&gt;
&lt;p&gt;这种模式，kube-proxy 会监视 Kubernetes master 对 service 对象和 Endpoints 对象的添加和移除。对每个 Service，它会在本地 Node 上打开一个端口（随机选择）。任何连接到“代理端口”的请求，都会被代理到 Service 的 backend Pods 中的某一个上面（如 Endpoints 所报告的一样）。使用哪个 backend Pod，是基于 Service 的 SessionAffinity 来确定的。最后，它安装 iptables 规则，捕获到达该 Service 的 clusterIP（是虚拟 IP）和 Port 的请求，并重定向到代理端口，代理端口再代理请求到 backend Pod。&lt;/p&gt;
&lt;p&gt;网络返回的结果是，任何到达 Service 的 IP：Port 的请求，都会被代理到一个合适的 backend，不需要客户端知道关于 Kubernetes，service 或 pod 的任何信息。默认的策略是，通过 round-robin 算法来选择 backend Pod。实现基于客户端 IP 的会话亲和性，可以通过设置 service.spec.sessionAffinity 的值为“C&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2019/12/08/Qam6un.png&#34; alt=&#34;userspace 代理模式&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;iptables-代理模式&#34;&gt;iptables 代理模式&lt;/h2&gt;
&lt;p&gt;这种模式，kube-proxy 会监视 Kubernetes master 对象和 Endpoinnts 对象的添加和移除。对每个 Service，它会安装 iptables 规则，从而捕获到达该 Service 的 clusterIP（虚拟 IP）和端口的请求，进而将请求重定向到 Service 的一组 backend 中某个上面。对于每个 Endpoints 对象，它也会安装 iptables 规则，这个规则会选择一个 backend Pod。&lt;/p&gt;
&lt;p&gt;默认的策略是，随机选择一个 backend。实现基于客户端 IP 的会话亲和性，可以将 service.spec.sessionAffinity 的值设置为“ClientIP”（默认值为“None”）&lt;/p&gt;
&lt;p&gt;和 userspace 代理类似，网络返回的结果是，任何到达 Service 的 IP：Port 的请求，都会被代理到一个合适的 backend，不需要客户端知道关于 Kubernetes，service 或 Pod 的任何信息。这应该比 userspace 代理更快，更可靠。然而，不想 userspace 代理，如果始出选择的 Pod 没有响应，iptables 代理不能自动地重试另一个 Pod，所以它需要依赖 readiness probes；&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2019/12/08/Qam7uR.jpg&#34; alt=&#34;iptables 代理模式&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h2 id=&#34;ipvs-代理模式&#34;&gt;ipvs 代理模式&lt;/h2&gt;
&lt;p&gt;这种模式，kube-proxy 会监视 Kubernetes service 对象和 Endpoints，调用 netlink 接口以相应地创建 ipvs 规则并定期与 Kubernetes service 对象和 Endpoints 对象同步 ipvs 规则，以确保 ipvs 状态与期望一致。访问服务时，流量将被重定向到其中一个后端 Pod。&lt;/p&gt;
&lt;p&gt;与 iptables 类似，ipvs 基于 netfilter 的 hook 功能，但使用哈希表作为底层数据结构并在内核空间中工作。这意味着 ipvs 可以更快地重定向流量，并且在同步代理规则时具有更好的性能。此外，ipvs 为负载均衡算法提供了更多的选项，例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;rr：轮询调度&lt;/li&gt;
&lt;li&gt;lc：最小连接数&lt;/li&gt;
&lt;li&gt;dh：目标哈希&lt;/li&gt;
&lt;li&gt;sh：源哈希&lt;/li&gt;
&lt;li&gt;sed：最短期望延迟&lt;/li&gt;
&lt;li&gt;nq：不排队调度&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意：ipvs 模式假定在运行 kube-proxy 之前在节点上都已经安装了 IPVS 内核模块。当 kube-proxy 以 ipvs 代理模式启动时，kube-proxy 将验证节点上是否安装了 IPVS 模块，如果未安装，则 kube-proxy 将回退到 iptables 代理模式。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s2.ax1x.com/2019/12/08/QamLE6.md.png&#34; alt=&#34;ipvs 代理模式&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h1 id=&#34;负载均衡&#34;&gt;负载均衡&lt;/h1&gt;
&lt;p&gt;在 Kubernetes 集群中微服务的负载均衡是由 kube-proxy 实现的。kube-proxy 是 Kubernetes 集群内部的负载均衡器。它是一个分布式代理服务器，在 Kubernetes 的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的 Kube-proxy 就越多，高可用节点也随之增多。与之相比，我们平时在服务器端做个反向代理做负载均衡，还要进一步解决反向代理的负载均衡和高可用问题。&lt;/p&gt;
&lt;p&gt;kubernetes 中通过 service 概念来对应用做多 POD 间的负载均衡，Service 是一组 Pod 的服务抽象，相当于一组 Pod 的 LB，负责将请求分发给对应的 Pod；Service 会为这个 LB 提供一个 IP，一般称为 ClusterIP。Service Cluster IP 是 Kubernetes 系统中的虚拟 IP 地址，由系统动态分配；Kubernetes 集群中的每个节点都会运行 kube-proxy，其负责为 ExternalName 以外的服务实现虚拟 IP 形式，v1.2 版本后默认使用的是 iptables。&lt;/p&gt;
&lt;p&gt;Kube-proxy 是一个简单的网络代理和负载均衡器，它的作用主要是负责 Service 的实现，具体来说，就是实现了内部从 Pod 到 Service 和外部的从 NodePort 向 Service 的访问。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5803001/45594895-ff2acb00-b9d4-11e8-89ed-a7b0f724c249.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;kubernetes 中的每个 node 都会运行一个 kube-proxy。他为每个 service 都映射一个本地 port，任何连接这个本地 port 的请求都会转到 backend 后的随机一个 pod，service 中的字段 SessionAffinity 决定了使用 backend 的哪个 pod，最后在本地建立一些 iptables 规则，这样访问 service 的 cluster ip 以及对应的 port 时，就能将请求映射到后端的 pod 中。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://user-images.githubusercontent.com/5803001/45594757-832f8380-b9d2-11e8-9e61-b696f63051fd.png&#34; alt=&#34;image&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在这种模式下，kube-proxy 监视 Kubernetes 主服务器添加和删除服务和端点对象。对于每个服务，它安装 iptables 规则，捕获到服务的 clusterIP（虚拟）和端口的流量，并将流量重定向到服务的后端集合之一。对于每个 Endpoints 对象，它安装选择后端 Pod 的 iptables 规则。默认情况下，后端的选择是随机的。可以通过将 service.spec.sessionAffinity 设置为“ClientIP”（默认为“无”）来选择基于客户端 IP 的会话关联。与用户空间代理一样，最终结果是绑定到服务的 IP:端口的任何流量被代理到适当的后端，而客户端不知道关于 Kubernetes 或服务或 Pod 的任何信息。这应该比用户空间代理更快，更可靠。然而，与用户空间代理不同，如果最初选择的 Pod 不响应，则 iptables 代理不能自动重试另一个 Pod，因此它取决于具有工作准备就绪探测。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
