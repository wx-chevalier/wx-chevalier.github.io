<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>特征降维 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/machinelearning-notes/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/</link><atom:link href="https://ng-tech.icu/books/machinelearning-notes/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/index.xml" rel="self" type="application/rss+xml"/><description>特征降维</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>特征降维</title><link>https://ng-tech.icu/books/machinelearning-notes/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/</link></image><item><title>PCA</title><link>https://ng-tech.icu/books/machinelearning-notes/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/pca/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/machinelearning-notes/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/pca/</guid><description>&lt;h1 id="pca">PCA&lt;/h1>
&lt;p>PCA(Principal Component Analysis)不仅仅是对高维数据进行降维，更重要的是经过降维去除了噪声，发现了数据中的模式。PCA 把原先的 n 个特征用数目更少的 m 个特征取代，新特征是旧特征的线性组合，这些线性组合最大化样本方差，尽量使新的 m 个特征互不相关。从旧特征到新特征的映射捕获数据中的固有变异性。&lt;/p>
&lt;p>根据上面对 PCA 的数学原理的解释，我们可以了解到一些 PCA 的能力和限制。PCA 本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。因此，PCA 也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以 考虑 Kernel PCA，通过 Kernel 函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA 假设数据各主特征是分布在正交方向上，如果在非正交方向上 存在几个方差较大的方向，PCA 的效果就大打折扣了。&lt;/p>
&lt;p>最后需要说明的是，PCA 是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以 PCA 便于通用实现，但是本身无法个性化的优化。&lt;/p></description></item></channel></rss>