<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>资源调度 | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/</link>
      <atom:link href="https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/index.xml" rel="self" type="application/rss+xml" />
    <description>资源调度</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>资源调度</title>
      <link>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/</link>
    </image>
    
    <item>
      <title>单机资源管理</title>
      <link>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/%E5%8D%95%E6%9C%BA%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/%E5%8D%95%E6%9C%BA%E8%B5%84%E6%BA%90%E7%AE%A1%E7%90%86/</guid>
      <description>&lt;h1 id=&#34;单机资源管理&#34;&gt;单机资源管理&lt;/h1&gt;
&lt;p&gt;大量的任务实例在物理机器上实际运行时，需要单机上的隔离保护机制，以有效保障不同任务对物理资源的需求，确保高低优先级不要互相影响。同时还需要保护物理机器，避免进入过载状态，保障整机的可用性。资源高压力下的 SLA 保障一直以来是学术界和工业界发力的方向，诸如 Borg、Heracles、autoscaler 等开源探索，都假设在资源冲突时，无条件向在线业务倾斜，离线业务可以随时被牺牲。但是，离线内部的 AppMaster、实时、准实时等业务也有强 SLA 需求，不能随意牺牲。对此，学术界和工业界尚无深入思考和实践，我们需要摸着石头过河，创造适合我们自己的道路。&lt;/p&gt;
&lt;p&gt;传统的基于磁盘文件的 Shuffle 方式，在机械硬盘上有明显的碎片读问题，严重影响性能与稳定性。Spark External Shuffle Service 虽然做了一些改进，但是引入了很大的 overhead，在部分作业上得不偿失。一些工业界和学术界的工作提出了通过聚合 Partition 数据的方式达到极致性能，但缺少异常状况下的处理方案，稳定性有很大缺失。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>计算调度</title>
      <link>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/%E8%AE%A1%E7%AE%97%E8%B0%83%E5%BA%A6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/%E8%AE%A1%E7%AE%97%E8%B0%83%E5%BA%A6/</guid>
      <description>&lt;h1 id=&#34;计算调度&#34;&gt;计算调度&lt;/h1&gt;
&lt;p&gt;每个 Job 抽象成一个 DAG（有向无环图），图中的节点有前后依赖关系。随着阿里大数据业务的增长和新计算模型的提出，DAG 框架需要更好的动态性，以更灵活的适应数据和资源的变化。此外，计算调度和 Shuffle 系统需要对不同规模都给出最优的调度效果和执行性能。&lt;/p&gt;
&lt;p&gt;业界各个分布式系统（包括 SPARK, FLINK, HIVE, SCOPE, TENSORFLOW）都包含 DAG 执行框架，这些执行框架的本源都可以归结于 Dryad 提出的 DAG 模型。目前业内的 DAG 执行框架都依赖于特定的分布式系统，要么缺少清晰的点，边，图的定义，要么缺少动态执行调整能力，很难用一套 DAG 执行框架来满足大数据所有计算场景的需要。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>任务调度</title>
      <link>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6/</guid>
      <description>&lt;h1 id=&#34;资源调度&#34;&gt;资源调度&lt;/h1&gt;
&lt;p&gt;群内的海量硬件资源，如 CPU、内存、磁盘、网络、GPU、FPGA 等，需要快速地分配给每天上千万的 job，几十亿的计算实例。既要满足 CPU 密集、内存密集、或者某种型号的 GPU 板卡、数据 locality、多租户配额等多种资源约束，同时又要优化集群的资源利用率，削峰填谷、取长补短、降低成本。尤其随着集群规模的急剧扩大，这些问题的解决需要架构上有新的突破。&lt;/p&gt;
&lt;p&gt;经过几十年的发展，YARN 和 Kubernetes 成为代表性的开源调度框架。YARN 提出的双层调度框架实现了资源管理和调度的分离，满足了中小规模离线作业频繁调度的需求，但在超大规模场景下调度性能存在不足，集群利用率不高，多租户间的资源公平性较差；而 Kubernetes 是面向容器场景的调度（容器只要一次启动、不需要频繁调度），主要解决容器的编排、管理等问题，更适合任务长时间运行的场景，但在大数据计算高并发作业的场景，没有有效的解决方案。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>数据调度</title>
      <link>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/cloud-series/2.%E9%9B%86%E7%BE%A4%E7%BC%96%E6%8E%92/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/%E6%95%B0%E6%8D%AE%E8%B0%83%E5%BA%A6/</guid>
      <description>&lt;h1 id=&#34;数据调度&#34;&gt;数据调度&lt;/h1&gt;
&lt;p&gt;互联网时代的一个重要特点是数据来源广泛，出于数据生产和容灾的需要，数据通常是跨地域摆放在不同地区的不同机房。大数据服务希望容纳所有数据，做到一条 SQL 可以访问到全球任何数据，对这些数据做分析、聚合等操作。但要做到这一点需要面临极大的挑战，其中最大的挑战来自网络。不同于大数据机房内部网络，广域网的网络延迟高、带宽小、价格高、稳定性差，如果每天将全球产生的数据全部集中在一个集群需要极大的网络消耗。考虑到数据量增长的速度远大于广域网发展的速度，很多的大数据计算引擎并没有建一个超大的数据中心将所有数据容纳进来，而是在全球范围根据数据产生的位置就近建设数据中心。&lt;/p&gt;
&lt;p&gt;目前工业界成熟的大数据分析系统大都运行在一个数据中心里，跨多数据中心这个方向并没有得到很好地研究。学术界近几年有一些多数据中心调度的工作，但大多致力于 DAG 内 Task 的调度，如 Iridium 期望通过 Task 调度和数据摆放减少跨域网络对任务时效性的影响，Clarinet 提出一种 WAN-aware 的 Query Optimizer，Tetrium 在 Task 和 Job 调度时同时考虑了计算和网络资源因素。这些研究绝大部分更关注单个作业的性能影响，而没有从大数据服务提供商的角度，关注带宽成本和整体性能。&lt;/p&gt;
&lt;h1 id=&#34;数据调度架构&#34;&gt;数据调度架构&lt;/h1&gt;
&lt;p&gt;在数据调度中，我们考虑的有优化数据的摆放位置，确定采取什么迁移策略，以及当需要的数据不在本地时，如何进行数据复制和直读、写回，最终实现跨地域长传带宽的流控和整体存储成本的降低。&lt;/p&gt;
&lt;p&gt;较为粗放的策略，即考虑数据中心的业务独立。大部分的计算只需要读取内部产生的数据，数据中心之间有少量的数据依赖，由于量比较少，所以作业跨域直接访问数据也不会产生太大流量压力。数据中心各自根据业务需求和增速进行容量规划，包括计算和存储能力的扩容等。初期各数据中心整体负载不高，因此独立规划基本能满足业务的需要。&lt;/p&gt;
&lt;p&gt;不过随着业务的不断增长和变化，越来越多的作业需要依赖其他数据中心的数据进行计算，跨域带宽逐渐成为系统瓶颈。另一方面，随着机器规模不断增长，机房、网络等基础设施逐渐成为数据中心进一步扩大规模的障碍，各数据中心独立规划的弊端逐渐暴露，开始出现各数据中心忙闲不均、资源浪费的情况。&lt;/p&gt;
&lt;p&gt;为了解决跨域带宽和各数据中心规划中遇到的问题，我们可以在数据中心上层增加了一层调度层，用于调度数据和计算。这层调度独立于数据中心内部的调度，主要目的在于：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;调度数据，包括数据的迁移和复制。通过迁移数据，均衡各数据中心存储负载，实现集群层面的存储计算分离，并保证不会由于访问远程数据造成带宽雪崩；通过复制（缓存）数据，避免对同一数据的频繁跨域访问，减少带宽消耗；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调度计算，包括整体业务的迁移和 SQL 粒度的调度。通过整体业务的迁移，均衡数据中心计算负载，通过将联系紧密的业务放在一起从而减少跨域数据依赖。但业务整体迁移需要迁移大量的历史数据，会消耗大量带宽。因此我们加入了 SQL 粒度的跨机房调度，希望在不迁移或复制数据的情况下，减少带宽消耗（将计算调度到数据所在数据中心）和均衡集群负载。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
