<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SentenceAnalysis | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/</link>
      <atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/index.xml" rel="self" type="application/rss+xml" />
    <description>SentenceAnalysis</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>SentenceAnalysis</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/</link>
    </image>
    
    <item>
      <title>Representation-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/representation-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/representation-list/</guid>
      <description>&lt;h1 id=&#34;document-representation-list&#34;&gt;Document Representation List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://blog.districtdatalabs.com/nlp-research-lab-part-1-distributed-representations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2016-NLP Research Lab Part 1: Distributed Representations&lt;/a&gt;: How I Learned To Stop Worrying And Love Word Embeddings&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;word-vectors&#34;&gt;Word Vectors&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Understanding word vectors&lt;/a&gt;: Understanding word vectors: A tutorial for &amp;ldquo;Reading and Writing Electronic Text,&amp;rdquo; a class I teach at ITP.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;word2vec&#34;&gt;Word2Vec&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Code Word2Vec Tutorial Part I: The SkipGram Model&lt;/a&gt;，&lt;a href=&#34;http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Word2Vec Tutorial Part 2 - Negative Sampling&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1402.3722.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1411.2738.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;word2vec Parameter Learning Explained&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/itplus/article/details/37998797&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;论文翻译章节：基于 Negative Sampling 的模型 &lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;word2vec: negative sampling (in layman term)?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.quora.com/Deep-Learning/Deep-Learning-What-is-meant-by-a-distributed-representation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep-Learning-What-is-meant-by-a-distributed-representation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://code.google.com/p/word2vec/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google - Word2Vec&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://techblog.youdao.com/?p=915#LinkTarget_699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Learning 实战之 word2vec&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.csdn.net/lingerlanlan/article/details/38048335&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;word2vector 学习笔记(一)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://licstar.net/archives/328#s20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;词向量和语言模型&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/licstar/compare&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;关于多个词向量算法的实现对比&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/21391710&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;斯坦福深度学习课程第二弹：词向量内部和外部任务评价&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>StatisticalLanguageModel-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/statisticallanguagemodel-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/statisticallanguagemodel-list/</guid>
      <description>&lt;h1 id=&#34;统计语言模型资料索引&#34;&gt;统计语言模型资料索引&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://jon.dehdari.org/tutorials/lm_overview.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Short Overview of Statistical Language Models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Language_model#Continuous_space_language_models&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wiki 上关于语言模型的阐述&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://6me.us/Eo4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Elegant N-gram Generation in Python&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/1703.01619.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-Neural Machine Translation and Sequence-to-sequence Models: A Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kamidox/blogs/blob/master/content/notes/stanford_cs224d.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford CS224d Lecture 1&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;embedding&#34;&gt;Embedding&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Neural Network Embeddings Explained&lt;/a&gt;: How deep learning can represent War and Peace as a vector&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>TopicModel-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/topicmodel-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/syntaxsemanticanalysis/sentenceanalysis/topicmodel-list/</guid>
      <description>&lt;h1 id=&#34;主题模型资料索引&#34;&gt;主题模型资料索引&lt;/h1&gt;
&lt;p&gt;朴素贝叶斯可以胜任许多文本的分类问题，但是无法解决语料中一词多义和多词一义的问题，它更像是词法分析，而不是语义分析。而如果使用词向量作为文档的特征，可以较好地解决了一词多义和多词一义的问题，但是就好像过拟合一样，会造成计算文档间相似度的不准确性。而通过添加主题这个隐藏变量，一个词可能被映射到多个主题，而多个主题也可能被映射到一个词中，从而解决一定程度上的语义问题。&lt;/p&gt;
&lt;p&gt;主题模型经历从基于 SVD 的简单的 LSA(隐含语义分析)，到基于概率模型与 EM 的 pLSA，再到基于 Dirichlet 分布的 LDA。目前，经典的主题模型一般都会基于 BOW(Bag-of-Words)假设。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.cad.zju.edu.cn/home/vagblog/?p=4151&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Task-Driven Comparison of Topic Models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.infoq.com/cn/news/2016/07/technical-details-for-topic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LinkedIn 文本分析平台：主题挖掘的四大技术步骤&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;lda&#34;&gt;LDA&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://6me.us/idj2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LDA 算法理解&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tdhopper/notes-on-dirichlet-processes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Notes Nonparametric Bayesian Methods and Dirichlet Processes&lt;/a&gt;: &lt;a href=&#34;https://parg.co/bsl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nonparametric Latent Dirichlet Allocation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://parg.co/bsl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nonparametric Latent Dirichlet Allocation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;lda2vec&#34;&gt;Lda2Vec&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.datasciencecentral.com/profiles/blogs/a-tale-about-lda2vec-when-lda-meets-word2vec?xg_source=activity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A tale about LDA2vec: when LDA meets word2vec&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;word2vec, LDA, and introducing a new hybrid algorithm: lda2vec&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;topic-model-evaluation&#34;&gt;Topic Model Evaluation&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://qpleple.com/perplexity-to-evaluate-topic-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Perplexity To Evaluate Topic Models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://qpleple.com/topic-coherence-to-evaluate-topic-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Topic Coherence To Evaluate Topic Models&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2015-Exploring the Space of Topic Coherence Measures&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://epubs.siam.org/doi/pdf/10.1137/1.9781611972733.6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hierarchical Document Clustering Using Frequent Itemsets&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
