<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/</link><atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/index.xml" rel="self" type="application/rss+xml"/><description>Transformer</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>Transformer</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/</link></image><item><title>Transformer-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/transformer-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/transformer-list/</guid><description>&lt;h1 id="transformer-list">Transformer List&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">2018-The Illustrated Transformer&lt;/a>: In this post, we will look at The Transformer â€“ a model that uses attention to boost the speed with which these models can be trained.&lt;/li>
&lt;/ul>
&lt;h1 id="resource">Resource&lt;/h1>
&lt;h2 id="course">Course&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/huggingface/course" target="_blank" rel="noopener">2022-The Hugging Face Course #Course#&lt;/a>: This repo contains the content that&amp;rsquo;s used to create the Hugging Face course. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you&amp;rsquo;ll learn how to use the Hugging Face ecosystem â€” ðŸ¤— Transformers, ðŸ¤— Datasets, ðŸ¤— Tokenizers, and ðŸ¤— Accelerate â€” as well as the Hugging Face Hub. It&amp;rsquo;s completely free and open-source!&lt;/li>
&lt;/ul>
&lt;h2 id="series">Series&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/" target="_blank" rel="noopener">2021-åŸºäºŽ Transformers çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨ #Series#&lt;/a>: Natural Language Processing with transformers. æœ¬é¡¹ç›®é¢å‘çš„å¯¹è±¡æ˜¯ï¼šNLP åˆå­¦è€…ã€transformer åˆå­¦è€…ï¼Œæœ‰ä¸€å®šçš„ pythonã€pytorch ç¼–ç¨‹åŸºç¡€ï¼Œå¯¹å‰æ²¿çš„ transformer æ¨¡åž‹æ„Ÿå…´è¶£ï¼Œäº†è§£å’ŒçŸ¥é“ç®€å•çš„æ·±åº¦å­¦ä¹ æ¨¡åž‹ã€‚&lt;/li>
&lt;/ul></description></item></channel></rss>