<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Transformer | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/</link><atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/index.xml" rel="self" type="application/rss+xml"/><description>Transformer</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>Transformer</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/</link></image><item><title>Transformer-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/transformer-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/transformer/transformer-list/</guid><description>&lt;h1 id="transformer-list">Transformer List&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">2018-The Illustrated Transformer&lt;/a>: In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained.&lt;/li>
&lt;/ul>
&lt;h1 id="resource">Resource&lt;/h1>
&lt;h2 id="course">Course&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/huggingface/course" target="_blank" rel="noopener">2022-The Hugging Face Course #Course#&lt;/a>: This repo contains the content that&amp;rsquo;s used to create the Hugging Face course. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you&amp;rsquo;ll learn how to use the Hugging Face ecosystem — 🤗 Transformers, 🤗 Datasets, 🤗 Tokenizers, and 🤗 Accelerate — as well as the Hugging Face Hub. It&amp;rsquo;s completely free and open-source!&lt;/li>
&lt;/ul>
&lt;h2 id="series">Series&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://datawhalechina.github.io/learn-nlp-with-transformers/#/" target="_blank" rel="noopener">2021-基于 Transformers 的自然语言处理(NLP)入门 #Series#&lt;/a>: Natural Language Processing with transformers. 本项目面向的对象是：NLP 初学者、transformer 初学者，有一定的 python、pytorch 编程基础，对前沿的 transformer 模型感兴趣，了解和知道简单的深度学习模型。&lt;/li>
&lt;/ul></description></item></channel></rss>