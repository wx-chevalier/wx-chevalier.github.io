<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLaMA | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llama/</link><atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llama/index.xml" rel="self" type="application/rss+xml"/><description>LLaMA</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>LLaMA</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llama/</link></image><item><title>Dolly-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llama/dolly-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llama/dolly-list/</guid><description>&lt;h1 id="dolly-list">Dolly List&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kw2828/Dolly-2.0-Series" target="_blank" rel="noopener">2023-Dolly-2.0-Series
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/kw2828/Dolly-2.0-Series" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Democratizing resources for running, fine-tuning, and inferencing Dolly 2.0.&lt;/li>
&lt;/ul></description></item><item><title>LLaMA-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llama/llama-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llama/llama-list/</guid><description>&lt;h1 id="llama">LLaMA&lt;/h1>
&lt;h1 id="opensource">OpenSource&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/shawwn/llama-dl" target="_blank" rel="noopener">2023-llama-dl
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/shawwn/llama-dl" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: High-speed download of LLaMA, Facebook&amp;rsquo;s 65B parameter GPT model&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/jerryjliu/llama_index" target="_blank" rel="noopener">2023-LlamaIndex
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/jerryjliu/llama_index" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: LlamaIndex (GPT Index) is a project that provides a central interface to connect your LLM&amp;rsquo;s with external data.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/cocktailpeanut/dalai" target="_blank" rel="noopener">2023-dalai
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/cocktailpeanut/dalai" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: The simplest way to run LLaMA on your local machine&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/antimatter15/alpaca.cpp" target="_blank" rel="noopener">2023-Alpaca.cpp
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/antimatter15/alpaca.cpp" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Run a fast ChatGPT-like model locally on your device. The screencast below is not sped up and running on an M2 Macbook Air with 4GB of weights.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/Alpaca-LoRA" target="_blank" rel="noopener">2023-Alpaca-LoRA
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/Alpaca-LoRA" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Instruct-tuning LLaMA on consumer hardware.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/setzer22/llama-rs" target="_blank" rel="noopener">2023-llama-rs
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/setzer22/llama-rs" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: LLaMA-rs is a Rust port of the llama.cpp project. This allows running inference for Facebook&amp;rsquo;s LLaMA model on a CPU with good performance using full precision, f16 or 4-bit quantized versions of the model.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/nsarrazin/serge" target="_blank" rel="noopener">2023-Serge
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/nsarrazin/serge" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: A chat interface based on llama.cpp for running Alpaca models. Entirely self-hosted, no API keys needed. Fits on 4GB of RAM and runs on the CPU.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/ggerganov/llama.cpp" target="_blank" rel="noopener">2023-llama.cpp
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/ggerganov/llama.cpp" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: The main goal is to run the model using 4-bit quantization on a MacBook.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/nomic-ai/gpt4all" target="_blank" rel="noopener">2023-gpt4all
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/nomic-ai/gpt4all" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Demo, data and code to train an assistant-style large language model with ~800k GPT-3.5-Turbo Generations based on LLaMa&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="chinese">Chinese&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/ymcui/Chinese-LLaMA-Alpaca" target="_blank" rel="noopener">2023-Chinese-LLaMA-Alpaca
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/ymcui/Chinese-LLaMA-Alpaca" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: 为了促进大模型在中文 NLP 社区的开放研究，本项目开源了中文 LLaMA 模型和经过指令精调的 Alpaca 大模型。这些模型在原版 LLaMA 的基础上扩充了中文词表并使用了中文数据进行二次预训练，进一步提升了中文基础语义理解能力。同时，在中文 LLaMA 的基础上，本项目使用了中文指令数据进行指令精调，显著提升了模型对指令的理解和执行能力。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/project-baize/baize-chatbot" target="_blank" rel="noopener">2023-Baize
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/project-baize/baize-chatbot" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Baize is an open-source chat model trained with LoRA. It uses 100k dialogs generated by letting ChatGPT chat with itself. We also use Alpaca&amp;rsquo;s data to improve its performance. We have released 7B, 13B and 30B models. Please refer to the paper for more details.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="alpaca">Alpaca&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/deep-diver/Alpaca-LoRA-Serve" target="_blank" rel="noopener">2023-deep-diver/Alpaca-LoRA-Serve
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/deep-diver/Alpaca-LoRA-Serve" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: This repository demonstrates Alpaca-LoRA as a Chatbot service with Alpaca-LoRA and Gradio. It comes with the following features:&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/LianjiaTech/BELLE" target="_blank" rel="noopener">2023-LianjiaTech/BELLE
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/LianjiaTech/BELLE" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: 本项目基于 Stanford Alpaca ，Stanford Alpaca 的目标是构建和开源一个基于 LLaMA 的模型。Stanford Alpaca 的种子任务都是英语，收集的数据也都是英文，因此训练出来的模型未对中文优化。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/LC1332/Chinese-alpaca-lora" target="_blank" rel="noopener">2023-Chinese-alpaca-lora
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/LC1332/Chinese-alpaca-lora" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: CamelBell(驼铃), tuning Chinese Data on Chinese based model GLM is now an individual repo. We may move original Luotuo into a new repo also.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/databrickslabs/dolly" target="_blank" rel="noopener">2023-Dolly
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/databrickslabs/dolly" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: This fine-tunes the GPT-J 6B model on the Alpaca dataset using a Databricks notebook. Please note that while GPT-J 6B is Apache 2.0 licensed, the Alpaca dataset is licensed under Creative Commons NonCommercial (CC BY-NC 4.0).&lt;/p>
&lt;/li>
&lt;/ul></description></item></channel></rss>