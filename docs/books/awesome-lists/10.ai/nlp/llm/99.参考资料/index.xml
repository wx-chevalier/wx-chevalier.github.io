<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>99.参考资料 | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</link>
      <atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/index.xml" rel="self" type="application/rss+xml" />
    <description>99.参考资料</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>99.参考资料</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</link>
    </image>
    
    <item>
      <title>2023-AI Canon</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-ai-canon/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-ai-canon/</guid>
      <description>&lt;h1 id=&#34;ai-canon&#34;&gt;AI Canon&lt;/h1&gt;
&lt;p&gt;Research in artificial intelligence is increasing at an exponential rate. It’s difficult for AI experts to keep up with everything new being published, and even harder for beginners to know where to start.&lt;/p&gt;
&lt;p&gt;So, in this post, we’re sharing a curated list of resources we’ve relied on to get smarter about modern AI. We call it the “AI Canon” because these papers, blog posts, courses, and guides have had an outsized impact on the field over the past several years.&lt;/p&gt;
&lt;p&gt;We start with a gentle introduction to &lt;em&gt;transformer&lt;/em&gt; and &lt;em&gt;latent diffusion&lt;/em&gt; models, which are fueling the current AI wave. Next, we go deep on technical learning resources; practical guides to building with large language models (LLMs); and analysis of the AI market. Finally, we include a reference list of landmark research results, starting with “Attention is All You Need” — the 2017 paper by Google that introduced the world to transformer models and ushered in the age of generative AI.&lt;/p&gt;
&lt;h1 id=&#34;a-gentle-introduction&#34;&gt;A gentle introduction…&lt;/h1&gt;
&lt;p&gt;These articles require no specialized background and can help you get up to speed quickly on the most important parts of the modern AI wave.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://karpathy.medium.com/software-2-0-a64152b37c35&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Software 2.0&lt;/a&gt;&lt;/strong&gt;: Andrej Karpathy was one of the first to clearly explain (in 2017!) why the new AI wave really matters. His argument is that AI is a new and powerful way to program computers. As LLMs have improved rapidly, this thesis has proven prescient, and it gives a good mental model for how the AI market may progress.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;State of GPT&lt;/a&gt;&lt;/strong&gt;: Also from Karpathy, this is a very approachable explanation of how ChatGPT / GPT models in general work, how to use them, and what directions R&amp;amp;D may take.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;What is ChatGPT doing … and why does it work?&lt;/strong&gt;&lt;/a&gt;: Computer scientist and entrepreneur Stephen Wolfram gives a long but highly readable explanation, from first principles, of how modern AI models work. He follows the timeline from early neural nets to today’s LLMs and ChatGPT.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://daleonai.com/transformers-explained&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transformers, explained&lt;/a&gt;&lt;/strong&gt;: This post by Dale Markowitz is a shorter, more direct answer to the question “what is an LLM, and how does it work?” This is a great way to ease into the topic and develop intuition for the technology. It was written about GPT-3 but still applies to newer models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://mccormickml.com/2022/12/21/how-stable-diffusion-works/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Stable Diffusion works&lt;/a&gt;&lt;/strong&gt;: This is the computer vision analogue to the last post. Chris McCormick gives a layperson’s explanation of how Stable Diffusion works and develops intuition around text-to-image models generally. For an even &lt;em&gt;gentler&lt;/em&gt; introduction, check out this &lt;a href=&#34;https://www.reddit.com/r/StableDiffusion/comments/zs5dk5/i_made_an_infographic_to_explain_how_stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;comic&lt;/a&gt; from r/StableDiffusion.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;foundational-learning-neural-networks-backpropagation-and-embeddings&#34;&gt;Foundational learning: neural networks, backpropagation, and embeddings&lt;/h1&gt;
&lt;p&gt;These resources provide a base understanding of fundamental ideas in machine learning and AI, from the basics of deep learning to university-level courses from AI experts.&lt;/p&gt;
&lt;h3 id=&#34;explainers&#34;&gt;Explainers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://developer.nvidia.com/blog/deep-learning-nutshell-core-concepts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep learning in a nutshell: core concepts&lt;/strong&gt;&lt;/a&gt;: This four-part series from Nvidia walks through the basics of deep learning as practiced in 2015, and is a good resource for anyone just learning about AI.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Practical deep learning for coders&lt;/a&gt;&lt;/strong&gt;: Comprehensive, free course on the fundamentals of AI, explained through practical examples and code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://towardsdatascience.com/word2vec-explained-49c52b4ccb71&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Word2vec explained&lt;/a&gt;&lt;/strong&gt;: Easy introduction to embeddings and tokens, which are building blocks of LLMs (and all language models).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yes you should understand backprop&lt;/a&gt;&lt;/strong&gt;: More in-depth post on back-propagation if you want to understand the details. If you want even more, try the &lt;a href=&#34;https://www.youtube.com/watch?v=i94OvYb6noo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford CS231n lecture&lt;/a&gt; on Youtube.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;courses&#34;&gt;Courses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford CS229&lt;/a&gt;&lt;/strong&gt;: Introduction to Machine Learning with Andrew Ng, covering the fundamentals of machine learning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford CS224N&lt;/a&gt;&lt;/strong&gt;: NLP with Deep Learning with Chris Manning, covering NLP basics through the first generation of LLMs.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;tech-deep-dive-understanding-transformers-and-large-models&#34;&gt;Tech deep dive: understanding transformers and large models&lt;/h1&gt;
&lt;p&gt;There are countless resources — some better than others — attempting to explain how LLMs work. Here are some of our favorites, targeting a wide range of readers/viewers.&lt;/p&gt;
&lt;h3 id=&#34;explainers-1&#34;&gt;Explainers&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The illustrated transformer&lt;/strong&gt;&lt;/a&gt;: A more technical overview of the transformer architecture by Jay Alammar.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;http://nlp.seas.harvard.edu/annotated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The annotated transformer&lt;/a&gt;&lt;/strong&gt;: In-depth post if you want to understand transformers at a source code level. Requires some knowledge of PyTorch.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=kCc8FmEb1nY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Let’s build GPT: from scratch, in code, spelled out&lt;/strong&gt;&lt;/a&gt;: For the engineers out there, Karpathy does a video walkthrough of how to build a GPT model.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://jalammar.github.io/illustrated-stable-diffusion/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The illustrated Stable Diffusion&lt;/a&gt;**&lt;/strong&gt;:** Introduction to latent diffusion models, the most common type of generative AI model for images.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huyenchip.com/2023/05/02/rlhf.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RLHF: Reinforcement Learning from Human Feedback&lt;/a&gt;&lt;/strong&gt;: Chip Huyen explains RLHF, which can make LLMs behave in more predictable and human-friendly ways. This is one of the most important but least well-understood aspects of systems like ChatGPT.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hhiLw5Q_UFg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reinforcement learning from human feedback&lt;/strong&gt;&lt;/a&gt;: Computer scientist and OpenAI cofounder John Shulman goes deeper in this great talk on the current state, progress and limitations of LLMs with RLHF.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;courses-1&#34;&gt;Courses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=P127jhj-8-Y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Stanford CS25&lt;/strong&gt;&lt;/a&gt;: Transformers United, an online seminar on Transformers.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://stanford-cs324.github.io/winter2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stanford CS324&lt;/a&gt;&lt;/strong&gt;: Large Language Models with Percy Liang, Tatsu Hashimoto, and Chris Re, covering a wide range of technical and non-technical aspects of LLMs.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reference-and-commentary&#34;&gt;Reference and commentary&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=Ount2Y4qxQo&amp;amp;t=1072s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Predictive learning, NIPS 2016&lt;/a&gt;&lt;/strong&gt;: In this early talk, Yann LeCun makes a strong case for unsupervised learning as a critical element of AI model architectures at scale. Skip to &lt;a href=&#34;https://youtu.be/Ount2Y4qxQo?t=1160&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;19:20&lt;/a&gt; for the famous cake analogy, which is still one of the best mental models for modern AI.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=hx7BXih7zx8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AI for full-self driving at Tesla&lt;/strong&gt;&lt;/a&gt;: Another classic Karpathy talk, this time covering the Tesla data collection engine. Starting at &lt;a href=&#34;https://youtu.be/hx7BXih7zx8?t=515&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;8:35&lt;/a&gt; is one of the great all-time AI rants, explaining why long-tailed problems (in this case stop sign detection) are so hard.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://gwern.net/scaling-hypothesis&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The scaling hypothesis&lt;/a&gt;&lt;/strong&gt;: One of the most surprising aspects of LLMs is that scaling — adding more data and compute — just keeps increasing accuracy. GPT-3 was the first model to demonstrate this clearly, and Gwern’s post does a great job explaining the intuition behind it.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Chinchilla’s wild implications&lt;/strong&gt;&lt;/a&gt;: Nominally an explainer of the important Chinchilla paper (see below), this post gets to the heart of the big question in LLM scaling: are we running out of data? This builds on the post above and gives a refreshed view on scaling laws.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/2303.18223v4.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A survey of large language models&lt;/a&gt;&lt;/strong&gt;: Comprehensive breakdown of current LLMs, including development timeline, size, training strategies, training data, hardware, and more.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.12712&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Sparks of artificial general intelligence: Early experiments with GPT-4&lt;/strong&gt;&lt;/a&gt;: Early analysis from Microsoft Research on the capabilities of GPT-4, the current most advanced LLM, relative to human intelligence.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pub.towardsai.net/the-ai-revolution-how-auto-gpt-unleashes-a-new-era-of-automation-and-creativity-2008aa2ca6ae&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The AI revolution: How Auto-GPT unleashes a new era of automation and creativity&lt;/strong&gt;&lt;/a&gt;: An introduction to Auto-GPT and AI agents in general. This technology is very early but important to understand — it uses internet access and self-generated sub-tasks in order to solve specific, complex problems or goals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Waluigi Effect&lt;/a&gt;&lt;/strong&gt;: Nominally an explanation of the “Waluigi effect” (i.e., why “alter egos” emerge in LLM behavior), but interesting mostly for its deep dive on the theory of LLM prompting.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;practical-guides-to-building-with-llms&#34;&gt;Practical guides to building with LLMs&lt;/h1&gt;
&lt;p&gt;A new application stack is emerging with LLMs at the core. While there isn’t a lot of formal education available on this topic yet, we pulled out some of the most useful resources we’ve found.&lt;/p&gt;
&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dagster.io/blog/chatgpt-langchain&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Build a GitHub support bot with GPT3, LangChain, and Python&lt;/strong&gt;&lt;/a&gt;: One of the earliest public explanations of the modern LLM app stack. Some of the advice in here is dated, but in many ways it kicked off widespread adoption and experimentation of new AI apps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huyenchip.com/2023/04/11/llm-engineering.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Building LLM applications for production&lt;/a&gt;&lt;/strong&gt;: Chip Huyen discusses many of the key challenges in building LLM apps, how to address them, and what types of use cases make the most sense.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.promptingguide.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prompt Engineering Guide&lt;/a&gt;&lt;/strong&gt;: For anyone writing LLM prompts — including app devs — this is the most comprehensive guide, with specific examples for a handful of popular models. For a lighter, more conversational treatment, try &lt;a href=&#34;https://github.com/brexhq/prompt-engineering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brex’s prompt engineering guide&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://simonwillison.net/2023/Apr/14/worst-that-can-happen/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Prompt injection: What’s the worst that can happen?&lt;/strong&gt;&lt;/a&gt; Prompt injection is a potentially serious security vulnerability lurking for LLM apps, with no perfect solution yet. Simon Willison gives the definitive description of the problem in this post. Nearly everything Simon writes on AI is outstanding.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/openai/openai-cookbook/tree/main&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;OpenAI cookbook&lt;/strong&gt;&lt;/a&gt;: For developers, this is the definitive collection of guides and code examples for working with the OpenAI API. It’s updated continually with new code examples.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Pinecone learning center&lt;/strong&gt;&lt;/a&gt;: Many LLM apps are based around a vector search paradigm. Pinecone’s learning center — despite being branded vendor content — offers some of the most useful instruction on how to build in this pattern.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://python.langchain.com/en/latest/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;LangChain docs&lt;/strong&gt;&lt;/a&gt;: As the default orchestration layer for LLM apps, LangChain connects to just about all other pieces of the stack. So their docs are a real reference for the full stack and how the pieces fit together.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;courses-2&#34;&gt;Courses&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://fullstackdeeplearning.com/llm-bootcamp/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLM Bootcamp&lt;/a&gt;&lt;/strong&gt;: A practical course for building LLM-based applications with Charles Frye, Sergey Karayev, and Josh Tobin.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/learn/nlp-course/chapter1/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face Transformers&lt;/a&gt;&lt;/strong&gt;: Guide to using open-source LLMs in the Hugging Face transformers library.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;llm-benchmarks&#34;&gt;LLM benchmarks&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://lmsys.org/blog/2023-05-03-arena/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chatbot Arena&lt;/a&gt;&lt;/strong&gt;: An Elo-style ranking system of popular LLMs, led by a team at UC Berkeley. Users can also participate by comparing models head to head.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open LLM Leaderboard&lt;/a&gt;&lt;/strong&gt;: A ranking by Hugging Face, comparing open source LLMs across a collection of standard benchmarks and tasks.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;market-analysis&#34;&gt;Market analysis&lt;/h1&gt;
&lt;p&gt;We’ve all marveled at what generative AI can produce, but there are still a lot of questions about &lt;em&gt;what it all means&lt;/em&gt;. Which products and companies will survive and thrive? What happens to artists? How should companies use it? How will it affect literally jobs and society at large? Here are some attempts at answering these questions.&lt;/p&gt;
&lt;h3 id=&#34;a16z-thinking&#34;&gt;a16z thinking&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://a16z.com/2023/01/19/who-owns-the-generative-ai-platform/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Who owns the generative AI platform?&lt;/strong&gt;&lt;/a&gt;: Our flagship assessment of where value is accruing, and might accrue, at the infrastructure, model, and application layers of generative AI.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://a16z.com/2023/04/27/navigating-the-high-cost-of-ai-compute/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Navigating the high cost of AI compute&lt;/strong&gt;&lt;/a&gt;: A detailed breakdown of why generative AI models require so many computing resources, and how to think about acquiring those resources (i.e., the right GPUs in the right quantity, at the right cost) in a high-demand market.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://a16z.com/2022/11/16/creativity-as-an-app/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Art isn’t dead, it’s just machine-generated&lt;/a&gt;&lt;/strong&gt;: A look at how AI models were able to reshape creative fields — often assumed to be the last holdout against automation — much faster than fields such as software development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://a16z.com/2022/11/17/the-generative-ai-revolution-in-games/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The generative AI revolution in games&lt;/a&gt;&lt;/strong&gt;: An in-depth analysis from our Games team at how the ability to easily create highly detailed graphics will change how game designers, studios, and the entire market function. &lt;a href=&#34;https://a16z.com/2023/03/17/the-generative-ai-revolution/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This follow-up piece&lt;/a&gt; from our Games team looks specifically at the advent of AI-generated content vis à vis user-generated content.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://a16z.com/2023/03/30/b2b-generative-ai-synthai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;For B2B generative AI apps, is less more?&lt;/strong&gt;&lt;/a&gt;: A prediction for how LLMs will evolve in the world of B2B enterprise applications, centered around the idea that summarizing information will ultimately be more valuable than producing text.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://a16z.com/2023/04/19/financial-services-will-embrace-generative-ai-faster-than-you-think/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Financial services will embrace generative AI faster than you think&lt;/a&gt;&lt;/strong&gt;: An argument that the financial services industry is poised to use generative AI for personalized consumer experiences, cost-efficient operations, better compliance, improved risk management, and dynamic forecasting and reporting.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://a16z.com/2023/02/07/everyday-ai-consumer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Generative AI: The next consumer platform&lt;/strong&gt;&lt;/a&gt;: A look at opportunities for generative AI to impact the consumer market across a range of sectors from therapy to ecommerce.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://time.com/6274752/ai-health-care/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;To make a real difference in health care, AI will need to learn like we do&lt;/strong&gt;&lt;/a&gt;: AI is poised to irrevocably change how we look to prevent and treat illness. However, to truly transform drug discovery to care delivery, we should invest in creating an ecosystem of “specialist” AIs — that learn like our best physicians and drug developers do today.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://a16z.com/2023/05/17/the-new-industrial-revolution-bio-x-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The new industrial revolution: Bio x AI&lt;/strong&gt;&lt;/a&gt;: The next industrial revolution in human history will be biology powered by artificial intelligence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-perspectives&#34;&gt;Other perspectives&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.07258&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On the opportunities and risks of foundation models&lt;/a&gt;&lt;/strong&gt;: Stanford overview paper on Foundation Models. Long and opinionated, but this shaped the term.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stateof.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;State of AI Report&lt;/strong&gt;&lt;/a&gt;: An annual roundup of everything going on in AI, including technology breakthroughs, industry development, politics/regulation, economic implications, safety, and predictions for the future.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.10130&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;GPTs are GPTs: An early look at the labor market impact potential of large language models&lt;/strong&gt;&lt;/a&gt;: This paper from researchers at OpenAI, OpenResearch, and the University of of Pennsylvania predicts that “around 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while approximately 19% of workers may see at least 50% of their tasks impacted.”&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Deep-Medicine-Eric-Topol-audiobook/dp/B07PJ21V5N/ref=sr_1_1?hvadid=580688888836&amp;amp;hvdev=c&amp;amp;hvlocphy=9031955&amp;amp;hvnetw=g&amp;amp;hvqmt=e&amp;amp;hvrand=13698160037271563598&amp;amp;hvtargid=kwd-646099228782&amp;amp;hydadcr=15524_13517408&amp;amp;keywords=eric&amp;#43;topol&amp;#43;deep&amp;#43;medicine&amp;amp;qid=1684965845&amp;amp;sr=8-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Deep medicine: How artificial intelligence can make healthcare human again&lt;/strong&gt;&lt;/a&gt;: Dr. Eric Topol reveals how artificial intelligence has the potential to free physicians from the time-consuming tasks that interfere with human connection. The doctor-patient relationship is restored. (&lt;a href=&#34;https://a16z.com/2019/06/13/ai-doctor-deep-medicine-topol/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a16z podcast&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;landmark-research-results&#34;&gt;Landmark research results&lt;/h1&gt;
&lt;p&gt;Most of the amazing AI products we see today are the result of no-less-amazing research, carried out by experts inside large companies and leading universities. Lately, we’ve also seen impressive work from individuals and the open source community taking popular projects into new directions, for example by creating automated agents or porting models onto smaller hardware footprints.&lt;/p&gt;
&lt;p&gt;Here’s a collection of many of these papers and projects, for folks who really want to dive deep into generative AI. (For research papers and projects, we’ve also included links to the accompanying blog posts or websites, where available, which tend to explain things at a higher level. And we’ve included original publication years so you can track foundational research over time.)&lt;/p&gt;
&lt;h3 id=&#34;large-language-models&#34;&gt;&lt;strong&gt;Large language models&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;New models&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention is all you need&lt;/a&gt;&lt;/strong&gt; (2017): The original transformer work and research paper from Google Brain that started it all. (&lt;a href=&#34;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT: pre-training of deep bidirectional transformers for language understanding&lt;/a&gt;&lt;/strong&gt; (2018): One of the first publicly available LLMs, with many variants still in use today. (&lt;a href=&#34;https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving language understanding by generative pre-training&lt;/a&gt;&lt;/strong&gt; (2018): The first paper from OpenAI covering the GPT architecture, which has become the dominant development path in LLMs. (&lt;a href=&#34;https://openai.com/research/language-unsupervised&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Language models are few-shot learners&lt;/strong&gt;&lt;/a&gt; (2020): The OpenAI paper that describes GPT-3 and the decoder-only architecture of modern LLMs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.02155&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/strong&gt; (2022): OpenAI’s paper explaining InstructGPT, which utilizes humans in the loop to train models and, thus, better follow the instructions in prompts. This was one of the key unlocks that made LLMs accessible to consumers (e.g., via ChatGPT). (&lt;a href=&#34;https://openai.com/research/instruction-following&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.08239&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;LaMDA: language models for dialog applications&lt;/strong&gt;&lt;/a&gt; (2022): A model form Google specifically designed for free-flowing dialog between a human and chatbot across a wide variety of topics. (&lt;a href=&#34;https://blog.google/technology/ai/lamda/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.02311&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PaLM: Scaling language modeling with pathways&lt;/a&gt;&lt;/strong&gt; (2022): PaLM, from Google, utilized a new system for training LLMs across thousands of chips and demonstrated larger-than-expected improvements for certain tasks as model size scaled up. (&lt;a href=&#34;https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;). See also the &lt;a href=&#34;https://arxiv.org/abs/2305.10403&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PaLM-2 technical report&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.01068&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OPT: Open Pre-trained Transformer language models&lt;/a&gt;&lt;/strong&gt; (2022): OPT is one of the top performing fully open source LLMs. The release for this 175-billion-parameter model comes with code and was trained on publicly available datasets. (&lt;a href=&#34;https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.15556&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Training compute-optimal large language models&lt;/a&gt;&lt;/strong&gt; (2022): The Chinchilla paper. It makes the case that most models are data limited, not compute limited, and changed the consensus on LLM scaling. (&lt;a href=&#34;https://www.deepmind.com/blog/an-empirical-analysis-of-compute-optimal-large-language-model-training&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.08774&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;GPT-4 technical report&lt;/strong&gt;&lt;/a&gt; (2023): The latest and greatest paper from OpenAI, known mostly for how little it reveals! (&lt;a href=&#34;https://openai.com/research/gpt-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;). The &lt;a href=&#34;https://cdn.openai.com/papers/gpt-4-system-card.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-4 system card&lt;/a&gt; sheds some light on how OpenAI treats hallucinations, privacy, security, and other issues.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.13971&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;LLaMA: Open and efficient foundation language models&lt;/strong&gt;&lt;/a&gt; (2023): The model from Meta that (almost) started an open-source LLM revolution. Competitive with many of the best closed-source models but only opened up to researchers on a restricted license. (&lt;a href=&#34;https://ai.facebook.com/blog/large-language-model-llama-meta-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://crfm.stanford.edu/2023/03/13/alpaca.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Alpaca: A strong, replicable instruction-following model&lt;/strong&gt;&lt;/a&gt; (2023): Out of Stanford, this model demonstrates the power of instruction tuning, especially in smaller open-source models, compared to pure scale.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Model improvements (e.g. fine-tuning, retrieval, attention)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://proceedings.neurips.cc/paper_files/paper/2017/file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep reinforcement learning from human preferences&lt;/a&gt;&lt;/strong&gt; (2017): Research on reinforcement learning in gaming and robotics contexts, that turned out to be a fantastic tool for LLMs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.11401&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Retrieval-augmented generation for knowledge-intensive NLP tasks&lt;/a&gt;&lt;/strong&gt; (2020): Developed by Facebook, RAG is one of the two main research paths for improving LLM accuracy via information retrieval. (&lt;a href=&#34;https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.04426&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving language models by retrieving from trillions of tokens&lt;/a&gt;&lt;/strong&gt; (2021): RETRO, for “Retrieval Enhanced TRansfOrmers,” is another approach — this one by DeepMind — to improve LLM accuracy by accessing information not included in their training data. (&lt;a href=&#34;https://www.deepmind.com/blog/improving-language-models-by-retrieving-from-trillions-of-tokens&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.09685&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;LoRA: Low-rank adaptation of large language models&lt;/strong&gt;&lt;/a&gt; (2021): This research out of Microsoft introduced a more efficient alternative to fine-tuning for training LLMs on new data. It’s now become a standard for community fine-tuning, especially for image models.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.08073&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Constitutional AI (2022)&lt;/a&gt;&lt;/strong&gt;: The Anthropic team introduces the concept of reinforcement learning from AI Feedback (RLAIF). The main idea is that we can develop a harmless AI assistant with the supervision of other AIs.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;FlashAttention: Fast and memory-efficient exact attention with IO-awareness&lt;/strong&gt;&lt;/a&gt; (2022): This research out of Stanford opened the door for state-of-the-art models to understand longer sequences of text (and higher-resolution images) without exorbitant training times and costs. (&lt;a href=&#34;https://ai.stanford.edu/blog/longer-sequences-next-leap-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.14052&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hungry hungry hippos: Towards language modeling with state space models&lt;/a&gt;&lt;/strong&gt; (2022): Again from Stanford, this paper describes one of the leading alternatives to attention in language modeling. This is a promising path to better scaling and training efficiency. (&lt;a href=&#34;https://hazyresearch.stanford.edu/blog/2023-01-20-h3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;image-generation-models&#34;&gt;&lt;strong&gt;Image generation models&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2103.00020&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Learning transferable visual models from natural language supervision&lt;/strong&gt;&lt;/a&gt; (2021): Paper that introduces a base model — CLIP — that links textual descriptions to images. One of the first effective, large-scale uses of foundation models in computer vision. (&lt;a href=&#34;https://openai.com/research/clip&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2102.12092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Zero-shot text-to-image generation&lt;/strong&gt;&lt;/a&gt; (2021): This is the paper that introduced DALL-E, a model that combines the aforementioned CLIP and GPT-3 to automatically generate images based on text prompts. Its successor, DALL-E 2, would kick off the image-based generative AI boom in 2022. (&lt;a href=&#34;https://openai.com/research/dall-e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.10752&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;High-resolution image synthesis with latent diffusion models&lt;/a&gt;&lt;/strong&gt; (2021): The paper that described Stable Diffusion (after the launch and explosive open source growth).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.11487&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Photorealistic text-to-image diffusion models with deep language understanding&lt;/a&gt;&lt;/strong&gt; (2022): Imagen was Google’s foray into AI image generation. More than a year after its announcement, the model has yet to be released publicly as of the publish date of this piece. (&lt;a href=&#34;https://imagen.research.google/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2208.12242&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DreamBooth: Fine tuning text-to-image diffusion models for subject-driven generation&lt;/a&gt;&lt;/strong&gt; (2022): DreamBooth is a system, developed at Google, for training models to recognize user-submitted subjects and apply them to the context of a prompt (e.g. [USER] smiling at the Eiffel Tower). (&lt;a href=&#34;https://dreambooth.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.05543&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Adding conditional control to text-to-image diffusion models&lt;/strong&gt;&lt;/a&gt; (2023): This paper from Stanford introduces ControlNet, a now very popular tool for exercising fine-grained control over image generation with latent diffusion models.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agents&#34;&gt;&lt;strong&gt;Agents&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/pdf?id=BZ5a1r-kVsf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;A path towards autonomous machine intelligence&lt;/strong&gt;&lt;/a&gt; (2022): A proposal from Meta AI lead and NYU professor Yann LeCun on how to build autonomous and intelligent agents that truly understand the world around them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.03629&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ReAct: Synergizing reasoning and acting in language models&lt;/a&gt;&lt;/strong&gt; (2022): A project out of Princeton and Google to test and improve the reasoning and planning abilities of LLMs. (&lt;a href=&#34;https://ai.googleblog.com/2022/11/react-synergizing-reasoning-and-acting.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2304.03442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Generative agents: Interactive simulacra of human behavior&lt;/strong&gt;&lt;/a&gt; (2023): Researchers at Stanford and Google used LLMs to power agents, in a setting akin to “The Sims,” whose interactions are emergent rather than programmed.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2303.11366&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Reflexion: an autonomous agent with dynamic memory and self-reflection&lt;/strong&gt;&lt;/a&gt; (2023): Work from researchers at Northeastern University and MIT on teaching LLMs to solve problems more reliably by learning from their mistakes and past experiences.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2302.04761&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Toolformer: Language models can teach themselves to use tools&lt;/a&gt;&lt;/strong&gt; (2023): This project from Meta trained LLMs to use external tools (APIs, in this case, pointing to things like search engines and calculators) in order to improve accuracy without increasing model size.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/Significant-Gravitas/Auto-GPT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Auto-GPT: An autonomous GPT-4 experiment&lt;/a&gt;&lt;/strong&gt;: An open source experiment to expand on the capabilities of GPT-4 by giving it a collection of tools (internet access, file storage, etc.) and choosing which ones to use in order to solve a specific task.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://github.com/yoheinakajima/babyagi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BabyAGI&lt;/a&gt;&lt;/strong&gt;: This Python script utilizes GPT-4 and vector databases (to store context) in order to plan and executes a series of tasks that solve a broader objective.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;other-data-modalities&#34;&gt;&lt;strong&gt;Other data modalities&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Code generation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.03374&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evaluating large language models trained on code&lt;/a&gt;&lt;/strong&gt; (2021): This is OpenAI’s research paper for Codex, the code-generation model behind the GitHub Copilot product. (&lt;a href=&#34;https://openai.com/blog/openai-codex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://www.science.org/stoken/author-tokens/ST-905/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Competition-level code generation with AlphaCode&lt;/a&gt;&lt;/strong&gt; (2021): This research from DeepMind demonstrates a model capable of writing better code than human programmers. (&lt;a href=&#34;https://www.deepmind.com/blog/competitive-programming-with-alphacode&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2203.13474&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CodeGen: An open large language model for code with multi-turn program synthesis&lt;/a&gt;&lt;/strong&gt; (2022): CodeGen comes out of the AI research arm at Salesforce, and currently underpins the Replit Ghostwriter product for code generation. (&lt;a href=&#34;https://blog.salesforceairesearch.com/codegen/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Video generation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.14792&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Make-A-Video: Text-to-video generation without text-video data&lt;/a&gt;&lt;/strong&gt; (2022): A model from Meta that creates short videos from text prompts, but also adds motion to static photo inputs or creates variations of existing videos. (&lt;a href=&#34;https://makeavideo.studio/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.02303&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Imagen Video: High definition video generation with diffusion models&lt;/strong&gt;&lt;/a&gt; (2022): Just what it sounds like: a version of Google’s image-based Imagen model optimized for producing short videos from text prompts. (&lt;a href=&#34;https://imagen.research.google/video/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Human biology and medical data&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1905.12265.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Strategies for pre-training graph neural networks&lt;/strong&gt;&lt;/a&gt; (2020): This publication laid the groundwork for effective pre-training methods useful for applications across drug discovery, such as molecular property prediction and protein function prediction. (&lt;a href=&#34;https://snap.stanford.edu/gnn-pretrain/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nature.com/articles/s41586-019-1923-7&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Improved protein structure prediction using potentials from deep learning&lt;/strong&gt;&lt;/a&gt; (2020): DeepMind’s protein-centric transformer model, AlphaFold, made it possible to predict protein structure from sequence — a true breakthrough which has already had far-reaching implications for understanding biological processes and developing new treatments for diseases. (&lt;a href=&#34;https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;) (&lt;a href=&#34;https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;explainer&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.13138&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Large language models encode clinical knowledge&lt;/strong&gt;&lt;/a&gt; (2022): Med-PaLM is a LLM capable of correctly answering US Medical License Exam style questions. The team has since published results on the performance of Med-PaLM2, which achieved a score on par with “expert” test takers. Other teams have performed similar experiments with &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2022.12.19.22283643v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2303.13375&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-4&lt;/a&gt;. (&lt;a href=&#34;https://www.youtube.com/watch?v=saWEFDRuNJc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;video&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Audio generation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2005.00341&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Jukebox: A generative model for music&lt;/strong&gt;&lt;/a&gt; (2020): OpenAI’s foray into music generation using transformers, capable of producing music, vocals, and lyrics with minimal training. (&lt;a href=&#34;https://openai.com/research/jukebox&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.03143.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;AudioLM: a language modeling approach to audio generation&lt;/strong&gt;&lt;/a&gt; (2022): AudioLM is a Google project for generating multiple types of audio, including speech and instrumentation. (&lt;a href=&#34;https://ai.googleblog.com/2022/10/audiolm-language-modeling-approach-to.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.11325&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;MusicLM: Generating nusic from text&lt;/strong&gt;&lt;/a&gt; (2023): Current state of the art in AI-based music generation, showing higher quality and coherence than previous attempts. (&lt;a href=&#34;https://google-research.github.io/seanet/musiclm/examples/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;blog post&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Multi-dimensional image generation&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2003.08934&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;NeRF: Representing scenes as neural radiance fields for view synthesis&lt;/strong&gt;&lt;/a&gt; (2020): Research from a UC-Berkeley-led team on “synthesizing novel views of complex scenes” using 5D coordinates. (&lt;a href=&#34;https://www.matthewtancik.com/nerf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;a href=&#34;https://arxiv.org/pdf/2209.14988.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DreamFusion: Text-to-3D using 2D diffusion&lt;/a&gt;&lt;/strong&gt; (2022): Work from researchers at Google and UC-Berkeley that builds on NeRF to generate 3D images from 2D inputs. (&lt;a href=&#34;https://dreamfusion3d.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Special thanks to&lt;/em&gt; &lt;a href=&#34;https://a16z.com/author/jack-soslow/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Jack Soslow&lt;/em&gt;&lt;/a&gt;&lt;em&gt;,&lt;/em&gt; &lt;a href=&#34;https://a16z.com/author/jay-rughani/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Jay Rughani&lt;/em&gt;&lt;/a&gt;&lt;em&gt;,&lt;/em&gt; &lt;a href=&#34;https://a16z.com/author/marco-mascorro/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Marco Mascorro&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, &lt;a href=&#34;https://a16z.com/author/martin-casado/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Martin Casado&lt;/a&gt;,&lt;/em&gt; &lt;a href=&#34;https://a16z.com/author/rajko-radovanovic/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Rajko Radovanovic&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, and&lt;/em&gt; &lt;a href=&#34;https://a16z.com/author/vijay-pande/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Vijay Pande&lt;/em&gt;&lt;/a&gt; &lt;em&gt;for their contributions to this piece, and to the entire a16z team for an always informative discussion about the latest in AI. And thanks to&lt;/em&gt; &lt;a href=&#34;https://a16z.com/author/sonal-chokshi/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Sonal Chokshi&lt;/em&gt;&lt;/a&gt; &lt;em&gt;and the crypto team for building a long series of canons at the firm.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;* * *&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The views expressed here are those of the individual AH Capital Management, L.L.C. (“a16z”) personnel quoted and are not the views of a16z or its affiliates. Certain information contained in here has been obtained from third-party sources, including from portfolio companies of funds managed by a16z. While taken from sources believed to be reliable, a16z has not independently verified such information and makes no representations about the enduring accuracy of the information or its appropriateness for a given situation. In addition, this content may include third-party advertisements; a16z has not reviewed such advertisements and does not endorse any advertising content contained therein.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This content is provided for informational purposes only, and should not be relied upon as legal, business, investment, or tax advice. You should consult your own advisers as to those matters. References to any securities or digital assets are for illustrative purposes only, and do not constitute an investment recommendation or offer to provide investment advisory services. Furthermore, this content is not directed at nor intended for use by any investors or prospective investors, and may not under any circumstances be relied upon when making a decision to invest in any fund managed by a16z. (An offering to invest in an a16z fund will be made only by the private placement memorandum, subscription agreement, and other relevant documentation of any such fund and should be read in their entirety.) Any investments or portfolio companies mentioned, referred to, or described are not representative of all investments in vehicles managed by a16z, and there can be no assurance that the investments will be profitable or that other investments made in the future will have similar characteristics or results. A list of investments made by funds managed by Andreessen Horowitz (excluding investments for which the issuer has not provided permission for a16z to disclose publicly as well as unannounced investments in publicly traded digital assets) is available at &lt;a href=&#34;https://a16z.com/investments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://a16z.com/investments/&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Charts and graphs provided within are for informational purposes solely and should not be relied upon when making any investment decision. Past performance is not indicative of future results. The content speaks only as of the date indicated. Any projections, estimates, forecasts, targets, prospects, and/or opinions expressed in these materials are subject to change without notice and may differ or be contrary to opinions expressed by others. Please see &lt;a href=&#34;https://a16z.com/disclosures&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://a16z.com/disclosures&lt;/a&gt; for additional important information.&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
