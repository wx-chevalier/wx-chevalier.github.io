<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/</link><atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/index.xml" rel="self" type="application/rss+xml"/><description>LLM</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>LLM</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/</link></image><item><title>GPT-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/gpt-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/gpt-list/</guid><description>&lt;h1 id="gpt-list">GPT List&lt;/h1>
&lt;h1 id="resource">Resource&lt;/h1>
&lt;h2 id="course">Course&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/karpathy/ng-video-lecture" target="_blank" rel="noopener">2023-nanogpt-lecture 🎥&lt;/a>: Code created in the Neural Networks: Zero To Hero video lecture series, specifically on the first lecture on nanoGPT. Publishing here as a Github repo so people can easily hack it, walk through the git log history of it, etc.&lt;/li>
&lt;/ul>
&lt;h1 id="chatgpt">ChatGPT&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://drive.google.com/file/d/1UOfN0iB_A0rEGYc2CbYnpIF44FupQn2I/view" target="_blank" rel="noopener">2022-The ChatGPT Cheat Sheet&lt;/a>: This cheat sheet illustrates the diverse abilities of OpenAI’s ChatGPT for developers and content creators to enhance their proficiency in large language model prompting across various domains including media content creation, natural language processing, and programming.&lt;/li>
&lt;/ul>
&lt;h1 id="opensource">OpenSource&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://github.com/jaymody/picoGPT" target="_blank" rel="noopener">2023-picoGPT
&lt;img src="https://martrix-usa.oss-accelerate.aliyuncs.com/logo/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/jaymody/picoGPT" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: An unnecessarily tiny and minimal implementation of GPT-2 in NumPy.&lt;/li>
&lt;/ul></description></item><item><title>LLM-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llm-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/llm-list/</guid><description>&lt;h1 id="llm-list">LLM List&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://github.com/howl-anderson/unlocking-the-power-of-llms" target="_blank" rel="noopener">2023-使用 Prompts 和 Chains 让 ChatGPT 成为神奇的生产力工具！&lt;/a>: ChatGPT 诞生后，因其非常强大的又难以置信的的能力，得到了非常广泛的关注。用户将 ChatGPT 视作一种有趣且知识渊博的聊天工具。但事实上，使用合适的 Prompts 和 Chains，可以将 ChatGPT 作为一个神奇的生产力工具，能够处理各种各样复杂的任务。本仓库将详细介绍如何使用 ChatGPT 完成各种任务。&lt;/li>
&lt;/ul>
&lt;h1 id="opensource">OpenSource&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/cfortuner/promptable" target="_blank" rel="noopener">2023-Promptable
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/cfortuner/promptable" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Promptable is library that enables you to build powerful AI applications with LLMs and Embeddings providers such as OpenAI, Hugging Face, Cohere and Anthropic. It provides a flexible and extensible API that makes it easy to compose LLMs with data and tools to build complex applications quickly and easily.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/amazon-science/mm-cot" target="_blank" rel="noopener">2023-mm-cot
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/amazon-science/mm-cot" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Multimodal-CoT incorporates vision features in a decoupled training framework. The framework consists of two training stages: (i) rationale generation and (ii) answer inference. Both stages share the same model architecture but differ in the input and output.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/hwchase17/langchain" target="_blank" rel="noopener">2023-LangChains
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/hwchase17/langchain" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Large language models (LLMs) are emerging as a transformative technology, enabling developers to build applications that they previously could not. But using these LLMs in isolation is often not enough to create a truly powerful app - the real power comes when you can combine them with other sources of computation or knowledge.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/logspace-ai/langflow" target="_blank" rel="noopener">LangFlow
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/logspace-ai/langflow" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: ⛓️ LangFlow is a UI for LangChain, designed with react-flow to provide an effortless way to experiment and prototype flows.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="finetuning--inference">Finetuning &amp;amp; Inference&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/stochasticai/xturing" target="_blank" rel="noopener">2023-xturing
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/stochasticai/xturing" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: xturing provides fast, efficient and simple fine-tuning of LLMs, such as LLaMA, GPT-J, GPT-2, OPT, Cerebras-GPT, Galactica, and more. By providing an easy-to-use interface for personalizing LLMs to your own data and application, xTuring makes it simple to build and control LLMs. The entire process can be done inside your computer or in your private cloud, ensuring data privacy and security.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/OptimalScale/LMFlow" target="_blank" rel="noopener">2023-LMFlow
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/OptimalScale/LMFlow" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: An extensible, convenient, and efficient toolbox for finetuning large machine learning models, designed to be user-friendly, speedy and reliable, and accessible to the entire community.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h2 id="chatglm">ChatGLM&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/THUDM/ChatGLM-6B" target="_blank" rel="noopener">2023-ChatGLM-6B
&lt;img src="https://ng-tech.icu/assets/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/THUDM/ChatGLM-6B" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。&lt;/li>
&lt;/ul></description></item><item><title>Prompt-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/prompt-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/llm/prompt-list/</guid><description>&lt;h1 id="prompt-list">Prompt List&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://github.com/microsoft/prompt-engine" target="_blank" rel="noopener">2022-prompt-engine
&lt;img src="https://martrix-usa.oss-accelerate.aliyuncs.com/logo/code.svg" style="max-width: 100px;display: inline-flex;"/>
&lt;img src="https://img.shields.io/github/stars/microsoft/prompt-engine" style="max-width: 100px;display: inline-flex;"/>&lt;/a>: Prompt engineering can be as simple as formatting a question and passing it to the model, but it can also get quite complex - requiring substantial code to manipulate and update strings. This library aims to make that easier. It also aims to codify patterns and practices around prompt engineering.&lt;/li>
&lt;/ul></description></item></channel></rss>