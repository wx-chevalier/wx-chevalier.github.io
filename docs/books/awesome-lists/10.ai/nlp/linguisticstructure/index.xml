<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LinguisticStructure | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/</link><atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/index.xml" rel="self" type="application/rss+xml"/><description>LinguisticStructure</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>LinguisticStructure</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/</link></image><item><title>Representation-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/representation-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/representation-list/</guid><description>&lt;h1 id="document-representation-list">Document Representation List&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="http://blog.districtdatalabs.com/nlp-research-lab-part-1-distributed-representations" target="_blank" rel="noopener">2016-NLP Research Lab Part 1: Distributed Representations&lt;/a>: How I Learned To Stop Worrying And Love Word Embeddings&lt;/li>
&lt;/ul>
&lt;h1 id="word-vectors">Word Vectors&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469" target="_blank" rel="noopener">2018-Understanding word vectors&lt;/a>: Understanding word vectors: A tutorial for &amp;ldquo;Reading and Writing Electronic Text,&amp;rdquo; a class I teach at ITP.&lt;/li>
&lt;/ul>
&lt;h1 id="word2vec">Word2Vec&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf" target="_blank" rel="noopener">The Code Word2Vec Tutorial Part I: The SkipGram Model&lt;/a>，&lt;a href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" target="_blank" rel="noopener">Word2Vec Tutorial Part 2 - Negative Sampling&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1402.3722.pdf" target="_blank" rel="noopener">word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank" rel="noopener">word2vec Parameter Learning Explained&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://blog.csdn.net/itplus/article/details/37998797" target="_blank" rel="noopener">论文翻译章节：基于 Negative Sampling 的模型 &lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term" target="_blank" rel="noopener">word2vec: negative sampling (in layman term)?&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.quora.com/Deep-Learning/Deep-Learning-What-is-meant-by-a-distributed-representation" target="_blank" rel="noopener">Deep-Learning-What-is-meant-by-a-distributed-representation&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://code.google.com/p/word2vec/" target="_blank" rel="noopener">Google - Word2Vec&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://techblog.youdao.com/?p=915#LinkTarget_699" target="_blank" rel="noopener">Deep Learning 实战之 word2vec&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://blog.csdn.net/lingerlanlan/article/details/38048335" target="_blank" rel="noopener">word2vector 学习笔记(一)&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://licstar.net/archives/328#s20" target="_blank" rel="noopener">词向量和语言模型&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/licstar/compare" target="_blank" rel="noopener">关于多个词向量算法的实现对比&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/21391710" target="_blank" rel="noopener">斯坦福深度学习课程第二弹：词向量内部和外部任务评价&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>StatisticalLanguageModel-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/statisticallanguagemodel-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/statisticallanguagemodel-list/</guid><description>&lt;h1 id="统计语言模型资料索引">统计语言模型资料索引&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://jon.dehdari.org/tutorials/lm_overview.pdf" target="_blank" rel="noopener">A Short Overview of Statistical Language Models&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/Language_model#Continuous_space_language_models" target="_blank" rel="noopener">Wiki 上关于语言模型的阐述&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://6me.us/Eo4" target="_blank" rel="noopener">Elegant N-gram Generation in Python&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://arxiv.org/pdf/1703.01619.pdf" target="_blank" rel="noopener">2017-Neural Machine Translation and Sequence-to-sequence Models: A Tutorial&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kamidox/blogs/blob/master/content/notes/stanford_cs224d.md" target="_blank" rel="noopener">Stanford CS224d Lecture 1&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="embedding">Embedding&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526" target="_blank" rel="noopener">2018-Neural Network Embeddings Explained&lt;/a>: How deep learning can represent War and Peace as a vector&lt;/li>
&lt;/ul></description></item><item><title>TopicModel-List</title><link>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/topicmodel-list/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-lists/10.ai/nlp/linguisticstructure/topicmodel-list/</guid><description>&lt;h1 id="主题模型资料索引">主题模型资料索引&lt;/h1>
&lt;p>朴素贝叶斯可以胜任许多文本的分类问题，但是无法解决语料中一词多义和多词一义的问题，它更像是词法分析，而不是语义分析。而如果使用词向量作为文档的特征，可以较好地解决了一词多义和多词一义的问题，但是就好像过拟合一样，会造成计算文档间相似度的不准确性。而通过添加主题这个隐藏变量，一个词可能被映射到多个主题，而多个主题也可能被映射到一个词中，从而解决一定程度上的语义问题。&lt;/p>
&lt;p>主题模型经历从基于 SVD 的简单的 LSA(隐含语义分析)，到基于概率模型与 EM 的 pLSA，再到基于 Dirichlet 分布的 LDA。目前，经典的主题模型一般都会基于 BOW(Bag-of-Words)假设。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://www.cad.zju.edu.cn/home/vagblog/?p=4151" target="_blank" rel="noopener">Task-Driven Comparison of Topic Models&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://www.infoq.com/cn/news/2016/07/technical-details-for-topic" target="_blank" rel="noopener">LinkedIn 文本分析平台：主题挖掘的四大技术步骤&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="lda">LDA&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://6me.us/idj2" target="_blank" rel="noopener">LDA 算法理解&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/tdhopper/notes-on-dirichlet-processes" target="_blank" rel="noopener">Notes Nonparametric Bayesian Methods and Dirichlet Processes&lt;/a>: &lt;a href="https://parg.co/bsl" target="_blank" rel="noopener">Nonparametric Latent Dirichlet Allocation&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://parg.co/bsl" target="_blank" rel="noopener">Nonparametric Latent Dirichlet Allocation&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="lda2vec">Lda2Vec&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://www.datasciencecentral.com/profiles/blogs/a-tale-about-lda2vec-when-lda-meets-word2vec?xg_source=activity" target="_blank" rel="noopener">A tale about LDA2vec: when LDA meets word2vec&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994" target="_blank" rel="noopener">word2vec, LDA, and introducing a new hybrid algorithm: lda2vec&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="topic-model-evaluation">Topic Model Evaluation&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="http://qpleple.com/perplexity-to-evaluate-topic-models/" target="_blank" rel="noopener">Perplexity To Evaluate Topic Models&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://qpleple.com/topic-coherence-to-evaluate-topic-models/" target="_blank" rel="noopener">Topic Coherence To Evaluate Topic Models&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf" target="_blank" rel="noopener">2015-Exploring the Space of Topic Coherence Measures&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="http://epubs.siam.org/doi/pdf/10.1137/1.9781611972733.6" target="_blank" rel="noopener">Hierarchical Document Clustering Using Frequent Itemsets&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item></channel></rss>