<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Transformer | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/awesome-lists/10.ai/deeplearning/transformer/</link>
      <atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/deeplearning/transformer/index.xml" rel="self" type="application/rss+xml" />
    <description>Transformer</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>Transformer</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/deeplearning/transformer/</link>
    </image>
    
    <item>
      <title>Transformer-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/deeplearning/transformer/transformer-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/deeplearning/transformer/transformer-list/</guid>
      <description>&lt;h1 id=&#34;transformer-list&#34;&gt;Transformer List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-The Transformer Family Version 2.0&lt;/a&gt;: Many new Transformer architecture improvements have been proposed since my last post on ‚ÄúThe Transformer Family‚Äù about three years ago. Here I did a big refactoring and enrichment of that 2020 post ‚Äî restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Transformer-OpenSource-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/deeplearning/transformer/transformer-opensource-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/deeplearning/transformer/transformer-opensource-list/</guid>
      <description>&lt;h1 id=&#34;transformer-opensource-list&#34;&gt;Transformer OpenSource List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/xenova/transformers.js&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-transformers.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/xenova/transformers.js&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Run ü§ó Transformers in your browser! We currently support BERT, ALBERT, DistilBERT, T5, T5v1.1, FLAN-T5, GPT2, BART, CodeGen, Whisper, CLIP, Vision Transformer, and VisionEncoderDecoder models, for a variety of tasks including: masked language modelling, text classification, text-to-text generation, translation, summarization, question answering, text generation, automatic speech recognition, image classification, zero-shot image classification, and image-to-text.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
