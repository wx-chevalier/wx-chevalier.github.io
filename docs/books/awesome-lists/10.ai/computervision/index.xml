<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ComputerVision | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/</link>
      <atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/computervision/index.xml" rel="self" type="application/rss+xml" />
    <description>ComputerVision</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>ComputerVision</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/</link>
    </image>
    
    <item>
      <title>ComputerVision-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/computervision-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/computervision-list/</guid>
      <description>&lt;h1 id=&#34;computer-vision-list&#34;&gt;Computer Vision List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kjw0612/awesome-deep-vision&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A curated list of deep learning resources for computer vision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/kLYm3hNFiEXNAlSW3Zaq5g&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-å›¾åƒå¤„ç†ï¼Œè®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½ä¹‹é—´çš„å·®å¼‚&lt;/a&gt;: å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å¸®åŠ©ä½ äº†è§£å›¾åƒå¤„ç†ï¼Œè®¡ç®—æœºè§†è§‰å’Œäººå·¥æ™ºèƒ½ä¹‹é—´çš„åŒºåˆ«ã€‚&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;case-study&#34;&gt;Case Study&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/hPzB0gpbJax3b65nx1Ovdw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-è®¡ç®—æœºè§†è§‰åœ¨åˆ¶é€ ä¸šåº”ç”¨çš„åå¤§æœ€æ–°æ¡ˆä¾‹&lt;/a&gt;: æ˜¨å¤©å‘ç°ä¸€ç¯‡æ–‡ç« ï¼Œåˆ†äº«äº†è®¡ç®—æœºè§†è§‰åœ¨åˆ¶é€ ä¸šåº”ç”¨ä¸­çš„ 10 ä¸ªæ¡ˆä¾‹ï¼Œç‰¹æ­¤è½¬è½½è¿‡æ¥åˆ†äº«ç»™å¤§å®¶ï¼Œå¸Œæœ›å¯¹å¤§å®¶æœ‰å¸®åŠ©ï¼Œæœ‰ç–‘é—®çš„å¯ä»¥åœ¨æ–‡æœ«ç•™è¨€äº’ç›¸äº¤æµï½&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;collection&#34;&gt;Collection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/computervision-recipes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-computervision-recipes #Collection#&lt;/a&gt;: This repository provides examples and best practice guidelines for building computer vision systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;series&#34;&gt;Series&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/computervision-recipes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-computervision-recipes #Series#&lt;/a&gt;: Best Practices, code samples, and documentation for Computer Vision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/WZMIAOMIAO/deep-learning-for-image-processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;æ·±åº¦å­¦ä¹ åœ¨å›¾åƒå¤„ç†ä¸­çš„åº”ç”¨æ•™ç¨‹ #Series#&lt;/a&gt;: æœ¬æ•™ç¨‹æ˜¯å¯¹æœ¬äººç ”ç©¶ç”ŸæœŸé—´çš„ç ”ç©¶å†…å®¹è¿›è¡Œæ•´ç†æ€»ç»“ï¼Œæ€»ç»“çš„åŒæ—¶ä¹Ÿå¸Œæœ›èƒ½å¤Ÿå¸®åŠ©æ›´å¤šçš„å°ä¼™ä¼´ã€‚åæœŸå¦‚æœæœ‰å­¦ä¹ åˆ°æ–°çš„çŸ¥è¯†ä¹Ÿä¼šä¸å¤§å®¶ä¸€èµ·åˆ†äº«ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;image-search&#34;&gt;Image Search&lt;/h1&gt;
&lt;h1 id=&#34;opensource&#34;&gt;OpenSource&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bryandlee/animegan2-pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;animegan2-pytorch 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: PyTorch implementation of AnimeGANv2.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ComputerVision-OpenSource-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/computervision-opensource-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/computervision-opensource-list/</guid>
      <description>&lt;h1 id=&#34;computer-vision&#34;&gt;Computer Vision&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/opencv/cvat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CVAT 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Powerful and efficient Computer Vision Annotation Tool (CVAT).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleGAN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PaddleGAN 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: PaddlePaddle GAN library, including lots of interesting applications like DeepFake First-Order motion transfer, Mai-ha-hiï¼ˆèš‚èšå‘€å˜¿), faceswap wav2lip, picture repair, image editing, photo2cartoon, image style transfer, and so on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/google/mediapipe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-MediaPipe 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/google/mediapipe&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Cross-platform, customizable ML solutions for live and streaming media.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;showcase&#34;&gt;Showcase&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/roboflow/notebooks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-roboflow/notebooks 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/roboflow/notebooks&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Examples and tutorials on using SOTA computer vision models and techniques. Learn everything from old-school ResNet, through YOLO and object-detection transformers like DETR, to the latest models like Grounding DINO and SAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;image--text&#34;&gt;Image &amp;amp; Text&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SHI-Labs/Versatile-Diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Versatile-Diffusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: We built Versatile Diffusion (VD), the first unified multi-flow multimodal diffusion framework, as a step towards Universal Generative AI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;å›¾åƒå¤„ç†&#34;&gt;å›¾åƒå¤„ç†&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/danielgatis/rembg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rembg 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Rembg is a tool to remove images background. That is it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;å›¾åƒåˆ†ç±»&#34;&gt;å›¾åƒåˆ†ç±»&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/infinitered/nsfwjs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nsfw JS 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A simple JavaScript library to help you quickly identify unseemly images; all in the client&amp;rsquo;s browser. NSFWJS isn&amp;rsquo;t perfect, but it&amp;rsquo;s pretty accurate (~90% from our test set of 15,000 test images)&amp;hellip; and it&amp;rsquo;s getting more accurate all the time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deeppomf/DeepCreamPy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepCreamPy 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A deep learning-based tool to automatically replace censored artwork in hentai with plausible reconstructions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ocr&#34;&gt;OCR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nguyenq/tess4j&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tess4j 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Java JNA wrapper for Tesseract OCR API.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sergiomsilva/alpr-unconstrained&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-alpr-unconstrained&lt;/a&gt;: License Plate Detection and Recognition in Unconstrained Scenarios.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-PaddleOCR 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: PaddleOCR aims to create rich, leading, and practical OCR tools that help users train better models and apply them into practice.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JaidedAI/EasyOCR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-EasyOCR 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Ready-to-use OCR with 40+ languages supported including Chinese, Japanese, Korean and Thai&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/faustomorales/keras-ocr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keras-ocr 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A packaged and flexible version of the CRAFT text detector and Keras CRNN recognition model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-PaddleOCR 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Awesome OCR toolkits based on PaddlePaddleï¼ˆ8.6M ultra-lightweight pre-trained model, support training and deployment among server, mobile, embeded and IoT devicesï¼‰.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-mmocr 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: OpenMMLab Text Detection, Recognition and Understanding Toolbox&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;segment&#34;&gt;Segment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Segment Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/facebookresearch/segment-anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: The Segment Anything Model (SAM) produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a dataset of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Grounded-Segment-Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Marrying Grounding DINO with Segment Anything &amp;amp; Stable Diffusion &amp;amp; BLIP - Automatically Detect , Segment and Generate Anything with Image and Text Inputs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kevmo314/magic-copy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Magic Copy 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/kevmo314/magic-copy&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Magic Copy is a Chrome extension that uses Meta&amp;rsquo;s Segment Anything Model to extract a foreground object from an image and copy it to the clipboard.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/fudan-zvg/Semantic-Segment-Anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Semantic-Segment-Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/fudan-zvg/Semantic-Segment-Anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Automated dense category annotation engine that serves as the initial semantic labeling for the Segment Anything dataset (SA-1B).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ZrrSkywalker/Personalize-SAM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-ZrrSkywalker/Personalize-SAM 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/ZrrSkywalker/Personalize-SAM&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: How to customize SAM to automatically segment your pet dog in a photo album?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/opengeos/segment-geospatial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-opengeos/segment-geospatial 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/opengeos/segment-geospatial&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A Python package for segmenting geospatial data with the Segment Anything Model (SAM)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SysCV/sam-hq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-SysCV/sam-hq 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/SysCV/sam-hq&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: We propose HQ-SAM to upgrade SAM for high-quality zero-shot segmentation. Refer to our paper for more details. Our code and models will be released in two weeks. Stay tuned!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/baaivision/Painter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Painter 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/baaivision/Painter&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Painter &amp;amp; SegGPT Series: Vision Foundation Models from BAAI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/geekyutao/Inpaint-Anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Inpaint-Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/geekyutao/Inpaint-Anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Users can select any object in an image by clicking on it. With powerful vision models, e.g., SAM, LaMa and Stable Diffusion (SD), Inpaint Anything is able to remove the object smoothly (i.e., Remove Anything). Further, prompted by user input text, Inpaint Anything can fill the object with any desired content (i.e., Fill Anything) or replace the background of it arbitrarily (i.e., Replace Anything).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sail-sg/EditAnything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-EditAnything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/sail-sg/EditAnything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Edit anything in images powered by segment-anything, ControlNet, StableDiffusion, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-GroundingDINO 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/IDEA-Research/GroundingDINO&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: The official implementation of &amp;ldquo;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Segment-Everything-Everywhere-All-At-Once 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: We introduce SEEM that can Segment Everything Everywhere with Multi-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/xinyu1205/Recognize_Anything-Tag2Text&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Recognize_Anything-Tag2Text 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/xinyu1205/Recognize_Anything-Tag2Text&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A Strong Image Tagging Model &amp;amp; Tag2Text: Guiding Vision-Language Model via Image Tagging&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;object-detection&#34;&gt;Object Detection&lt;/h2&gt;
&lt;h3 id=&#34;åŠ¨ä½œè¯†åˆ«&#34;&gt;åŠ¨ä½œè¯†åˆ«&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/Detectron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-Detectron 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Detectron is Facebook AI Research&amp;rsquo;s software system that implements state-of-the-art object detection algorithms, including Mask R-CNN.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/adipandas/multi-object-tracker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-Multi Object Tracker 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Object detection using deep learning and multi-object tracking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-OpenPose 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021-CLIP 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;yolov5 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: This repository represents Ultralytics open-source research into future object detection methods, and incorporates our lessons learned and best practices evolved over training thousands of models on custom client datasets with our previous YOLO repository.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLOX 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with ONNX, TensorRT, ncnn, and OpenVINO supported.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLOv6 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: a single-stage object detection framework dedicated to industrial applications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://victordibia.github.io/handtrack.js/#/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Handtrack.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: å®ƒå¯ä»¥è®©å¼€å‘äººå‘˜ä½¿ç”¨ç»è¿‡è®­ç»ƒçš„æ‰‹éƒ¨æ£€æµ‹æ¨¡å‹å¿«é€Ÿåˆ›å»ºæ‰‹åŠ¿äº¤äº’åŸå‹ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MMDetection 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022-MMYOLO 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: MMYOLO is an open source toolbox for YOLO series algorithms based on PyTorch and MMDetection. It is a part of the OpenMMLab project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IDEA-Research/detrex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;detrex 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: IDEA Open Source Toolbox for Transformer Based Object Detection Algorithms&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;äººè„¸æ£€æµ‹&#34;&gt;äººè„¸æ£€æµ‹&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tehnokv/picojs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-pico.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: a face-detection library in 200 lines of JavaScript&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;face-api.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: JavaScript API for Face Recognition in the Browser with tensorflow.js.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepfakes/faceswap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Faceswap 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Faceswap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vipstone/faceai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-faceai 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: ä¸€æ¬¾å…¥é—¨çº§çš„äººè„¸ã€è§†é¢‘ã€æ–‡å­—æ£€æµ‹ä»¥åŠè¯†åˆ«çš„é¡¹ç›®ã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/seetafaceengine/SeetaFace2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SeetaFace 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Open source, full stack face recognization toolkit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-è¶…è½»é‡çº§äººè„¸æ£€æµ‹æ¨¡å‹ 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: è¯¥æ¨¡å‹è®¾è®¡æ˜¯é’ˆå¯¹è¾¹ç¼˜è®¡ç®—è®¾å¤‡æˆ–ä½ç®—åŠ›è®¾å¤‡(å¦‚ç”¨ ARM æ¨ç†)è®¾è®¡çš„å®æ—¶è¶…è½»é‡çº§é€šç”¨äººè„¸æ£€æµ‹æ¨¡å‹ï¼Œå¯ä»¥åœ¨ä½ç®—åŠ›è®¾å¤‡ä¸­å¦‚ç”¨ ARM è¿›è¡Œå®æ—¶çš„é€šç”¨åœºæ™¯çš„äººè„¸æ£€æµ‹æ¨ç†ï¼ŒåŒæ ·é€‚ç”¨äºç§»åŠ¨ç«¯ã€PCã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tg-bomze/Face-Depixelizer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-Face Depixelizer 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Face Depixelizer based on &amp;ldquo;PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models&amp;rdquo; repository.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JDAI-CV/FaceX-Zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-FaceX Zoo 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: FaceX-Zoo is a PyTorch toolbox for face recognition. It provides a training module with various supervisory heads and backbones towards state-of-the-art face recognition, as well as a standardized evaluation module which enables to evaluate the models in most of the popular benchmarks just by editing a simple configuration.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tracker&#34;&gt;Tracker&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ifzhang/ByteTrack&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021-ByteTrack 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: ByteTrack: Multi-Object Tracking by Associating Every Detection Box.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/gaomingqi/Track-Anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Track-Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/gaomingqi/Track-Anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/videoflow/videoflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-videoflow 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Python framework that facilitates the quick development of complex video analysis applications and other series-processing based applications in a multiprocessing environment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021-RobustVideoMatting 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Robust Video Matting in PyTorch, TensorFlow, TensorFlow.js, ONNX, CoreML!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-DINOv2 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/facebookresearch/dinov2&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>FaceRecognition-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/facerecognition-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/facerecognition-list/</guid>
      <description>&lt;h1 id=&#34;face-recognition-äººè„¸è¯†åˆ«&#34;&gt;Face Recognition: äººè„¸è¯†åˆ«&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://trackingjs.com/docs.html#introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tracking.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Tracking.js å¯ä»¥å±•ç¤ºæ•ˆæœç±»ä¼¼ Kinect æˆ–è€… Wii çš„ä½“æ„Ÿåº”ç”¨ï¼Œä¸”è¯¥ JavaScript åº“ä½“ç§¯å° (~7k)ï¼Œéå¸¸è½»é‡çº§ï¼Œä¸”æ¥å£ç®€æ´ã€‚Tracking.js èƒ½å¤Ÿåœ¨ç§»åŠ¨ Web åº”ç”¨ã€æ¡Œé¢åº”ç”¨ä¸­å·¥ä½œï¼Œç”šè‡³å¯ä»¥å’ŒåŸºäº Node.js çš„æœåŠ¡å™¨è¿›è¡Œé…å¯¹ã€‚å®ƒä¼šç»™æµè§ˆå™¨å¸¦æ¥è®¡ç®—æœºå›¾å½¢å­¦ç®—æ³•å’ŒæŠ€æœ¯ï¼Œå…¶æ‹¥æœ‰åŠŸèƒ½ï¼šè„¸éƒ¨è¯†åˆ« ( æŸä¸ªç‰¹å®šçš„é¢œè‰²æ—¶æˆ–äººç‰© / è„¸åº / èº«ä½“å‡ºç°ç§»åŠ¨çš„æ—¶å€™ )ã€å®æ—¶è‰²å½©è·Ÿè¸ªã€‚å¯¹äº Web å¼€å‘è€Œ è¨€ï¼Œä»¥å‰éœ€è¦é€šè¿‡ C æˆ– C++ çš„æŠ€æœ¯æ‰èƒ½å®ç°ç±»ä¼¼æ•ˆæœã€‚è€Œç°åœ¨ Traking.js æä¾›äº†ä¸€ä¸ª Web ç»„ä»¶ï¼Œå› æ­¤ Web å‰ç«¯å¼€å‘äººå‘˜å¯ä»¥è®¿é—® HTML æ ‡ç­¾ç»„ä»¶ æ¥å®ç°ç±»ä¼¼åŠŸèƒ½ï¼Œè€Œæ— éœ€äº†è§£ JavaScriptï¼Œè¿™æå¤§çš„ç®€åŒ–äº† Web å¼€å‘ã€‚Tracking.js åŒ…æ‹¬ä¸€ä¸ªè‰²å½©è·Ÿè¸ªç®—æ³•å’Œå¯¹è±¡è·Ÿè¸ªç»„ä»¶ï¼Œå®ƒèƒ½ä½¿ Web æµè§ˆå™¨è¯†åˆ«è„¸éƒ¨åŠçœ¼ç›çš„å˜åŒ–ã€‚ä¾‹å¦‚ï¼ŒWeb å‰ç«¯è¿˜å¯ä»¥å¯¹äºç”¨è¿™ä¸ªåŠŸèƒ½ æ¥è®¾ç½®ç”¨æˆ·å¤´åƒï¼Œå¯¹ä¸€äº›ç½‘ç«™è€Œè¨€ï¼Œè¿™ä¹Ÿæ˜¯ä¸ªå¾ˆç‚«çš„åŠŸèƒ½ï¼›åŒæ—¶å¯¹è·Ÿè¸ªçš„è„¸éƒ¨æ•°æ®å’Œåå°æ•°æ®åº“è¿›è¡ŒåŒ¹é…ï¼Œä»è€Œå’Œåé¦ˆç»™ç”¨æˆ·æ›´å¤šæœ‰ç”¨çš„æ•°æ®ã€‚&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MidJourney-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/midjourney-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/midjourney-list/</guid>
      <description>&lt;h1 id=&#34;midjourney-list&#34;&gt;MidJourney List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/willwulfken/MidJourney-Styles-and-Keywords-Reference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-MidJourney Styles and Keywords Reference&lt;/a&gt;: A reference containing Styles and Keywords that you can use with MidJourney AI. There are also pages showing resolution comparison, image weights, and much more!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://enchanting-trader-463.notion.site/Midjourney-AI-Guide-41eca43809dd4d8fa676e648436fc29c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Midjourney AI Guide&lt;/a&gt;: So letâ€™s begin our journey towards filled with surprises and fun tricks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Object-Detection-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/object-detection-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/object-detection-list/</guid>
      <description>&lt;h1 id=&#34;object-detection-list&#34;&gt;Object Detection List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/move-cursor-with-tensorflow-3727ed5e2795&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Move Your Cursor with Webcam Using TensorFlow Object Detection API&lt;/a&gt;: TensorMouse is a small open source Python application that allows you to move your cursor by moving a random household object (like a cup, apple or banana) in front of webcam and acts as a replacement for computer mouse or trackpad.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.algorithmia.com/deep-dive-into-object-detection-with-open-images-using-tensorflow/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Deep Dive into Object Detection with Open Images, using TensorFlow&lt;/a&gt;: TensorFlowâ€™s Object Detection API and its ability to handle large volumes of data make it a perfect choice, so letâ€™s jump right inâ€¦&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OCR-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/ocr-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/ocr-list/</guid>
      <description>&lt;h1 id=&#34;ocr-list&#34;&gt;OCR List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;2017-Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/DmitryUlyanov/deep-image-prior&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-Deep image prior 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Image restoration with neural networks but without learning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://parg.co/UsP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-How to break a CAPTCHA system in 15 minutes with Machine Learning&lt;/a&gt;: Letâ€™s hack the worldâ€™s most popular Wordpress CAPTCHA Plug-in.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/karandesai-96/digit-classifier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;digit-classifier&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://leadtools.gcpowertools.com.cn/orders/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LEADTOOLs&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/garnele007/SwiftOCR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SwiftOCR&lt;/a&gt;: Fast and simple OCR library written in Swift&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.a9t9.com/2015/02/ocr-online-converter-review.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Best Online OCR Software for Converting Images to Text&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.newocr.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NewOCR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.onlineocr.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OnlineOCR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gnu.org/software/ocrad/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ocrad&lt;/a&gt;: is an OCR (Optical Character Recognition) program based on a feature extraction method. It reads images in pbm (bitmap), pgm (greyscale) or ppm (color) formats and produces text in byte (8-bit) or UTF-8 formats.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/A9T9/OCR-Benchmark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OCR-Benchmark&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wanghaisheng/awesome-ocr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;awesome-ocr&lt;/a&gt;: A curated list of promising OCR resources.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;engineering-practices&#34;&gt;Engineering Practices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.dropbox.com/tech/2018/10/using-machine-learning-to-index-text-from-billions-of-images/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Dropbox-Using machine learning to index text from billions of images&lt;/a&gt;: So now, when a user searches for English text that appears in one of these files, it will show up in the search results. This blog post describes how we built this feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;case-study&#34;&gt;Case Study&lt;/h2&gt;
&lt;h1 id=&#34;resource&#34;&gt;Resource&lt;/h1&gt;
&lt;h2 id=&#34;collection&#34;&gt;Collection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/OXmWLuZR2mzEz7drn4xGDQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-æœ€å…¨ OCR ç›¸å…³èµ„æ–™æ•´ç† ğŸ—ƒï¸&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://my.oschina.net/zhouxiang/blog/161619&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Java ä½¿ç”¨ Tess4J è¿›è¡Œ å›¾ç‰‡æ–‡å­—è¯†åˆ« ç¬”è®°&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OpenCV-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/opencv-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/opencv-list/</guid>
      <description>&lt;h1 id=&#34;opencv-list&#34;&gt;OpenCV List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2015-LearnOpenCV ğŸ“š&lt;/a&gt;: This repo contains code for Computer Vision, Deep learning, and AI articles shared on our blog LearnOpenCV.com.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>StableDiffusion-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/stablediffusion-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/stablediffusion-list/</guid>
      <description>&lt;h1 id=&#34;stablediffusion-list&#34;&gt;StableDiffusion List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/diffusion-models-class&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face Diffusion Models Course ğŸ¥&lt;/a&gt;: Materials for the Hugging Face Diffusion Models Course.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/FurkanGozukara/Stable-Diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FurkanGozukara/Stable-Diffusion #Series# 
















  &lt;img src=&#34;https://img.shields.io/github/stars/FurkanGozukara/Stable-Diffusion&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Best Stable Diffusion and AI Tutorials, Guides, News, Tips and Tricks&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;opensource&#34;&gt;OpenSource&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/divamgupta/diffusionbee-stable-diffusion-ui&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diffusion Bee 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Diffusion Bee is the easiest way to run Stable Diffusion locally on your M1 Mac. Comes with a one-click installer. No dependencies or technical knowledge needed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/AbdBarho/stable-diffusion-webui-docker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable Diffusion WebUI Docker 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Run Stable Diffusion on your machine with a nice UI without any hassle! This repository provides multiple UIs for you to play around with stable diffusion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.charl-e.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CHARL-E 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: CHARL-E packages Stable Diffusion into a simple app. No complex setup, dependencies, or internet required â€” just download and say what you want to see.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ahrm/UnstableFusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UnstableFusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A Stable Diffusion desktop frontend with inpainting, img2img and more!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stablediffusion-infinity 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Outpainting with Stable Diffusion on an infinite canvas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/divamgupta/diffusionbee-stable-diffusion-ui&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diffusion Bee 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Diffusion Bee is the easiest way to run Stable Diffusion locally on your M1 Mac. Comes with a one-click installer. No dependencies or technical knowledge needed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/invoke-ai/InvokeAI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InvokeAI 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: This version of Stable Diffusion features a slick WebGUI, an interactive command-line script that combines text2img and img2img functionality in a &amp;ldquo;dream bot&amp;rdquo; style interface, and multiple features and other enhancements. For more info, see the website link below.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GuyTevet/motion-diffusion-model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MDM: Human Motion Diffusion Model 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: The official PyTorch implementation of the paper &amp;ldquo;Human Motion Diffusion Model&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/amotile/stable-diffusion-studio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stable-diffusion-studio 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: An animation focused workflow frontend for Stable Diffusion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JingShing/novelai-colab-ver&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;novelai-colab-ver 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: You can use this version to experience how novelai works without a good gpu.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/TheLastBen/fast-stable-diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fast-stable-diffusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: fast-stable-diffusion colabs, +25-50% speed increase + memory efficient + DreamBooth&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/civitai/civitai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-civitai 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/civitai/civitai&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Our goal with this project is to create a platform where people can share their stable diffusion models (textual inversions, hypernetworks, aesthetic gradients, VAEs, and any other crazy stuff people do to customize their AI generations), collaborate with others to improve them, and learn from each other&amp;rsquo;s work. The platform allows users to create an account, upload their models, and browse models that have been shared by others. Users can also leave comments and feedback on each other&amp;rsquo;s models to facilitate collaboration and knowledge sharing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;showcase&#34;&gt;Showcase&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/KKGo1999/Stable-diffusion-person&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Stable-diffusion-person 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/KKGo1999/Stable-diffusion-person&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: æœ¬æ–‡ä»‹ç»ç”±åŸºäº Stable-diffusion çš„ Chilloutmix æ¨¡å‹ï¼ˆä»¥åŠæœ€æ–°çš„ ControlNetï¼‰ç”Ÿæˆé«˜æ¸…çœŸå®äººåƒçš„æ–¹æ³•åŠ Demoã€‚&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/replicate/scribble-diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-scribble-diffusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/replicate/scribble-diffusion&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Turn your rough sketch into a refined image using AI.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/camenduru/stable-diffusion-webui-colab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-stable-diffusion-webui-colab 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/camenduru/stable-diffusion-webui-colab&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: stable diffusion webui colab.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;stablediffusion-implementation&#34;&gt;StableDiffusion Implementation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/axodox/axodox-machinelearning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;axodox/axodox-machinelearning 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/axodox/axodox-machinelearning&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: This repository contains a fully C++ implementation of Stable Diffusion-based image synthesis, including the original txt2img, img2img and inpainting capabilities and the safety checker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gui--app&#34;&gt;GUI &amp;amp; APP&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/justjake/Gauss&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022-Gauss 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/justjake/Gauss&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A Stable Diffusion app for macOS built with SwiftUI and Apple&amp;rsquo;s ml-stable-diffusion CoreML models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-ComfyUI 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/comfyanonymous/ComfyUI&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A powerful and modular stable diffusion GUI with a graph/nodes interface.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/mlc-ai/web-stable-diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Web Stable Diffusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/mlc-ai/web-stable-diffusion&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Bringing stable diffusion models to web browsers. Everything runs inside the browser with no server support.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
