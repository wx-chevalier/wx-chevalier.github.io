<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ComputerVision | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/</link>
      <atom:link href="https://ng-tech.icu/books/awesome-lists/10.ai/computervision/index.xml" rel="self" type="application/rss+xml" />
    <description>ComputerVision</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>ComputerVision</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/</link>
    </image>
    
    <item>
      <title>ComputerVision-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/computervision-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/computervision-list/</guid>
      <description>&lt;h1 id=&#34;computer-vision-list&#34;&gt;Computer Vision List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kjw0612/awesome-deep-vision&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A curated list of deep learning resources for computer vision&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/kLYm3hNFiEXNAlSW3Zaq5g&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-图像处理，计算机视觉和人工智能之间的差异&lt;/a&gt;: 因此，在本文中，我将帮助你了解图像处理，计算机视觉和人工智能之间的区别。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;case-study&#34;&gt;Case Study&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/hPzB0gpbJax3b65nx1Ovdw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-计算机视觉在制造业应用的十大最新案例&lt;/a&gt;: 昨天发现一篇文章，分享了计算机视觉在制造业应用中的 10 个案例，特此转载过来分享给大家，希望对大家有帮助，有疑问的可以在文末留言互相交流～&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;collection&#34;&gt;Collection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/microsoft/computervision-recipes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-computervision-recipes #Collection#&lt;/a&gt;: This repository provides examples and best practice guidelines for building computer vision systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;series&#34;&gt;Series&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/computervision-recipes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-computervision-recipes #Series#&lt;/a&gt;: Best Practices, code samples, and documentation for Computer Vision.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/WZMIAOMIAO/deep-learning-for-image-processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;深度学习在图像处理中的应用教程 #Series#&lt;/a&gt;: 本教程是对本人研究生期间的研究内容进行整理总结，总结的同时也希望能够帮助更多的小伙伴。后期如果有学习到新的知识也会与大家一起分享。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;image-search&#34;&gt;Image Search&lt;/h1&gt;
&lt;h1 id=&#34;opensource&#34;&gt;OpenSource&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bryandlee/animegan2-pytorch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;animegan2-pytorch 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: PyTorch implementation of AnimeGANv2.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>ComputerVision-OpenSource-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/computervision-opensource-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/computervision-opensource-list/</guid>
      <description>&lt;h1 id=&#34;computer-vision&#34;&gt;Computer Vision&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/opencv/cvat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CVAT 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Powerful and efficient Computer Vision Annotation Tool (CVAT).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleGAN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PaddleGAN 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: PaddlePaddle GAN library, including lots of interesting applications like DeepFake First-Order motion transfer, Mai-ha-hi（蚂蚁呀嘿), faceswap wav2lip, picture repair, image editing, photo2cartoon, image style transfer, and so on.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/google/mediapipe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-MediaPipe 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/google/mediapipe&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Cross-platform, customizable ML solutions for live and streaming media.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;showcase&#34;&gt;Showcase&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/roboflow/notebooks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-roboflow/notebooks 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/roboflow/notebooks&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Examples and tutorials on using SOTA computer vision models and techniques. Learn everything from old-school ResNet, through YOLO and object-detection transformers like DETR, to the latest models like Grounding DINO and SAM.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;image--text&#34;&gt;Image &amp;amp; Text&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SHI-Labs/Versatile-Diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Versatile-Diffusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: We built Versatile Diffusion (VD), the first unified multi-flow multimodal diffusion framework, as a step towards Universal Generative AI.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;图像处理&#34;&gt;图像处理&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/danielgatis/rembg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rembg 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Rembg is a tool to remove images background. That is it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;图像分类&#34;&gt;图像分类&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/infinitered/nsfwjs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nsfw JS 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A simple JavaScript library to help you quickly identify unseemly images; all in the client&amp;rsquo;s browser. NSFWJS isn&amp;rsquo;t perfect, but it&amp;rsquo;s pretty accurate (~90% from our test set of 15,000 test images)&amp;hellip; and it&amp;rsquo;s getting more accurate all the time.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deeppomf/DeepCreamPy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepCreamPy 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A deep learning-based tool to automatically replace censored artwork in hentai with plausible reconstructions.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;ocr&#34;&gt;OCR&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nguyenq/tess4j&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tess4j 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Java JNA wrapper for Tesseract OCR API.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sergiomsilva/alpr-unconstrained&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-alpr-unconstrained&lt;/a&gt;: License Plate Detection and Recognition in Unconstrained Scenarios.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-PaddleOCR 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: PaddleOCR aims to create rich, leading, and practical OCR tools that help users train better models and apply them into practice.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JaidedAI/EasyOCR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-EasyOCR 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Ready-to-use OCR with 40+ languages supported including Chinese, Japanese, Korean and Thai&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/faustomorales/keras-ocr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keras-ocr 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A packaged and flexible version of the CRAFT text detector and Keras CRNN recognition model.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PaddlePaddle/PaddleOCR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-PaddleOCR 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Awesome OCR toolkits based on PaddlePaddle（8.6M ultra-lightweight pre-trained model, support training and deployment among server, mobile, embeded and IoT devices）.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmocr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-mmocr 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: OpenMMLab Text Detection, Recognition and Understanding Toolbox&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;segment&#34;&gt;Segment&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/segment-anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Segment Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/facebookresearch/segment-anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: The Segment Anything Model (SAM) produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a dataset of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IDEA-Research/Grounded-Segment-Anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Grounded-Segment-Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Marrying Grounding DINO with Segment Anything &amp;amp; Stable Diffusion &amp;amp; BLIP - Automatically Detect , Segment and Generate Anything with Image and Text Inputs&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kevmo314/magic-copy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Magic Copy 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/kevmo314/magic-copy&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Magic Copy is a Chrome extension that uses Meta&amp;rsquo;s Segment Anything Model to extract a foreground object from an image and copy it to the clipboard.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/fudan-zvg/Semantic-Segment-Anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Semantic-Segment-Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/fudan-zvg/Semantic-Segment-Anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Automated dense category annotation engine that serves as the initial semantic labeling for the Segment Anything dataset (SA-1B).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ZrrSkywalker/Personalize-SAM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-ZrrSkywalker/Personalize-SAM 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/ZrrSkywalker/Personalize-SAM&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: How to customize SAM to automatically segment your pet dog in a photo album?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/opengeos/segment-geospatial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-opengeos/segment-geospatial 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/opengeos/segment-geospatial&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A Python package for segmenting geospatial data with the Segment Anything Model (SAM)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/SysCV/sam-hq&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-SysCV/sam-hq 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/SysCV/sam-hq&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: We propose HQ-SAM to upgrade SAM for high-quality zero-shot segmentation. Refer to our paper for more details. Our code and models will be released in two weeks. Stay tuned!&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/baaivision/Painter&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Painter 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/baaivision/Painter&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Painter &amp;amp; SegGPT Series: Vision Foundation Models from BAAI&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/geekyutao/Inpaint-Anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Inpaint-Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/geekyutao/Inpaint-Anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Users can select any object in an image by clicking on it. With powerful vision models, e.g., SAM, LaMa and Stable Diffusion (SD), Inpaint Anything is able to remove the object smoothly (i.e., Remove Anything). Further, prompted by user input text, Inpaint Anything can fill the object with any desired content (i.e., Fill Anything) or replace the background of it arbitrarily (i.e., Replace Anything).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/sail-sg/EditAnything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-EditAnything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/sail-sg/EditAnything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Edit anything in images powered by segment-anything, ControlNet, StableDiffusion, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IDEA-Research/GroundingDINO&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-GroundingDINO 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/IDEA-Research/GroundingDINO&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: The official implementation of &amp;ldquo;Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection&amp;rdquo;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Segment-Everything-Everywhere-All-At-Once 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: We introduce SEEM that can Segment Everything Everywhere with Multi-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/xinyu1205/Recognize_Anything-Tag2Text&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Recognize_Anything-Tag2Text 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/xinyu1205/Recognize_Anything-Tag2Text&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A Strong Image Tagging Model &amp;amp; Tag2Text: Guiding Vision-Language Model via Image Tagging&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;object-detection&#34;&gt;Object Detection&lt;/h2&gt;
&lt;h3 id=&#34;动作识别&#34;&gt;动作识别&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/Detectron&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-Detectron 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Detectron is Facebook AI Research&amp;rsquo;s software system that implements state-of-the-art object detection algorithms, including Mask R-CNN.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/adipandas/multi-object-tracker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-Multi Object Tracker 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Object detection using deep learning and multi-object tracking.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/CMU-Perceptual-Computing-Lab/openpose&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-OpenPose 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/openai/CLIP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021-CLIP 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ultralytics/yolov5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;yolov5 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: This repository represents Ultralytics open-source research into future object detection methods, and incorporates our lessons learned and best practices evolved over training thousands of models on custom client datasets with our previous YOLO repository.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Megvii-BaseDetection/YOLOX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLOX 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with ONNX, TensorRT, ncnn, and OpenVINO supported.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/meituan/YOLOv6&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;YOLOv6 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: a single-stage object detection framework dedicated to industrial applications.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://victordibia.github.io/handtrack.js/#/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Handtrack.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: 它可以让开发人员使用经过训练的手部检测模型快速创建手势交互原型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmdetection&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MMDetection 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/open-mmlab/mmyolo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022-MMYOLO 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: MMYOLO is an open source toolbox for YOLO series algorithms based on PyTorch and MMDetection. It is a part of the OpenMMLab project.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/IDEA-Research/detrex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;detrex 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: IDEA Open Source Toolbox for Transformer Based Object Detection Algorithms&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;人脸检测&#34;&gt;人脸检测&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tehnokv/picojs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-pico.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: a face-detection library in 200 lines of JavaScript&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;face-api.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: JavaScript API for Face Recognition in the Browser with tensorflow.js.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepfakes/faceswap&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Faceswap 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Faceswap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/vipstone/faceai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-faceai 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: 一款入门级的人脸、视频、文字检测以及识别的项目。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/seetafaceengine/SeetaFace2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SeetaFace 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Open source, full stack face recognization toolkit.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/Linzaer/Ultra-Light-Fast-Generic-Face-Detector-1MB&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-超轻量级人脸检测模型 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: 该模型设计是针对边缘计算设备或低算力设备(如用 ARM 推理)设计的实时超轻量级通用人脸检测模型，可以在低算力设备中如用 ARM 进行实时的通用场景的人脸检测推理，同样适用于移动端、PC。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/tg-bomze/Face-Depixelizer&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-Face Depixelizer 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Face Depixelizer based on &amp;ldquo;PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of Generative Models&amp;rdquo; repository.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JDAI-CV/FaceX-Zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2020-FaceX Zoo 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: FaceX-Zoo is a PyTorch toolbox for face recognition. It provides a training module with various supervisory heads and backbones towards state-of-the-art face recognition, as well as a standardized evaluation module which enables to evaluate the models in most of the popular benchmarks just by editing a simple configuration.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;tracker&#34;&gt;Tracker&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ifzhang/ByteTrack&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021-ByteTrack 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: ByteTrack: Multi-Object Tracking by Associating Every Detection Box.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/gaomingqi/Track-Anything&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Track-Anything 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/gaomingqi/Track-Anything&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;video&#34;&gt;Video&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/videoflow/videoflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-videoflow 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Python framework that facilitates the quick development of complex video analysis applications and other series-processing based applications in a multiprocessing environment.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/PeterL1n/RobustVideoMatting&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021-RobustVideoMatting 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Robust Video Matting in PyTorch, TensorFlow, TensorFlow.js, ONNX, CoreML!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/facebookresearch/dinov2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-DINOv2 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/facebookresearch/dinov2&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>FaceRecognition-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/facerecognition-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/facerecognition-list/</guid>
      <description>&lt;h1 id=&#34;face-recognition-人脸识别&#34;&gt;Face Recognition: 人脸识别&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://trackingjs.com/docs.html#introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tracking.js 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Tracking.js 可以展示效果类似 Kinect 或者 Wii 的体感应用，且该 JavaScript 库体积小 (~7k)，非常轻量级，且接口简洁。Tracking.js 能够在移动 Web 应用、桌面应用中工作，甚至可以和基于 Node.js 的服务器进行配对。它会给浏览器带来计算机图形学算法和技术，其拥有功能：脸部识别 ( 某个特定的颜色时或人物 / 脸庞 / 身体出现移动的时候 )、实时色彩跟踪。对于 Web 开发而 言，以前需要通过 C 或 C++ 的技术才能实现类似效果。而现在 Traking.js 提供了一个 Web 组件，因此 Web 前端开发人员可以访问 HTML 标签组件 来实现类似功能，而无需了解 JavaScript，这极大的简化了 Web 开发。Tracking.js 包括一个色彩跟踪算法和对象跟踪组件，它能使 Web 浏览器识别脸部及眼睛的变化。例如，Web 前端还可以对于用这个功能 来设置用户头像，对一些网站而言，这也是个很炫的功能；同时对跟踪的脸部数据和后台数据库进行匹配，从而和反馈给用户更多有用的数据。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>MidJourney-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/midjourney-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/midjourney-list/</guid>
      <description>&lt;h1 id=&#34;midjourney-list&#34;&gt;MidJourney List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/willwulfken/MidJourney-Styles-and-Keywords-Reference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-MidJourney Styles and Keywords Reference&lt;/a&gt;: A reference containing Styles and Keywords that you can use with MidJourney AI. There are also pages showing resolution comparison, image weights, and much more!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://enchanting-trader-463.notion.site/Midjourney-AI-Guide-41eca43809dd4d8fa676e648436fc29c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Midjourney AI Guide&lt;/a&gt;: So let’s begin our journey towards filled with surprises and fun tricks.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Object-Detection-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/object-detection-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/object-detection-list/</guid>
      <description>&lt;h1 id=&#34;object-detection-list&#34;&gt;Object Detection List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/move-cursor-with-tensorflow-3727ed5e2795&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Move Your Cursor with Webcam Using TensorFlow Object Detection API&lt;/a&gt;: TensorMouse is a small open source Python application that allows you to move your cursor by moving a random household object (like a cup, apple or banana) in front of webcam and acts as a replacement for computer mouse or trackpad.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://blog.algorithmia.com/deep-dive-into-object-detection-with-open-images-using-tensorflow/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Deep Dive into Object Detection with Open Images, using TensorFlow&lt;/a&gt;: TensorFlow’s Object Detection API and its ability to handle large volumes of data make it a perfect choice, so let’s jump right in…&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OCR-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/ocr-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/ocr-list/</guid>
      <description>&lt;h1 id=&#34;ocr-list&#34;&gt;OCR List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;&#34;&gt;2017-Creating a Modern OCR Pipeline Using Computer Vision and Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/DmitryUlyanov/deep-image-prior&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-Deep image prior 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Image restoration with neural networks but without learning.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://parg.co/UsP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2017-How to break a CAPTCHA system in 15 minutes with Machine Learning&lt;/a&gt;: Let’s hack the world’s most popular Wordpress CAPTCHA Plug-in.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/karandesai-96/digit-classifier&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;digit-classifier&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://leadtools.gcpowertools.com.cn/orders/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LEADTOOLs&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/garnele007/SwiftOCR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SwiftOCR&lt;/a&gt;: Fast and simple OCR library written in Swift&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://blog.a9t9.com/2015/02/ocr-online-converter-review.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Best Online OCR Software for Converting Images to Text&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.newocr.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NewOCR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.onlineocr.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OnlineOCR&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;http://www.gnu.org/software/ocrad/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ocrad&lt;/a&gt;: is an OCR (Optical Character Recognition) program based on a feature extraction method. It reads images in pbm (bitmap), pgm (greyscale) or ppm (color) formats and produces text in byte (8-bit) or UTF-8 formats.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;overview&#34;&gt;Overview&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/A9T9/OCR-Benchmark&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OCR-Benchmark&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wanghaisheng/awesome-ocr&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;awesome-ocr&lt;/a&gt;: A curated list of promising OCR resources.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;engineering-practices&#34;&gt;Engineering Practices&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://blogs.dropbox.com/tech/2018/10/using-machine-learning-to-index-text-from-billions-of-images/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Dropbox-Using machine learning to index text from billions of images&lt;/a&gt;: So now, when a user searches for English text that appears in one of these files, it will show up in the search results. This blog post describes how we built this feature.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;case-study&#34;&gt;Case Study&lt;/h2&gt;
&lt;h1 id=&#34;resource&#34;&gt;Resource&lt;/h1&gt;
&lt;h2 id=&#34;collection&#34;&gt;Collection&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/OXmWLuZR2mzEz7drn4xGDQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-最全 OCR 相关资料整理 🗃️&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;tutorial&#34;&gt;Tutorial&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://my.oschina.net/zhouxiang/blog/161619&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Java 使用 Tess4J 进行 图片文字识别 笔记&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OpenCV-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/opencv-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/opencv-list/</guid>
      <description>&lt;h1 id=&#34;opencv-list&#34;&gt;OpenCV List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/spmallick/learnopencv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2015-LearnOpenCV 📚&lt;/a&gt;: This repo contains code for Computer Vision, Deep learning, and AI articles shared on our blog LearnOpenCV.com.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>StableDiffusion-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/stablediffusion-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/10.ai/computervision/stablediffusion-list/</guid>
      <description>&lt;h1 id=&#34;stablediffusion-list&#34;&gt;StableDiffusion List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/huggingface/diffusion-models-class&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face Diffusion Models Course 🎥&lt;/a&gt;: Materials for the Hugging Face Diffusion Models Course.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/FurkanGozukara/Stable-Diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FurkanGozukara/Stable-Diffusion #Series# 
















  &lt;img src=&#34;https://img.shields.io/github/stars/FurkanGozukara/Stable-Diffusion&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Best Stable Diffusion and AI Tutorials, Guides, News, Tips and Tricks&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;opensource&#34;&gt;OpenSource&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/divamgupta/diffusionbee-stable-diffusion-ui&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diffusion Bee 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Diffusion Bee is the easiest way to run Stable Diffusion locally on your M1 Mac. Comes with a one-click installer. No dependencies or technical knowledge needed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/AbdBarho/stable-diffusion-webui-docker&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable Diffusion WebUI Docker 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Run Stable Diffusion on your machine with a nice UI without any hassle! This repository provides multiple UIs for you to play around with stable diffusion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.charl-e.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CHARL-E 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: CHARL-E packages Stable Diffusion into a simple app. No complex setup, dependencies, or internet required — just download and say what you want to see.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ahrm/UnstableFusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UnstableFusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A Stable Diffusion desktop frontend with inpainting, img2img and more!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/lkwq007/stablediffusion-infinity&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stablediffusion-infinity 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Outpainting with Stable Diffusion on an infinite canvas.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/divamgupta/diffusionbee-stable-diffusion-ui&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Diffusion Bee 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Diffusion Bee is the easiest way to run Stable Diffusion locally on your M1 Mac. Comes with a one-click installer. No dependencies or technical knowledge needed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/invoke-ai/InvokeAI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InvokeAI 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: This version of Stable Diffusion features a slick WebGUI, an interactive command-line script that combines text2img and img2img functionality in a &amp;ldquo;dream bot&amp;rdquo; style interface, and multiple features and other enhancements. For more info, see the website link below.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/GuyTevet/motion-diffusion-model&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MDM: Human Motion Diffusion Model 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: The official PyTorch implementation of the paper &amp;ldquo;Human Motion Diffusion Model&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/amotile/stable-diffusion-studio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;stable-diffusion-studio 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: An animation focused workflow frontend for Stable Diffusion.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/JingShing/novelai-colab-ver&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;novelai-colab-ver 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: You can use this version to experience how novelai works without a good gpu.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/TheLastBen/fast-stable-diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fast-stable-diffusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: fast-stable-diffusion colabs, +25-50% speed increase + memory efficient + DreamBooth&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/civitai/civitai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-civitai 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/civitai/civitai&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Our goal with this project is to create a platform where people can share their stable diffusion models (textual inversions, hypernetworks, aesthetic gradients, VAEs, and any other crazy stuff people do to customize their AI generations), collaborate with others to improve them, and learn from each other&amp;rsquo;s work. The platform allows users to create an account, upload their models, and browse models that have been shared by others. Users can also leave comments and feedback on each other&amp;rsquo;s models to facilitate collaboration and knowledge sharing.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;showcase&#34;&gt;Showcase&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/KKGo1999/Stable-diffusion-person&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Stable-diffusion-person 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/KKGo1999/Stable-diffusion-person&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: 本文介绍由基于 Stable-diffusion 的 Chilloutmix 模型（以及最新的 ControlNet）生成高清真实人像的方法及 Demo。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/replicate/scribble-diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-scribble-diffusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/replicate/scribble-diffusion&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Turn your rough sketch into a refined image using AI.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/camenduru/stable-diffusion-webui-colab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-stable-diffusion-webui-colab 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/camenduru/stable-diffusion-webui-colab&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: stable diffusion webui colab.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;stablediffusion-implementation&#34;&gt;StableDiffusion Implementation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/axodox/axodox-machinelearning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;axodox/axodox-machinelearning 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/axodox/axodox-machinelearning&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: This repository contains a fully C++ implementation of Stable Diffusion-based image synthesis, including the original txt2img, img2img and inpainting capabilities and the safety checker.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;gui--app&#34;&gt;GUI &amp;amp; APP&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/justjake/Gauss&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022-Gauss 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/justjake/Gauss&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A Stable Diffusion app for macOS built with SwiftUI and Apple&amp;rsquo;s ml-stable-diffusion CoreML models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/comfyanonymous/ComfyUI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-ComfyUI 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/comfyanonymous/ComfyUI&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A powerful and modular stable diffusion GUI with a graph/nodes interface.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/mlc-ai/web-stable-diffusion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-Web Stable Diffusion 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/mlc-ai/web-stable-diffusion&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Bringing stable diffusion models to web browsers. Everything runs inside the browser with no server support.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
