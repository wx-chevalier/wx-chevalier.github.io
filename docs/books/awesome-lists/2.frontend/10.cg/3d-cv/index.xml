<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>3D-CV | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/awesome-lists/2.frontend/10.cg/3d-cv/</link>
      <atom:link href="https://ng-tech.icu/books/awesome-lists/2.frontend/10.cg/3d-cv/index.xml" rel="self" type="application/rss+xml" />
    <description>3D-CV</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>3D-CV</title>
      <link>https://ng-tech.icu/books/awesome-lists/2.frontend/10.cg/3d-cv/</link>
    </image>
    
    <item>
      <title>3D-CV-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/2.frontend/10.cg/3d-cv/3d-cv-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/2.frontend/10.cg/3d-cv/3d-cv-list/</guid>
      <description>&lt;h1 id=&#34;awesome-3d-cv-list&#34;&gt;Awesome 3D CV List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/mint-lab/3dv_tutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-An Invitation to 3D Vision: A Tutorial for Everyone üóÉÔ∏è&lt;/a&gt;: An Invitation to 3D Vision is an introductory tutorial on 3D vision (a.k.a. geometric vision or visual geometry or multi-view geometry). It aims to make beginners understand basic theory of 3D vision and implement their own applications using OpenCV. In addition to tutorial slides, example codes are provided in the purpose of education. They include simple but interesting and practical applications. The example codes are written as short as possible (mostly less than 100 lines) to be clear and easy to understand.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nmwsharp/diffusion-net&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022-DiffusionNet 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/nmwsharp/diffusion-net&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: DiffusionNet is a general-purpose method for deep learning on surfaces such as 3D triangle meshes and point clouds. It is well-suited for tasks like segmentation, classification, feature extraction, etc.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;classification&#34;&gt;Classification&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/charlesq34/pointnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2016-Pointnet 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/charlesq34/pointnet&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Deep Learning on Point Sets for 3D Classification and Segmentation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;mesh-generation&#34;&gt;Mesh Generation&lt;/h1&gt;
&lt;h2 id=&#34;image-to-mesh&#34;&gt;Image To Mesh&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/mlivesu/slice2mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-mlivesu/slice2mesh 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/mlivesu/slice2mesh&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: a Meshing Tool for the Simulation of Additive Manufacturing Processes&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/ThibaultGROUEIX/AtlasNet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-ThibaultGROUEIX/AtlasNet 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/ThibaultGROUEIX/AtlasNet&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: This repository contains the source codes for the paper &amp;ldquo;AtlasNet: A Papier-M√¢ch√© Approach to Learning 3D Surface Generation &amp;ldquo;. The network is able to synthesize a mesh (point cloud + connectivity) from a low-resolution point cloud, or from an image.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/xingyuansun/pix3d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018-Pix3d 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/xingyuansun/pix3d&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Dataset and Methods for Single-Image 3D Shape Modeling.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/nywang16/Pixel2Mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-Pixel2Mesh 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/nywang16/Pixel2Mesh&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Generating 3D Mesh Models from Single RGB Images. In ECCV2018.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/AOT-AG/DicomToMesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021-AOT-AG/DicomToMesh 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/AOT-AG/DicomToMesh&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: A command line tool to transform a DICOM volume into a 3d surface mesh (obj, stl or ply). Several mesh processing routines can be enabled, such as mesh reduction, smoothing or cleaning. Works on Linux, OSX and Windows.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/dunbar12138/pix2pix3D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023-pix2pix3D 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/dunbar12138/pix2pix3D&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: This is the official PyTorch implementation of &amp;ldquo;3D-aware Conditional Image Synthesis&amp;rdquo;. Pix2pix3D synthesizes 3D objects (neural fields) given a 2D label map, such as a segmentation or edge map. We also provide an interactive 3D editing demo.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;text-to-mesh&#34;&gt;Text To Mesh&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/threedle/text2mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2021-text2mesh 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/threedle/text2mesh&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: 3D mesh stylization driven by a text input in PyTorch.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/openai/point-e&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022-Point¬∑E 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/openai/point-e&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Point cloud diffusion for 3D model synthesis&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>NeRF-List</title>
      <link>https://ng-tech.icu/books/awesome-lists/2.frontend/10.cg/3d-cv/nerf-list/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/awesome-lists/2.frontend/10.cg/3d-cv/nerf-list/</guid>
      <description>&lt;h1 id=&#34;nerf-list&#34;&gt;NeRF List&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/megvii-research/introduction-neural-3d-reconstruction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022-introduction-neural-3d-reconstruction #Course#&lt;/a&gt;: Êó∑‰∏ñ Course materials for Introduction to Neural 3D Reconstruction.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;opensource&#34;&gt;OpenSource&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/alicevision/Meshroom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2019-Meshroom 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/alicevision/Meshroom&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Meshroom is a free, open-source 3D Reconstruction Software based on the AliceVision Photogrammetric Computer Vision framework.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/NVlabs/instant-ngp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2022-instant-ngp 
















  &lt;img src=&#34;https://ng-tech.icu/assets/code.svg&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt; 
















  &lt;img src=&#34;https://img.shields.io/github/stars/NVlabs/instant-ngp&#34; style=&#34;max-width: 100px;display: inline-flex;&#34;/&gt;&lt;/a&gt;: Here you will find an implementation of four neural graphics primitives, being neural radiance fields (NeRF), signed distance functions (SDFs), neural images, and neural volumes. In each case, we train and render a MLP with multiresolution hash input encoding using the tiny-cuda-nn framework.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
