<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>调度和驱逐 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/</link><atom:link href="https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/index.xml" rel="self" type="application/rss+xml"/><description>调度和驱逐</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>调度和驱逐</title><link>https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/</link></image><item><title>调度架构</title><link>https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/%E8%B0%83%E5%BA%A6%E6%9E%B6%E6%9E%84/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/%E8%B0%83%E5%BA%A6%E6%9E%B6%E6%9E%84/</guid><description>&lt;h1 id="kubernetes-调度架构">Kubernetes 调度架构&lt;/h1>
&lt;h1 id="调度单元">调度单元&lt;/h1>
&lt;h2 id="pod-调度">Pod 调度&lt;/h2>
&lt;p>当新增一个 Pod 时，集群会在可用的集群节点中寻找最合适的节点来运行相应的容器。Kubernetes 首先会排除无效节点:&lt;/p>
&lt;ul>
&lt;li>节点状态为不可用的，如，节点不通或者 K8s 服务运行异常等；&lt;/li>
&lt;li>节点剩余的 CPU,内存资源不足以运行容器的；&lt;/li>
&lt;li>容器运行时占用的宿主机端口出现冲突的；&lt;/li>
&lt;li>按照节点选择 label 不匹配的；&lt;/li>
&lt;/ul>
&lt;p>Pod.spec.nodeSelector 通过 kubernetes 的 label-selector 机制选择节点，由调度器调度策略匹配 label，而后调度 pod 到目标节点，该匹配规则属于强制约束。&lt;/p>
&lt;p>然后通过打分机制决定将 Pod 具体调度到剩余机器中的那一台，默认调度节点选择策略权重为 1，节点的调度规则是采用 plugin 方式，允许自行编写调度策略进行打分处理。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 标准打分公式&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">score&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">(&lt;/span>权重 * 评价策略分值&lt;span class="o">)&lt;/span> + &lt;span class="o">(&lt;/span>weight1 * priorityFunc1&lt;span class="o">)&lt;/span> + &lt;span class="o">(&lt;/span>weight2 * priorityFunc2&lt;span class="o">)&lt;/span> + ...
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># LeastRequestedPriority&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">score&lt;/span> &lt;span class="o">=&lt;/span> cpu&lt;span class="o">((&lt;/span>capacity - sum&lt;span class="o">(&lt;/span>requested&lt;span class="o">))&lt;/span> * &lt;span class="m">10&lt;/span> / capacity&lt;span class="o">)&lt;/span> + memory&lt;span class="o">((&lt;/span>capacity - sum&lt;span class="o">(&lt;/span>requested&lt;span class="o">))&lt;/span> * &lt;span class="m">10&lt;/span> / capacity&lt;span class="o">)&lt;/span> /2
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># BalanceResourceAllocation&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">score&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">10&lt;/span> -abs &lt;span class="o">(&lt;/span>cpuFraction - memoryFraction&lt;span class="o">)&lt;/span> * &lt;span class="m">10&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">cpuFraction&lt;/span> &lt;span class="o">=&lt;/span> requested / capacity&lt;span class="p">;&lt;/span> &lt;span class="nv">memoryFraction&lt;/span> &lt;span class="o">=&lt;/span> requested / capacity
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># CalculateSpreadPriority&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">score&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="m">10&lt;/span> * &lt;span class="o">((&lt;/span>maxCount -counts&lt;span class="o">)&lt;/span> / &lt;span class="o">(&lt;/span>maxCount&lt;span class="o">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>
&lt;p>LeastRequestedPriority: CPU 可用资源为 100，运行容器申请的资源为 15，则 cpu 分值为 8.5 分，内存可用资源为 100，运行容器申请资源为 20，则内存分支为 8 分。则此评价规则在此节点的分数为(8.5 +8) / 2 = 8.25 分。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>BalanceResourceAllocation: CPU 可用资源为 100，申请 10，则 cpuFraction 为 0.1，而内存可用资源为 20，申请 10，则 memoryFraction 为 0.5，这样由于 CPU 和内存使用不均衡，此节点的得分为 10-abs ( 0.1 - 0.5 ) * 10 = 6 分。假如 CPU 和内存资源比较均衡，例如两者都为 0.5，那么代入公式，则得分为 10 分。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>CalculateSpreadPriority: 一个 web 服务，可能存在 5 个实例，例如当前节点已经分配了 2 个实例了，则本节点的得分为 10 _ ((5-2) / 5) = 6 分，而没有分配实例的节点，则得分为 10 _ ((5-0) / 5) = 10 分。没有分配实例的节点得分越高。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="资源保障">资源保障&lt;/h1>
&lt;p>目前，Kubernetes 只支持 CPU 和 Memory 两种资源的申请，Kubernetes 中，根据应用对资源的诉求不同，把应用的 QoS 按照优先级从高到低分为三大类：Guaranteed, Burstable 和 Best-Effort。三种类别分别表示资源配额必须交付、尽量交付以及不保障。QoS 等级是通过 resources 的 limits 和 requests 参数间接计算出来的。CPU 为可压缩资源，Node 上的所有 Pods 共享 CPU 时间片，原理是通过设置 cpu.cfs_quota_us 和 cpu.cfs_period_us 实现，一个 CPU 逻辑核嘀嗒时间被切了 N 份，只要按照百分比例设置 cpu.cfs_quota_us 的值就可以实现 CPU 时间片的比例分配，如设置 2N 表示利用两个 CPU 逻辑核的时间。Memory 为不可压缩资源，kubernetes 中主要利用 memory.limit_in_bytes 实现内存的限制。当应用内存超过了它的 limits，那么会被系统 OOM。内存是不可压缩资源，它的保障的机制最为复杂，kubernetes 利用内核 oom_score 机制，实现了对 Pod 容器内(进程)内存 oom kill 的优先级管控，内核中 OOM Score 的取值范围是[-1000, 1000]，值越大，被系统 KILL 的概率就越高。&lt;/p>
&lt;h2 id="guaranteed">Guaranteed&lt;/h2>
&lt;p>如果 Pod 中每一个容器都只设置了 limits 参数，或者 同时设置了 limits 和 requests 并且 limits 和 requests 的值一样，那么这个 Pod 就是 Guaranteed 类型的。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">foo&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">//只设置了limits&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">10m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">1Gi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">bar&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">100m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">100Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requests&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">//requests 和 limits均已设定，并且值相同&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">100m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">100Mi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="burstable">Burstable&lt;/h2>
&lt;p>当以下情形设置，Pod 会为定位成 Burstable 类型, Busrtable 类型保障了资源的最小需求，但不会超过&lt;code>limits&lt;/code>。&lt;/p>
&lt;ul>
&lt;li>Pod 里的一个或多个容器只设置了&lt;code>requests&lt;/code>参数。&lt;/li>
&lt;li>Pod 里的一个或多个容器同时设置了&lt;code>requests&lt;/code>和&lt;code>limits&lt;/code>参数，但是两者值不一样。&lt;/li>
&lt;li>Pod 里的所有容器均设置了&lt;code>limits&lt;/code>，但是他们的类型不一样，不如容器 1 只定义了 CPU，容器 2 只定义了 Memory。&lt;/li>
&lt;li>Pod 里存在多个容器时，其中存在容器可被定义为 Bustable 条件的 Pod 也是 Bustable 类型，比如有两个容器，容器 1 设置了&lt;code>limits&lt;/code>，容器 2 没有任何设置。&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">foo&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">memory&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">1Gi&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">bar&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">limits&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">cpu&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">100m&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">duck&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="besteffort">BestEffort&lt;/h2>
&lt;p>当 Pod 中所有的容器均没设置 requests 和 limits，那么这个 Pod 即为 BestEffort 类型，他们可消费所在 Node 上所有资源，但在资源紧张的时候，也是最优先被杀死。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">foo&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">bar&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">resources&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://yq.aliyun.com/articles/689495?utm_content=g_1000040917#" target="_blank" rel="noopener">https://yq.aliyun.com/articles/689495?utm_content=g_1000040917#&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>亲和性调度</title><link>https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/%E4%BA%B2%E5%92%8C%E6%80%A7%E8%B0%83%E5%BA%A6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/%E4%BA%B2%E5%92%8C%E6%80%A7%E8%B0%83%E5%BA%A6/</guid><description>&lt;h1 id="亲和性调度">亲和性调度&lt;/h1>
&lt;p>除了让 K8s 集群调度器自动为 Pod 资源选择某个节点（默认调度考虑的是资源足够，并且 load 尽量平均），有些情况我们希望能更多地控制 Pod 应该如何调度。比如，集群中有些机器的配置更好（SSD，更好的内存等），我们希望比较核心的服务（比如说数据库）运行在上面；或者某两个服务的网络传输很频繁，我们希望它们最好在同一台机器上，或者同一个机房。&lt;/p>
&lt;p>这种调度在 K8s 中分为两类：node affinity 和 Pod affinity。&lt;/p>
&lt;h1 id="选择-node">选择 Node&lt;/h1>
&lt;p>K8s 中有很多对 label 的使用，node 就是其中一例。label 可以让用户非常灵活地管理集群中的资源，service 选择 Pod 就用到了 label。这篇文章介绍到的调度也是如此，可以根据节点的各种不同的特性添加 label，然后在调度的时候选择特定 label 的节点。&lt;/p>
&lt;p>在使用这种方法之前，需要先给 Node 加上 label，通过 kubectl 非常容易做：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">$ kubectl label nodes &amp;lt;Node-name&amp;gt; &amp;lt;label-key&amp;gt;&lt;span class="o">=&lt;/span>&amp;lt;label-value&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>列出 Node 的时候指定 &amp;ndash;show-labels 参数就能查看 Node 都添加了哪些 label：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">$ kubectl get nodes --show-labels
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Node 有了 label，在调度的时候就可以用到这些信息，用法也很简单，在 Pod 的 spec 字段下面加上 nodeSelector，它里面保存的是多个键值对，表示节点上有对应的 label，并且值也匹配。还是举个例子看得明白，比如原来简单的 nginx pod：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Pod&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">env&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">test&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>添加上 Node 选择信息，就变成了下面这样：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Pod&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">labels&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">env&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">test&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">nginx&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">imagePullPolicy&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">IfNotPresent&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">nodeSelector&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">disktype&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">ssd&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这个例子就是告诉 K8s 调度的时候把 Pod 放到有 SSD 磁盘的机器上。除了自己定义的 label 之外，K8s 还会自动给集群中的节点添加一些 label，比如：&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubernetes.io/hostname&lt;/code>：节点的 hostname 名称&lt;/li>
&lt;li>&lt;code>beta.kubernetes.io/os&lt;/code>：节点安装的操作系统&lt;/li>
&lt;li>&lt;code>beta.kubernetes.io/arch&lt;/code>：节点的架构类型&lt;/li>
&lt;li>……&lt;/li>
&lt;/ul>
&lt;p>不同版本添加的 label 会有不同，这些 label 和手动添加的没有区别，可以通过 &lt;code>--show-labels&lt;/code> 查看，也能够用在 &lt;code>nodeSelector&lt;/code> 中。&lt;/p>
&lt;h1 id="node-affinity">Node Affinity&lt;/h1>
&lt;p>Affinity 翻译成中文是“亲和性”，它对应的是 Anti-Affinity，我们翻译成“互斥”。这两个词比较形象，可以把 Pod 选择 Node 的过程类比成磁铁的吸引和互斥，不同的是除了简单的正负极之外，pod 和 Node 的吸引和互斥是可以灵活配置的。&lt;/p>
&lt;p>kubernetes 1.2 版本开始引入这个概念，目前（1.6 版本）处于 beta 阶段，相信后面会变成核心的功能。这种方法比 nodeSelector 复杂，但是也更灵活，提供了更精细的调度控制。它的优点包括：&lt;/p>
&lt;ul>
&lt;li>匹配有更多的逻辑组合，不只是字符的完全相等&lt;/li>
&lt;li>调度分成软策略（soft）和硬策略（hard），在软策略的情况下，如果没有满足调度条件的节点，pod 会忽略这条规则，继续完成调度过程&lt;/li>
&lt;/ul>
&lt;h2 id="策略">策略&lt;/h2>
&lt;p>目前有两种主要的 Node affinity：requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution。前者表示 Pod 必须部署到满足条件的节点上，如果没有满足条件的节点，就不断重试；后者表示优先部署在满足条件的节点上，如果没有满足条件的节点，就忽略这些条件，按照正常逻辑部署。&lt;/p>
&lt;p>IgnoredDuringExecution 正如名字所说，pod 部署之后运行的时候，如果节点标签发生了变化，不再满足 pod 指定的条件，pod 也会继续运行。与之对应的是 requiredDuringSchedulingRequiredDuringExecution，如果运行的 pod 所在节点不再满足条件，kubernetes 会把 pod 从节点中删除，重新选择符合要求的节点。&lt;/p>
&lt;p>软策略和硬策略的区分是有用处的，硬策略适用于 pod 必须运行在某种节点，否则会出现问题的情况，比如集群中节点的架构不同，而运行的服务必须依赖某种架构提供的功能；软策略不同，它适用于满不满足条件都能工作，但是满足条件更好的情况，比如服务最好运行在某个区域，减少网络传输等。这种区分是用户的具体需求决定的，并没有绝对的技术依赖。&lt;/p>
&lt;h2 id="匹配规则">匹配规则&lt;/h2>
&lt;p>Node Affinity 的典型示例如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-yaml" data-lang="yaml">&lt;span class="line">&lt;span class="cl">&lt;span class="nt">apiVersion&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">v1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">kind&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">Pod&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">metadata&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">with-node-affinity&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="nt">spec&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">affinity&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">nodeAffinity&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">requiredDuringSchedulingIgnoredDuringExecution&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">nodeSelectorTerms&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">matchExpressions&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">kubernetes.io/e2e-az-name&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">e2e-az1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">e2e-az2&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">preferredDuringSchedulingIgnoredDuringExecution&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">weight&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="m">1&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">preference&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">matchExpressions&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">key&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">another-node-label-key&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">operator&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">In&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">values&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="l">another-node-label-value&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">containers&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>- &lt;span class="nt">name&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">with-node-affinity&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="nt">image&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="l">gcr.io/google_containers/pause:2.0&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>这个 pod 同时定义了 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution 两种 nodeAffinity。第一个要求 pod 运行在特定 AZ 的节点上，第二个希望节点最好有对应的 another-node-label-key:another-node-label-value 标签。&lt;/p>
&lt;p>这里的匹配逻辑是 label 的值在某个列表中，可选的操作符有：&lt;/p>
&lt;ul>
&lt;li>&lt;code>In&lt;/code>：label 的值在某个列表中&lt;/li>
&lt;li>&lt;code>NotIn&lt;/code>：label 的值不在某个列表中&lt;/li>
&lt;li>&lt;code>Exists&lt;/code>：某个 label 存在&lt;/li>
&lt;li>&lt;code>DoesNotExist&lt;/code>: 某个 label 不存在&lt;/li>
&lt;li>&lt;code>Gt&lt;/code>：label 的值大于某个值（字符串比较）&lt;/li>
&lt;li>&lt;code>Lt&lt;/code>：label 的值小于某个值（字符串比较）&lt;/li>
&lt;/ul>
&lt;p>如果 nodeAffinity 中 nodeSelectorTerms 有多个选项，如果节点满足任何一个条件就可以；如果 matchExpressions 有多个选项，则只有同时满足这些逻辑选项的节点才能运行 pod。&lt;/p>
&lt;h1 id="pod-affinity">Pod Affinity&lt;/h1>
&lt;p>通过上一部分内容的介绍，我们知道怎么在调度的时候让 pod 灵活地选择 node；但有些时候我们希望调度能够考虑 pod 之间的关系，而不只是 pod-node 的关系。pod affinity 是在 kubernetes1.4 版本引入的，目前在 1.6 版本也是 beta 功能。&lt;/p>
&lt;p>举个例子，我们系统服务 A 和服务 B 尽量部署在同个主机、机房、城市，因为它们网络沟通比较多；再比如，我们系统数据服务 C 和数据服务 D 尽量分开，因为如果它们分配到一起，然后主机或者机房出了问题，会导致应用完全不可用，如果它们是分开的，应用虽然有影响，但还是可用的。&lt;/p>
&lt;p>pod affinity 可以这样理解：调度的时候选择（或者不选择）这样的节点 N，这些节点上已经运行了满足条件 X。条件 X 是一组 label 选择器，它必须指明作用的 namespace（也可以作用于所有的 namespace），因为 pod 是运行在某个 namespace 中的。&lt;/p>
&lt;p>和 node affinity 相似，pod affinity 也有 requiredDuringSchedulingIgnoredDuringExecution 和 preferredDuringSchedulingIgnoredDuringExecution，意义也和之前一样。如果有使用亲和性，在 affinity 下面添加 podAffinity 字段，如果要使用互斥性，在 affinity 下面添加 podAntiAffinity 字段。&lt;/p>
&lt;p>下面是一个例子：&lt;/p>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: Pod
metadata:
name: with-pod-affinity
spec:
affinity:
podAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: security
operator: In
values:
- S1
topologyKey: failure-domain.beta.kubernetes.io/zone
podAntiAffinity:
preferredDuringSchedulingIgnoredDuringExecution:
- weight: 100
podAffinityTerm:
labelSelector:
matchExpressions:
- key: security
operator: In
values:
- S2
topologyKey: kubernetes.io/hostname
containers:
- name: with-pod-affinity
image: gcr.io/google_containers/pause:2.0
&lt;/code>&lt;/pre>&lt;p>这个例子中，pod 需要调度到某个 zone（通过 &lt;code>failure-domain.beta.kubernetes.io/zone&lt;/code> 指定），这个 zone 至少有一个节点上运行了这样的 pod：这个 pod 有 &lt;code>security:S1&lt;/code> label。互斥性保证节点最好不要调度到这样的节点，这个节点上运行了某个 pod，而且这个 pod 有 &lt;code>security:S2&lt;/code> label。&lt;/p>
&lt;p>在 &lt;code>labelSelector&lt;/code> 和 &lt;code>topologyKey&lt;/code> 同级，还可以定义 &lt;code>namespaces&lt;/code> 列表，表示匹配哪些 namespace 里面的 pod，默认情况下，会匹配定义的 pod 所在的 namespace；如果定义了这个字段，但是它的值为空，则匹配所有的 namespaces。&lt;/p></description></item><item><title>租户隔离</title><link>https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/%E7%A7%9F%E6%88%B7%E9%9A%94%E7%A6%BB/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/k8s-notes/03.%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5/%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/%E8%B0%83%E5%BA%A6%E5%92%8C%E9%A9%B1%E9%80%90/%E7%A7%9F%E6%88%B7%E9%9A%94%E7%A6%BB/</guid><description>&lt;h1 id="租户隔离">租户隔离&lt;/h1>
&lt;h1 id="基于-kubernetes-的租户隔离">基于 Kubernetes 的租户隔离&lt;/h1>
&lt;p>在规划和实施多租户集群时，我们首先可以利用的是 Kubernetes 自身的资源隔离层，包括集群本身，命名空间，节点，pod 和容器均是不同层次的资源隔离模型。当不同租户的应用负载能够共享相同的资源模型时，就会存在彼此之间的安全隐患。为此，我们需要在实施多租时控制每个租户能够访问到的资源域，同时在资源调度层面尽可能的保证处理敏感信息的容器运行在相对独立的资源节点内；如果出于资源开销的角度，当有来自不同租户的负载共享同一个资源域时，可以通过运行时刻的安全和资源调度控制策略减少跨租户攻击的风险。&lt;/p>
&lt;p>在实施多租户架构时首先需要确定对应的应用场景，包括判断租户内用户和应用负载的可信程度以及对应的安全隔离程度。在此基础上以下几点是安全隔离的基本需求：&lt;/p>
&lt;ul>
&lt;li>开启 Kubernetes 集群的默认安全配置
&lt;ul>
&lt;li>开启 RBAC 鉴权，禁止匿名用户访问&lt;/li>
&lt;li>开启 secrets encryption 能力，增强敏感信息保护&lt;/li>
&lt;li>基于 CIS kubernetes benchmarks 进行相应的安全配置&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>开启 NodeRestriction，AlwaysPullImages, PodSecurityPolicy 等相关 admission controllers&lt;/li>
&lt;li>通过 PSP 限制 pod 部署的特权模式，同时控制其运行时刻 SecurityContext&lt;/li>
&lt;li>配置 NetworkPolicy&lt;/li>
&lt;li>Docker 运行时刻开启 Seccomp/AppArmor/SELinux 配置&lt;/li>
&lt;li>尽量实现监控、日志等服务的多租隔离&lt;/li>
&lt;/ul>
&lt;p>而对于如 SaaS、KaaS 等服务模型下，或者我们无法保证租户内用户的可信程度时，我们需要采取一些更强有力的隔离手段，比如：&lt;/p>
&lt;ul>
&lt;li>使用如 OPA 等动态策略引擎进行网络或 Object 级别的细粒度访问控制&lt;/li>
&lt;li>使用安全容器实现容器运行时刻内核级别的安全隔离&lt;/li>
&lt;li>完备的监控，日志，存储等服务的多租隔离方案&lt;/li>
&lt;/ul>
&lt;h1 id="场景分析">场景分析&lt;/h1>
&lt;h2 id="内部共享集群的软隔离">内部共享集群的软隔离&lt;/h2>
&lt;p>该场景下集群的所有用户均来自企业内部，这也是当前很多 K8s 集群客户的使用模式，因为服务使用者身份的可控性，相对来说这种业务形态的安全风险是相对可控的；我们可以通过命名空间对不同部门或团队进行资源的逻辑隔离，同时定义以下几种角色的业务人员：&lt;/p>
&lt;ul>
&lt;li>集群管理员：具有集群的管理能力（扩缩容、添加节点等操作）、负责为租户管理员创建和分配命名空间、负责各类策略（RAM/RBAC/networkpolicy/quota&amp;hellip;）的 CRUD&lt;/li>
&lt;li>租户管理员：至少具有集群的 RAM 只读权限、管理租户内相关人员的 RBAC 配置&lt;/li>
&lt;li>租户内用户：在租户对应命名空间内使用权限范围内的 K8s 资源&lt;/li>
&lt;/ul>
&lt;p>在建立了基于用户角色的访问控制基础上，我们还需要保证命名空间之间的网络隔离，在不同的命名空间之间只能够允许白名单范围内的跨租户应用请求。另外，对于业务安全等级要求较高的应用场景，我们需要限制应用容器的内核能力，可以配合 seccomp/AppArmor/SELinux 等策略工具达到限制容器运行时刻 capabilities 的目的。当然 Kubernetes 现有的命名空间单层逻辑隔离还不足以满足一部分大型企业应用复杂业务模型对隔离需求，我们可以关注 Virtual Cluster，它通过抽象出更高级别的租户资源模型来实现更精细化的多租管理，以此弥补原生命名空间能力上的不足。&lt;/p>
&lt;h2 id="saas--kaas-服务模型下的多租户">SaaS &amp;amp; KaaS 服务模型下的多租户&lt;/h2>
&lt;p>在 SaaS 多租场景下，kubernetes 集群中的租户对应为 SaaS 平台中各服务应用实例和 SaaS 自身控制平面，该场景下可以将平台各服务应用实例划分到彼此不同的命名空间中。而服务的最终用户是无法与 Kubernetes 的控制平面组件进行交互，这些最终用户能够看到和使用的是 SaaS 自身控制台，他们通过上层定制化的 SaaS 控制平面使用服务或部署业务（如下左图所示）。例如，某博客平台部署在多租户集群上运行。在该场景下，租户是每个客户的博客实例和平台自己的控制平面。平台的控制平面和每个托管博客都将在不同的命名空间中运行。客户将通过平台的界面来创建和删除博客、更新博客软件版本，但无法了解集群的运作方式。&lt;/p>
&lt;p>KaaS 多租场景常见于云服务提供商，该场景下业务平台的服务直接通过 Kubernetes 控制平面暴露给不同租户下的用户，最终用户可以使用 K8s 原生 API 或者服务提供商基于 CRDs/controllers 扩展出的接口。出于隔离的最基本需求，这里不同租户也需要通过命名空间进行访问上的逻辑隔离，同时保证不同租户间网络和资源配额上的隔离。&lt;/p>
&lt;p>与企业内部共享集群不同，这里的最终用户均来自非受信域，他们当中不可避免的存在恶意租户在服务平台上执行恶意代码，因此对于 SaaS/KaaS 服务模型下的多租户集群，我们需要更高标准的安全隔离，而 kubernetes 现有原生能力还不足以满足安全上的需求，为此我们需要如安全容器这样在容器运行时刻内核级别的隔离来强化该业务形态下的租户安全。&lt;/p>
&lt;h1 id="访问控制">访问控制&lt;/h1>
&lt;h2 id="authn--authz--admission">AuthN &amp;amp; AuthZ &amp;amp; Admission&lt;/h2>
&lt;p>ACK 集群的授权分为 RAM 授权和 RBAC 授权两个步骤，其中 RAM 授权作用于集群管理接口的访问控制，包括对集群的 CRUD 权限（如集群可见性、扩缩容、添加节点等操作），而 RBAC 授权用于集群内部 kubernetes 资源模型的访问控制，可以做到指定资源在命名空间粒度的细化授权。&lt;/p>
&lt;p>ACK 授权管理为租户内用户提供了不同级别的预置角色模板，同时支持绑定多个用户自定义的集群角色，此外支持对批量用户的授权。&lt;/p>
&lt;h2 id="networkpolicy">NetworkPolicy&lt;/h2>
&lt;p>NetworkPolicy 可以控制不同租户业务 pod 之间的网络流量，另外可以通过白名单的方式打开跨租户之间的业务访问限制。&lt;/p>
&lt;p>您可以在使用了 Terway 网络插件的容器服务集群上配置 NetworkPolicy，&lt;a href="https://github.com/ahmetb/kubernetes-network-policy-recipes" target="_blank" rel="noopener">这里&lt;/a>可以获得一些策略配置的示例。&lt;/p>
&lt;h2 id="podsecuritypolicy">PodSecurityPolicy&lt;/h2>
&lt;p>PSP 是 K8s 原生的集群维度的资源模型，它可以在创建 pod 请求的 admission 阶段校验其行为是否满足对应 PSP 策略的要求，比如检查 pod 是否使用了 host 的（网络，文件系统，指定端口，PID namespace）等，同时可以限制租户内的用户开启特权（privileged）容器，限制挂盘类型，强制只读挂载等能力；不仅如此，PSP 还可以基于绑定的策略给 pod 添加对应的 SecurityContext，包括容器运行时刻的 uid，gid 和添加或删除的内核 capabilities 等多种设置。&lt;/p>
&lt;p>关于如何开启 PSP admission 和相关策略及权限绑定的使用，可以参阅&lt;a href="https://kubernetes.io/zh/docs/concepts/policy/pod-security-policy/#%e4%bb%80%e4%b9%88%e6%98%af-pod-%e5%ae%89%e5%85%a8%e7%ad%96%e7%95%a5" target="_blank" rel="noopener">这里&lt;/a>&lt;/p>
&lt;h2 id="opa">OPA&lt;/h2>
&lt;p>OPA（Open Policy Agent）是一种功能强大的策略引擎，支持解耦式的 policy decisions 服务并且社区已经有了相对成熟的与 kubernetes 的&lt;a href="https://www.openpolicyagent.org/docs/latest/kubernetes-admission-control" target="_blank" rel="noopener">集成方案&lt;/a>。当现有 RBAC 在命名空间粒度的隔离不能够满足企业应用复杂的安全需求时，可以通过 OPA 提供 object 模型级别的细粒度访问策略控制。&lt;/p>
&lt;p>同时 OPA 支持七层的 NetworkPolicy 策略定义及基于 labels/annotation 的跨命名空间访问控制，可以作为 K8s 原生 NetworkPolicy 的有效增强。&lt;/p>
&lt;h1 id="资源调度">资源调度&lt;/h1>
&lt;h4 id="resource-quotas--limit-range">Resource Quotas &amp;amp; Limit Range&lt;/h4>
&lt;p>在多租户场景下，不同团队或部门共享集群资源，难免会有资源竞争的情况发生，为此我们需要对每个租户的资源使用配额做出限制。其中 ResourceQuota 用于限制租户对应命名空间下所有 pod 占用的总资源 request 和 limit，LimitRange 用来设置租户对应命名空间中部署 pod 的默认资源 request 和 limit 值。另外我们还可以对租户的存储资源配额和对象数量配额进行限制。&lt;/p>
&lt;p>关于资源配额的详细指导可以参见&lt;a href="https://kubernetes.io/zh/docs/concepts/policy/resource-quotas/" target="_blank" rel="noopener">这里&lt;/a>&lt;/p>
&lt;h4 id="pod-prioritypreemption">Pod Priority/Preemption&lt;/h4>
&lt;p>从 1.14 版本开始 pod 的优先级和抢占已经从 beta 成为稳定特性，其中 pod priority 标识了 pod 在 pending 状态的调度队列中等待的优先级；而当节点资源不足等原因造成高优先的 pod 无法被调度时，scheduler 会尝试驱逐低优先级的 pod 来保证高优先级 pod 可以被调度部署。&lt;/p>
&lt;p>在多租户场景下，可以通过优先级和抢占设置确保租户内重要业务应用的可用性；同时 pod priority 可以和 ResouceQuota 配合使用，完成租户在指定优先级下有多少配额的限制。&lt;/p>
&lt;h4 id="dedicated-nodes">Dedicated Nodes&lt;/h4>
&lt;p>恶意租户可以规避由节点污点和容忍机制强制执行的策略。以下说明仅用于企业内部受信任租户集群，或租户无法直接访问 Kubernetes 控制平面的集群。&lt;/p>
&lt;p>通过对集群中的某些节点添加污点，可以将这些节点用于指定几个租户专门使用。在多租户场景下，例如集群中的 GPU 节点可以通过污点的方式保留给业务应用中需要使用到 GPU 的服务团队使用。集群管理员可以通过如 effect: &amp;ldquo;NoSchedule&amp;quot;这样的标签给节点添加污点，同时只有配置了相应容忍设置的 pod 可以被调度到该节点上。&lt;/p>
&lt;p>当然恶意租户可以同样通过给自身 pod 添加同样的容忍配置来访问该节点，因此仅使用节点污点和容忍机制还无法在非受信的多租集群上保证目标节点的独占性。&lt;/p>
&lt;p>关于如何使用节点污点机制来控制调度，请参阅&lt;a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/" target="_blank" rel="noopener">这里&lt;/a>。&lt;/p>
&lt;h2 id="敏感信息保护">敏感信息保护&lt;/h2>
&lt;h3 id="secrets-encryption-at-rest">secrets encryption at REST&lt;/h3>
&lt;p>在多租户集群中不同租户用户共享同一套 etcd 存储，在最终用户可以访问 Kubernetes 控制平面的场景下，我们需要保护 secrets 中的数据，避免在访问控制策略配置不当情况下的敏感信息泄露。为此可以参考 K8s 原生的 secret 加密能力，请参阅&lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/" target="_blank" rel="noopener">这里&lt;/a>。&lt;/p></description></item></channel></rss>