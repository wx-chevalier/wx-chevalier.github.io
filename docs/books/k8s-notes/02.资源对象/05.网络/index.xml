<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>05.网络 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/k8s-notes/02.%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/05.%E7%BD%91%E7%BB%9C/</link><atom:link href="https://ng-tech.icu/books/k8s-notes/02.%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/05.%E7%BD%91%E7%BB%9C/index.xml" rel="self" type="application/rss+xml"/><description>05.网络</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>05.网络</title><link>https://ng-tech.icu/books/k8s-notes/02.%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/05.%E7%BD%91%E7%BB%9C/</link></image><item><title>netfilter</title><link>https://ng-tech.icu/books/k8s-notes/02.%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/05.%E7%BD%91%E7%BB%9C/netfilter/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/k8s-notes/02.%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/05.%E7%BD%91%E7%BB%9C/netfilter/</guid><description>&lt;h1 id="kubernetes-中反向代理的实现">Kubernetes 中反向代理的实现&lt;/h1>
&lt;p>K8S 集群节点实现服务反向代理的方法，目前主要有三种，即 userspace、iptables 以及 ipvs。&lt;/p>
&lt;p>横向来看，节点上的网络环境，被分割成不同的网络命名空间，包括主机网络命名空间和 Pod 网络命名空间；纵向来看，每个网络命名空间包括完整的网络栈，从应用到协议栈，再到网络设备。在网络设备这一层，我们通过 cni0 虚拟网桥，组建出系统内部的一个虚拟局域网。Pod 网络通过 veth 对连接到这个虚拟局域网内。cni0 虚拟局域网通过主机路由以及网口 eth0 与外部通信。在网络协议栈这一层，我们可以通过编程 netfilter 过滤器框架，来实现集群节点的反向代理。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://assets.ng-tech.icu/item/20230430223341.png" alt="主机网络命名空间、POD 网络命名空间" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>实现反向代理，归根结底，就是做 DNAT，即把发送给集群服务 IP 和端口的数据包，修改成发给具体容器组的 IP 和端口。参考 netfilter 过滤器框架的图，我们知道，在 netfilter 里，可以通过在 PREROUTING，OUTPUT 以及 POSTROUGING 三个位置加入 NAT 规则，来改变数据包的源地址或目的地址。&lt;/p>
&lt;p>因为这里需要做的是 DNAT，即改变目的地址，这样的修改，必须在路由（ROUTING）之前发生以保证数据包可以被路由正确处理，所以实现反向代理的规则，需要被加到 PREROUTING 和 OUTPUT 两个位置。&lt;/p>
&lt;p>其中，PREOURTING 的规则，用来处理从 Pod 访问服务的流量。数据包从 Pod 网络 veth 发送到 cni0 之后，进入主机协议栈，首先会经过 netfilter PREROUTING 来做处理，所以发给服务的数据包，会在这个位置做 DNAT。经过 DNAT 处理之后，数据包的目的地址变成另外一个 Pod 的地址，从而经过主机路由，转发到 eth0，发送给正确的集群节点。&lt;/p>
&lt;p>而添加在 OUTPUT 这个位置的 DNAT 规则，则用来处理从主机网络发给服务的数据包，原理也是类似，即经过路由之前，修改目的地址，以方便路由转发。&lt;/p>
&lt;h1 id="netfilter">netfilter&lt;/h1>
&lt;p>为了实现管道和过滤功能两者的解耦，netfilter 用了表这个概念。表就是 netfilter 的过滤中心，其核心功能是过滤方式的分类（表），以及每种过滤方式中，过滤规则的组织（链）。&lt;/p>
&lt;p>把过滤功能和管道解耦之后，所有对数据包的处理，都变成了对表的配置。而管道上的 5 个切口，仅仅变成了流量的出入口，负责把流量发送到过滤中心，并把处理之后的流量沿着管道继续传送下去。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://assets.ng-tech.icu/item/20230430223427.png" alt="MANGLE、NAT、FILTER" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>在表中，netfilter 把规则组织成为链。表中有针对每个管道切口的默认链，也有我们自己加入的自定义链。默认链是数据的入口，默认链可以通过跳转到自定义链来完成一些复杂的功能。这里允许增加自定义链的好处是显然的。为了完成一个复杂过滤功能，比如实现 K8S 集群节点的反向代理，我们可以使用自定义链来模块化我们规则。&lt;/p>
&lt;h1 id="自定义链">自定义链&lt;/h1>
&lt;p>集群服务的反向代理，实际上就是利用自定义链，模块化地实现了数据包的 DNAT 转换。KUBE-SERVICE 是整个反向代理的入口链，其对应所有服务的总入口；KUBE-SVC-XXXX 链是具体某一个服务的入口链，KUBE-SERVICE 链会根据服务 IP，跳转到具体服务的 KUBE-SVC-XXXX 链；而 KUBE-SEP-XXXX 链代表着某一个具体 Pod 的地址和端口，即 endpoint，具体服务链 KUBE-SVC-XXXX 会以一定的负载均衡算法，跳转到 endpoint 链。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://assets.ng-tech.icu/item/20230430223455.png" alt="KUBE-SERVICE" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p></description></item><item><title>容器网络演化</title><link>https://ng-tech.icu/books/k8s-notes/02.%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/05.%E7%BD%91%E7%BB%9C/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%BC%94%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/k8s-notes/02.%E8%B5%84%E6%BA%90%E5%AF%B9%E8%B1%A1/05.%E7%BD%91%E7%BB%9C/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E6%BC%94%E5%8C%96/</guid><description>&lt;h1 id="容器网络的发展与演讲">容器网络的发展与演讲&lt;/h1>
&lt;p>容器技术很火，经常为人所提及，尤其是开源容器工具 Docker，已在不少数据中心里有广泛应用。容器主要是对软件和其依赖环境的标准化打包，将应用之间相互隔离，并能运行在很多主流操作系统上。这样看来容器和虚拟机技术很类似，容器是 APP 层面的隔离，而虚拟化是物理资源层面的隔离，容器解决了虚拟技术的不少痛点问题，很多时候容器可以和虚拟机结合在一起使用，这也是目前数据中心主流的做法。&lt;/p>
&lt;p>除了容器网络安全，如何更好的链接不同 Kubernetes 集群孤岛，如何链接异构容器云平台，这些都是一系列的网络问题需要考虑。容器网络技术也在持续演进，从 Docker 本身的动态端口映射网络模型，到 CNCF 的 CNI 容器网络接口到 Service Mesh+CNI 层次化 SDN。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s1.ax1x.com/2020/10/19/0votgO.png" alt="网络转化" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h1 id="端口映射">端口映射&lt;/h1>
&lt;h2 id="bridge-模式">Bridge 模式&lt;/h2>
&lt;p>Bridge 模式，即 Linux 的网桥模式，Docker 在安装完成后，便会在系统上默认创建一个 Linux 网桥，名称为 docker0 并为其分配一个子网，针对有 Docker 创建的每一个容器，均为其创建一个虚拟的以太网设备（veth peer）。其中一端关联到网桥上，另一端映射到容器类的网络空间中。然后从这个虚拟网段中分配一个 IP 地址给这个接口。其网络模型如下：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s1.ax1x.com/2020/10/19/0vor5t.png" alt="网络模型" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="host-模式">Host 模式&lt;/h2>
&lt;p>Host 模式，即共用主机的网络，它的网络命名空间和主机是同一个，使用宿主机 Namespace、IP 和端口。&lt;/p>
&lt;h2 id="container-模式">Container 模式&lt;/h2>
&lt;p>Container 模式，使用已经存在容器的网络的 Namespace，相当于多个容器使用同一个网络协议栈，Kubernetes 中的 Pod 中多个容器之间的网络和存储的贡献就是使用这种模式。&lt;/p>
&lt;h2 id="none-模式">None 模式&lt;/h2>
&lt;p>None 模式，在容器创建时，不指定任何网络模式。由用户自己在适当的时候去指定。&lt;/p>
&lt;h1 id="容器网络接口">容器网络接口&lt;/h1>
&lt;p>CNI（Container Network Interface）是 Google 和 CoreOS 主导制定的容器网络标准，它是在 RKT 网络提议 的基础上发展起来的，综合考虑了灵活性、扩展性、IP 分配、多网卡等因素。CNI 旨在为容器平台提供网络的标准化。不同的容器平台（比如目前的 Kubernetes、Mesos 和 RKT）能够通过相同的接口调用不同的网络组件。简单来说，容器 runtime 为容器提供 network namespace，网络插件负责将 network interface 插入该 network namespace 中并且在宿主机做一些必要的配置，最后对 namespace 中的 interface 进行 IP 和路由的配置。&lt;/p>
&lt;p>这个协议连接了两个组件：容器管理系统和网络插件，具体的事情都是插件来实现的，包括：创建容器网络空间（network namespace）、把网络接口（interface）放到对应的网络空间、给网络接口分配 IP 等。目前采用 CNI 提供的方案一般分为两种，隧道方案和路由方案。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s1.ax1x.com/2020/10/19/0vTgF1.md.png" alt="各种网络方案比较" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="callico">Callico&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s1.ax1x.com/2020/10/19/0vTuJP.png" alt="Callico Network" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Callico 容器网络和其他虚拟网络最大的不同是，它没有采用 Overlay 网络做报文转发，提供了纯三层网络模型。三层通信模型表示每个容器都通过 IP 直接通信，要想路由工作能够正常，每个容器所在的主机节点必须有某种方法知道整个集群的路由信息，Callico 采用 BGP 路由协议，使得全网所有的 Node 和网络设备都记录到全网路由，然而这种方式会产生很多的无效路由，对网络设备路由规格要求较大，整网不能有路由规格低的设备。另外，Callico 实现了从源容器经过源宿主机，经过数据中心路由，然后到达目的宿主机，最后分配到目的容器，整个过程中始终都是根据 BGP 协议进行路由转发，并没有进行封包，解包过程，这样转发效率就会快得多，这是 Callico 容器网络的技术优势。&lt;/p>
&lt;h2 id="flannel">Flannel&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s1.ax1x.com/2020/10/19/0vT1sg.png" alt="Flannel Network" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Flannel 是 CoreOS 提出用于解决容器集群跨主机通讯的网络解决方案。Flannel 实质上是一种覆盖网络 Overlay network，也就是将 TCP 数据包装在另一种网络包里面进行路由转发和通信，目前已支持 UDP、VXLAN、AWS VPC、GCE 路由等数据转发方式，其中以 VXLAN 技术最为流行，很多数据中心在考虑引入容器时，也考虑将网络切换到 Flannel 的 VXLAN 网络中来。Flannel 为每个主机分配一个 subnet，容器从此 subnet 中分配 IP，这些 IP 可在主机间路由，容器间无需 NAT 和端口映射就可以跨主机通讯。Flannel 让集群中不同节点主机创建容器时都具有全集群唯一虚拟 IP 地址，并连通主机节点网络。&lt;/p>
&lt;p>Flannel 可为集群中所有节点重新规划 IP 地址使用规则，从而使得不同节点上的容器能够获得“同属一个内网”且“不重复的”的 IP 地址，让不同节点上的容器能够直接通过内网 IP 通信，网络封装部分对容器是不可见的。源主机服务将原本数据内容 UDP 封装后根据自己的路由表投递给目的节点，数据到达以后被解包，然后直接进入目的节点虚拟网卡，然后直接达到目的主机容器虚拟网卡，实现网络通信目的。Flannel 虽然对网络要求较高，要引入封装技术，转发效率也受到影响，但是却可以平滑过渡到 SDN 网络，VXLAN 技术可以和 SDN 很好地结合起来，值得整个网络实现自动化部署，智能化运维和管理，较适合于新建数据中心网络部署。&lt;/p>
&lt;h2 id="weave">Weave&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s1.ax1x.com/2020/10/19/0vTDL4.png" alt="Weave" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Weave 实质上也是覆盖网络，Weave 可以把不同主机上容器互相连接的网络虚拟成一个类似于本地网络的网络，不同主机之间都使用自己的私有 IP 地址，当容器分布在多个不同的主机上时，通过 Weave 可以简化这些容器之间的通信。Weave 网络中的容器使用标准的端口提供服务（如 MySQL 默认使用 3306），管理微服务是十分直接简单的。每个容器都可以通过域名来与另外的容器通信，也可以直接通信而无需使用 NAT，也不需要使用端口映射或者复杂的联接。部署 Weave 容器网络最大的好处是无需修改你的应用代码。&lt;/p>
&lt;p>Weave 通过在容器集群的每个主机上启动虚拟路由器，将主机作为路由器，形成互联互通的网络拓扑，在此基础上，实现容器的跨主机通信。要部署 Weave 需要确保主机 Linux 内核版本在 3.8 以上，Docker 1.10 以上，主机间访问如果有防火墙，则防火墙必须彼此放行 TCP 6783 和 UDP 6783/6784 这些端口号，这些是 Weave 控制和数据端口，主机名不能相同，Weave 要通过主机名识别子网。Weave 网络类似于主机 Overlay 技术，直接在主机上进行报文流量的封装，从而实现主机到主机的跨 Underlay 三层网络的互访，这是和 Flannel 网络的最大区别，Flannel 是一种网络 Overlay 方案。&lt;/p>
&lt;h2 id="macvlan-网络方案">Macvlan 网络方案&lt;/h2>
&lt;p>Macvlan 是 Linux kernel 比较新的特性，允许在主机的一个网络接口上配置多个虚拟的网络接口，这些网络 interface 有自己独立的 MAC 地址，也可以配置上 IP 地址进行通信。Macvlan 下的虚拟机或者容器网络和主机在同一个网段中，共享同一个广播域。Macvlan 和 Bridge 比较相似，但因为它省去了 Bridge 的存在，所以配置和调试起来比较简单，而且效率也相对高。除此之外，Macvlan 自身也完美支持 VLAN。&lt;/p>
&lt;p>不同的容器网络方案，适用于不同的应用场景，就看企业如何选择了，从难易度上来讲，Callico 最简单，其次 Flannel，Weave 最复杂，从网络技术来看，Weave 和 Flannel 都是网络封装技术，区别在于封装的位置在网络设备上还是主机上。&lt;/p>
&lt;h1 id="服务网格-cni">服务网格 CNI&lt;/h1>
&lt;p>服务网格是当下的一个热点，业界有些声音说服务网格是下一代 SDN，但是 Service Mesh 并不能替代 CNI，与 CNI 一起提供层次化微服务应用所需要的网络服务。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s1.ax1x.com/2020/10/19/0vThQO.png" alt="服务网格 CNI" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>CNI 需要交付给容器云 L2-4 层细化至微服务内部的每个 Pod 容器应用终端交付所需要的 L2 网络连接，L3 路由，L2-4 层安全隔离，容器云整体安全，负载均衡，etc……虽然现有的 CNI 仅提供非常有限的功能，此部分可通过后边介绍的 NSX Datacenter 完整交付。&lt;/p>
&lt;p>Service Mesh 更多的致力于微服务应用层面的服务治理，致力于 L5-7 层网络服务，服务网格在每一个应用容器前部署一个 Sidecar Envoy 应用代理，提供微服务间的智能路由，分布式负载均衡，流量管理，蓝绿，金丝雀发布，微服务弹性，限流熔断，超时重试，微服务间的可视化，安全等等。但是 Service Mesh 并不会替代 CNI，他们工作在不同的 SDN 层次，CNI 更多工作在 L2-4 层，Mesh 在 5-7 层 application SDN。Mesh 不能独立于 CNI 部署。根据 Gartner 报告指出，在 2020 年，几乎 100%容器云都将内置 Service mesh 技术。而目前开源的 Istio Service Mesh 仅提供单一 Kubernetes 集群内部微服务治理，缺失异构容器云，跨云能力。&lt;/p>
&lt;p>从物理机到虚拟机，再到容器，这是服务器虚拟化技术发展的必然趋势，容器解决虚拟机的使用限制，但也将网络引入更复杂的境地，数据中心网络要去适应这种变化，要去适配容器，所以才出现了百花齐放的容器网络方案，这些方案都是为容器而生，从网络层面去适配容器。希望本文能作为一个引子，让业界的同仁能更好在容器化的浪潮下更好的劈波斩浪。&lt;/p></description></item></channel></rss>