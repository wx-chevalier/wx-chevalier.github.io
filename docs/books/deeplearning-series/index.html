<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="Next-gen Tech Edu"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/deeplearning-series/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=alternate href=/books/deeplearning-series/index.xml type=application/rss+xml title="Next-gen Tech Edu"><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/deeplearning-series/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/deeplearning-series/"><meta property="og:title" content="DeepLearning-Series | Next-gen Tech Edu"><meta property="og:description" content="Next-gen Tech Edu"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>DeepLearning-Series | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=eb806282837630457ae117be4d307809><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Books</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idd3e29c108296daf2010433becac7f12e")' href=#idd3e29c108296daf2010433becac7f12e aria-expanded=false aria-controls=idd3e29c108296daf2010433becac7f12e aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link active" href=/books/deeplearning-series/>DeepLearning-Series</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idd3e29c108296daf2010433becac7f12e aria-expanded=false aria-controls=idd3e29c108296daf2010433becac7f12e><i class="fa-solid fa-angle-right" id=caret-idd3e29c108296daf2010433becac7f12e></i></a></div><ul class="nav docs-sidenav collapse show" id=idd3e29c108296daf2010433becac7f12e><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id44563bf40eebfee05364d37322728dda")' href=#id44563bf40eebfee05364d37322728dda aria-expanded=false aria-controls=id44563bf40eebfee05364d37322728dda aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/0.%E5%AF%BC%E8%AE%BA/>0.导论</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id44563bf40eebfee05364d37322728dda aria-expanded=false aria-controls=id44563bf40eebfee05364d37322728dda><i class="fa-solid fa-angle-right" id=caret-id44563bf40eebfee05364d37322728dda></i></a></div><ul class="nav docs-sidenav collapse" id=id44563bf40eebfee05364d37322728dda><li class="child level"><a href=/books/deeplearning-series/0.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8F%B2/>深度学习简史</a></li><li class="child level"><a href=/books/deeplearning-series/0.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%89%8D%E6%B2%BF/>深度学习前沿</a></li><li class="child level"><a href=/books/deeplearning-series/0.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/>深度学习算法</a></li></ul></div><li class="child level"><a href=/books/deeplearning-series/introduction/>INTRODUCTION</a></li><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id4a98f8c933a585f6327329c359469a5e")' href=#id4a98f8c933a585f6327329c359469a5e aria-expanded=false aria-controls=id4a98f8c933a585f6327329c359469a5e aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/>计算机视觉</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id4a98f8c933a585f6327329c359469a5e aria-expanded=false aria-controls=id4a98f8c933a585f6327329c359469a5e><i class="fa-solid fa-angle-right" id=caret-id4a98f8c933a585f6327329c359469a5e></i></a></div><ul class="nav docs-sidenav collapse" id=id4a98f8c933a585f6327329c359469a5e><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id2b499c83144bd9a9a4f25fab5049b596")' href=#id2b499c83144bd9a9a4f25fab5049b596 aria-expanded=false aria-controls=id2b499c83144bd9a9a4f25fab5049b596 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/scikit-image/>scikit-image</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id2b499c83144bd9a9a4f25fab5049b596 aria-expanded=false aria-controls=id2b499c83144bd9a9a4f25fab5049b596><i class="fa-solid fa-angle-right" id=caret-id2b499c83144bd9a9a4f25fab5049b596></i></a></div><ul class="nav docs-sidenav collapse" id=id2b499c83144bd9a9a4f25fab5049b596><li class="child level"><a href=/books/deeplearning-series/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/scikit-image/%E7%BB%98%E5%88%B6%E4%B8%8E%E8%BD%AC%E5%8C%96/>绘制与转化</a></li><li class="child level"><a href=/books/deeplearning-series/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/scikit-image/%E5%9F%BA%E7%A1%80%E8%AF%BB%E5%86%99/>基础读写</a></li><li class="child level"><a href=/books/deeplearning-series/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/scikit-image/%E5%83%8F%E7%B4%A0%E5%A4%84%E7%90%86/>像素处理</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id2860e7359ada6d2715d255e69fd5c22b")' href=#id2860e7359ada6d2715d255e69fd5c22b aria-expanded=false aria-controls=id2860e7359ada6d2715d255e69fd5c22b aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id28bdeb6657cb1cc12228f529ff04f74a")' href=#id28bdeb6657cb1cc12228f529ff04f74a aria-expanded=false aria-controls=id28bdeb6657cb1cc12228f529ff04f74a aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%9F%BA%E7%A1%80/>图形学基础</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id28bdeb6657cb1cc12228f529ff04f74a aria-expanded=false aria-controls=id28bdeb6657cb1cc12228f529ff04f74a><i class="fa-solid fa-angle-right" id=caret-id28bdeb6657cb1cc12228f529ff04f74a></i></a></div><ul class="nav docs-sidenav collapse" id=id28bdeb6657cb1cc12228f529ff04f74a><li class="child level"><a href=/books/deeplearning-series/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%9F%BA%E7%A1%80/%E5%9B%BE%E5%83%8F%E6%BB%A4%E6%B3%A2/>图像滤波</a></li><li class="child level"><a href=/books/deeplearning-series/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%9F%BA%E7%A1%80/%E4%BF%A1%E5%8F%B7%E4%B8%8E%E5%99%AA%E5%A3%B0/>信号与噪声</a></li></ul></div></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idf009b2e2dbfc4b1fff4ea09673337d72")' href=#idf009b2e2dbfc4b1fff4ea09673337d72 aria-expanded=false aria-controls=idf009b2e2dbfc4b1fff4ea09673337d72 aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id78ceec2c97f178409c30b667330767f1")' href=#id78ceec2c97f178409c30b667330767f1 aria-expanded=false aria-controls=id78ceec2c97f178409c30b667330767f1 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>神经网络</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id78ceec2c97f178409c30b667330767f1 aria-expanded=false aria-controls=id78ceec2c97f178409c30b667330767f1><i class="fa-solid fa-angle-right" id=caret-id78ceec2c97f178409c30b667330767f1></i></a></div><ul class="nav docs-sidenav collapse" id=id78ceec2c97f178409c30b667330767f1><li class="child level"><a href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E4%B8%A2%E5%BC%83%E6%B3%95/>丢弃法</a></li><li class="child level"><a href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>多层神经网络</a></li><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id1becb15ad4ce75514287d48260839f8e")' href=#id1becb15ad4ce75514287d48260839f8e aria-expanded=false aria-controls=id1becb15ad4ce75514287d48260839f8e aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/>反向传播</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id1becb15ad4ce75514287d48260839f8e aria-expanded=false aria-controls=id1becb15ad4ce75514287d48260839f8e><i class="fa-solid fa-angle-right" id=caret-id1becb15ad4ce75514287d48260839f8e></i></a></div><ul class="nav docs-sidenav collapse" id=id1becb15ad4ce75514287d48260839f8e><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id48930edff5cabd2591b152750fa3aa12")' href=#id48930edff5cabd2591b152750fa3aa12 aria-expanded=false aria-controls=id48930edff5cabd2591b152750fa3aa12 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/>999.参考资料</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id48930edff5cabd2591b152750fa3aa12 aria-expanded=false aria-controls=id48930edff5cabd2591b152750fa3aa12><i class="fa-solid fa-angle-right" id=caret-id48930edff5cabd2591b152750fa3aa12></i></a></div><ul class="nav docs-sidenav collapse" id=id48930edff5cabd2591b152750fa3aa12><li class="child level"><a href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/a-step-by-step-backpropagation-example/>A Step by Step Backpropagation Example</a></li></ul></div></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id5dedd3766cf580baa69ce8b3894bfcea")' href=#id5dedd3766cf580baa69ce8b3894bfcea aria-expanded=false aria-controls=id5dedd3766cf580baa69ce8b3894bfcea aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/>激活函数</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id5dedd3766cf580baa69ce8b3894bfcea aria-expanded=false aria-controls=id5dedd3766cf580baa69ce8b3894bfcea><i class="fa-solid fa-angle-right" id=caret-id5dedd3766cf580baa69ce8b3894bfcea></i></a></div><ul class="nav docs-sidenav collapse" id=id5dedd3766cf580baa69ce8b3894bfcea><li class="child level"><a href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu/>ReLU</a></li><li class="child level"><a href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid/>sigmoid</a></li><li class="child level"><a href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh/>tanh</a></li></ul></div><li class="child level"><a href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%AF%E8%A7%86%E5%8C%96/>神经网络可视化</a></li><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id4966b5631ae380c1cf8906d2c0b0a665")' href=#id4966b5631ae380c1cf8906d2c0b0a665 aria-expanded=false aria-controls=id4966b5631ae380c1cf8906d2c0b0a665 aria-hidden=false data-toggle=collapse></div></div><li class="child level"><a href=/books/deeplearning-series/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/>正向传播与反向传播</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-ide09a003936d086238a682fedf8b5b034")' href=#ide09a003936d086238a682fedf8b5b034 aria-expanded=false aria-controls=ide09a003936d086238a682fedf8b5b034 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-series/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/>生成模型</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#ide09a003936d086238a682fedf8b5b034 aria-expanded=false aria-controls=ide09a003936d086238a682fedf8b5b034><i class="fa-solid fa-angle-right" id=caret-ide09a003936d086238a682fedf8b5b034></i></a></div><ul class="nav docs-sidenav collapse" id=ide09a003936d086238a682fedf8b5b034><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id65bf48d2d27545aa2e1d9c3108fd31ef")' href=#id65bf48d2d27545aa2e1d9c3108fd31ef aria-expanded=false aria-controls=id65bf48d2d27545aa2e1d9c3108fd31ef aria-hidden=false data-toggle=collapse></div></div></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id010cb57ce2cd0bb81e8d2bb2c39a4b57")' href=#id010cb57ce2cd0bb81e8d2bb2c39a4b57 aria-expanded=false aria-controls=id010cb57ce2cd0bb81e8d2bb2c39a4b57 aria-hidden=false data-toggle=collapse></div></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#lstm--长短期记忆网络>LSTM | 长短期记忆网络</a></li></ul><ul><li><a href=#textcnn--文本卷积网络>TextCNN | 文本卷积网络</a></li><li><a href=#transformer>Transformer</a></li><li><a href=#bert>BERT</a></li></ul><ul><li><a href=#contributing>Contributing</a></li><li><a href=#acknowledgements>Acknowledgements</a></li><li><a href=#copyright--more--延伸阅读>Copyright & More | 延伸阅读</a></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>DeepLearning-Series</h1><div class=article-style><p><a href=https://github.com/wx-chevalier/DeepLearning-Series/graphs/contributors target=_blank rel=noopener><img src="https://img.shields.io/github/contributors/wx-chevalier/DeepLearning-Series.svg?style=flat-square" style=max-width:100px;display:inline-flex></a>
<a href=https://github.com/wx-chevalier/DeepLearning-Series/network/members target=_blank rel=noopener><img src="https://img.shields.io/github/forks/wx-chevalier/DeepLearning-Series.svg?style=flat-square" style=max-width:100px;display:inline-flex></a>
<a href=https://github.com/wx-chevalier/DeepLearning-Series/stargazers target=_blank rel=noopener><img src="https://img.shields.io/github/stars/wx-chevalier/DeepLearning-Series.svg?style=flat-square" style=max-width:100px;display:inline-flex></a>
<a href=https://github.com/wx-chevalier/DeepLearning-Series/issues target=_blank rel=noopener><img src="https://img.shields.io/github/issues/wx-chevalier/DeepLearning-Series.svg?style=flat-square" style=max-width:100px;display:inline-flex></a>
<a href=https://github.com/wx-chevalier/DeepLearning-Series/blob/master/LICENSE.txt target=_blank rel=noopener><img src=https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey.svg style=max-width:100px;display:inline-flex></a></p><br><p align=center><a href=https://github.com/wx-chevalier/DeepLearning-Series><img src=https://assets.ng-tech.icu/item/header.svg alt=Logo style=width:100vw;height:400px></a><p align=center><a href=https://ng-tech.icu/books/DeepLearning-Series><strong>在线阅读 >></strong></a><br><br><a href=https://github.com/wx-chevalier/Awesome-CheatSheets>速览手册</a>
·
<a href=./examples>代码案例</a>
·
<a href=https://github.com/wx-chevalier/Awesome-Lists>参考资料</a>
·
<a href=./README.en.md>English Version</a></p></p><h1 id=深度学习实战>深度学习实战</h1><p>在深度学习篇中，我们将了解深度学习的历史与理论。深度学习的起点即是所谓的神经网络（Neural Network）。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/pLpXL4pY/image.png alt=mindmap loading=lazy data-zoomable></div></div></figure></p><h1 id=nn--神经网络基础>NN | 神经网络基础</h1><p>神经网络层的基本组成成员为神经元，神经元包含两部分，一部分是上一层网络输出和当前网络层参数的一个线性乘积，另外一部分是线性乘积的非线性转换；如果缺少非线性转换，则多层线性乘积可以转化为一层的线性乘积。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/d3NJmvtq/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>$$
\begin{array}{l}{\mathrm{H}(\mathrm{in})=\Sigma=\mathrm{W}<em>{1} \star \mathrm{x}</em>{1}+\mathrm{W}<em>{2} \star \mathrm{x}</em>{2}+\mathrm{W}<em>{3} \star \mathrm{x}</em>{3}+\mathrm{b}} \ {\mathrm{H}(\mathrm{out})=\sigma(\Sigma)}\end{array}
$$</p><p>一个神经网络结构通常包含输入层，隐藏层，输出层。输入层是我们的 特征（features），输出层是我们的预测（prediction）。神经网络的目的是拟合一个函数 $f*: features -> prediction$，在训练期间，通过减小 prediction 和 实际 label 的差异的这种方式, 来更改网络参数，来使当前的网络能逼近于理想的函数 $f*$。而对于仅有单个隐藏层的神经网络，我们称为浅层神经网络：</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/fWqsPzDs/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>相对于浅层网络结构，有两层，三层及以上隐藏层的我们就可以称为深度网络：</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/wTJxdZ63/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>在通常的理解中，一个足够宽的网络，是能够拟合任何函数的。而一个深层网络，则能够用更少的参数来拟合该函数，因为深层的神经元可以获取比浅层神经元更复杂的特征表示。</p><p>而根据网络层级间不同的反馈方式，我们又可以将其区分为前馈神经网络、反馈神经网络、双向神经网络以及循环神经网络等：</p><h1 id=cnn--卷积网络>CNN | 卷积网络</h1><p>前文中提及的神经网络，不同层间的神经元之间两两皆有关联，隐藏层的神经元会和上一层所有的神经元输出相关。和全连接网络相对应的，是只和上一层部分神经元输出连接的网络，如卷积网络。</p><p>卷积网络神经元只和上一层的部分神经元输出是连接的，在直觉上，是因为人的视觉神经元触突只对局部信息敏感，而不是全局所有信息都对同一个触突产生等价作用。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/hj76dFZd/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>同一个卷积核从左到右，从上到下和输入做乘积，得到了不同强度的输出。从直觉上来理解，卷积核对原始数据的不同数据分布的敏感度是不一样的。如果把卷积核理解为是某种模式, 那么符合这种模式 的数据分布会得到比较强的输出，而不符合这种模式 的输出则得到弱的，甚至是不输出。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/D0kDQCRY/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>一个卷积核是一个模式提取器，多个卷积核就是多个模式提取器。通过多个特征提取器对原始数据做特征提取转换，就构成了一层卷积。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/pT8cdfyQ/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>Alex Net, 因为 GPU 内存的原因，Alex 使用了两块 GPU 对模型做了切割，本质上的卷积层是用于特征提取，最大池化层用于提取强特征及减少参数，全连接层则是所有高级特征参与到最后分类决策中去。</p><h1 id=rnn--循环神经网络>RNN | 循环神经网络</h1><p>CNN 是对空间上特征的提取，RNN 则是对时序上特征的提取。在直觉上，我们理解 RNN 网络是一个可模拟任何函数的一个神经网络(action)，加上同时有一份自己的历史存储(memory)，action+memory 两者让 RNN 成为了一个图灵机器。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/prHG5mnw/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>在 RNN 中，$x_1$, $x_2$, $x_3$, $x_t$ 是在时序上不一样的输入，而 $V$，$U$，$W$ 三个矩阵则是共享。同时 RNN 网络中保存了自己的状态 $S$。$S$ 随着输入而改变，不同的输入/不同时刻的输入或多或少影响 RNN 网络的状态 $S$。而 RNN 网络的状态 $S$ 则决定最后的输出。</p><h2 id=lstm--长短期记忆网络>LSTM | 长短期记忆网络</h2><p>RNN 的问题是非线性操作 $σ$ 的存在且每一步间通过连乘操作传递，会导致长序列历史信息不能很好的传递到最后，而有了 LSTM 网络。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/dV1WCPLV/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>在 LSTM Cell 中，包含了通常意义上的遗忘门（点乘，决定什么要从状态中去除），输入更新门（按位相加，决定什么要添加到状态中去），输出门（点乘，决定状态的输出是什么）。</p><p>LSTM 本质上是矩阵的运算，后续 LSTM 的变种 GRU 如下：</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/9QnLt87t/image.png alt loading=lazy data-zoomable></div></div></figure></p><h1 id=nlp--自然语言处理>NLP | 自然语言处理</h1><h2 id=textcnn--文本卷积网络>TextCNN | 文本卷积网络</h2><p>CNN 在计算机识别领域中应用广泛，其捕捉局部特征的能力非常强，为分析和利用图像数据的研究者提供了极大的帮助。TextCNN 是 2014 年 Kim 在 EMNLP 上提出将 CNN 应用于 NLP 的文本分类任务中。</p><p>从直观上理解，TextCNN 通过一维卷积来获取句子中 n-gram 的特征表示。TextCNN 对文本浅层特征的抽取能力很强，在短文本领域如搜索、对话领域专注于意图分类时效果很好，应用广泛，且速度快，一般是首选；对长文本领域，TextCNN 主要靠 filter 窗口抽取特征，在长距离建模方面能力受限，且对语序不敏感。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/W3MSVsfX/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>文本卷积与图像卷积的不同之处在于只在文本序列的一个方向做卷积。对句子单词每个可能的窗口做卷积操作得到特征图(feature map)。</p><p>$$
\mathrm{c}=\left[\mathrm{c}<em>{1}, \mathrm{c}</em>{2}, \ldots, \mathrm{c}_{\mathrm{n}-\mathrm{h}+1}\right]
$$</p><p>其中 $\mathrm{c} \in \mathrm{R}^{n-h+1}$，对 feature map 做最大池化(max-pooling)操作，取中最大值 <code>max{c}</code> 作为 filter 提取出的 feature。通过选择每个 feature map 的最大值，可捕获其最重要的特征。每个 filter 卷积核产生一个 feature,一个 TextCNN 网络包括很多不同窗口大小的卷积核，如常用的 <code>filter size ∈ {3,4,5}</code> 每个 filter 的 feature maps=100。</p><h2 id=transformer>Transformer</h2><p>Attention 此前就被用于众多 NLP 的任务，用于定位关键 token 或者特征，比如在文本分类的最后加一层 Attention 来提高性能。Transformer 起源自注意力机制（Attention），完全抛弃了传统的 RNN，整个网络结构完全是由 Attention 机制组成。Transformer 可以通过堆叠 Transformer Layer 进行搭建，作者的实验是通过搭建编码器和解码器各 6 层，总共 12 层的 Encoder-Decoder，并在机器翻译中取得了 BLEU 值的新高。</p><p>以 N=2 示例，Encoder 阶段：输入“Thinking Machines”，对应词向量,叠加位置向量 Positional Encoding，对每个位置做 Self-Attention 得到; Add&Norm 分两步,residual connection 即,layer Normalization 得到新的,对每个位置分别做 feed forward 全连接和 Add&Norm，得到一个 Encoder Layer 的输出，重复堆叠 2 次，最后将 Encoder Layer 输出到 Decoder 的 Encoder-Decoder Layer 层。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/9f9YPWVC/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>Decoder 阶段：先是对 Decoder 的输入做 Masked Self-Attention Layer,然后将 Encoder 阶段的输出与 Decoder 第一级的输出做 Encoder-Decoder Attention,最后接 FFN 全连接，堆叠 2 个 Decoder,最后接全连接+Softmax 输出当前位置概率最大的的词。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/Ss7L5DJP/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>Transformer 的优点：</p><ul><li><p>并行计算, 提高训练速度。这是相比 LSTM 很大的突破，LSTM 在训练的时候, 当前步的计算要依赖于上一步的隐状态, 这是一个连续过程, 每次计算都需要等之前的计算完成才能展开，限制模型并行能力。而 Transformer 不用 LSTM 结构, Attention 机制的每一步计算只是依赖上一层的输出，并不依赖上一词的信息，因而词与词之间是可以并行的，从而训练时可以并行计算, 提高训练速度。</p></li><li><p>一步到位的全局联系捕捉。顺序计算的过程中信息会丢失，尽管 LSTM 等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象,LSTM 依旧无能为力。Transformer 使用了 Attention 机制，从而将序列中的任意两个位置之间的距离是缩小为 1，这对解决 NLP 中棘手的长期依赖问题是非常有效的。</p></li></ul><p>总结对比 CNN、RNN 和 Self-Attention：</p><ul><li><p>CNN：只能看到局部领域，适合图像，因为在图像上抽象更高层信息仅仅需要下一层特征的局部区域，文本的话强在抽取局部特征，因而更适合短文本。</p></li><li><p>RNN：理论上能看到所有历史，适合文本，但是存在梯度消失问题。</p></li><li><p>Self-Attention：相比 RNN 不存在梯度消失问题。对比 CNN 更加适合长文本，因为能够看到更远距离的信息，CNN 叠高多层之后可以看到很远的地方，但是 CNN 本来需要很多层才能完成的抽象，Self-Attention 在很底层就可以做到，这无疑是非常巨大的优势。</p></li></ul><h2 id=bert>BERT</h2><p>BERT (Bidirectional Encoder Representations from Transformers)本质来讲是 NLP 领域最底层的语言模型，通过海量语料预训练，得到序列当前最全面的局部和全局特征表示。</p><p>BERT 网络结构如下所示，BERT 与 Transformer 的 Encoder 网络结构完全相同。假设 Embedding 向量的维度是，输入序列包含 n 个 token，则 BERT 模型一个 layer 的输入是一个的矩阵，而它的输出也同样是一个的矩阵，所以这样 N 层 BERT layer 就可以很方便的首尾串联起来。BERT 的 large model 使用了 N=24 层这样的 Transformer block。</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.postimg.cc/MpCyphB4/image.png alt loading=lazy data-zoomable></div></div></figure></p><p>BERT 的主要贡献有以下几个方面</p><ul><li><p>预训练的有效性：这方面来说 BERT 改变了游戏规则，是因为相比设计复杂巧妙的网络结构，在海量无监督数据上预训练得到的 BERT 语言表示+少量训练数据微调的简单网络模型的实验结果取得了很大的优势。</p></li><li><p>网络深度：基于 DNN 语言模型(NNLM，CBOW 等)获取词向量的表示已经在 NLP 领域获得很大成功，而 BERT 预训练网络基于 Transformer 的 Encoder，可以做的很深。</p></li><li><p>双向语言模型：在 BERT 之前，ELMo 和 GPT 的主要局限在于标准语言模型是单向的，GPT 使用 Transformer 的 Decoder 结构，只考虑了上文的信息。ELMo 从左往右的语言模型和从右往左的语言模型其实是独立开来训练的，共享 embedding，将两个方向的 LSTM 拼接并不能真正表示上下文，其本质仍是单向的，且多层 LSTM 难训练。</p></li><li><p>目标函数：对比语言模型任务只做预测下一个位置的单词，想要训练包含更多信息的语言模型，就需要让语言模型完成更复杂的任务，BERT 主要完成完形填空和句对预测的任务，即两个 loss：一个是 Masked Language Model，另一个是 Next Sentence Prediction。</p></li></ul><h1 id=nav--关联导航>Nav | 关联导航</h1><h1 id=about--关于>About | 关于</h1><h2 id=contributing>Contributing</h2><p>Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are <strong>greatly appreciated</strong>.</p><ol><li>Fork the Project</li><li>Create your Feature Branch (<code>git checkout -b feature/AmazingFeature</code>)</li><li>Commit your Changes (<code>git commit -m 'Add some AmazingFeature'</code>)</li><li>Push to the Branch (<code>git push origin feature/AmazingFeature</code>)</li><li>Open a Pull Request</li></ol><h2 id=acknowledgements>Acknowledgements</h2><ul><li><p><a href=https://github.com/wx-chevalier/Awesome-Lists target=_blank rel=noopener>Awesome-Lists</a>: 📚 Guide to Galaxy, curated, worthy and up-to-date links/reading list for ITCS-Coding/Algorithm/SoftwareArchitecture/AI. 💫 ITCS-编程/算法/软件架构/人工智能等领域的文章/书籍/资料/项目链接精选。</p></li><li><p><a href=https://github.com/wx-chevalier/Awesome-CS-Books target=_blank rel=noopener>Awesome-CS-Books</a>: :books: Awesome CS Books/Series(.pdf by git lfs) Warehouse for Geeks, ProgrammingLanguage, SoftwareEngineering, Web, AI, ServerSideApplication, Infrastructure, FE etc. :dizzy: 优秀计算机科学与技术领域相关的书籍归档。</p></li></ul><h2 id=copyright--more--延伸阅读>Copyright & More | 延伸阅读</h2><p>笔者所有文章遵循<a href=https://creativecommons.org/licenses/by-nc-nd/4.0/deed.zh target=_blank rel=noopener>知识共享 署名 - 非商业性使用 - 禁止演绎 4.0 国际许可协议</a>，欢迎转载，尊重版权。您还可以前往 <a href=https://ng-tech.icu/books-gallery/ target=_blank rel=noopener>NGTE Books</a> 主页浏览包含知识体系、编程语言、软件工程、模式与架构、Web 与大前端、服务端开发实践与工程架构、分布式基础架构、人工智能与深度学习、产品运营与创业等多类目的书籍列表：</p><p><a href=https://ng-tech.icu/books-gallery/ target=_blank rel=noopener><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://s2.ax1x.com/2020/01/18/19uXtI.png alt="NGTE Books" loading=lazy data-zoomable></div></div></figure></a></p></div><div class=article-widget><div class="container-xl row post-nav"></div></div><div class=body-footer><p>最近更新于 2023-03-05</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>