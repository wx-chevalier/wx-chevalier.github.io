<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/</link><atom:link href="https://ng-tech.icu/books/awesome-cheatsheets/ai/index.xml" rel="self" type="application/rss+xml"/><description>AI</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>AI</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/</link></image><item><title>AI-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/ai-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/ai-cheatsheet/</guid><description>&lt;h1 id="ai-cheatsheet--ai">AI CheatSheet | AI&lt;/h1>
&lt;p>所谓 AI，即能够感知（Aware/Sense），然后做出决策（Decision），并且对于环境做出反应（Act）。&lt;/p>
&lt;p>机器学习的目标是让机器能够自动根据数据分析预测我们想要的结果。　　机器学习有时会和人工智能（AI）混淆。但严格来讲，机器学习只是人工智能的子域。机器学习有许多不同的形式，涉及许多不同的名字：模式识别、统计建模、数据挖掘、知识发现、预测分析、数据科学、适应系统、自组织系统等。&lt;/p>
&lt;p>人工智能，与机器学习的思维模式，不同于软件工程中以逻辑与数学思维去思考，以断言来证明程序各属性的正确性，并且最终得到确定性产品的开发思维模式；它的关注点从数学科学转移到自然科学，观察不确定的未知世界，开展实验，并使用统计信息 而非逻辑来分析实验结果。&lt;/p>
&lt;p>佩德罗·多明戈斯在《终极算法》一书中将机器学习领域划分了五大思想学派（符号学派、联结学派、进化学派、贝叶斯学派和类推学派）。&lt;/p>
&lt;p>深度学习是机器学习中使用深度神经网络的的子领域。用机器学习解决一个问题的时候，首先我们拿到一个数据，比如说这个数据对象是个图像，然后我们就用很多特征把它描述出来，比如说颜色、纹理等等。这些特征都是我们人类专家通过手工来设计的，表达出来之后我们再去进行学习。而今天我们有了深度学习之后，现在不再需要手工去设计特征了。你把数据从一端扔进去，模型从另外一端就出来了，中间所有的特征完全可以通过学习自己来解决。所以这就是我们所谓的特征学习，或者说表示学习。这和以往的机器学习技术相比可以说是一个很大的进步。我们不再需要依赖人类专家去设计特征了。&lt;/p>
&lt;p>表示学习最关键的又是什么呢？我们现在有这么一个答案，就是逐层的处理。第一，我们要有逐层的处理；第二，我们要有特征的内部变换；第三，我们要有足够的模型复杂度。&lt;/p>
&lt;p>AI 从感知层大致分为两大块，一块是计算机视觉，这一块已经比较成熟，无论是人脸识别、物体检测、运动检测都已经能用于实际场景中。另一块则是 NLP，虽然微软、Google 等宣称它们的 AI 翻译准确率已经极高，但实际上仍然不太好用，而多轮会话的问题没有解决，Chatbot 还是难以与人展开正常对话。&lt;/p>
&lt;h2 id="意义与应用">意义与应用&lt;/h2>
&lt;p>首先，它会为您提供一个可缩短编程时间的工具。假设我想编写一个程序来纠正拼写错误。我可以通过大量示例和经验法则（例如，I 位于 E 之前，但出现在 C 之后时例外），取得一定的进展，然后经过数周的努力，编写出一个合理的程序。&lt;/p>
&lt;p>其次，借助机器学习，您可以自定义自己的产品，使其更适合特定的用户群体。假设我手动编写了一个英文拼写纠正程序，这个程序很成功，因此我打算 针对 100 种最常用语言提供相应的版本。这样一来，每种语言版本几乎都需要从头开始，这将需要付出数年的努力。但如果我使用机器学习技术构建该程序，然后迁移到其他语言，基本上就相当于，我只需收集该特定语言的数据，并将这些数据提供给完全一样的机器学习模型即可。&lt;/p>
&lt;p>第三，借助机器学习，您可以解决自己作为编程人员不知道如何用人工方法解决的问题。作为人类，我可以认出朋友的面孔，理解他们所说的话，但所有这些都是在潜意识下完成的，如果让我编写一个程序来做这些事，我会完全不知所措。但是，机器学习算法对此却很擅长；我不需要告诉算法应该怎么做，只需向其展示大量样本，问题就可以迎刃而解。&lt;/p>
&lt;h2 id="发展与变迁">发展与变迁&lt;/h2>
&lt;p>人工智能发展有三个阶段：计算智能、感知智能和认知智能。&lt;/p>
&lt;p>第一阶段的计算智能即快速计算和记忆存储，像机器人战胜围棋大师，靠的就是超强的记忆能力和运算速度。人脑的逻辑能力再强大，也敌不过人工智能每天和自己下几百盘棋，通过强大的计算能力对十几步后的结果做出预测，从这一角度来说，人工智能多次战败世界级围棋选手，足以证明这一领域发展之成熟。&lt;/p>
&lt;p>第二阶段的感知智能，即让机器拥有视觉、听觉、触觉等感知能力。自动驾驶汽车做的就是这一方面的研究，使机器通过传感器对周围的环境进行感知和处理，从而实现自动驾驶。感知智能方面的技术目前发展比较成熟的领域有语音识别和图像识别，比如做安全领域人脸识别技术的 Face++，以成熟的计算机视觉技术深耕电商、短视频等领域的 Yi+，能够对多种语言进行准确识别翻译的科大讯飞等。&lt;/p>
&lt;p>第三阶段的认知智能与前面在人工智能的 3 大分支里提到的认知 AI 类似，就是让机器拥有自己的认知，能理解会思考。认知智能是目前机器和人差距最大的领域，因为这不仅涉及逻辑和技术，还涉及心理学、哲学和语言学等学科。&lt;/p>
&lt;h1 id="知识领域">知识领域&lt;/h1>
&lt;h2 id="mathematics--数学基础">Mathematics | 数学基础&lt;/h2>
&lt;h2 id="machine-learning--机器学习">Machine Learning | 机器学习&lt;/h2>
&lt;p>Maximum Objective Function&lt;/p>
&lt;p>常见机器学习的任务可以分解为以下七个步骤：&lt;/p>
&lt;p>Data Collection&lt;/p>
&lt;p>Data Preparation&lt;/p>
&lt;p>Build Model&lt;/p>
&lt;p>Train Model&lt;/p>
&lt;p>Evaluation&lt;/p>
&lt;p>Tune&lt;/p>
&lt;p>Predict&lt;/p>
&lt;h2 id="deep-learning--深度学习">Deep Learning | 深度学习&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/43595548-846b86e0-96af-11e8-951b-ae913482c19c.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>传统的机器学习往往需要大量的领域知识与计算时间，而深度学习为我们带来了无限灵活的函数、通用的参数拟合、高速可扩展等特性的算法。&lt;/p>
&lt;p>Traditional statistical models do very well on structured data, i.e. tabular data, but have notoriously struggled with unstructured data like images, audio, and natural language. Neural networks that contain many layers of neurons embody the research that is popularly called Deep Learning. The key insight and property of deep neural networks that make them suitable for modeling unstructured data is that complex data, like images, generally have many layers of unique features that are composed to produce the data. As a classic example: images have edges which form the basis for textures, textures form the basis for simple objects, simple objects form the basis for more complex objects, and so on. In deep neural networks we aim to learn these many layers of composable features.&lt;/p>
&lt;p>Traditional statistical models do very well on structured data, i.e. tabular data, but have notoriously struggled with unstructured data like images, audio, and natural language. Neural networks that contain many layers of neurons embody the research that is popularly called Deep Learning. The key insight and property of deep neural networks that make them suitable for modeling unstructured data is that complex data, like images, generally have many layers of unique features that are composed to produce the data. As a classic example: images have edges which form the basis for textures, textures form the basis for simple objects, simple objects form the basis for more complex objects, and so on. In deep neural networks we aim to learn these many layers of composable features.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/43685359-4a84c7d4-98e4-11e8-8bce-7ef4cd2aa686.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="nlp--自然语言处理">NLP | 自然语言处理&lt;/h2>
&lt;h2 id="computer-vision--计算机视觉">Computer Vision | 计算机视觉&lt;/h2>
&lt;h1 id="terminology--通用概念">Terminology | 通用概念&lt;/h1>
&lt;h2 id="function--函数">Function | 函数&lt;/h2>
&lt;p>💡 Sigmod $\sigma$ 💡&lt;/p>
&lt;p>神经网络中的激活函数，其作用就是引入非线性。具体的非线性形式，则有多种选择。sigmoid 的优点在于输出范围有限，所以数据在传递的过程中不容易发散。当然也有相应的缺点，就是饱和的时候梯度太小。sigmoid 还有一个优点是输出范围为 (0, 1)，所以可以用作输出层，输出表示概率。Sigmoid 函数是一个在生物学中常见的 S 型的函数，也称为 S 形生长曲线。Sigmoid 函数由下列公式定义，其导数可以节约计算时间:&lt;/p>
&lt;p>$$
S(x) = \frac{1}{1+ e^{-x}} \
S&amp;rsquo;(x)=S(x)[1-S(x)]
$$&lt;/p>
&lt;p>Geoff Hinton covered exactly this topic in his coursera course on neural nets. The problem with sigmoids is that as you reach saturation (values get close to 1 or 0), the gradients vanish. This is detrimental to optimization speed. Softmax doesn&amp;rsquo;t have this problem, and in fact if you combine softmax with a cross entropy error function the gradients are just (z-y), as they would be for a linear output with least squares error.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># sigmoid function&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">deriv&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">deriv&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="model--模型">Model | 模型&lt;/h2>
&lt;h2 id="optimization--优化">Optimization | 优化&lt;/h2>
&lt;h2 id="networks--网络">Networks | 网络&lt;/h2>
&lt;p>Gradient ∇ (微分算符)：梯度&lt;/p>
&lt;p>梯度即是某个函数的偏导数，其允许输入多个向量然后输出单个值，某个典型的函数即是神经网络中的损失函数。梯度会显示出随着变量输入的增加输出值增加的方向，换言之，如果我们要降低损失值则反梯度逆向前行即可。&lt;/p>
&lt;p>梯度下降法 Gradient Descent 是一种常用的一阶(first-order)优化方法，是求解无约束优化问题最简单、最经典的方法之一。考虑无约束优化问题$min_xf(x)$，其中$f(x)$为连续可微函数。如果能构造出一个序列$x^0,x^1,&amp;hellip;,x^t$满足：&lt;/p>
&lt;p>$$
f(x^{t+1}) &amp;lt; f(x^t),t=0,1,2&amp;hellip;
$$&lt;/p>
&lt;p>则不断执行该过程即可以收敛到局部极小点。而根据泰勒展示我们可以知道:&lt;/p>
&lt;p>$$
f(x+\Delta x) \simeq f(x) + \Delta x^T \nabla f(x)
$$&lt;/p>
&lt;p>于是，如果要满足 $f(x+\Delta x) &amp;lt; f(x)$，可以选择:&lt;/p>
&lt;p>$$
\Delta x = -{step} \nabla f(x)
$$&lt;/p>
&lt;p>其中$step$是一个小常数，表示步长。以求解目标函数最小化为例，梯度下降算法可能存在一下几种情况：&lt;/p>
&lt;ul>
&lt;li>当目标函数为凸函数时，局部极小点就对应着函数全局最小值时，这种方法可以快速的找到最优解；&lt;/li>
&lt;li>当目标函数存在多个局部最小值时，可能会陷入局部最优解。因此需要从多个随机的起点开始解的搜索。&lt;/li>
&lt;li>当目标函数不存在最小值点，则可能陷入无限循环。因此，有必要设置最大迭代次数。&lt;/li>
&lt;/ul>
&lt;h1 id="back-propagation反向传播">Back Propagation：反向传播&lt;/h1>
&lt;p>简称为 Back prop，即将前向传播输入值计算得出的误差反向传递到输入值中，经常用于微积分中的链式调用。&lt;/p>
&lt;h1 id="rectified-linear-units-or-relu">Rectified Linear Units or ReLU&lt;/h1>
&lt;p>Sigmoid 函数的输出间隔为&lt;code>[0,1]&lt;/code>，而 ReLU 的输出范围为&lt;code>[0,infinity]&lt;/code>，换言之 Sigmoid 更合适 Logistic 回归而 ReLU 更适合于表示正数。深度学习中 ReLU 并不会受制于所谓的梯度消失问题(Vanishing Gradient Problem)。&lt;/p>
&lt;h1 id="tanh">Tanh&lt;/h1>
&lt;p>Tanh 函数有助于将你的网络权重控制在&lt;code>[-1,1]&lt;/code>之间，而且从上图中可以看出，越靠近 0 的地方梯度值越大，并且梯度的范围位于&lt;code>[0,1]&lt;/code>之间，和 Sigmoid 函数的范围一致，这一点也能有助于避免梯度偏差。&lt;/p>
&lt;h1 id="lstmgru">LSTM/GRU&lt;/h1>
&lt;p>最早见于 Recurrent Neural Networks，不过同样可以用于其他内存单元较少的地方。其主要可以在训练中保持输入的状态，从而避免之前因为 RNN 丢失输入先验上下文而导致的梯度消失问题。&lt;/p>
&lt;h1 id="softmax">Softmax&lt;/h1>
&lt;p>Softmax 函数常用于神经网络的末端以添加分类功能，该函数主要是进行多元逻辑斯蒂回归，也就可以用于多元分类问题。通常会使用交叉熵作为其损失函数。&lt;/p>
&lt;h1 id="l1--l2-regularization">L1 &amp;amp; L2 Regularization&lt;/h1>
&lt;p>正则化项通过对系数添加惩罚项来避免过拟合，正则化项也能够指明模型复杂度。L1 与 L2 的区别在于 L1 能够保证模型的稀疏性。引入正则化项能够保证模型的泛化能力并且避免在训练数据中过拟合。&lt;/p>
&lt;h1 id="drop-out">Drop out&lt;/h1>
&lt;p>Drop out 同样可以避免过拟合，并且能以近似指数的时间来合并多个不同的神经网络结构。该方法会随机地在每一层中选择一些显性层与隐层，在我们的实践中通常会由固定比例的层 Drop out 决定。&lt;/p>
&lt;h1 id="batch-normalization">Batch Normalization&lt;/h1>
&lt;p>在深度学习中，如果有太多的层次会导致所谓的 Internal Covariate Shift，也就是训练过程中因为网络参数的变化导致网络激活分布的变化。如果我们能减少这种变量迁移，我们能够更快地训练网络。Batch Normalization 则通过将每个处理块进行正则化处理来解决这个问题。&lt;/p>
&lt;h1 id="objective-functions">Objective Functions&lt;/h1>
&lt;p>也就是损失函数或者 Optimization Score Function，某个深度学习网络的目标即是最小化该函数值从而提升网络的准确度。&lt;/p>
&lt;h1 id="f1f-score">F1/F Score&lt;/h1>
&lt;p>用于衡量某个模型的准确度的标准:&lt;/p>
&lt;pre tabindex="0">&lt;code>F1 = 2 * (Precision * Recall) / (Precision + Recall)
Precision = True Positives / (True Positives + False Positives)
Recall = True Positives / (True Positives + False Negatives)
&lt;/code>&lt;/pre>&lt;h1 id="cross-entropy">Cross Entropy&lt;/h1>
&lt;p>用于计算预测标签值与真实标签值之间的差距，基本的定义如下:&lt;/p>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/TAj6zrxEpQjp0BcwcaEWdQ" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/TAj6zrxEpQjp0BcwcaEWdQ&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>CommunityDetection-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/communitydetection-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/communitydetection-cheatsheet/</guid><description>&lt;h1 id="社团发现">社团发现&lt;/h1>
&lt;p>社会网络构建方案：&lt;/p>
&lt;p>基于群聊内容构建&lt;/p>
&lt;p>基于点对点聊天内容构建&lt;/p>
&lt;p>基于好友关系构建&lt;/p>
&lt;p>基于群组之间关系构建关系网&lt;/p>
&lt;p>基于聊天中的情感倾向构建情感网络&lt;/p>
&lt;p>基于关键词的传播路径构建网络&lt;/p>
&lt;p>基于命令的传播路径构建网络(包含回应词的构建)&lt;/p></description></item><item><title>DeepLearning-Application-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/deeplearning-application-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/deeplearning-application-cheatsheet/</guid><description>&lt;blockquote>
&lt;p>本文翻译自&lt;a href="https://medium.com/@olivercameron?source=post_header_lockup" target="_blank" rel="noopener">Oliver Cameron&lt;/a>的&lt;a href="https://medium.com/@olivercameron/deep-learning-is-revolutionary-d0f3667bafa0#.g4c7xvd6r" target="_blank" rel="noopener">DeepLearning-Is-Revolutionary&lt;/a>。本文中涉及到一些语音播放等推荐直接阅读原文听取。本文从属于笔者的&lt;a href="https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders" target="_blank" rel="noopener">程序猿的数据科学与机器学习实战手册&lt;/a>，推荐阅读&lt;a href="https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders/blob/master/machinelearning-is-fun-for-anyone-curious-about-ml.md" target="_blank" rel="noopener">有趣的机器学习：从多项拟合到深度学习&lt;/a>、&lt;a href="https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders/blob/master/datascience-machinelearning.md" target="_blank" rel="noopener">数据科学与机器学习概论&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>现在已经有很多人作了关于 DeepLearning 以及其如何重要的讲座，我非常同意他们的看法。工作在 DeepLearning 这个领域是让我觉得最接近魔法师的事，我觉得未来三年内很多软件的重要部分都会由深度学习推动前行。不过，现在好像 DeepLearning 还非主流，因此我想分享些优秀的开发者的一些贡献。&lt;/p>
&lt;h1 id="图片增强httpsgithubcomalexjcneural-enhance">&lt;a href="https://github.com/alexjc/neural-enhance" target="_blank" rel="noopener">图片增强&lt;/a>&lt;/h1>
&lt;p>如果你手里只有一大堆低分辨率的图片，DeepLearning 可以帮你预测高分辨率的图片会是什么样子，然后自动地帮你添加遗失的细节。&lt;/p>
&lt;h1 id="文本转换为语音httpsdeepmindcomblogwavenet-generative-model-raw-audio">&lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" rel="noopener">文本转换为语音&lt;/a>&lt;/h1>
&lt;p>DeepLearning 能够构建一个文本转语音的系统，基本上合成出来的语音效果和真人没啥区别。可以参考&lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio" target="_blank" rel="noopener">wavenet-generative-model-raw-audio&lt;/a>这篇文章。&lt;/p>
&lt;h1 id="音乐合成httpsdeepmindcomblogwavenet-generative-model-raw-audio">&lt;a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/" target="_blank" rel="noopener">音乐合成&lt;/a>&lt;/h1>
&lt;p>DeepLearning 能够帮你合成传统的音乐，效果跟真人合成的相比也是分辨不出来。&lt;/p>
&lt;h1 id="绘画风格替换httpsresearchgoogleblogcom201610supercharging-style-transferhtml">&lt;a href="https://research.googleblog.com/2016/10/supercharging-style-transfer.html" target="_blank" rel="noopener">绘画风格替换&lt;/a>&lt;/h1>
&lt;p>DeepLearning 能够帮你给选定的图片替换风格样式。&lt;/p>
&lt;h1 id="字体生成httpserikberncom20160121analyzing-50k-fonts-using-deep-neural-networks">&lt;a href="https://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deep-neural-networks/" target="_blank" rel="noopener">字体生成&lt;/a>&lt;/h1>
&lt;h1 id="图片填充httpsbamosgithubio20160809deep-completion">&lt;a href="https://bamos.github.io/2016/08/09/deep-completion/" target="_blank" rel="noopener">图片填充&lt;/a>&lt;/h1>
&lt;p>DeepLearning 能够自动帮你补全图片中遗失的部分。&lt;/p>
&lt;h1 id="机器人httpsblogsnvidiacomblog20160115deep-learning-robot-walk">&lt;a href="https://blogs.nvidia.com/blog/2016/01/15/deep-learning-robot-walk/" target="_blank" rel="noopener">机器人&lt;/a>&lt;/h1>
&lt;p>DeepLearning 能够辅助进行机器人训练，让它像个人类一样走路&lt;/p>
&lt;h1 id="图片摘要httpkarpathygithubio20150521rnn-effectiveness">&lt;a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">图片摘要&lt;/a>&lt;/h1>
&lt;h1 id="sketchhttpsgithubcomalexjcneural-doodle">&lt;a href="https://github.com/alexjc/neural-doodle" target="_blank" rel="noopener">Sketch&lt;/a>&lt;/h1>
&lt;h1 id="自动驾驶httpsarxivorgabs160407316">&lt;a href="https://arxiv.org/abs/1604.07316" target="_blank" rel="noopener">自动驾驶&lt;/a>&lt;/h1></description></item><item><title>DeepLearning-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/deeplearning-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/deeplearning-cheatsheet/</guid><description>&lt;h1 id="deeplearning-cheatsheet--深度学习概念备忘">DeepLearning CheatSheet | 深度学习概念备忘&lt;/h1>
&lt;p>深度学习正在逐步地改变世界，从网络搜索、广告推荐这样传统的互联网业务到健康医疗、自动驾驶等不同的行业领域。百年前的电气革命为社会带来了新的支柱产业，而如今 AI 正是新时代的电力基础，驱动社会技术的快速发展。&lt;/p>
&lt;p>对于支持向量机、Logistics 回归这样经典的机器学习算法而言，在数据量从零递增的初始阶段，其性能会不断提升；不过很快就会触碰到天花板，此时性能很难再随着数据集的增长而提升。而伴随着移动互联网时代的到来，我们能够从网站、移动应用或者其他安装在电子终端设备上的传感器中获取到海量的数据；这些数据在开启大数据时代的同时也为深度学习的发展提供了坚实的基础。我们在上图中也可以看出，越是大型的神经网络随着数据量的增加，其性能提升的越快，并且其性能天花板也是越高。&lt;/p>
&lt;p>深度学习崛起的另一个重要基石就是计算能力的提升，这里不仅指新一代 CPU 或者 GPU 设备，还有是在许多基础优化算法上的革新，都使得我们能够更快地训练出神经网络。譬如早期我们会使用 Sigmod 函数作为神经网络的激活函数，随着 x 的增大其梯度会逐渐趋近于零，这就导致了模型收敛变得相对缓慢；而 ReLU 则能较好地避免这个问题，其在正无穷大时梯度值依然保持恒定。简单地从 Sigmod 函数迁移到 ReLU 即能够为模型训练带来极大的效率提升，这也方便了我们构建出更复杂的神经网络。&lt;/p>
&lt;p>神经网络的分类很多，不过截止到目前大多数的有价值的神经网络都还是基于机器学习中所谓的有监督学习(Supervised Learning)。在有监督学习中，我们的训练数据集中已知了特征与结果输出之间的对应关系，而目标就是寻找正确的输入与输出之间的关系表示。譬如目前最赚钱的深度学习应用之一，在线广告中就是输入有关于网站展示的信息以及部分用户的信息，神经网络会预测用户是否会点击该广告；通过为不同的用户展示他们最感兴趣的广告，来增加用户的实际点击率。&lt;/p>
&lt;p>深度学习是目前实现图像识别、语音识别类任务的最佳技术路线，至于自然语言处理、商业趋势的分析预测等场景虽然都取得了不错的成绩，但要取得更好的效果往往需要领域的强力支持。&lt;/p>
&lt;h1 id="neural-networks--神经网络">Neural Networks | 神经网络&lt;/h1>
&lt;p>神经网络其实是一个非常宽泛的称呼，它包括两类，一类是用计算机的方式去模拟人脑，这就是我们常说的 ANN（人工神经网络），另一类是研究生物学上的神经网络，又叫生物神经网络；在人工神经网络之中，又分为前馈神经网络和反馈神经网络这两种。神经网络由大量神经元组成。每个神经元获得线性组合的输入，经过非线性的激活函数，然后得到非线性的输出；线性结果加上值相关(Element wise)非线性结果，我们才能去模拟任意的复杂图形。&lt;/p>
&lt;p>生物神经元可以通过刺激产生冲动、传导冲动；并可以合成化学物质（神经递质、神经激素等），通过他的轴突输送到特定部位并释放，从而产生一个反应。&lt;/p>
&lt;p>第一个正式的人工神经元模型是由沃伦·麦卡洛克（Warren Maculloach）和沃尔特·皮茨（Walter Pitts）于 1943 年提出的。这个模型看起来很像组成计算机的逻辑门。人工神经元示意图如图 1-2 所示。人工神经元本质上是一组向量的加权求和，并通过激活函数产生一个输出。单个的神经元模型无法从数据中学习。第一个正式的人工神经元模型是由沃伦·麦卡洛克（Warren Maculloach）和沃尔特·皮茨（Walter Pitts）于 1943 年提出的。这个模型看起来很像组成计算机的逻辑门。人工神经元示意图如图 1-2 所示。人工神经元本质上是一组向量的加权求和，并通过激活函数产生一个输出。单个的神经元模型无法从数据中学习。&lt;/p>
&lt;p>2006 年，Hinton 在《Science》期刊上发表了论文《用神经网络降低数据维数》，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/44627904-00824e00-a969-11e8-8524-f1f9ad40efad.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>神经网络往往由多层堆叠而成，主要分为输入层(Input Layer)，隐层(Hidden Layer)以及输出层(Output Layer)这三种。输入层中的每个节点代表了数据集中的每条数据的某个特征，隐层中的每个节点则是接收上层传来的特征向量，将其与权重向量进行点乘，传入激活函数得到该节点对应的特征值；输出层则是将前隐层的输出转化为单一的输出值。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/44627910-18f26880-a969-11e8-83b0-40ea1f682f2c.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>在实际计算中，假设 $X$ 是 $N _ features_num$ 维度的数据集，即包含了 $N$ 条数据，每条数据包含了 $features_num$ 个特征。而隐层总的权重矩阵维度为 $features_num _ hidden&lt;em>layer_nodes_num$，即每个隐层节点包含了形为 $features_num * 1$ 的权重向量；输入的数据特征向量与权重向量点乘后得到单一值。换言之，如果隐层共包含了 $hidden&lt;/em>layer_nodes_num$ 个节点，那么某条数据经过隐层变换后输入的数据就成了形为 $1 * hidden_layer_nodes_num$ 的特征向量。在包含偏置值(bais)的隐层中，每个节点的偏置值为 b，那么整个隐层的 偏置向量就是形为 $1 \times hidden_layer_nodes_num$ 的向量，与点乘得到的特征向量相加，得到某条数据经过该隐层后最终的特征向量。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 数据集&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">shape&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">features_num&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Weight, 权重矩阵&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">W&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random_normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">features_num&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_layer_nodes_num&lt;/span>&lt;span class="p">]))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 偏移量矩阵&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">hidden_layer_nodes_num&lt;/span>&lt;span class="p">])))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 构建神经网络计算图&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">XW&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matmul&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">W&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Activation Function | 激活函数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sigmoid&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">Z&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="激活函数">激活函数&lt;/h2>
&lt;p>在早期，Sigmoid 函数和 tanh 函数是人们经常使用的激活函数。近年来，随着神经网络的隐层不断增加后，人们偏向于使用 ReLU 函数作为激活函数，这是因为 ReLU 函数的在 x 轴正向的导数是 1，不容易产生梯度消失问题（见 8.3 节）。这里值得一提的是，如果使用 ReLU 函数作为激活函数，则最后生成的拟合函数为分片线性函数。&lt;/p>
&lt;p>关于激活函数的解释和讨论，机器学习领域及互联网上有很多文章进行过解释，都是解释为“激活函数的主要作用是提供网络的非线性建模能力，提供了分层的非线性映射学习能力”等等。&lt;/p>
&lt;p>从数学上看，深度神经网络就是一个多层复合函数。在机器学习中，这个复合函数的好坏依赖于对目标函数最大化或者最小化的过程，我们常常把最小化的目标函数称为损失函数（Loss function），它主要用于衡量机器学习模型（此处为深度神经网络）的预测能力。常见的损失函数有均方误差（L2 误差）、平均绝对误差（L1 误差）、分位数损失、交叉熵、对比损失等。损失函数的选取依赖于参数的数量、异常值、机器学习算法、梯度下降的效率、导数求取的难易和预测的置信度等若干方面。这里就不展开讨论。&lt;/p>
&lt;p>给定了一批训练数据，计算神经网络的问题就是通过极小化损失函数来找到网络中的所有权值参数（包括偏置参数）。这个过程本质上就是利用这个神经网络函数来拟合这些数据，这个过程也称为“训练”或“学习”。&lt;/p>
&lt;p>误差反向传播算法（Error Back Propagation，BP）是神经网络训练时采用的一种通用方法。本质是随机梯度下降法。最早见于如下论文：&lt;/p>
&lt;p>[8-1] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by back-propagating errors. Nature, 323(99): 533-536, 1986.&lt;/p>
&lt;p>由于整个网络是一个复合函数，因此，训练的过程就是不断地应用“复合函数求导”（链式法则）及“梯度下降法”。反向传播算法从神经网络的输出层开始，利用递推公式根据后一层的误差计算本层的误差，通过误差计算本层参数的梯度值，然后将差项传播到前一层。然后根据这些误差来更新这些系数参数。&lt;/p>
&lt;p>，算法过程中计算误差项时每一层都要乘以本层激活函数的导数，因此，会发生很多次的导数连乘。如果激活函数的导数的绝对值小于 1，多次连乘之后误差项很快会衰减到接近于 0；而参数的梯度值由误差项计算得到，从而导致前面层的权重梯度接近于 0，参数不能得到有效更新，这称为“梯度消失”问题。与之相反，如果激活函数导数的绝对值大于 1，多次连乘后权重值会趋向于非常大的数，这称为“梯度爆炸”。长期以来，这两个问题一直困扰神经网络层次无法变得很深。&lt;/p>
&lt;p>最近几年，ReLU 函数被经常使用作为深度神经网络的激活函数。有两个主要理由：&lt;/p>
&lt;ol>
&lt;li>
&lt;pre>&lt;code> 该函数的导数为sgn（忽略在0处不可导的情形），计算简单，在正半轴导数为1，有效的缓解了梯度消失问题；
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;pre>&lt;code> 虽然它是一个分段线性函数，但它具有非线性逼近能力；使用ReLU激活函数的深度网络的本质是用分段（分片）线性函数（超平面）去逼近目标。
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ol>
&lt;p>后来，人们在 ReLU 的基础上又改进得到各种其他形式的激活函数，包括 ELU、PReLU 等。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/44628032-42ac8f00-a96b-11e8-8072-f360af7814dc.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>激活函数计算量大，反向传播求误差梯度时，求导涉及除法。反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。一个非常大的梯度流过一个 ReLU 神经元，更新过参数之后，这个神经元再也不会对任何数据有激活现象了，那么这个神经元的梯度就永远都会是 0。&lt;/p>
&lt;p>Softmax 函数则是用于多分类神经网络输出：&lt;/p>
&lt;p>$$
\sigma(z)&lt;em>j = \frac{e^{z_j}}{\sum&lt;/em>{k=1}^Ke^{z_k}}
$$&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/44628064-d54d2e00-a96b-11e8-89e9-3c6cccebf039.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="梯度下降">梯度下降&lt;/h2>
&lt;p>梯度下降法 Gradient Descent 是一种常用的一阶(first-order)优化方法，是求解无约束优化问题最简单、最经典的方法之一。考虑无约束优化问题$min_xf(x)$，其中$f(x)$为连续可微函数。如果能构造出一个序列$x^0,x^1,&amp;hellip;,x^t$满足：&lt;/p>
&lt;p>$$
f(x^{t+1}) &amp;lt; f(x^t),t=0,1,2&amp;hellip;
$$&lt;/p>
&lt;p>则不断执行该过程即可以收敛到局部极小点。而根据泰勒展示我们可以知道:&lt;/p>
&lt;p>$$
f(x+\Delta x) \simeq f(x) + \Delta x^T \nabla f(x)
$$&lt;/p>
&lt;p>于是，如果要满足 $f(x+\Delta x) &amp;lt; f(x)$，可以选择:&lt;/p>
&lt;p>$$
\Delta x = -{step} \nabla f(x)
$$&lt;/p>
&lt;p>其中$step$是一个小常数，表示步长。以求解目标函数最小化为例，梯度下降算法可能存在一下几种情况：&lt;/p>
&lt;ul>
&lt;li>当目标函数为凸函数时，局部极小点就对应着函数全局最小值时，这种方法可以快速的找到最优解；&lt;/li>
&lt;li>当目标函数存在多个局部最小值时，可能会陷入局部最优解。因此需要从多个随机的起点开始解的搜索。&lt;/li>
&lt;li>当目标函数不存在最小值点，则可能陷入无限循环。因此，有必要设置最大迭代次数。&lt;/li>
&lt;/ul>
&lt;h1 id="cnn--卷积神经网络">CNN | 卷积神经网络&lt;/h1>
&lt;p>传统的图像特征提取（特征工程）主要是基于各种先验模型，通过提取图像关键点、生成描述子特征数据、进行数据匹配或者机器学习方法对特征数据二分类/多分类实现图像的对象检测与识别。卷积神经网络通过计算机自动提取特征（表示工程）实现图像特征的提取与抽象，通过 MLP 实现数据的回归与分类。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/47838226-9adb9380-dde9-11e8-8353-94ebc1166a58.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>卷积神经网络经过池化层操作对单位像素迁移和亮度影响进行了校正，做到了图像的迁移与亮度不变性的特征提取、而且在池化过程中通过不断的降低图像分辨率，构建了图像的多尺度特征，所以还具备尺度空间不变性，完成了图像不变性特征提取工作。&lt;/p>
&lt;h2 id="卷积层">卷积层&lt;/h2>
&lt;p>卷积层是一系列滤波器集合(filters set)、它的输出结果被称为特征映射(feature maps)，每个 feature map 都一个 filter 在图像上卷积得到的输出。一般情况下都会输出结果加线性修正，对卷积层常用就是 ReLU。&lt;/p>
&lt;p>卷积是一个线性操作，我们需要一个非线性组合，否则两个卷积卷积层还不如一个卷积层。&lt;/p>
&lt;h2 id="池化层">池化层&lt;/h2>
&lt;p>在卷积层提取到的特征数据不具备空间不变性（尺度与迁移不变性特征），只有通过了池化层之后才会具备空间不变性特征。池化层是针对每个 feature map 进行池化操作，池化操作的窗口大小可以指定为任意尺寸，主要有两种类型的池化操作：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>下采样池化（均值池化）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最大值池化&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>无论是选择哪种池化方式都会输出一个新低分辨率 feature map，多数时候这个过程中会包含一定的信息损失，所以卷积神经网络一般通过扩展深度（增加 feature map 的数量）来补偿。&lt;/p>
&lt;p>在进行池化的时候我们如果选择步长为 1 进行池化，通过这样的池化方式输出的结果我们称为重叠池化输出，它不利于特征的稀疏生成，重叠窗口池化与均值池化都有这样的缺点，所以经常采样的是最大值池化，同时不会进行窗口重叠，有实验结果表明，在卷积层保持相同 feature map 与参数的情况下，最大值池化的结果明显优于重叠池化与均值池化，而且网络的深度越深，两者之间的准确度差异越大。&lt;/p>
&lt;h1 id="rnn--循环神经网络">RNN | 循环神经网络&lt;/h1>
&lt;p>循环神经网络最大的特点就在于循环神经网路的隐含层的每个节点的状态或输出除了与当前时刻的输入有关，还有上一个时刻的状态或者输出有关，隐含层节点之间存在循环连接。这使得循环神经网络具有对时间序列的记忆能力。
实际应用中最有效的序列模型称为门控 RNN（gated RNN）。包括基于长短期记忆（LSTM：long short-term memory）和基于门控循环单元(GRU：gated recurrent unit)的网络。门控 RNN 主要是解决一般 RNN 的梯度消失或者梯度爆炸的问题。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/44517585-b09e4f80-a6fa-11e8-8177-407607d84fd7.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="lstm">LSTM&lt;/h2>
&lt;p>不同于普通的 RNN 节点单元，LSTM 引入了遗忘门（总共有 3 个门，输入们、遗忘门和输出们）用来决定我们需要从节点单元中抛弃哪些信息以及保留哪些信息。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/44517720-05da6100-a6fb-11e8-9ffe-c018b9bf5baf.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://www.cnblogs.com/charlotte77/p/7759802.html" target="_blank" rel="noopener">https://www.cnblogs.com/charlotte77/p/7759802.html&lt;/a>, &lt;a href="https://www.cnblogs.com/charlotte77/p/7783261.html#4089029" target="_blank" rel="noopener">https://www.cnblogs.com/charlotte77/p/7783261.html#4089029&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://www.cnblogs.com/charlotte77/p/5629865.html#4100293" target="_blank" rel="noopener">https://www.cnblogs.com/charlotte77/p/5629865.html#4100293&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>EnsembleLearning-CheatCheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/ensemblelearning-cheatcheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/ensemblelearning-cheatcheatsheet/</guid><description>&lt;h1 id="集成学习">集成学习&lt;/h1>
&lt;p>机器学习中的集成模型（Ensemble Models）采用了类似的思想。集成模型结合多个模型的决策，以提高整体性能。&lt;/p></description></item><item><title>FeatureEngineering-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/featureengineering-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/featureengineering-cheatsheet/</guid><description>&lt;h1 id="feature-engineering-cheatsheet--特征工程理论盘点">Feature Engineering CheatSheet | 特征工程理论盘点&lt;/h1>
&lt;!-- prettier-ignore-start -->
&lt;p>PCA 和 LDA 是最常见的线性降维方法，它们按照某种准则为数据集 ${x_i}^n_{i=1}$ 找到一个最优投影方向 $W$ 和截距 $b$，然后做变换 $z_i = Wx_i + b$ 得到降维后的数据集 ${z_i}^n_{i=1}$。因为 $z_i = Wx_i + b$ 是一个线性变换（严格来说叫仿射变换，因为有截距项），所以这两种方法叫做线性降维。&lt;/p>
&lt;!-- prettier-ignore-end -->
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h1 id="distance--similarity距离与相似度">Distance &amp;amp; Similarity(距离与相似度)&lt;/h1>
&lt;p>最常见的标准化方法就是 z-score 标准化，也叫标准差标准化，这种方法给予原始数据的均值（mean）和标准差（standard deviation）进行数据的标准化。常用的标准化方法还有 min-max 标准化（又叫离差标准化）、atan 反正切函数标准化、log 函数标准化等。
　　我们使用的是 z-score 标准化方法，与离差标准化的不同之处在于，离差标准化仅仅仅仅对原数据的的方差与均差进行了倍数缩减，而标准差标准化则使标准化的数据方差为一。这对许多的算法更加有利，但是其缺点在于假如原始数据没有呈高斯分布，标准化的数据分布效果并不好。所以我们使用 z-score 标准化方法的前提假设是数据分布符合正态分布（及高斯分布）。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://dzone.com/articles/machine-learning-measuring" target="_blank" rel="noopener">Machine Learning: Measuring Similarity and Distance&lt;/a>
&lt;blockquote>
&lt;/blockquote>
&lt;/li>
&lt;li>&lt;a href="http://www.cnblogs.com/daniel-D/p/3244718.html" target="_blank" rel="noopener">漫谈：机器学习中距离和相似性度量方法&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="数值点距离numeric-data-points">数值点距离(numeric data points)&lt;/h2>
&lt;h3 id="闵可夫斯基距离">闵可夫斯基距离&lt;/h3>
&lt;p>&lt;strong>闵可夫斯基距离&lt;/strong>(Minkowski distance)是衡量数值点之间距离的一种非常常见的方法，假设数值点 P 和 Q 坐标如下：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220422-b6c5a38eccb74824b92ba1b40c9dd92f.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>那么，闵可夫斯基距离定义为：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220504-12655edb08dc45ae8a036d8028743042.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>该距离最常用的 p 是 2 和 1, 前者是&lt;strong>欧几里得距离&lt;/strong>(Euclidean distance)，后者是&lt;strong>曼哈顿距离&lt;/strong>(Manhattan distance)。假设在曼哈顿街区乘坐出租车从 P 点到 Q 点，白色表示高楼大厦，灰色表示街道：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220530-1c87c470c5984305932cb5f5fc91656f.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>绿色的斜线表示欧几里得距离，在现实中是不可能的。其他三条折线表示了曼哈顿距离，这三条折线的长度是相等的。&lt;/p>
&lt;p>当 p 趋近于无穷大时，闵可夫斯基距离转化成&lt;strong>切比雪夫距离&lt;/strong>(Chebyshev distance)：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220549-4fb4c30e7fb84ca290d04f44f75dea7b.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>我们知道平面上到原点欧几里得距离(p = 2)为 1 的点所组成的形状是一个圆，当 p 取其他数值的时候呢？&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220559-ae662025d1394f90bfd62f7c21c3d895.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>注意，当 &lt;code>p &amp;lt; 1&lt;/code> 时，闵可夫斯基距离不再符合三角形法则，举个例子：当 p &lt;code>&amp;lt;&lt;/code> 1, (0,0) 到 (1,1) 的距离等于&lt;code>(1+1)^{1/p} &amp;gt; 2&lt;/code>, 而 (0,1) 到这两个点的距离都是 1。&lt;/p>
&lt;p>闵可夫斯基距离比较直观，但是它与数据的分布无关，具有一定的局限性，如果 x 方向的幅值远远大于 y 方向的值，这个距离公式就会过度放大 x 维度的作用。所以，在计算距离之前，我们可能还需要对数据进行 &lt;strong>z-transform&lt;/strong> 处理，即减去均值，除以标准差：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?%28x_1,%20y_1%29%5Cmapsto%20%28%5Cfrac%7Bx_1%20-%20%5Cmu%20_x%7D%7B%5Csigma%20_x%7D,%20%5Cfrac%7By_1%20-%20%5Cmu%20_y%7D%7B%5Csigma%20_y%7D%29" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;blockquote>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?%5Cmu" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure> : 该维度上的均值&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?%5Csigma" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure> : 该维度上的标准差&lt;/p>
&lt;/blockquote>
&lt;p>可以看到，上述处理开始体现数据的统计特性了。这种方法在假设数据各个维度不相关的情况下利用数据分布的特性计算出不同的距离。如果维度相互之间数据相关(例如：身高较高的信息很有可能会带来体重较重的信息，因为两者是有关联的)，这时候就要用到&lt;strong>马氏距离&lt;/strong>(Mahalanobis distance)了。&lt;/p>
&lt;h3 id="马氏距离">马氏距离&lt;/h3>
&lt;p>考虑下面这张图，椭圆表示等高线，从欧几里得的距离来算，绿黑距离大于红黑距离，但是从马氏距离，结果恰好相反：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220637-f472bb13a779481bbfa45a9d79bd2175.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>马氏距离实际上是利用 Cholesky transformation 来消除不同维度之间的&lt;strong>相关性&lt;/strong>和&lt;strong>尺度不同&lt;/strong>的性质。假设样本点(列向量)之间的协方差对称矩阵是
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?%5CSigma" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure> ，通过 Cholesky Decomposition(实际上是对称矩阵 LU 分解的一种特殊形式，可参考之前的&lt;a href="http://www.cnblogs.com/daniel-D/p/3204508.html" target="_blank" rel="noopener">博客&lt;/a>)可以转化为下三角矩阵和上三角矩阵的乘积：
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?%5CSigma%20=%20LL%5ET" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure> 。消除不同维度之间的相关性和尺度不同，只需要对样本点 x 做如下处理：
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?z%20=%20L%5E%7B-1%7D%28x%20-%20%5Cmu%20%29" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure> 。处理之后的欧几里得距离就是原样本的马氏距离：为了书写方便，这里求马氏距离的平方)：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220659-e3270d8a52ef45c1b457d9af19b1aad1.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>下图蓝色表示原样本点的分布，两颗红星坐标分别是(3, 3)，(2, -2):&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220711-7c326cd8835a446d94684e6adb7ff75a.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>由于 x，y 方向的尺度不同，不能单纯用欧几里得的方法测量它们到原点的距离。并且，由于 x 和 y 是相关的(大致可以看出斜向右上)，也不能简单地在 x 和 y 方向上分别减去均值，除以标准差。最恰当的方法是对原始数据进行 Cholesky 变换，即求马氏距离(可以看到，右边的红星离原点较近)：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07220737-b9ab6c4b19d64590998685325ae49bd1.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>将上面两个图的绘制代码和求马氏距离的代码贴在这里，以备以后查阅：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># -*- coding=utf-8 -*-&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># code related at: http://www.cnblogs.com/daniel-D/&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">pylab&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">pl&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">scipy.spatial.distance&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">dist&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">plotSamples&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">z&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stars&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matrix&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mf">3.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">2.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mf">3.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">2.&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mf">0.&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">z&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">z&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matrix&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">stars&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">z&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">stars&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pl&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 画 gaussian 随机点&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pl&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">scatter&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">stars&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">stars&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="n">s&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">200&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">marker&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;*&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;r&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 画三个指定点&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pl&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">axhline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">linewidth&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;g&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 画 x 轴&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pl&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">axvline&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">linewidth&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">color&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;g&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># 画 y 轴&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pl&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;equal&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pl&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">axis&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pl&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">show&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 产生高斯分布的随机点&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">mean&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="c1"># 平均值&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">cov&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">]]&lt;/span> &lt;span class="c1"># 协方差&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">multivariate_normal&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cov&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1000&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plotSamples&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">covMat&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matrix&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cov&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">))&lt;/span> &lt;span class="c1"># 求 x 与 y 的协方差矩阵&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Z&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">linalg&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cholesky&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">covMat&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">I&lt;/span> &lt;span class="c1"># 仿射矩阵&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">plotSamples&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">Z&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 求马氏距离&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s1">到原点的马氏距离分别是：&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mahalanobis&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">covMat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">I&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mahalanobis&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">covMat&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">I&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 求变换后的欧几里得距离&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dots&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">Z&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">matrix&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">]]))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">T&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="s1">&amp;#39;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s1">变换后到原点的欧几里得距离分别是：&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">minkowski&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dots&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dist&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">minkowski&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dots&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]),&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>马氏距离的变换和 PCA 分解的&lt;a href="http://deeplearning.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96" target="_blank" rel="noopener">白化处理&lt;/a>颇有异曲同工之妙，不同之处在于：就二维来看，PCA 是将数据主成分旋转到 x 轴(正交矩阵的酉变换)，再在尺度上缩放(对角矩阵)，实现尺度相同。而马氏距离的 L 逆矩阵是一个下三角，先在 x 和 y 方向进行缩放，再在 y 方向进行错切(想象矩形变平行四边形)，总体来说是一个没有旋转的仿射变换。&lt;/p>
&lt;h2 id="类别点距离categorical-data-points">类别点距离(categorical data points)&lt;/h2>
&lt;p>$distance*{final} = α.distance*{numeric} + (1- α).distance_{categorical}$&lt;/p>
&lt;h3 id="汉明距离">汉明距离&lt;/h3>
&lt;p>&lt;strong>汉明距离&lt;/strong>(Hamming distance)是指，两个等长字符串 s1 与 s2 之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。举个维基百科上的例子：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07221109-c683a8f31c9a4e31a93e5d04fdab3443.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>还可以用简单的&lt;strong>匹配系数&lt;/strong>来表示两点之间的相似度——匹配字符数/总字符数。在一些情况下，某些特定的值相等并不能代表什么。举个例子，用 1 表示用户看过该电影，用 0 表示用户没有看过，那么用户看电影的的信息就可用 0,1 表示成一个序列。考虑到电影基数非常庞大，用户看过的电影只占其中非常小的一部分，如果两个用户都没有看过某一部电影(两个都是 0)，并不能说明两者相似。反而言之，如果两个用户都看过某一部电影(序列中都是 1)，则说明用户有很大的相似度。在这个例子中，序列中等于 1 所占的权重应该远远大于 0 的权重，这就引出下面要说的&lt;strong>杰卡德相似系数&lt;/strong>(Jaccard similarity)。&lt;/p>
&lt;h3 id="jacard-相似度">Jacard 相似度&lt;/h3>
&lt;p>Jacard 相似性直观的概念来自，两个集合有多相似，显然，Jacard 最好是应用在离散的变量几何上。先看公式：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://upload.wikimedia.org/math/1/8/6/186c7f4e83da32e889d606140fae25a0.png" alt=" J(A,B) = {{|A /cap B|}/over{|A /cup B|}}." loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>分子是集合交集，分母是集合并集，画个图，马上就明白咋回事了。&lt;/p>
&lt;p>和 Jacard index 相似的一个公式是 Dice‘ coefficient, 它也很直观，&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://upload.wikimedia.org/math/2/3/5/2354a9c697d2bf4ae114b8f1f72d5090.png" alt="s = /frac{2 | X /cap Y |}{| X | &amp;#43; | Y |} " loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="向量内积">向量内积&lt;/h2>
&lt;p>向量内积是线性代数里最为常见的计算，实际上它还是一种有效并且直观的相似性测量手段。向量内积的定义如下：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?Inner%28x,y%29%20=%20%5Clangle%20x,%20y%20%5Crangle%20=%20%5Csum_i%20x_i%20y_i" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>直观的解释是：如果 x 高的地方 y 也比较高，x 低的地方 y 也比较低，那么整体的内积是偏大的，也就是说 x 和 y 是相似的。举个例子，在一段长的序列信号 A 中寻找哪一段与短序列信号 a 最匹配，只需要将 a 从 A 信号开头逐个向后平移，每次平移做一次内积，内积最大的相似度最大。信号处理中 DFT 和 DCT 也是基于这种内积运算计算出不同频域内的信号组分(DFT 和 DCT 是正交标准基，也可以看做投影)。向量和信号都是离散值，如果是连续的函数值，比如求区间&lt;code>[-1, 1]&lt;/code> 两个函数之间的相似度，同样也可以得到(系数)组分，这种方法可以应用于多项式逼近连续函数，也可以用到连续函数逼近离散样本点(最小二乘问题，&lt;strong>OLS coefficients&lt;/strong>)中，扯得有点远了- -!。&lt;/p>
&lt;h3 id="余弦相似度">余弦相似度&lt;/h3>
&lt;p>向量内积的结果是没有界限的，一种解决办法是除以长度之后再求内积，这就是应用十分广泛的&lt;strong>余弦相似度&lt;/strong>(Cosine similarity)：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?CosSim%28x,y%29%20=%20%5Cfrac%7B%5Csum_i%20x_i%20y_i%7D%7B%20%5Csqrt%7B%20%5Csum_i%20x_i%5E2%7D%20%5Csqrt%7B%20%5Csum_i%20y_i%5E2%20%7D%20%7D%20=%20%5Cfrac%7B%20%5Clangle%20x,y%20%5Crangle%20%7D%7B%20%7C%7Cx%7C%7C%5C%20%7C%7Cy%7C%7C%20%7D" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>余弦相似度与向量的幅值无关，只与向量的方向相关，在文档相似度(&lt;a href="http://www.ruanyifeng.com/blog/2013/03/cosine_similarity.html" target="_blank" rel="noopener">TF-IDF&lt;/a>)和图片相似性(&lt;a href="http://www.ruanyifeng.com/blog/2013/03/similar_image_search_part_ii.html" target="_blank" rel="noopener">histogram&lt;/a>)计算上都有它的身影。&lt;/p>
&lt;h3 id="皮尔逊相关系数pearson-correlation-coefficient">皮尔逊相关系数(Pearson Correlation Coefficient)&lt;/h3>
&lt;p>需要注意一点的是，余弦相似度受到向量的平移影响，上式如果将 x 平移到 x+1, 余弦值就会改变。怎样才能实现平移不变性？这就是下面要说的&lt;strong>皮尔逊相关系数&lt;/strong>(Pearson correlation)，有时候也直接叫&lt;strong>相关系数&lt;/strong>。皮尔逊相关系数是一种度量两个变量间相关程度的方法。它是一个介于 1 和 -1 之间的值，其中，1 表示变量完全正相关，0 表示无关，-1 表示完全负相关。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?%5Cbegin%7Balign%7D%20Corr%28x,y%29%20&amp;amp;=%20%5Cfrac%7B%20%5Csum_i%20%28x_i-%5Cbar%7Bx%7D%29%20%28y_i-%5Cbar%7By%7D%29%20%7D%7B%20%5Csqrt%7B%5Csum%20%28x_i-%5Cbar%7Bx%7D%29%5E2%7D%20%5Csqrt%7B%20%5Csum%20%28y_i-%5Cbar%7By%7D%29%5E2%20%7D%20%7D%20&amp;amp;=%20%5Cfrac%7B%5Clangle%20x-%5Cbar%7Bx%7D,%5C%20y-%5Cbar%7By%7D%20%5Crangle%7D%7B%20%7C%7Cx-%5Cbar%7Bx%7D%7C%7C%5C%20%7C%7Cy-%5Cbar%7By%7D%7C%7C%7D%20%20%20&amp;amp;=%20CosSim%28x-%5Cbar%7Bx%7D,%20y-%5Cbar%7By%7D%29%20%5Cend%7Balign%7D" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>=&lt;/p>
&lt;p>$\frac{\sum x_iy_i-\frac{\sum x_i\sum y_i}{n}}{\sqrt{\sum x_i^2-\frac{(\sum x_i)^2}{n}}\sqrt{\sum y_i^2-\frac{(\sum y_i)^2}{n}}}$&lt;/p>
&lt;p>在推荐系统中，我们常用皮尔逊相关系数来衡量两个用户兴趣的相似度，它是判断两组数据与某一直线拟合程度的一种度量。它在用户对物品的评分数据差别大时(如有些用户评分普遍较高，有些用户评分普遍偏低)时的效果更好。也即它修正了“夸大分值”的情况，如果某个用户总是倾向于给出比另一个人更高的分值，而两者的分值之差又始终保持一致，则两者间依然可能存在很好地相关性。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://7xlv6k.com1.z0.glb.clouddn.com/cdn_chapter2_2.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>在&lt;a href="http://zh.wikipedia.org/wiki/%E7%BB%9F%E8%AE%A1%E5%AD%A6" target="_blank" rel="noopener">统计学&lt;/a>中，&lt;strong>皮尔逊积矩相关系数&lt;/strong>(Pearson product-moment correlation coefficient)用于度量两个变量 X 和 Y 之间的&lt;a href="http://zh.wikipedia.org/wiki/%E7%9B%B8%E5%85%B3" target="_blank" rel="noopener">相关&lt;/a>(线性相关)，其值介于-1 与 1 之间。系数的值为 1 意味着&lt;em>X&lt;/em> 和 &lt;em>Y&lt;/em>可以很好的由直线方程来描述，所有的数据点都很好的落在一条 &lt;a href="http://zh.wikipedia.org/w/index.php?title=Line_%28mathematics%29&amp;amp;action=edit&amp;amp;redlink=1" target="_blank" rel="noopener">直线&lt;/a>上，且 &lt;em>Y&lt;/em> 随着 &lt;em>X&lt;/em> 的增加而增加。系数的值为?1 意味着所有的数据点都落在直线上，且 &lt;em>Y&lt;/em> 随着 &lt;em>X&lt;/em> 的增加而减少。系数的值为 0 意味着两个变量之间没有线性关系。当两个变量独立时，相关系数为 0.但反之并不成立。这是因为相关系数仅仅反映了两个变量之间是否线性相关。比如说，&lt;em>X&lt;/em>是区间［－１，１］上的一个均匀分布的随机变量。&lt;em>Y&lt;/em> = &lt;em>X&lt;/em>2. 那么&lt;em>Y&lt;/em>是完全由&lt;em>X&lt;/em>确定。因此&lt;em>Y&lt;/em> 和&lt;em>X&lt;/em>是不独立的。但是相关系数为 0。或者说他们是不相关的。当&lt;em>Y&lt;/em> 和&lt;em>X&lt;/em>服从联合正态分布时，其相互独立和不相关是等价的。当且仅当 &lt;em>X**i&lt;/em> and &lt;em>Y**i&lt;/em> 均落在他们各自的均值的同一侧，则(&lt;em>X**i&lt;/em> ? &lt;em>X&lt;/em>)(&lt;em>Y**i&lt;/em> ? &lt;em>Y&lt;/em>) 的值为正。也就是说，如果&lt;em>X**i&lt;/em> 和 &lt;em>Y**i&lt;/em> 同时趋向于大于, 或同时趋向于小于他们各自的均值，则相关系数为正。如果 &lt;em>X**i&lt;/em> 和 &lt;em>Y**i&lt;/em> 趋向于落在他们均值的相反一侧，则相关系数为负。&lt;/p>
&lt;p>皮尔逊相关系数具有平移不变性和尺度不变性，计算出了两个向量(维度)的相关性。不过，一般我们在谈论相关系数的时候，将 x 与 y 对应位置的两个数值看作一个样本点，皮尔逊系数用来表示这些样本点分布的相关性。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07221044-45cc00fed26d4c6796f4d9b9072dc177.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>由于皮尔逊系数具有的良好性质，在各个领域都应用广泛，例如，在推荐系统根据为某一用户查找喜好相似的用户,进而&lt;a href="http://www.cnblogs.com/daniel-D/p/3192180.html" target="_blank" rel="noopener">提供推荐&lt;/a>，优点是可以不受每个用户评分标准不同和观看影片数量不一样的影响。&lt;/p>
&lt;blockquote>
&lt;p>例如，假设五个国家的国民生产总值分别是 1、2、3、5、8(单位 10 亿美元)，又假设这五个国家的贫困比例分别是 11%、12%、13%、15%、18%。&lt;/p>
&lt;/blockquote>
&lt;p>创建 2 个向量.(R 语言)&lt;/p>
&lt;pre tabindex="0">&lt;code>x&amp;lt;-c(1,2,3,5,8)
y&amp;lt;-c(0.11,0.12,0.13,0.15,0.18)
&lt;/code>&lt;/pre>&lt;p>按照维基的例子,应计算出相关系数为 1 出来.我们看看如何一步一步计算出来的.&lt;/p>
&lt;p>x 的平均数是:3.8&lt;/p>
&lt;p>y 的平均数是 0.138&lt;/p>
&lt;p>所以,&lt;/p>
&lt;pre tabindex="0">&lt;code>sum((x-mean(x))*(y-mean(y)))=0.308
&lt;/code>&lt;/pre>&lt;p>用大白话来写就是:&lt;/p>
&lt;p>(1-3.8)*(0.11-0.138)=0.0784&lt;/p>
&lt;p>(2-3.8)*(0.12-0.138)=0.0324&lt;/p>
&lt;p>(3-3.8)*(0.13-0.138)=0.0064&lt;/p>
&lt;p>(5-3.8)*(0.15-0.138)=0.0144&lt;/p>
&lt;p>(8-3.8)*(0.18-0.138)=0.1764&lt;/p>
&lt;p>0.0784+0.0324+0.0064+0.0144+0.1764=0.308&lt;/p>
&lt;p>同理, 分号下面的,分别是:&lt;/p>
&lt;p>sum((x-mean(x))^2)=30.8&lt;/p>
&lt;p>sum((y-mean(y))^2)= 0.00308&lt;/p>
&lt;p>用大白话来写,分别是:&lt;/p>
&lt;p>(1-3.8)^2=7.84 #平方&lt;/p>
&lt;p>(2-3.8)^2=3.24 #平方&lt;/p>
&lt;p>(3-3.8)^2=0.64 #平方&lt;/p>
&lt;p>(5-3.8)^2=1.44 #平方&lt;/p>
&lt;p>(8-3.8)^2=17.64 #平方&lt;/p>
&lt;p>7.84+3.24+0.64+1.44+17.64=30.8&lt;/p>
&lt;p>同理,求得:&lt;/p>
&lt;pre tabindex="0">&lt;code>sum((y-mean(y))^2)= 0.00308
&lt;/code>&lt;/pre>&lt;p>然后再开平方根,分别是:&lt;/p>
&lt;p>30.8^0.5=5.549775&lt;/p>
&lt;p>0.00308^0.5=0.05549775&lt;/p>
&lt;p>用分子除以分母,就计算出最终结果:&lt;/p>
&lt;p>0.308/(5.549775*0.05549775)=1&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1">#皮尔逊相关度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">sim_pearson&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prefs&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">p1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">p2&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">si&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">item&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">prefs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">p1&lt;/span>&lt;span class="p">]:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">item&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">prefs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">p2&lt;/span>&lt;span class="p">]:&lt;/span> &lt;span class="n">si&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">si&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">n&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">si&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#计算开始&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sum1&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">prefs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">p1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">it&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">it&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">si&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sum2&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">prefs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">p2&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">it&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">it&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">si&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sum1Sq&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="nb">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prefs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">p1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">it&lt;/span>&lt;span class="p">],&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">it&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">si&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sum2Sq&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="nb">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prefs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">p2&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">it&lt;/span>&lt;span class="p">],&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">it&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">si&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pSum&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nb">sum&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">prefs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">p1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">it&lt;/span>&lt;span class="p">]&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">prefs&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">p2&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">it&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">it&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">si&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">num&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">pSum&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sum1&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">sum2&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">den&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">sqrt&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">sum1Sq&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nb">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sum1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sum2Sq&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="nb">pow&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sum2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">n&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#计算结束&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">den&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">r&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">num&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">den&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">r&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="序列距离stringtimeseries">序列距离(String,TimeSeries)&lt;/h2>
&lt;h3 id="dtwdynamic-time-warp">DTW(Dynamic Time Warp)&lt;/h3>
&lt;p>汉明距离可以度量两个长度相同的字符串之间的相似度，如果要比较两个不同长度的字符串，不仅要进行替换，而且要进行插入与删除的运算，在这种场合下，通常使用更加复杂的&lt;strong>编辑距离&lt;/strong>(Edit distance, Levenshtein distance)等算法。编辑距离是指两个字串之间，由一个转成另一个所需的最少编辑操作次数。许可的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。编辑距离求的是最少编辑次数，这是一个动态规划的问题，有兴趣的同学可以自己研究研究。&lt;/p>
&lt;p>时间序列是序列之间距离的另外一个例子。&lt;strong>DTW 距离&lt;/strong>(Dynamic Time Warp)是序列信号在时间或者速度上不匹配的时候一种衡量相似度的方法。神马意思？举个例子，两份原本一样声音样本 A、B 都说了“你好”，A 在时间上发生了扭曲，“你”这个音延长了几秒。最后 A:“你~~~~~~~好”，B：“你好”。DTW 正是这样一种可以用来匹配 A、B 之间的最短距离的算法。&lt;/p>
&lt;p>DTW 距离在保持信号先后顺序的限制下对时间信号进行“膨胀”或者“收缩”，找到最优的匹配，与编辑距离相似，这其实也是一个动态规划的问题:&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07221153-ea76b098f70a4a68b4929789c032ef69.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="ch">#!/usr/bin/python2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># -*- coding:UTF-8 -*-&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># code related at: http://blog.mckelv.in/articles/1453.html&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">sys&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">distance&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">lambda&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">b&lt;/span> &lt;span class="p">:&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="n">b&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">dtw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sa&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">sb&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s1">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1"> &amp;gt;&amp;gt;&amp;gt;dtw(u&amp;#34;干啦今今今今今天天气气气气气好好好好啊啊啊&amp;#34;, u&amp;#34;今天天气好好啊&amp;#34;)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1"> 2
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1"> &amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">MAX_COST&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">&amp;lt;&amp;lt;&lt;/span>&lt;span class="mi">32&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">#初始化一个len(sb) 行(i)，len(sa)列(j)的二维矩阵&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">len_sa&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sa&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">len_sb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sb&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># BUG:这样是错误的(浅拷贝): dtw_array = [[MAX_COST]*len(sa)]*len(sb)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dtw_array&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="n">MAX_COST&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">len_sa&lt;/span>&lt;span class="p">)]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">len_sb&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dtw_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">distance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sa&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">],&lt;/span>&lt;span class="n">sb&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">xrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">len_sb&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">xrange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">len_sa&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">==&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">continue&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">nb&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">nb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtw_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">nb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtw_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span> &lt;span class="ow">and&lt;/span> &lt;span class="n">j&lt;/span> &lt;span class="o">&amp;gt;&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">nb&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">append&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dtw_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">min_route&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">min&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nb&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">cost&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">distance&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sa&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">],&lt;/span>&lt;span class="n">sb&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">dtw_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">j&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">cost&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">min_route&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">dtw_array&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">len_sb&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="n">len_sa&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">main&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">argv&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="sa">u&lt;/span>&lt;span class="s1">&amp;#39;干啦今今今今今天天气气气气气好好好好啊啊啊&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">s2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="sa">u&lt;/span>&lt;span class="s1">&amp;#39;今天天气好好啊&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">d&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">dtw&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">s1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">s2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span> &lt;span class="n">d&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="vm">__name__&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s1">&amp;#39;__main__&amp;#39;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sys&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">exit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">main&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sys&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argv&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="网络节点距离">网络节点距离&lt;/h2>
&lt;h2 id="分布距离">分布距离&lt;/h2>
&lt;p>前面我们谈论的都是两个数值点之间的距离，实际上两个概率分布之间的距离是可以测量的。在统计学里面经常需要测量两组样本分布之间的距离，进而判断出它们是否出自同一个 population，常见的方法有&lt;strong>卡方检验&lt;/strong>(Chi-Square)和 &lt;strong>KL 散度&lt;/strong>( KL-Divergence)，下面说一说 KL 散度吧。&lt;/p>
&lt;p>先从信息熵说起，假设一篇文章的标题叫做“黑洞到底吃什么”，包含词语分别是 {黑洞, 到底, 吃什么}, 我们现在要根据一个词语推测这篇文章的类别。哪个词语给予我们的信息最多？很容易就知道是“黑洞”，因为“黑洞”这个词语在所有的文档中出现的概率太低啦，一旦出现，就表明这篇文章很可能是在讲科普知识。而其他两个词语“到底”和“吃什么”出现的概率很高，给予我们的信息反而越少。如何用一个函数 h(x) 表示词语给予的信息量呢？第一，肯定是与 p(x) 相关，并且是负相关。第二，假设 x 和 y 是独立的(黑洞和宇宙不相互独立，谈到黑洞必然会说宇宙),即 p(x,y) = p(x)p(y), 那么获得的信息也是叠加的，即 h(x, y) = h(x) + h(y)。满足这两个条件的函数肯定是负对数形式：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?h%28x%29%20=%20-ln%5C%20p%28x%29" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>对假设一个发送者要将随机变量 X 产生的一长串随机值传送给接收者，接受者获得的平均信息量就是求它的数学期望：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?H[x]%20=%20-%5Csum%20_x%7Bp%28x%29%5Cln%20p%28x%29%7D" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://latex.codecogs.com/gif.latex?H[x]%20=%20-%5Cint%20%7B%20p%28x%29%5Cln%20%7B%20p%28x%29%20%7D%20dx%20%7D" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>这就是熵的概念。另外一个重要特点是，熵的大小与字符平均最短编码长度是一样的(shannon)。设有一个未知的分布 p(x), 而 q(x) 是我们所获得的一个对 p(x) 的近似，按照 q(x) 对该随机变量的各个值进行编码，平均长度比按照真实分布的 p(x) 进行编码要额外长一些，多出来的长度这就是 KL 散度(之所以不说距离，是因为不满足对称性和三角形法则)，即：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/533521/201308/07221311-03bee2dca7e040e4889582d8182f4dde.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>KL 散度又叫&lt;strong>相对熵&lt;/strong>(relative entropy)。了解机器学习的童鞋应该都知道，在 Softmax 回归(或者 Logistic 回归)，最后的输出节点上的值表示这个样本分到该类的概率，这就是一个概率分布。对于一个带有标签的样本，我们期望的概率分布是：分到标签类的概率是 1， 其他类概率是 0。但是理想很丰满，现实很骨感，我们不可能得到完美的概率输出，能做的就是尽量减小总样本的 KL 散度之和(目标函数)。这就是 Softmax 回归或者 Logistic 回归中 Cost function 的优化过程啦。(PS：因为概率和为 1，一般的 logistic 二分类的图只画了一个输出节点，隐藏了另外一个)&lt;/p>
&lt;h1 id="feature-extraction特征抽取">Feature Extraction(特征抽取)&lt;/h1>
&lt;h2 id="tf-idf">TF-IDF&lt;/h2>
&lt;h2 id="word2vec">Word2Vec&lt;/h2>
&lt;h2 id="standardscaler">StandardScaler&lt;/h2>
&lt;h1 id="feature-selection特征选择">Feature Selection(特征选择)&lt;/h1>
&lt;h1 id="pca">PCA&lt;/h1>
&lt;h1 id="transform">Transform&lt;/h1>
&lt;h2 id="normalizerhttpsparkapacheorgdocslatestmllib-feature-extractionhtmlnormalizer">&lt;a href="http://spark.apache.org/docs/latest/mllib-feature-extraction.html#normalizer" target="_blank" rel="noopener">Normalizer&lt;/a>&lt;/h2>
&lt;h1 id="dimensionality-reduction降维">Dimensionality Reduction(降维)&lt;/h1>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Dimensional-Reduction-Algorithms.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Like clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarise or describe data using less information.&lt;/p>
&lt;p>This can be useful to visualize dimensional data or to simplify data which can then be used in a supervized learning method. Many of these methods can be adapted for use in classification and regression.&lt;/p>
&lt;ul>
&lt;li>Principal Component Analysis (PCA)&lt;/li>
&lt;li>Principal Component Regression (PCR)&lt;/li>
&lt;li>Partial Least Squares Regression (PLSR)&lt;/li>
&lt;li>Sammon Mapping&lt;/li>
&lt;li>Multidimensional Scaling (MDS)&lt;/li>
&lt;li>Projection Pursuit&lt;/li>
&lt;li>Linear Discriminant Analysis (LDA)&lt;/li>
&lt;li>Mixture Discriminant Analysis (MDA)&lt;/li>
&lt;li>Quadratic Discriminant Analysis (QDA)&lt;/li>
&lt;li>Flexible Discriminant Analysis (FDA)&lt;/li>
&lt;/ul>
&lt;h1 id="association-rule-learning-algorithms">Association Rule Learning Algorithms&lt;/h1>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Assoication-Rule-Learning-Algorithms.png" alt="Assoication Rule Learning Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Association rule learning are methods that extract rules that best explain observed relationships between variables in data.&lt;/p>
&lt;p>These rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organisation.&lt;/p>
&lt;p>The most popular association rule learning algorithms are:&lt;/p>
&lt;ul>
&lt;li>Apriori algorithm&lt;/li>
&lt;li>Eclat algorithm&lt;/li>
&lt;/ul>
&lt;h1 id="principal-component-analysis-pca--主成分分析">Principal Component Analysis, PCA | 主成分分析&lt;/h1>
&lt;p>主成分分析（PCA）和自编码器（AutoEncoders, AE）是无监督学习中的两种代表性方法。它与 LDA 同为机器学习中最基础的线性降维算法。&lt;/p>
&lt;p>PCA 和 LDA 是最常见的线性降维方法，它们按照某种准则为数据集找到一个最优投影方向 W 和截距 b，然后做变换得到降维后的数据集。因为是一个线性变换（严格来说叫仿射变换，因为有截距项），所以这两种方法叫做线性降维。&lt;/p>
&lt;p>非线性降维的两类代表方法是流形降维和 AutoEncoders，这两类方法也体现出了两种不同角度的“非线性”。流形方法的非线性体现在它认为数据分布在一个低维流形上，而流形本身就是非线性的，流形降维的代表方法是两篇 2000 年的 Science 论文提出的：多维放缩（multidimensional scaling, MDS）和局部线性嵌入（locally linear embedding, LLE）。不得不说实在太巧了，两种流形方法发表在同一年的 Science 上。&lt;/p>
&lt;p>AutoEncoders 的非线性和神经网络的非线性是一回事，都是利用堆叠非线性激活函数来近似任意函数。事实上，AutoEncoders 就是一种神经网络，只不过它的输入和输出相同，真正有意义的地方不在于网络的输出，而是在于网络的权重。&lt;/p></description></item><item><title>MachineLearning-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/machinelearning-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/machinelearning-cheatsheet/</guid><description>&lt;h1 id="machine-learning-cheatsheet--机器学习概念基础理论与常用算法速览">Machine Learning CheatSheet | 机器学习概念、基础理论与常用算法速览&lt;/h1>
&lt;p>对于某给定的任务 T，在合理的性能度量方案 P 的前提下，某计算机程序可以自主学习任务 T 的经验 E；随着提供合适、优质、大量的经验 E，该程序对于任务 T 的性能逐步提高。即随着任务的不断执行，经验的累积会带来计算机性能的提升。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://blogs.sas.com/content/subconsciousmusings/files/2017/04/machine-learning-cheet-sheet.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://api.ning.com/files/4*70048ZsE4d4vAJg-aTE95xugJ5lTBq5r9WVsZ54EvqkwPajzn6AeprtzImBldqq*dbze8ZGDtuyoeewh4MamSwpz8EHC5Q/Capture.PNG" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>判别式模型(Discriminative Model )是直接对条件概率 p(y|x;θ) 建模。常见的判别式模型有 线性回归模型、线性判别分析、支持向量机 SVM、神经网络等。　　生成式模型(Generative Model )则会对 x 和 y 的联合分布 p(x,y) 建模，然后通过贝叶斯公式来求得 p(yi|x)，然后选取使得 p(yi|x) 最大的 yi，即：&lt;/p>
&lt;p>简单来讲，前者不依赖于数据，能精确的解决问题；后者依赖于数据，目的是找到一个可接受的解。&lt;/p>
&lt;p>经典排序或者搜索算法中的快排、堆排、二叉、B 树等等，它们的存在价值不依赖于输入数据：有固定的步骤和解法，对于不同的输入数据都能正常运行(可能对特殊的数据有时候性能会差一些)；但是机器学习算法不同，你使用一个算法，做了一个模型，只是为了解决某个特定数据集下的某个需求；换一个数据和场合，这一套东西很可能就没有存在的意义了。&lt;/p>
&lt;p>第一个 community，是把机器学习看作人工智能分支的一个群体，这群人的主体是计算机科学家。现在的机器学习研究者可能很少有人读过 1983 年出的 Machine Learning: An Artificial Intelligence Approach 这本书。这本书的出版标志着机器学习成为人工智能中一个独立的领域。它其实是一部集早期机器学习研究之大成的文集，收罗了若干先贤(例 如 Herbert Simon，那位把诺贝尔奖、图灵奖以及各种各样和他相关的奖几乎拿遍了的科学天才)的大作，主编是 Ryszard S. Michalski(此君已去世多年了，他可算是机器学习的奠基人之一)、Jaime G. Carbonell(此君曾是 Springer 的 LNAI 的总编)、Tom Mitchell(此君是 CMU 机器学习系首任系主任、著名教材的作者，机器学习界没人不知道他吧)。Machine Learning 杂志的创刊，正是这群人努力的结果。这本书值得一读。虽然技术手段早就日新月异了，但有一些深刻的思想现在并没有过时。各个学科领域总有不少东西，换了新装之后又粉墨登场，现在热火朝天的 transfer learning，其实就是 learning by analogy 的升级版。&lt;/p>
&lt;p>人工智能的研究从以推理为重点到以知识为重点，再到以学习为重点，是有一条自然、清晰的脉络。人工智能出身的机器学习研究者，绝大部分 是把机器学习作为实现人工智能的一个途径，正如 1983 年的书名那样。他们关注的是人工智能中的问题，希望以机器学习为手段，但具体采用什么样的学习手段，是基于统计的、代数的、还是逻辑的、几何的，他们并不 care。这群人可能对统计学习目前 dominating 的地位未必满意。靠统计学习是不可能解决人工智能中大部分问题的，如果统计学习压制了对其他手段的研究，可能不是好事。这群人往往也不 care 在文章里 show 自己的数学水平，甚至可能是以简化表达自己的思想为荣。人工智能问题不是数学问题，甚至未必是依靠数学能够解决的问题。人工智能中许多事情的难处，往往在于我们不知道困难的本质在哪里，不知道问题在哪里。一旦问题清楚了，解决起来可能并不困难。&lt;/p>
&lt;p>第二个 community，是把机器学习看作应用统计学的一个群体，这群人的主体是统计学家。和纯数学相比，统计学不太干净，不少数学家甚至拒绝承认统计学是数学。但如果和人工智能相比，统计学就太干净了，统计学研究的问题是清楚的，不象人工智能那样，连问题到底在哪里都不知道。在相当长时间里，统计学家和机器学习一直保持着距离。慢慢地，不少统计学家逐渐意识到，统计学本来就该面向应用，而机器学习天生就是一个很好的切入点。因为机器学习虽然用到各种各样的数学，但要分析大量数据中蕴涵的规律，统计学是必不可少的。统计学出身的机器学习研究者，绝大部分是把机器学习当作应用统计学。他们关注的是如何把统计学中的理论和方法变成可以在计算机上有效实现的算法，至于这样的算法对人工智能中的什么问题有用，他们并不 care。&lt;/p>
&lt;p>这群人可能对人工智能毫无兴趣，在他们眼中，机器学习就是统计学习，是统计学比较偏向应用的一个分支，充其量是统计学与计算机科学的交叉。这群人对统计学习之外的学习手段往往是排斥的，这很自然，基于代数的、逻辑的、几何的学习，很难纳入统计学的范畴。&lt;/p>
&lt;p>两个群体的文化和价值观完全不同。第一个群体认为好的工作，对于第二个群体而言可能觉得没有技术含量，但第一个群体可能恰恰认为，简单的才好，正因为很好地抓住了问题本质，所以问题变得容易解决。第二个群体欣赏的工作，第一个群体可能觉得是故弄玄虚，看不出他想解决什么人工智能问题，根本就不是在搞人工智 能、搞计算机，但别人本来也没说自己是在搞人工智能、搞计算机，本来就不是在为人工智能做研究。两个群体各有其存在的意义，应该宽容一点，不需要去互较什么短长。但是既然顶着 Machine Learning 这个帽子的不是一伙儿，而是两伙儿，那么要跟进的新人就要谨慎了，先搞清楚自己更喜欢哪伙儿。&lt;/p>
&lt;ul>
&lt;li>加强概率与统计的基础课程，建议采用莫里斯 · 德格鲁特 (Morris H.DeGroot) 和马克 · 舍维什 (Mark J.Schervish) 合著的第四版《概率论与数理统计》(Probability and Statistics) 为教材。&lt;/li>
&lt;li>在线性代数课程里，加强矩阵分析的内容。教材建议使用吉尔伯特 · 斯特朗 (Gilbert Strang) 的《线性代数导论》(Introduction to Linear Algebra)。吉尔伯特 · 斯特朗在麻省理工学院一直讲述线性代数，他的网上视频课程堪称经典。后续建议开设矩阵计算，采用特雷费森 · 劳埃德 (Trefethen N.Lloyd) 和戴维 · 鲍 (David Bau lll) 著作的《数值线性代数》(Numerical Linear Algebra) 为教科书。&lt;/li>
&lt;li>开设机器学习课程。机器学习有许多经典的书籍，但大多不太适宜做本科生的教材。最近，麻省理工学院出版的约翰 · 凯莱赫 (John D.Kelleher) 和布瑞恩 · 麦克 · 纳米 (Brian Mac Namee) 等人著作的《机器学习基础之预测数据分析》(Fundamentals of Machine Learning for Predictive Data Analytics )，或者安得烈 · 韦伯 (Andrew R.Webb) 和基思 · 科普塞 (Keith D.Copsey) 合著的第三版《统计模式识别》(Statistical Pattern Recognition ) 比较适合作为本科生的教科书。同时建议课程设置实践环节，让学生尝试将机器学习方法应用到某些特定问题中。&lt;/li>
&lt;li>开设数值优化课程，建议参考教材乔治 · 诺塞达尔 (Jorge Nocedal) 和史蒂芬 · 赖特 (Stephen J.Wright) 的第二版《数值优化》(Numerical Optimization )，或者开设数值分析，建议采用蒂莫西 · 索尔的《数值分析》(Numerical Analysis) 为教材。&lt;/li>
&lt;li>加强算法课程，增加高级算法，比如随机算法，参考教材是迈克尔 · 米曾马克 (Michael Mitzenmacher) 和伊莱 · 阿普法 (Eli Upfal) 的《概率与计算: 随机算法与概率分析》(Probability and Computing: Randomized Algorithms and Probabilistic Analysis)。&lt;/li>
&lt;li>在程序设计方面，增加或加强并行计算的内容。特别是在深度学习技术的执行中，通常需要 GPU 加速，可以使用戴维 · 柯克 (David B.Kirk) 和胡文美 (Wen-mei W.Hwu) 的教材 《大规模并行处理器编程实战》(第二版)(Programming Massively Parallel Processors:A Hands-on Approach,Second Edition )；另外，还可以参考优达学城 (Udacity) 上英伟达 (Nvidia) 讲解 CUDA 计算的公开课。&lt;/li>
&lt;/ul>
&lt;p>在机器学习领域称为流形学习（Manifold learning），或降维问题（Dimension reduction）。如图 18 所示，数据是在三维空间给出的（B），每个点是 3 个坐标，看起来是三维数据；但其本质上位于三维空间的一张曲面上，即二维流形曲面上（A），其本征维数是 2。因此，可以将其一一映射到平面上（C）。&lt;/p>
&lt;p>传统机器学习的流程一般由两个主要的模块组成。第一个模块称为特征工程，将原始输入数据（高维向量）变换为一个低维向量，即，用这个低维向量来作为表达输入数据的特征（或描述子，feature or descriptor）。不同的人有不同的方法来计算这个低维特征，称为特征抽取（Feature extraction），如图 19 显示了不同的图像特征抽取方法。各种计算的特征都有一定的实用范围，因此，有人就提出如何选择或组合各种特征得到更好的特征，这是特征工程的另一个子问题，称为特征选择（Feature selection）。针对三维形状分割，我们发现对三维局部块的众多描述子并不是串联起来一起使用更好，而是针对不同的形状的部位使用不同的描述子会更好，于是我们使用稀疏选择的方法来选择形状描述子用于形状分割，详细可见如下论文。&lt;/p>
&lt;p>由于这种特征是通过人为设计的算法来得到的，因此这种特征称为人工特征（Hand-crafted features）。人工特征的抽取需要人们对输入数据的认知或者领域知识（Domain knowledge），因此在很多情况下会局限于人的经验和认知。&lt;/p>
&lt;p>第二个模块称为预测模型，即选用什么样的预测方法来拟合给定的数据，我们前面所讲的拟合就是指这个过程。预测模型的目标就是希望学习到的模型在预测未知目标时越精确越好。一般要考虑模型的复杂度及精确度等因素。在机器学习领域有非常多的方法，这里不一一介绍。&lt;/p>
&lt;p>拟合函数所带的参数的个数与样本数据的个数之间的差代表着这个拟合函数的“自由度”。网络越来越“深”后，拟合模型中的可调整参数的数量就非常大，通常能达到百万计甚至几十亿级。因此，层数很大的深度网络（模型过于复杂）能够表达一个自由度非常大的函数空间，甚至远高于目标函数空间（过完备空间），即自由度远大于 0。这样就很容易导致过拟合（Overfitting），&lt;/p>
&lt;h1 id="模型评估与正则化">模型评估与正则化&lt;/h1>
&lt;p>机器学习的算法或者模型的分类有很多种，其中有一种分法把模型分为 Discriminative Modeling (判别模型)和 Generative Modeling (生成模型)两种。为了写起来方便我们以下简称 DM 和 GM。在一个基本的机器学习问题中，我们通常有输入 $x \in X$和输出 $y \in Y$两个部分，简单地来说，DM 关注于 $x$和 $y$的关系，或者说在给定某个 $x$的情况下所对应的 $y$应该满足的规律或分布，即评估对象是最大化条件概率$p(y|x)$并直接对其建模；而 GM 则试图描述 $x$和 $y$的联合分布，即模型评估对象是最大化联合概率$p(x,y)$并对其建模。其实两者的评估目标都是要得到最终的类别标签$Y$，而$Y=argmax p(y|x)$，不同的是判别式模型直接通过解在满足训练样本分布下的最优化问题得到模型参数，主要用到拉格朗日乘算法、梯度下降法，常见的判别式模型如最大熵模型、CRF、LR、SVM 等；而生成式模型先经过贝叶斯转换成$Y = argmax p(y|x) = argmax p(x|y)*p(y)$，然后分别学习$p(y)$和$p(x|y)$的概率分布，主要通过极大似然估计的方法学习参数，如 NGram、HMM、Naive Bayes。比较而言，判别式模型会更加灵活，而生成式模型一般需要将特征加入马尔科夫链。但是判别式模型需要有指导训练，而生成式模型可以无指导训练。&lt;/p>
&lt;p>给定某系统的若干样本，求取该系统的参数，主要分为频率学派与贝叶斯学派。&lt;/p>
&lt;h4 id="频率学派">频率学派&lt;/h4>
&lt;p>假定参数是某个或者某些未知的定值，求这些参数如何取值，能够使得某目标函数取得极大或者极小值，典型的代表有矩估计、MLE、MaxEnt 以及 EM。&lt;/p>
&lt;h4 id="贝叶斯学派">贝叶斯学派&lt;/h4>
&lt;p>基于贝叶斯模型，假设参数本身是变化的，服从某个分布。求在这个分布约束下使得某目标函数极大/极小。大数据下是频率学派对于贝叶斯学派的一次有效逆袭。&lt;/p>
&lt;h1 id="算法模型">算法模型&lt;/h1>
&lt;h2 id="判别式模型与生成式模型">判别式模型与生成式模型&lt;/h2>
&lt;h2 id="有监督模型与无监督模型">有监督模型与无监督模型&lt;/h2>
&lt;h1 id="数据拟合">数据拟合&lt;/h1>
&lt;p>在科学技术的各领域中，我们所研究的事件一般都是有规律（因果关系）的，即自变量集合 X 与应变量集合 Y 之间存在的对应关系通常用映射来描述（特殊情况：实数集合 R1 到实数集合 R2 之间的映射称为函数）。这样能根据映射（函数）规律作出预测并用于实际应用。&lt;/p>
&lt;p>但是，很多工程问题难以直接推导出变量之间的函数表达式；或者即使能得出表达式，公式也十分复杂，不利于进一步的分析与计算。这时可以通过诸如采样、实验等方法获得若干离散的数据（称为样本数据点），然后根据这些数据，希望能得到这些变量之间的函数关系，这个过程称为数据拟合（Data fitting），在数理统计中也称为回归分析（Regression analysis）。&lt;/p>
&lt;p>根据 Lagrange 插值定理，一定能找到一个次多项式来插值给定的个样本点。但如果较大，则这样得到的高次多项式很容易造成“过拟合”(Overfitting)。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/45597742-24cec900-ba03-11e8-8489-1afd50b0b9ea.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="loss-function--损失函数">Loss Function | 损失函数&lt;/h2>
&lt;h2 id="model-evaluation--模型评估">Model Evaluation | 模型评估&lt;/h2>
&lt;h2 id="overfitting--regularization--过拟合与正则化">Overfitting &amp;amp; Regularization | 过拟合与正则化&lt;/h2>
&lt;h1 id="classification-分类">Classification: 分类&lt;/h1>
&lt;p>分类问题的核心即是希望能够在特征空间内寻找一条决策边界线或者是一条曲线，来将特征空间里区分为我们想要的类别。在分类算法领域，最常见的即是 Logistic 回归、决策树以及 SVM。&lt;/p>
&lt;pre tabindex="0">&lt;code> Condition: A Not A
Test says “A” True positive (TP) | False positive (FP)
----------------------------------
Test says “Not A” False negative (FN) | True negative (TN)
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>
&lt;p>TP&amp;ndash; 将正类预测为正类数；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>FN&amp;ndash; 将正类预测为负类数；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>FP&amp;ndash; 将负类预测为正类数；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>TN&amp;ndash; 将负类预测为负类数；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>准确率(Precision，$P$)定义为被正确预测为正例的数目占所有被预测为正例的数目的比重: $$ P = \frac{T_p}{T_p + F_p} $$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>召回率(Recall，$R$)定义为被正确预测为正例的数目占所有实际正例数目的比重: $$ R = \frac{T_p}{T_p + F_n} $$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>F1 则是相对综合的评价值，定义为了准确率与召回率的调和平均数: $$ F1 = 2 \frac{P * R}{P + R} $$&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>当量化算法的好坏时，首先需要看以下几个指标：&lt;/p>
&lt;ul>
&lt;li>True Positives(TP)：即实际为正例且被分类器划分为正例的样本数；&lt;/li>
&lt;li>False Positives(FP)：即实际为负例但被分类器划分为正例的样本数；&lt;/li>
&lt;li>True Negatives(TN)：即实际为负例且被分类器划分为负例的样本数；&lt;/li>
&lt;li>False Negatives(FN)：即实际为负例但被分类器划分为负例的样本数；&lt;/li>
&lt;/ul>
&lt;p>一般进行评价时，有两个指标：(1)准确率&lt;/p>
&lt;p>$$
accuracy = \frac{TP+TN}{P+N}
$$&lt;/p>
&lt;p>就是被分队的样本除以所有的样本数，通常来说，准确率越高，分类器越好。(2)召回率&lt;/p>
&lt;p>$$
recall = \frac{TP}{TP+FN}
$$&lt;/p>
&lt;p>召回率是覆盖面的度量，度量有多少个正例被分为正例。&lt;/p>
&lt;h2 id="regression-回归模型">Regression: 回归模型&lt;/h2>
&lt;p>$$
\begin{align*}
&amp;amp; h*{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 \
&amp;amp; h*{\theta}(x) = \sum_{i=0}^n \theta_i x_i = \theta^Tx
\end{align*}
$$&lt;/p>
&lt;h1 id="algorithms-算法">Algorithms: 算法&lt;/h1>
&lt;p>算法基本上从功能或者形式上来分类。比如，基于树的算法，神经网络算法。这是一个很有用的分类方式，但并不完美。因为有许多算法可以轻易地被分到两类中去，比如说 Learning Vector Quantization 就同时是神经网络类的算法和基于实例的方法。正如机器学习算法本身没有完美的模型一样，算法的分类方法也没有完美的。&lt;/p>
&lt;h3 id="grouped-by-learning-style根据学习风格分类">Grouped By Learning Style(根据学习风格分类)&lt;/h3>
&lt;h4 id="supervised-learning">Supervised Learning&lt;/h4>
&lt;p>输入数据被称为训练数据，并且有已知的结果或被标记。比如说一封邮件是否是垃圾邮件，或者说一段时间内的股价。模型做出预测，如果错了就会被修正，这个过程一直持续到对于训练数据它能够达到一定的正确标准。问题例子包括分类和回归问题，算法例子包括逻辑回归和反向神经网络。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Supervised-Learning-Algorithms.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>有监督学习的通俗例子可以以人类认知月亮的过程为例，小时候会认为黑夜里天空中明亮的圆盘状物体为月亮，后来随着时间的推移，发现圆盘状这个特点并非一个决定性因素。&lt;/p>
&lt;h4 id="unsupervised-learninghttpsgithubcomokulbilisimawesome-datascienceblobmasteralgorithmsmdunsupervised-learning">&lt;a href="https://github.com/okulbilisim/awesome-datascience/blob/master/Algorithms.md#unsupervised-learning" target="_blank" rel="noopener">Unsupervised Learning&lt;/a>&lt;/h4>
&lt;p>输入数据没有被标记，也没有确定的结果。模型对数据的结构和数值进行归纳。问题例子包括 Association rule learning 和聚类问题，算法例子包括 Apriori 算法和 K-均值算法。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Unsupervised-Learning-Algorithms.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h4 id="semi-supervised-learning">Semi-Supervised Learning&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Semi-supervised-Learning-Algorithms.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>输入数据是被标记的和不被标记的数据的混合，有一些预测问题但是模型也必须学习数据的结构和组成。问题例子包括分类和回归问题，算法例子基本上是无监督学习算法的延伸。&lt;/p>
&lt;h3 id="grouped-by-similarity根据算法相似度分类">Grouped By Similarity(根据算法相似度分类)&lt;/h3>
&lt;h4 id="regression-algorithms">Regression Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Regression-Algorithms.png" alt="Regression Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Regression is concerned with modelling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model.&lt;/p>
&lt;p>Regression methods are a workhorse of statistics and have been cooped into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm. Really, regression is a process.&lt;/p>
&lt;p>The most popular regression algorithms are:&lt;/p>
&lt;ul>
&lt;li>Ordinary Least Squares Regression (OLSR)&lt;/li>
&lt;li>Linear Regression&lt;/li>
&lt;li>Logistic Regression&lt;/li>
&lt;li>Stepwise Regression&lt;/li>
&lt;li>Multivariate Adaptive Regression Splines (MARS)&lt;/li>
&lt;li>Locally Estimated Scatterplot Smoothing (LOESS)&lt;/li>
&lt;/ul>
&lt;h4 id="instance-based-algorithms">Instance-based Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Instance-based-Algorithms.png" alt="Instance-based Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>Instance based learning model a decision problem with instances or examples of training data that are deemed important or required to the model.&lt;/p>
&lt;p>Such methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning. Focus is put on representation of the stored instances and similarity measures used between instances.&lt;/p>
&lt;p>The most popular instance-based algorithms are:&lt;/p>
&lt;ul>
&lt;li>k-Nearest Neighbour (kNN)&lt;/li>
&lt;li>Learning Vector Quantization (LVQ)&lt;/li>
&lt;li>Self-Organizing Map (SOM)&lt;/li>
&lt;li>Locally Weighted Learning (LWL)&lt;/li>
&lt;/ul>
&lt;h4 id="regularization-algorithms">Regularization Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Regularization-Algorithms.png" alt="Regularization Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing.&lt;/p>
&lt;p>I have listed regularization algorithms separately here because they are popular, powerful and generally simple modifications made to other methods.&lt;/p>
&lt;p>The most popular regularization algorithms are:&lt;/p>
&lt;ul>
&lt;li>Ridge Regression&lt;/li>
&lt;li>Least Absolute Shrinkage and Selection Operator (LASSO)&lt;/li>
&lt;li>Elastic Net&lt;/li>
&lt;li>Least-Angle Regression (LARS)&lt;/li>
&lt;/ul>
&lt;h4 id="decision-tree-algorithms">Decision Tree Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Decision-Tree-Algorithms.png" alt="Decision Tree Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>Decision tree methods construct a model of decisions made based on actual values of attributes in the data.&lt;/p>
&lt;p>Decisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems. Decision trees are often fast and accurate and a big favorite in machine learning.&lt;/p>
&lt;p>The most popular decision tree algorithms are:&lt;/p>
&lt;ul>
&lt;li>Classification and Regression Tree (CART)&lt;/li>
&lt;li>Iterative Dichotomiser 3 (ID3)&lt;/li>
&lt;li>C4.5 and C5.0 (different versions of a powerful approach)&lt;/li>
&lt;li>Chi-squared Automatic Interaction Detection (CHAID)&lt;/li>
&lt;li>Decision Stump&lt;/li>
&lt;li>M5&lt;/li>
&lt;li>Conditional Decision Trees&lt;/li>
&lt;/ul>
&lt;h4 id="bayesian-algorithms">Bayesian Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Bayesian-Algorithms.png" alt="Bayesian Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>Bayesian methods are those that are explicitly apply Bayes’ Theorem for problems such as classification and regression.&lt;/p>
&lt;p>The most popular Bayesian algorithms are:&lt;/p>
&lt;ul>
&lt;li>Naive Bayes&lt;/li>
&lt;li>Gaussian Naive Bayes&lt;/li>
&lt;li>Multinomial Naive Bayes&lt;/li>
&lt;li>Averaged One-Dependence Estimators (AODE)&lt;/li>
&lt;li>Bayesian Belief Network (BBN)&lt;/li>
&lt;li>Bayesian Network (BN)&lt;/li>
&lt;/ul>
&lt;h4 id="clustering-algorithms">Clustering Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Clustering-Algorithms.png" alt="Clustering Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>Clustering, like regression describes the class of problem and the class of methods.&lt;/p>
&lt;p>Clustering methods are typically organized by the modelling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.&lt;/p>
&lt;p>The most popular clustering algorithms are:&lt;/p>
&lt;ul>
&lt;li>k-Means&lt;/li>
&lt;li>k-Medians&lt;/li>
&lt;li>Expectation Maximisation (EM)&lt;/li>
&lt;li>Hierarchical Clustering&lt;/li>
&lt;/ul>
&lt;h4 id="artificial-neural-network-algorithms">Artificial Neural Network Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Artificial-Neural-Network-Algorithms.png" alt="Artificial Neural Network Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Artificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks.&lt;/p>
&lt;p>They are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.&lt;/p>
&lt;p>Note that I have separated out Deep Learning from neural networks because of the massive growth and popularity in the field. Here we are concerned with the more classical methods.&lt;/p>
&lt;p>The most popular artificial neural network algorithms are:&lt;/p>
&lt;ul>
&lt;li>Perceptron&lt;/li>
&lt;li>Back-Propagation&lt;/li>
&lt;li>Hopfield Network&lt;/li>
&lt;li>Radial Basis Function Network (RBFN)&lt;/li>
&lt;/ul>
&lt;h4 id="deep-learning-algorithms">Deep Learning Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Deep-Learning-Algorithms.png" alt="Deep Learning Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>Deep Learning methods are a modern update to Artificial Neural Networks that exploit abundant cheap computation.&lt;/p>
&lt;p>They are concerned with building much larger and more complex neural networks, and as commented above, many methods are concerned with semi-supervised learning problems where large datasets contain very little labelled data.&lt;/p>
&lt;p>The most popular deep learning algorithms are:&lt;/p>
&lt;ul>
&lt;li>Deep Boltzmann Machine (DBM)&lt;/li>
&lt;li>Deep Belief Networks (DBN)&lt;/li>
&lt;li>Convolutional Neural Network (CNN)&lt;/li>
&lt;li>Stacked Auto-Encoders&lt;/li>
&lt;/ul>
&lt;h4 id="support-vector-machines">Support Vector Machines&lt;/h4>
&lt;h4 id="ensemble-algorithms">Ensemble Algorithms&lt;/h4>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://3qeqpr26caki16dnhd19sv6by6v.wpengine.netdna-cdn.com/wp-content/uploads/2013/11/Ensemble-Algorithms.png" alt="Ensemble Algorithms" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction.&lt;/p>
&lt;p>Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.&lt;/p>
&lt;ul>
&lt;li>Boosting&lt;/li>
&lt;li>Bootstrapped Aggregation (Bagging)&lt;/li>
&lt;li>AdaBoost&lt;/li>
&lt;li>Stacked Generalization (blending)&lt;/li>
&lt;li>Gradient Boosting Machines (GBM)&lt;/li>
&lt;li>Gradient Boosted Regression Trees (GBRT)&lt;/li>
&lt;li>Random Forest&lt;/li>
&lt;/ul>
&lt;h2 id="markov">Markov&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://user-images.githubusercontent.com/5803001/44188707-98a25b00-a151-11e8-8441-32439e186b91.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://parg.co/Ajm" target="_blank" rel="noopener">Data PreProcessing&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="模型评估与正则化-1">模型评估与正则化&lt;/h1>
&lt;h2 id="vc-维">VC 维&lt;/h2>
&lt;p>VC 维是一类函数，描述的是这类函数能够把多少个样本的所有组合都划分开来。当你选定了一个模型以及它对应的特征之后，你是大概可以知道这组模型和特征的选择能够对多大的数据集进行分类的。此外，一类函数的 VC 维的大小，还可以反应出这类函数过拟合的可能性。&lt;/p>
&lt;h1 id="最优化理论">最优化理论&lt;/h1>
&lt;p>绝大多数机器学习问题的解决，都可以划分为两个阶段：建模和优化。所谓建模就是后面我们会提到的各种用模型来描述问题的方法，而优化就是建模完成之后求得模型的最优参数的过程。机器学习中常用的模型有很多，但背后用到的优化方法却并没有那么多。换句话说，很多模型都是用的同一套优化方法，而同一个优化方法也可以用来优化很多不同模型。对各种常用优化方法的和思想有所有了解非常有必要，对于理解模型训练的过程，以及解释各种情况下模型训练的效果都很有帮助。这里面包括最大似然、最大后验、梯度下降、拟牛顿法、L-BFGS 等。&lt;/p>
&lt;h1 id="有无监督学习">有/无监督学习&lt;/h1>
&lt;p>在目前的工业实践中，有监督学习的应用面仍然是最广泛的，这是因为我们现实中遇到的很多问题都是希望对某个事物的某个属性做出预测，而这些问题通过合理的抽象和变换，都可以转化为有监督学习的问题。&lt;/p>
&lt;h1 id="贝叶斯模型">贝叶斯模型&lt;/h1>
&lt;p>朴素贝叶斯有很强的假设，这个假设很多问题都不满足，模型结构也很简单，所以其优化效果并不是最好的。&lt;/p>
&lt;h1 id="分类与回归">分类与回归&lt;/h1>
&lt;p>在掌握了机器学习模型的基础流程之后，需要学习两种最基础的模型形式：线性模型和树形模型，分别对应着线性回归/逻辑回归和决策回归/分类树。&lt;/p>
&lt;h1 id="ensemble--模型增强">Ensemble | 模型增强&lt;/h1></description></item><item><title>NLP-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/nlp-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/nlp-cheatsheet/</guid><description>&lt;h1 id="nlp-cheatsheet--自然语言处理概览">NLP CheatSheet | 自然语言处理概览&lt;/h1>
&lt;p>语言是生物同类之间由于沟通需要而制定的具有统一编码解码标准的声音(图像)指令。包含手势、表情、语音等肢体语言，文字是显像符号。&lt;/p>
&lt;p>自然语言通常是指一种自然地随文化演化的语言。例如英语、汉语、日语等。有别于人造语言，例如世界语、编程语言等。&lt;/p>
&lt;p>自然语言处理包括自然语言理解和自然语言生成。自然语言理解是将自然语言变成计算机能够理解的语言，及非结构化文本转变为结构化信息。&lt;/p>
&lt;p>自然语言处理的四大经典难题：问答、复述、文摘、翻译。理论上，这四个题目任何一个彻底解决了，其他三个也就自然解决了。&lt;/p>
&lt;p>自然语言处理经历了从规则的方法到基于统计的方法。基于统计的自然语言处理方法，在数学模型上和通信就是相同的，甚至相同的。但是科学家们也是用了几十年才认识到这个问题。统计语言模型的初衷是为了解决语音识别问题，在语音识别中，计算机需要知道一个文字序列能否构成一个有意义的句子。&lt;/p>
&lt;p>自然语言处理（NLP）旨在使计算机可以智能地处理人类语言，是跨越人工智能、计算科学、认知科学、信息处理和语言学的重要跨学科领域。由于计算机和人类语言之间的交互技术的进步，语音识别、对话系统、信息检索、问答和机器翻译等 NLP 应用已经开始重塑人们识别、获取和利用信息的方式。&lt;/p>
&lt;p>NLP 的发展经历了三次浪潮：理性主义、经验主义和深度学习。在第一次浪潮中，理性主义方法主张设计手工制作的规则，将知识融入 NLP 系统，这种主张假设人类思维中的语言知识是通过通用继承预先固定下来的。在第二次浪潮中，经验方法假设丰富的感官输入和表面形式的可观察语言数据是必需的，并且足以使大脑学习自然语言的详细结构。因此，人们开发了概率模型来发现大型语料库中语言的规律性。在第三次浪潮中，受生物神经系统的启发，深度学习利用非线性处理的层次模型，从语言数据中学习内在表征，旨在模拟人类的认知能力。&lt;/p>
&lt;p>深度学习和自然语言处理的交叉在实际任务中取得了惊人的成功。语音识别是深度学习深刻影响的第一个工业 NLP 应用。随着大规模训练数据变得可用，深度神经网络实现了比传统经验方法低得多的识别误差。深度学习在 NLP 领域的另一个成功应用是机器翻译。使用神经网络对人类语言之间的映射进行建模的端到端神经机器翻译已经证明可以大大提高翻译质量。因此，神经机器翻译已迅速成为大型科技公司（谷歌、微软、Facebook、百度等）提供的主要商业在线翻译服务的新技术。NLP 的许多其他领域，包括语言理解和对话、词法分析和解析、知识图谱、信息检索、文本问答、社交计算、语言生成和文本情感分析，也通过深度学习取得了很大的进步，掀起了 NLP 发展的第三次浪潮。如今，深度学习是应用于几乎所有 NLP 任务的主导方法。&lt;/p>
&lt;p>当前的深度学习技术是从前两大浪潮发展的 NLP 技术在概念和范式上的革命。这场革命的关键支柱包括语言实体（子词、单词、短语、句子、段落、文档等）的分布式表示，通过嵌入、嵌入的语义泛化、语言的长跨深度序列建模、有效地表示从低到高的语言水平的分层网络以及端到端的深度学习方法，来共同完成许多 NLP 任务。在深度学习浪潮之前，这些都不可能，不仅是因为在之前的浪潮中缺乏大数据和强大的计算，而且同样重要的是，近年来我们错过了正确的框架，直到深度学习范式出现。&lt;/p>
&lt;h1 id="language-model语言模型">Language Model(语言模型)&lt;/h1>
&lt;p>语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在 NLP 的其它任务里也都能用到。语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率 $P(w_1, w_2, …, w_t)$。$w_1$ 到 $w_t$ 依次表示这句话中的各个词。有个很简单的推论是：&lt;/p>
&lt;p>$P(w&lt;em>1, w_2, …, w_t) = P(w_1) \times P(w_2 | w_1) \times P(w_3 | w_1, w_2) \times … \times P(w_t | w_1, w_2, …, w&lt;/em>{t-1})$&lt;/p>
&lt;p>常用的语言模型都是在近似地求 $P(w&lt;em>t | w_1, w_2, …, w&lt;/em>{t-1})$。比如 n-gram 模型就是用 $P(w&lt;em>t | w&lt;/em>{t-n+1}, …, w_{t-1})$ 近似表示前者。&lt;/p>
&lt;h2 id="词表示word-representation">词表示(Word Representation)&lt;/h2>
&lt;p>自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。&lt;/p>
&lt;h3 id="one-hot-representation">One-hot Representation&lt;/h3>
&lt;p>NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。&lt;/p>
&lt;p>举个栗子，&lt;/p>
&lt;p>“话筒”表示为 [0 0 0 &lt;strong>1&lt;/strong> 0 0 0 0 0 0 0 0 0 0 0 0 …]&lt;/p>
&lt;p>“麦克”表示为 [0 0 0 0 0 0 0 0 &lt;strong>1&lt;/strong> 0 0 0 0 0 0 0 …]&lt;/p>
&lt;p>每个词都是茫茫 0 海中的一个 1。&lt;/p>
&lt;p>这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字&lt;/p>
&lt;p>ID。比如刚才的例子中，话筒记为 3，麦克记为 8(假设从 0 开始记)。如果要编程实现的话，用 Hash&lt;/p>
&lt;p>表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。&lt;/p>
&lt;p>当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。&lt;/p>
&lt;h3 id="词向量distributed-representation">词向量(Distributed Representation)&lt;/h3>
&lt;p>而是用 &lt;strong>Distributed Representation&lt;/strong>(不知道这个应该怎么翻译，因为还存在一种叫“Distributional Representation”的表示方法，又是另一个不同的概念)表示的一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177,−0.107, 0.109, −0.542, …]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的，后文会提到目前计算出这种向量的主流方法。(个人认为)Distributed representation&lt;/p>
&lt;p>最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。&lt;/p>
&lt;h2 id="统计语言模型">统计语言模型&lt;/h2>
&lt;p>传统的统计语言模型是表示语言基本单位(一般为句子)的概率分布函数，这个概率分布也就是该语言的生成模型。一般语言模型可以使用各个词语条件概率的形式表示：&lt;/p>
&lt;p>$p(s)=p(w&lt;em>1^T)=p(w_1,w_2,\dots,w_T)=\Pi^T&lt;/em>{t=1}p(w_t|Context)$&lt;/p>
&lt;p>目标也可以是采用极大似然估计来求取最大化的 Log 概率的平均值，公式为&lt;/p>
&lt;p>$$
\frac{1}{T}\sum^T_{t=1}\sum_{-c \le j\le c,j \ne0}log p(w_{t+j}|w_t)
$$&lt;/p>
&lt;p>其中：&lt;/p>
&lt;ul>
&lt;li>$c$是训练上下文的大小。譬如$c$取值为 5 的情况下，一次就拿 5 个连续的词语进行训练。一般来说$c$越大，效果越好，但是花费的时间也会越多。&lt;/li>
&lt;li>$p(w*{t+j}|w_t)$表示$w_t$条件下出现$w*{t+j}$的概率。&lt;/li>
&lt;/ul>
&lt;p>其中 Context 即为上下文，根据对 Context 不同的划分方法，可以分为五大类。&lt;/p>
&lt;h3 id="上下文无关模型contextnull">上下文无关模型(Context=NULL)&lt;/h3>
&lt;p>该模型仅仅考虑当前词本身的概率，不考虑该词所对应的上下文环境。这是一种最简单，易于实现，但没有多大实际应用价值的统计语言模型。&lt;/p>
&lt;p>$p(w&lt;em>t|Context)=p(w_t)=\frac{N&lt;/em>{w_t}}{N}$&lt;/p>
&lt;p>这个模型不考虑任何上下文信息，仅仅依赖于训练文本中的词频统计。它是 n-gram 模型中当 n=1 的特殊情形，所以有时也称作 Unigram Model (—元文法统计模型)。实际应用中，常被应用到一些商用语音识别系统中。&lt;/p>
&lt;h3 id="n-gram-模型contextwt-n1wt-n2dotsw_t-1">N-Gram 模型(Context=$w*{t-n+1},w*{t-n+2},\dots,w_{t-1}$)&lt;/h3>
&lt;p>N-Gram 模型时大词汇连续语音识别中常用的一种语言模型，对中文而言，我们称之为汉语语言模型(CLM, Chinese Language Model)。汉语语言模型利用上下文中相邻词间的搭配信息，在需要把连续无空格的拼音、笔画，或代表字母或笔画的数字，转换成汉字串(即句子)时，可以计算出最大概率的句子，从而实现从到汉字的自动转换，无需用户手动选择，避开了许多汉字对应一个相同的拼音(或笔画串、数字串)的重码问题。该模型基于这样一种假设，第 n 个词的出现只与前面 n-1 个词相关，而与其它任何词都不相关，整句的概率就是各个词出现的概率的乘积。这些概率可以通过直接从语料中统计 n 个词同时出现的次数得到。常用的是二元的 Bi-Gram 和三元的 Tri-Gram。&lt;/p>
&lt;p>n=1 时，就是上面所说的上下文无关模型，这里 N-Gram—般认为是$N\ge2$是 的上下文相关模型。当 n=2 时，也称为 Bigram 语言模型，直观的想，在自然语 言中“白色汽车”的概率比“白色飞翔”的概率要大很多，也就是 P(汽车|白色)&amp;gt; P(飞翔|白色)。n&amp;gt;2 也类似，只是往前看 n-1 个词而不是一个词。一般 N-Gram 模型优化的目标是最大 log 似然，即：&lt;/p>
&lt;p>$\Pi^T*{t=1}p_t(w_t|w*{t-n+1},w&lt;em>t|w&lt;/em>{t-n+2},\dots,w&lt;em>t|w&lt;/em>{t-1})$&lt;/p>
&lt;p>N-Gram 模型的优点包含了前 N-1 个词所能提供的全部信息，这些信息对当前 词出现具有很强的约束力。同时因为只看 N-1 个词而不是所有词也使得模型的效率较高。这里以 Bi-Gram 做一个实例，假设语料库总的词数为 13748：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/407700/201310/18171638-c87b895734e748ff9a188265bccbe6bb.png" alt="image" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://images.cnitblog.com/blog/407700/201310/18171638-c325ffe1717e4763838913964e3971fc.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>N-Gram 语言模型也存在一些问题：&lt;/p>
&lt;ul>
&lt;li>n-gram 语言模型无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型。大部分研究或工作都是使用 Trigram，就算使用高阶的模型，其统计 到的概率可信度就大打折扣，还有一些比较小的问题采用 Bigram。&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>这种模型无法建模出词之间的相似度，有时候两个具有某种相似性的词，如果一个词经常出现在某段词之后，那么也许另一个词出现在这段词后面的概率也比较大。比如“白色的汽车”经常出现，那完全可以认为“白色的轿车”也可能经常出现。&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>训练语料里面有些 n 元组没有出现过，其对应的条件概率就是 0,导致计算一整句话的概率为 0。&lt;/li>
&lt;/ul>
&lt;h4 id="平滑法">平滑法&lt;/h4>
&lt;p>方法一为平滑法。最简单的方法是把每个 n 元组的出现次数加 1，那么原来出现 k 次的某个 n 元组就会记为 k+1 次，原来出现 0 次的 n 元组就会记为出现 1 次。这种也称为 Laplace 平滑。当然还有很多更复杂的其他平滑方法，其本质都 是将模型变为贝叶斯模型，通过引入先验分布打破似然一统天下的局面。而引入 先验方法的不同也就产生了很多不同的平滑方法。&lt;/p>
&lt;h4 id="回退法">回退法&lt;/h4>
&lt;p>方法二是回退法。有点像决策树中的后剪枝方法，即如果 n 元的概率不到，那就往上回退一步，用 n-1 元的概率乘上一个权重来模拟。&lt;/p>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> &lt;a href="https://mp.weixin.qq.com/s/ceCopgMboWvNxl8qgmeB-Q" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/ceCopgMboWvNxl8qgmeB-Q&lt;/a> 提取其中的序列标注与条件随机场&lt;/li>
&lt;/ul></description></item><item><title>Numpy-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/numpy-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/numpy-cheatsheet/</guid><description>&lt;h1 id="numpy-cheatsheet">Numpy CheatSheet&lt;/h1>
&lt;h1 id="broadcasting">Broadcasting&lt;/h1>
&lt;p>Numpy 会自动进行矩阵扩展操作以适应指定的矩阵运算&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">8&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">6&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">7&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">8&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">7&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">6&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">a&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># a.shape = (4, 3)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># b.shape = (3, 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>No! In numpy the &amp;ldquo;*&amp;rdquo; operator indicates element-wise multiplication. It is different from &amp;ldquo;np.dot()&amp;rdquo;. If you would try &amp;ldquo;c = np.dot(a,b)&amp;rdquo; you would get c.shape = (4, 2).&lt;/p>
&lt;p>Also, the broadcasting cannot happen because of the shape of b. b should have been something like (4, 1) or (1, 3) to broadcast properly. So a*b leads to an error!&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">a&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># a.shape = (4, 3)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># b.shape = (3, 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">//&lt;/span> &lt;span class="ne">ValueError&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">operands&lt;/span> &lt;span class="n">could&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">be&lt;/span> &lt;span class="n">broadcast&lt;/span> &lt;span class="n">together&lt;/span> &lt;span class="k">with&lt;/span> &lt;span class="n">shapes&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>Personas-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/personas-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/personas-cheatsheet/</guid><description>&lt;h1 id="用户画像">用户画像&lt;/h1>
&lt;p>Alan Cooper (交互设计之父)最早提出了 persona 的概念:“Personas are a concrete representation of target users.”&lt;strong>Persona 是真实用户的虚拟代表,是建立在一系列真实数据(Marketing data,Usability data&lt;/strong>)之上的目标用户模型。通过用户调研去了解用户,根据他们的目标、行为和观点的差异,将他们区分为不同的类型,然后每种类型中抽取出典型特征,赋予名字、照片、一些人口统计学要素、场景等描述,就形成了一个人物原型(personas)。&lt;/p>
&lt;h1 id="fingerprinting--用户识别">Fingerprinting | 用户识别&lt;/h1>
&lt;p>Fingerprinting 的本意是指纹采集，在技术层面上唯一标识一个浏览器。开源的 fingerprintjs2 库，你就能感受到程序员们为了追踪用户能想出多么骚的操作。这些操作所涉及的维度主要包括但不限于：IP 地址、JavaScript 行为、Flash 与 Java 插件、字体、Canvas、WebGL。&lt;/p></description></item><item><title>PyTorch-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/pytorch-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/pytorch-cheatsheet/</guid><description>&lt;p>自 2017 年 1 月 PyTorch 推出以来，采用 Python 语言、动态图机制、网络构建灵活以及拥有强大的社群等，其热度持续上升，一度有赶超 TensorFlow 的趋势。在机器学习模型开发中，主要涉及三大部分，分别是数据、模型和损失函数及优化器，学习 PyTorch 也会按照这条脉络。&lt;/p>
&lt;h1 id="模型">模型&lt;/h1></description></item><item><title>R-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/r-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/r-cheatsheet/</guid><description>&lt;h1 id="r-cheatsheet">R CheatSheet&lt;/h1>
&lt;p>R 语言是基于 S 语言的一种开源实现。S 语言是贝尔实验室最早开发的一种用于统计的工具，后来成为商业的 S-PLUS 软件，是一种与 SAS 和 SPSS 齐名的统计软件。R 语言的一个重要的优势就是 R 的生态，有大量的高质量的第三方的统计和算法相关的包。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 查看帮助文档&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ help&lt;span class="o">(&lt;/span>sd&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 查看函数示例&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">$ example&lt;span class="o">(&lt;/span>sd&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>写好的 R 文件，可以通过 source(&amp;ldquo;filename.R&amp;rdquo;)的形式装载进来。可以通过 save 函数将 R 的内存数据保存到一个 Rdata 文件中。下次再通过 load()函数读取出来。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="nf">save&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gun_data&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">file&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s">&amp;#34;gun_data.Rdata&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>可以从 CRAN 上安装扩展包：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 安装扩展包&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">install.packages&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;fBasics&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 使用扩展包&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nf">library&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">timeDate&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="文件处理">文件处理&lt;/h1>
&lt;p>既然要处理数据，肯定要先从数据源读取数据。我们选取最简单的方式，从 csv 文件中读取。假设我们有这样一个 csv 文件：&lt;/p>
&lt;pre tabindex="0">&lt;code class="language-csv" data-lang="csv">times,total, copy
1,122.18138504,48.200
&lt;/code>&lt;/pre>&lt;p>我们使用 read.csv 函数将其读到 gun&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="n">gun_data&lt;/span> &lt;span class="o">&amp;lt;-&lt;/span> &lt;span class="nf">read.csv&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;gun-1128-2.csv&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">header&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="bp">T&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="n">col.names&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="nf">c&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s">&amp;#34;times&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#34;total&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#34;copy&amp;#34;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="统计">统计&lt;/h1>
&lt;h2 id="均值">均值&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gun_data3[[2]]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">103.1747&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">mean&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gun_data4[[2]]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">113.3303&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="中位数">中位数&lt;/h2>
&lt;p>均值的问题在于，如果异常值比较大，会把均值拉高或拉低。而中位数是排序后处于中间的数，不受异常值的影响。
R 语言中用 median 函数求中位数：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">median&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gun_data3[[2]]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">101.651&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="五数">五数&lt;/h2>
&lt;p>所谓五数，就是最小值，25%分位值，中位数，75%分位值，最大值。
这五个数可以通过 fivenum()函数一次性求出来；连同均值，summary 函数能一次将 6 个数都求出来。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-r" data-lang="r">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">fivenum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gun_data3[[&lt;/span>&lt;span class="s">&amp;#34;total&amp;#34;&lt;/span>&lt;span class="n">]]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">[1]&lt;/span> &lt;span class="m">98.92649&lt;/span> &lt;span class="m">100.48752&lt;/span> &lt;span class="m">101.65097&lt;/span> &lt;span class="m">105.94518&lt;/span> &lt;span class="m">116.74337&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&lt;/span> &lt;span class="nf">summary&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">gun_data3[&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s">&amp;#34;total&amp;#34;&lt;/span>&lt;span class="n">]&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">Min.&lt;/span> &lt;span class="m">1&lt;/span>&lt;span class="n">st&lt;/span> &lt;span class="n">Qu.&lt;/span> &lt;span class="n">Median&lt;/span> &lt;span class="n">Mean&lt;/span> &lt;span class="m">3&lt;/span>&lt;span class="n">rd&lt;/span> &lt;span class="n">Qu.&lt;/span> &lt;span class="n">Max.&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="m">98.93&lt;/span> &lt;span class="m">100.50&lt;/span> &lt;span class="m">101.70&lt;/span> &lt;span class="m">103.20&lt;/span> &lt;span class="m">105.80&lt;/span> &lt;span class="m">116.70&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="方差">方差&lt;/h2>
&lt;p>方差是各样本值与均值的差值的平方的和，反映了数据的离散程度。&lt;/p>
&lt;blockquote>
&lt;p>var(gun_data3[,&amp;ldquo;total&amp;rdquo;])&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>[1] 12.70904&lt;/li>
&lt;/ul>
&lt;h2 id="标准差">标准差&lt;/h2>
&lt;p>方差的平方根是标准差。R 语言用 sd()函数求标准差&lt;/p>
&lt;blockquote>
&lt;p>sd(gun_data3[,&amp;ldquo;total&amp;rdquo;])&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>[1] 3.564974&lt;/li>
&lt;/ul>
&lt;h2 id="离差">离差&lt;/h2>
&lt;p>离差是 R 中提供的一个特殊功能，它是相对于中位数的偏差的绝对值和：&lt;/p>
&lt;p>mad(x) = 1/qnorm(3/4) * median(abs(x-median(x)))
离差用 mad()函数计算。&lt;/p>
&lt;p>偏度
如果结果不符合正态分布，我们希望知道是向左偏还是向右偏，这个值用偏度 skewness 来表示。R 中用 skewness()函数来计算。如果值&amp;gt;0 为右偏，反之为左偏。&lt;/p>
&lt;p>求偏度的函数，首先要通过 install.packages 来下载 fBasics 库，然后引入 timeDate 库：&lt;/p>
&lt;blockquote>
&lt;p>library(timeDate)
skewness(gun_data3[,2])&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>[1] 1.109821
&lt;blockquote>
&lt;p>attr(,&amp;ldquo;method&amp;rdquo;)&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>[1] &amp;ldquo;moment&amp;rdquo;
&lt;blockquote>
&lt;p>skewness(gun_data4[,2])&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>[1] 2.40715
&lt;blockquote>
&lt;p>attr(,&amp;ldquo;method&amp;rdquo;)&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>[1] &amp;ldquo;moment&amp;rdquo;
&lt;blockquote>
&lt;p>从中可以看以，这两组数据都向右偏。gun_data4 偏得更厉害。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul>
&lt;p>峰度
峰度是判断这个分布是比正态分布的图更尖还是更平。
R 中用 kurtosis()函数来计算&lt;/p>
&lt;blockquote>
&lt;p>kurtosis(gun_data3[,2])&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>[1] 0.7986081
&lt;blockquote>
&lt;p>attr(,&amp;ldquo;method&amp;rdquo;)&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>[1] &amp;ldquo;excess&amp;rdquo;
&lt;blockquote>
&lt;p>kurtosis(gun_data4[,2])&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>[1] 7.060265
&lt;blockquote>
&lt;p>attr(,&amp;ldquo;method&amp;rdquo;)&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;li>[1] &amp;ldquo;excess&amp;rdquo;
&lt;blockquote>
&lt;p>上面的两个分布都&amp;gt;0，说明比正态分布都要尖。&lt;/p>
&lt;/blockquote>
&lt;/li>
&lt;/ul></description></item><item><title>RecommendSystem-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/recommendsystem-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/recommendsystem-cheatsheet/</guid><description>&lt;h1 id="recommend-system-cheatsheet--推荐系统术语算法与实践">Recommend System CheatSheet | 推荐系统术语、算法与实践&lt;/h1>
&lt;p>推荐系统在社交媒体、资讯阅读、电子商务等各类互联网产品中起到了越来越重要的作用。本质上，推荐系统是一种新的组织信息、传递信息的方式。这种方式火热的根本原因是网络信息指数级增长和信息多媒体化。&lt;/p>
&lt;p>信息组织和传递方式大概分为下面 4 代版本。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>1.0 版本：导航页面。在互联网发展初期，网络上的信息很少，所以简单的导航页面就可以满足用户对网络信息获取的需要。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>2.0 版本：门户网站。随着网民的增加、原来越多的专业作者和编辑在网络上生产内容，这些内容包罗万象，有关于汽车的、有关于体育的。体育里面还有分关于篮球的、关于田径的等等。因此，门户网站这种以树形结构组织网页、传递信息的方式开始爆发，得到用户的青睐。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>3.0 版本：检索系统。随着博客、论坛、垂直网站这些 UGC（用户产生内容）平台模式的崛起，大量的信息被非专业人士产生。这些信息大多用来表达自我，具有语言口语化、内容不确定等特点，很难被清晰地分门别类。这时，检索系统闪亮登场。用户无需知道自己想要的信息属于什么类目，只需要把自己的需要输入在一个简单的搜索框里，就可以得到想要的信息。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>4.0 版本，全面进入移动互联网时代，每一个拥有智能手机的人都可以轻松产生互联网信息，而且包含更多的图片、声音、视频等更多模态类型的信息。在这个大数据时代，用户甚至很难用语言表达自己想看什么样的信息，于是，推荐系统迎来了其前所未有的机遇。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="计算与系统架构">计算与系统架构&lt;/h1>
&lt;p>从系统架构来看，现如今推荐系统面临着三大挑战：超大规模数据、用户实时化反馈、多模态信息。&lt;/p>
&lt;p>大规模的数据来源于三个方面：网络信息数量、用户数量和用户交互。前文已经提到，网络信息数量是指数级增长的。用户数量虽然有人口上限，但是由于互联网产品的垂直化，动辄过亿用户量的产品已不再是少数。另外，随着前端人机交互技术的发展，用户在产品交互次数已经明显增加。大规模的数据对推荐系统的挑战主要体现在大数据的存储、处理与分析，大规模机器学习系统两个部分。大数据存储、处理与分析已经有主流的开源分布式工具，比如 HDFS、Hadoop、Spark。它们已经是业界标配，可以满足大部分公司大数据处理的需求。另一方面，大规模的机器学习架构大致分为三类解决方案：基于 Map-Reduce 的方案、基于参数服务器（Parameter Server）的方案、基于 All Reduce 的方案。其中，基于参数服务器的方案目前占据主流地位，但是没有垄断性的标配工具。这是由于机器学习任务的多样性，与各个公司业务数据的独特性造成的。很多公司、开源组织或个人相继发布了开源分布式机器学习框架，比如 MLlib、Angel、Multiverso、TensorFlow、MXNet 等等。由于推荐系统的业务场景和机器学习算法的选择多种多样，所以开源机器学习框架并没有明显的高低之分，适合业务自身发展阶段和算法选择的就是最好的。在推荐系统构建初期，算法通常会选择线性模型，这时候使用 Spark 的 MLlib 就是一个比较好的选择。它随 Spark 一起安装，社区相对比较成熟，方便开发人员学习、应用和维护。但如果是重量级的超大规模机器学习，就需要根据自身业务的特点，自主研发自己的机器学习平台，以便在存储、计算、通信等多个方面进行细致的优化。&lt;/p>
&lt;p>为了提供更好的用户体验，推荐系统需要实时获取到用户的行为，快速处理数据、进行在线机器学习，最终近乎实时地反馈到下次的推荐服务中。要完成这样的任务，需要从前端到后端整个链路的架构支持。在客户端，需要实时获取用户的操作并实时传回，这需要消息队列（例如，Kafka、ZeroMQ、RabbitMQ 等）的有力保障。收到数据后，需要并行的流式处理框架来进行实时处理，常用的有 Storm、Spark Streaming、Flink 等。随后，流数据处理框架对回传的数据进行清洗、计数、计算特征等处理，另外，计算的结果通常需要立即保存，以备机器学习系统使用。这时，基于内存的、支持快速读写的 NoSQL 数据库可以发挥作用，目前工业界比较成熟应用的有 Redis、Memcache 等。由于目前的推荐系统大多是基于机器学习模型的，所以这里的实时性要求必须由在线机器学习系统来保障。机器学习算法，特别是深度学习算法对计算效率的要求非常高，所以通常会对数据进行采样以减少训练数据规模。但与此同时还要保证模型的效果不会打折扣，所以在线实时机器学习系统的研发和算法研发通常是密不可分的。最后，当短时间内用户再次发出请求时，更新后的模型返回给用户新的推荐结果，完成闭环。一个实时推荐系统要求闭环中的每一个环节都实时高效，必须没有短板，保障整个数据流通畅、高效。&lt;/p>
&lt;p>多模态是一个比较新兴的名词，其大致的意思是文字、语言、图像、视频这些不同的内容形式共同表达语义。显而易见，互联网上的内容已经从文本占主体演进到了多媒体混合形态占主体，比如近两年，随着手机摄像的快速发展以及网络成本逐渐降低，涌现出一批短视频产品。在目前的大多数推荐系统中，文本、图片、视频通常被分别处理后送入机器学习系统，分别训练出模型。最后，推荐系统会综合应用这些子模型。而多模态数据要求机器学习推荐系统可以同时处理包含多种模态形式的内容，通过联合学习来提升算法效果。但由于图像等多媒体特征抽取速度比较慢，很容易成为瓶颈，所以架构会面临巨大的挑战。比如，传统参数服务器会将数据随机分配到各个 worker。但对于多模态数据，这种分配方式是不合理的。因为不同类型的数据适用的机器类型不同，图片、视频需要 GPU 机器才能高效计算，而文本和其他浮点型统计数据则使用 CPU 会比较节约成本。显然，这要求系统架构研发强依赖于模态类型的分布和算法研发的设计方案。&lt;/p></description></item><item><title>Scikit-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/scikit-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/scikit-cheatsheet/</guid><description>&lt;h1 id="scikit-learn">Scikit-Learn&lt;/h1>
&lt;p>Scikit-learn 是开源的 Python 机器学习库，提供了数据预处理、交叉验证、算法与可视化算法等一系列接口。&lt;/p>
&lt;h2 id="basic-example基本用例">Basic Example:基本用例&lt;/h2>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">neighbors&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">datasets&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">preprocessing&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.cross_validation&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">train_test_split&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.metrics&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">accuracy_score&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">iris&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">datasets&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">load_iris&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">iris&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="p">:&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">iris&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">target&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">X_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_test&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">train_test_split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">random_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">33&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">scaler&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">preprocessing&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">StandardScaler&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">X_train&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scaler&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">X_test&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">scaler&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_test&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">knn&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">neighbors&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">KNeighborsClassifier&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">n_neighbors&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">knn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">y_pred&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">knn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">predict&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X_test&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">accuracy_score&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_pred&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="数据加载与切分">数据加载与切分&lt;/h2>
&lt;p>我们一般使用 NumPy 中的数组或者 Pandas 中的 DataFrame 等数据结构来存放数据：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">X&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">array&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="s1">&amp;#39;M&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;M&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;F&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;F&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;M&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;F&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;M&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;M&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;F&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;F&amp;#39;&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="s1">&amp;#39;F&amp;#39;&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">X&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">X&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="mf">0.7&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>NumPy 还提供了方便的接口帮我们划分训练数据与测试数据：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.cross_validation&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">train_test_split&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">X_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">X_test&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_train&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y_test&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">train_test_split&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">X&lt;/span>&lt;span class="p">,&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">random_state&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="model模型">Model:模型&lt;/h1>
&lt;h2 id="模型创建">模型创建&lt;/h2>
&lt;h3 id="监督学习">监督学习&lt;/h3>
&lt;ul>
&lt;li>Linear Regression&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.linear_model&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">LinearRegression&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">lr&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">LinearRegression&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">normalize&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Support Vector Machines&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="kn">from&lt;/span> &lt;span class="nn">sklearn.svm&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">SVC&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">&amp;gt;&amp;gt;&amp;gt;&lt;/span> &lt;span class="n">svc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">SVC&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">kernel&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s1">&amp;#39;linear&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;ul>
&lt;li>Naive Bayes&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.naive_bayes import GaussianNB
&amp;gt;&amp;gt;&amp;gt; gnb = GaussianNB()
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>KNN&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn import neighbors
&amp;gt;&amp;gt;&amp;gt; knn = neighbors.KNeighborsClassifier(n_neighbors=5)
&lt;/code>&lt;/pre>&lt;h3 id="无监督学习">无监督学习&lt;/h3>
&lt;ul>
&lt;li>Principal Component Analysis&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.decomposition import PCA
&amp;gt;&amp;gt;&amp;gt; pca = PCA(n_components=0.95)
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>KMeans&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.cluster import KMeans
&amp;gt;&amp;gt;&amp;gt; k_means = KMeans(n_clusters=3, random_state=0)
&lt;/code>&lt;/pre>&lt;h2 id="模型拟合">模型拟合&lt;/h2>
&lt;h3 id="有监督学习">有监督学习&lt;/h3>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; lr.fit(X, y)
&amp;gt;&amp;gt;&amp;gt; knn.fit(X_train, y_train)
&amp;gt;&amp;gt;&amp;gt; svc.fit(X_train, y_train)
&lt;/code>&lt;/pre>&lt;h3 id="无监督学习-1">无监督学习&lt;/h3>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; k_means.fit(X_train)
&amp;gt;&amp;gt;&amp;gt; pca_model = pca.fit_transform(X_train)
&lt;/code>&lt;/pre>&lt;h2 id="模型预测">模型预测&lt;/h2>
&lt;h3 id="有监督预测">有监督预测&lt;/h3>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; y_pred = svc.predict(np.random.random((2,5)))
&amp;gt;&amp;gt;&amp;gt; y_pred = lr.predict(X_test)
&amp;gt;&amp;gt;&amp;gt; y_pred = knn.predict_proba(X_test)
&lt;/code>&lt;/pre>&lt;h3 id="无监督预测">无监督预测&lt;/h3>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; y_pred = k_means.predict(X_test)
&lt;/code>&lt;/pre>&lt;h2 id="模型评估">模型评估&lt;/h2>
&lt;h3 id="分类度量">分类度量&lt;/h3>
&lt;ul>
&lt;li>Accuracy Scope&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; knn.score(X_test, y_test)
&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import accuracy_score
&amp;gt;&amp;gt;&amp;gt; accuracy_score(y_test, y_pred)
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Classification Report&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import classification_report
&amp;gt;&amp;gt;&amp;gt; print(classification_report(y_test, y_pred))
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Confusion Matrix&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import confusion_matrix
&amp;gt;&amp;gt;&amp;gt; print(confusion_matrix(y_test, y_pred))
&lt;/code>&lt;/pre>&lt;h3 id="回归度量">回归度量&lt;/h3>
&lt;ul>
&lt;li>Mean Absolute Error&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import mean_absolute_error
&amp;gt;&amp;gt;&amp;gt; y_true = [3, -0.5, 2]
&amp;gt;&amp;gt;&amp;gt; mean_absolute_error(y_true, y_pred)
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Mean Squared Error&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import mean_squared_error
&amp;gt;&amp;gt;&amp;gt; mean_squared_error(y_test, y_pred)
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>R2 Score&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import r2_score
&amp;gt;&amp;gt;&amp;gt; r2_score(y_true, y_pred)
&lt;/code>&lt;/pre>&lt;h3 id="聚类度量">聚类度量&lt;/h3>
&lt;ul>
&lt;li>Adjusted Rand Index&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import adjusted_rand_score
&amp;gt;&amp;gt;&amp;gt; adjusted_rand_score(y_true, y_pred)
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>Homogeneity&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import homogeneity_score
&amp;gt;&amp;gt;&amp;gt; homogeneity_score(y_true, y_pred)
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>V-measure&lt;/li>
&lt;/ul>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.metrics import v_measure_score
&amp;gt;&amp;gt;&amp;gt; metrics.v_measure_score(y_true, y_pred)
&lt;/code>&lt;/pre>&lt;h3 id="交叉验证">交叉验证&lt;/h3>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.cross_validation import cross_val_score
&amp;gt;&amp;gt;&amp;gt; print(cross_val_score(knn, X_train, y_train, cv=4))
&amp;gt;&amp;gt;&amp;gt; print(cross_val_score(lr, X, y, cv=2))
&lt;/code>&lt;/pre>&lt;h1 id="数据预处理">数据预处理&lt;/h1>
&lt;h2 id="标准化">标准化&lt;/h2>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import StandardScaler
&amp;gt;&amp;gt;&amp;gt; scaler = StandardScaler().fit(X_train)
&amp;gt;&amp;gt;&amp;gt; standardized_X = scaler.transform(X_train)
&amp;gt;&amp;gt;&amp;gt; standardized_X_test = scaler.transform(X_test)
&lt;/code>&lt;/pre>&lt;h2 id="归一化">归一化&lt;/h2>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import Normalizer
&amp;gt;&amp;gt;&amp;gt; scaler = Normalizer().fit(X_train)
&amp;gt;&amp;gt;&amp;gt; normalized_X = scaler.transform(X_train)
&amp;gt;&amp;gt;&amp;gt; normalized_X_test = scaler.transform(X_test)
&lt;/code>&lt;/pre>&lt;h2 id="二值化">二值化&lt;/h2>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import Binarizer
&amp;gt;&amp;gt;&amp;gt; binarizer = Binarizer(threshold=0.0).fit(X)
&amp;gt;&amp;gt;&amp;gt; binary_X = binarizer.transform(X)
&lt;/code>&lt;/pre>&lt;h2 id="类条件编码">类条件编码&lt;/h2>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import LabelEncoder
&amp;gt;&amp;gt;&amp;gt; enc = LabelEncoder()
&amp;gt;&amp;gt;&amp;gt; y = enc.fit_transform(y)
&lt;/code>&lt;/pre>&lt;h2 id="缺失值推导">缺失值推导&lt;/h2>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import Imputer
&amp;gt;&amp;gt;&amp;gt; imp = Imputer(missing_values=0, strategy=&amp;#39;mean&amp;#39;, axis=0)
&amp;gt;&amp;gt;&amp;gt; imp.fit_transform(X_train)
&lt;/code>&lt;/pre>&lt;h2 id="多项式属性生成">多项式属性生成&lt;/h2>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.preprocessing import PolynomialFeatures
&amp;gt;&amp;gt;&amp;gt; poly = PolynomialFeatures(5)
&amp;gt;&amp;gt;&amp;gt; poly.fit_transform(X)
&lt;/code>&lt;/pre>&lt;h1 id="模型调优">模型调优&lt;/h1>
&lt;h2 id="grid-search">Grid Search&lt;/h2>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.grid_search import GridSearchCV
&amp;gt;&amp;gt;&amp;gt; params = {&amp;#34;n_neighbors&amp;#34;: np.arange(1,3), &amp;#34;metric&amp;#34;: [&amp;#34;euclidean&amp;#34;, &amp;#34;cityblock&amp;#34;]}
&amp;gt;&amp;gt;&amp;gt; grid = GridSearchCV(estimator=knn,
param_grid=params)
&amp;gt;&amp;gt;&amp;gt; grid.fit(X_train, y_train)
&amp;gt;&amp;gt;&amp;gt; print(grid.best_score_)
&amp;gt;&amp;gt;&amp;gt; print(grid.best_estimator_.n_neighbors)
&lt;/code>&lt;/pre>&lt;h2 id="randomized-parameter-optimization">Randomized Parameter Optimization&lt;/h2>
&lt;pre tabindex="0">&lt;code>&amp;gt;&amp;gt;&amp;gt; from sklearn.grid_search import RandomizedSearchCV
&amp;gt;&amp;gt;&amp;gt; params = {&amp;#34;n_neighbors&amp;#34;: range(1,5), &amp;#34;weights&amp;#34;: [&amp;#34;uniform&amp;#34;, &amp;#34;distance&amp;#34;]}
&amp;gt;&amp;gt;&amp;gt; rsearch = RandomizedSearchCV(estimator=knn,
param_distributions=params,
cv=4,
n_iter=8,
random_state=5)
&amp;gt;&amp;gt;&amp;gt; rsearch.fit(X_train, y_train)
&amp;gt;&amp;gt;&amp;gt; print(rsearch.best_score_)
&lt;/code>&lt;/pre></description></item><item><title>Statistics-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/statistics-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/statistics-cheatsheet/</guid><description>&lt;h1 id="mathematics-cheatsheet--机器学习深度学习中的数学基础">Mathematics CheatSheet | 机器学习、深度学习中的数学基础&lt;/h1>
&lt;p>实际上，概率统计知识和数据科学家的日常工作，以及一个人工智能项目的正常运作都密切相关，概率统计知识正在人工智能中发挥着越来越重要的作用。&lt;/p>
&lt;p>和机器学习一样，概率统计各个领域的知识以及研究成果浩如烟海。&lt;/p>
&lt;p>概率和统计可以说是机器学习领域的基石之一，从某个角度来看，机器学习可以看做是建立在概率思维之上的一种对不确定世界的系统性思考和认知方式。学会用概率的视角看待问题，用概率的语言描述问题，是深入理解和熟练运用机器学习技术的最重要基础之一。&lt;/p>
&lt;p>对于离散数据，伯努利分布、二项分布、多项分布、Beta 分布、Dirichlet 分布以及泊松分布都是需要理解掌握的内容；对于连续数据，高斯分布和指数分布族是比较重要的分布。还需要掌握假设检验与置信区间的相关理论，才能分辨数据结论的真伪，譬如两组数据是否真的存在差异，上线一个策略之后指标是否真的有提升等等。&lt;/p>
&lt;p>信息论也是必须掌握的基础&lt;/p>
&lt;h1 id="微积分">微积分&lt;/h1>
&lt;h3 id="两边夹定理">两边夹定理&lt;/h3>
&lt;p>夹逼定理英文原名 Squeeze Theorem。也称两边夹定理、夹逼准则、夹挤定理、挟挤定理、三明治定理，是判定极限存在的两个准则之一，是函数极限的定理。&lt;/p>
&lt;!-- prettier-ignore-start -->
&lt;p>当 $x \in U(x_0,r)$ 时，有 $g(x) \le f(x) \le h(x)$ 成立，并且如果 $\lim_{x-&amp;gt;x_0}g(x) = A$ 以及 $\lim_{x-&amp;gt;x_0}h(x)=A$，那么有:&lt;/p>
&lt;p>$$
\lim_{x-&amp;gt;x_0}f(x)=A
$$&lt;/p>
&lt;!-- prettier-ignore-end -->
&lt;h3 id="极限存在定理">极限存在定理&lt;/h3>
&lt;p>单调有界数列必有极限&lt;/p>
&lt;h2 id="导数">导数&lt;/h2>
&lt;p>简单来说，导数就是曲线的斜率，是曲线变化快慢的反应。而二阶导数是斜率变化快慢的反应，表征曲线的凹凸性。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://d.hiphotos.baidu.com/baike/c0%3Dbaike92%2C5%2C5%2C92%2C30/sign=51054ce208d162d991e36a4e70b6c289/e61190ef76c6a7ef9d16aa00fffaaf51f3de66e7.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h3 id="taylor-公式---maclaurin-公式">Taylor 公式 - Maclaurin 公式&lt;/h3>
&lt;p>$f(x) = f(x_0) + f&amp;rsquo;(x_0)(x-x_0) + \frac{f&amp;rsquo;&amp;rsquo;(x_0)}{2!}(x-x_0)^2 + \cdots + \ \frac{f^{(n)}(x_0)}{n!}(x-x_0)^n + R_n(x)$&lt;/p>
&lt;p>而 Maclaurin 公式就是当$x_0 = 0$的时候的 Taylor 公式&lt;/p>
&lt;p>$f(x) = f(0) + f&amp;rsquo;(0)x + \frac{f&amp;rsquo;&amp;rsquo;(0)}{2!}x^2 + \cdots + \frac{f^{(n)}(0)}{n!}x^n + o(x^n)$&lt;/p>
&lt;h3 id="方向导数">方向导数&lt;/h3>
&lt;p>如果函数$z=f(x,y)$在点$P(x,y)$是可微分的，那么该函数在该点沿任一方向$L$的方向导数都存在，且有：&lt;/p>
&lt;p>$\frac{\partial f}{\partial l} = \frac{\partial f}{\partial x}cos\varphi + \frac{\partial f}{\partial y}sin\varphi$&lt;/p>
&lt;p>其中，$\varphi$为$x$轴到方向$L$的夹角。&lt;/p>
&lt;h3 id="梯度">梯度&lt;/h3>
&lt;p>梯度的方向是函数在该点变化最快的方向。梯度经常用在梯度下降法中。设函数$z=f(x,y)$在平面区域$D$内具有一阶连续偏导数，则对于每一个点$P(x,y) \in D$，向量$\lgroup \frac{\partial f}{\partial x},\frac{\partial f}{\partial y} \rgroup$为函数在该点$P$的梯度，记作$grad f(x,y)$&lt;/p>
&lt;h4 id="凸函数">凸函数&lt;/h4>
&lt;p>割线位于函数之上。&lt;/p>
&lt;p>$\forall x,y \in dome, 0 \le \theta \le 1$，有&lt;/p>
&lt;p>$f(\theta x + (1 - \theta)y) \le \theta f(x) + (1 - \theta)f(y)$&lt;/p>
&lt;p>凸函数在高等数学中有时候被称为凹函数，但是在机器学习中统一称为凸函数。一元二阶可微的函数在区间上是凸的，当且仅当它的二阶导数是非负的。&lt;/p>
&lt;h1 id="概率论">概率论&lt;/h1>
&lt;p>对于概率的认知：$P(x) \in [0,1]$，需要注意的是，$P=0$并不意味着某件事情一定不会发生。譬如桌子上的任意一定相对于整个桌面的概率就是零，但是硬币落到该点还是可能发生的。所以对概率的定义时，如果$x$为离散/连续变量，则$P(x=x_{0})$表示$x_0$发生的概率/概率密度。&lt;/p>
&lt;p>累积分布函数：$\phi(x) = P(x \le x_0 )$&lt;/p>
&lt;ul>
&lt;li>$\phi(x)$一定为单调递增函数&lt;/li>
&lt;li>$min(\phi(x)) = 0$，$max(\phi(x))$ = 1&lt;/li>
&lt;li>将值域为$[0,1]$的某函数$y=f(x)$看成 y 事件的累积概率，若$y$可导，则称$f&amp;rsquo;(x)$为某个概率的概率密度函数&lt;/li>
&lt;/ul>
&lt;h3 id="古典概型">古典概型&lt;/h3>
&lt;h3 id="几何概型">几何概型&lt;/h3>
&lt;h2 id="概率公式">概率公式&lt;/h2>
&lt;ul>
&lt;li>条件概率&lt;/li>
&lt;/ul>
&lt;p>$P(A|B) = \frac{P(AB)}{P(B)}$&lt;/p>
&lt;ul>
&lt;li>全概率公式&lt;/li>
&lt;/ul>
&lt;p>$P(A) = \sum_iP(A|B_i)P(B_i)$&lt;/p>
&lt;ul>
&lt;li>贝叶斯公式&lt;/li>
&lt;/ul>
&lt;p>$P(B_i|A) = \frac{P(AB_i)}{P(A)} = \frac{P(A|B_i)P(B_i)}{\sum_jP(A|B_j)P(B_j)}$&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://7xlgth.com1.z0.glb.clouddn.com/EE3A5CDB-7286-4DC8-A8D4-3C97CE96CCE1.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>上述公式中的$P(B_i)$即为没有数据支持下，事件$B(i)$的发生概率，也就是先验概率。而$P(B_i|A)$即在数据集合 A 的情况下，可能是属于事件$B_i$的概率，也就是后验概率。而$P(A|B_i)$即给定某参数的概率分布，也就是似然函数。&lt;/p>
&lt;h2 id="参数估计">参数估计&lt;/h2>
&lt;p>给定某系统的若干样本，求该系统的参数。&lt;/p>
&lt;ul>
&lt;li>矩估计/MLE/MaxEnt/EM&lt;/li>
&lt;/ul>
&lt;p>频率学派：假定参数是某个/某些未知的定值，求这些参数如何取值，能够使得某目标函数取极大/极小。&lt;/p>
&lt;ul>
&lt;li>贝叶斯模型&lt;/li>
&lt;/ul>
&lt;p>贝叶斯学派：假定参数本身是变化的，服从某个分布。求在这个分布约束下使得某目标函数极大/极小。&lt;/p>
&lt;h2 id="常见分布">常见分布&lt;/h2>
&lt;h3 id="两点分布0-1-分布">两点分布(0-1 分布)&lt;/h3>
&lt;p>已知随机变量$X$的分布律为：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>$X$&lt;/th>
&lt;th>1&lt;/th>
&lt;th>0&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>$p$&lt;/td>
&lt;td>$p$&lt;/td>
&lt;td>$1-p$&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>则有 $E(X) = 1 _ p + 0 _ q = p$&lt;/p>
&lt;p>$D(X) = E(X^2) - [E(X)]^2 = pq$&lt;/p>
&lt;h1 id="数理统计与参数估计">数理统计与参数估计&lt;/h1>
&lt;p>某个分布的期望，对于离散型而言：&lt;/p>
&lt;p>$E(X)=\sum_i{x_i}{p_i}$&lt;/p>
&lt;p>连续型：$E(X)=\sum_{-\infty}^{\infty}xf(x)dx$&lt;/p>
&lt;p>概率运算中无条件成立的是：&lt;/p>
&lt;p>$E(kX)=kE(X)$&lt;/p>
&lt;p>$E(X+Y)=E(X)+E(Y)$&lt;/p>
&lt;p>如果$X$和$Y$相互独立&lt;/p>
&lt;p>$E(XY)=E(X)E(Y)$&lt;/p>
&lt;p>反之不成立，事实上，如果$E(XY)=E(X)E(Y)$，只能说明$X$和$Y$不相关。&lt;/p>
&lt;p>$Var(X)=E{[X-E(X)]^2}=E(X^2)-E^2(X)$&lt;/p>
&lt;p>无条件成立：&lt;/p>
&lt;p>$Var(c)=0$&lt;/p>
&lt;p>$Var(X+c)=Var(X)$&lt;/p>
&lt;p>$Var(kX)=k^2Var(X)$&lt;/p>
&lt;h3 id="协方差">协方差&lt;/h3>
&lt;p>$Cov(X,Y)=E{[X-E(X)][y-e(y)]}$&lt;/p>
&lt;p>性质&lt;/p>
&lt;p>$Cov(X,Y)=Cov(Y,X)$&lt;/p>
&lt;p>$Cov(aX+b,cY+d)=acCov(X,Y)$&lt;/p>
&lt;p>$Cov(X_1+X_Y)=Cov(X_1,Y)+Cov(X_2,Y)$&lt;/p>
&lt;p>$Cov(X,Y)=E(XY)-E(X)E(Y)$&lt;/p>
&lt;p>如果$Cov(X,Y)=0$，则$X$和$Y$不相关。&lt;/p>
&lt;p>协方差是两个随机变量具有相同方向变化趋势的度量：&lt;/p>
&lt;p>如果$Cov(X,Y)&amp;gt;0$，它们的变化趋势相同。&lt;/p>
&lt;p>如果$Cov(X,Y)&amp;lt;0$，它们的变化趋势相反。&lt;/p>
&lt;p>如果$Cov(X,Y)=0$，则称$X$和$Y$不相关。如果$X$和$Y$不相关，则说明$X$和$Y$之间没有线性关系，但是有可能存在其他函数关系，即不能保证$X$和$Y$相互独立。但是对于二维正态随机变量，$X$和$Y$不相关即等价于$X$和$Y$相互独立。&lt;/p>
&lt;h3 id="相关系数">相关系数&lt;/h3>
&lt;p>定义：$\rho=\frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}$，根据协方差的定义可知：$|\rho|\leq1$&lt;/p>
&lt;p>当且仅当$X$和$Y$有线性关系的时候，等号成立。&lt;/p>
&lt;h2 id="矩">矩&lt;/h2>
&lt;p>对于随机变量$X$,$X$的$k$阶原点矩为：&lt;/p>
&lt;p>$E(X^k)$&lt;/p>
&lt;h3 id="偏度">偏度&lt;/h3>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://www.fusioninvesting.com/wp-content/uploads/2010/09/Skewness_Statistics.svg_.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>偏度衡量随机变量概率分布的不对称性，是相对于平均值不对称程度的度量。偏度的值可以为正，可以为负或者无定义。偏度为负/正表示在概率密度函数左侧/右侧的尾部比右侧/左侧的长，长尾在左侧/右侧。&lt;/p>
&lt;p>偏度为零表示数值相对均匀地分布在平均值的两侧，但不一定意味着一定是对称分布。偏度有时候用$Skew[X]$表示：&lt;/p>
&lt;p>$\gamma_1=E[\lgroup\frac{X-\mu}{\sigma}\rgroup^3]=\frac{E[(X-\mu)^3]}{(E[(X-\mu)^2])^{3/2}}=\frac{E[X^3]-3\mu\sigma^2-\mu^3}{\sigma^3}$&lt;/p>
&lt;h3 id="峰度">峰度&lt;/h3>
&lt;p>峰度是概率密度在均指处峰值高低的特征，通常定义四阶中心矩除以方差的平方减 3。&lt;/p>
&lt;p>$$
\gamma&lt;em>2=\frac{\kappa_4}{\kappa_2^2}=\frac{\mu_4}{\sigma^4}-3=\frac{\frac{1}{n}\sum&lt;/em>{i=1}^n(x&lt;em>i-\bar x)^4}{(\frac{1}{n}\sum&lt;/em>{i=1}^n(x_i-\bar x)^2)^2}-3
$$&lt;/p>
&lt;p>注意，减 3 是为了使得高斯分布的峰度为 0。&lt;/p>
&lt;h3 id="切比雪夫不等式">切比雪夫不等式&lt;/h3>
&lt;p>假设随机变量$X$的期望为$\mu$，方差为$\sigma^2$，对于任意整数$\varepsilon$，有：&lt;/p>
&lt;p>$P{|X-\mu|\geq\varepsilon}\le\frac{\sigma^2}{\varepsilon^2}$&lt;/p>
&lt;p>切比雪夫不等式说明，$X$的方差越小，事件${|X-\mu|&amp;lt;\varepsilon}$发生的概率越大，即$X$取值基本上集中在期望$\mu$附近。&lt;/p>
&lt;h3 id="大数定理">大数定理&lt;/h3>
&lt;p>假设随机变量$X_1,X_2,\dots X_n \dots$互相独立，并且具有相同的期望$\mu$和方差$\sigma^2$。作前$n$个随机变量的平均：&lt;/p>
&lt;p>$Y&lt;em>n=\frac{1}{n}\sum&lt;/em>{i=1}^{n}X_i$&lt;/p>
&lt;p>，则对于任意整数$\varepsilon$，存在有：&lt;/p>
&lt;p>$lim_{n \to \infty}P{|Y_n-\mu|&amp;lt;\varepsilon}=1$&lt;/p>
&lt;p>一次试验中事件 A 发生的概率为$p$，重复$n$次独立实验中，事件$A$发生了$n_A$次，则$p$、$n$、$n_A$的关系满足，对于任意整数$\varepsilon$则&lt;/p>
&lt;p>$lim_{n \to \infty}P{|\frac{n_A}{n}-p|&amp;lt;\varepsilon}=1$&lt;/p>
&lt;h3 id="中心极限定理">中心极限定理&lt;/h3>
&lt;p>假设随机变量$X_1,X_2,\dots X_n \dots$互相独立，并且具有相同的期望$\mu$和方差$\sigma^2$。则随机变量：&lt;/p>
&lt;p>$Y&lt;em>n=\frac{\sum&lt;/em>{i=1}^{n}X_i-n\mu}{\sqrt{n}\sigma}$&lt;/p>
&lt;p>的分布收敛到标准正态分布，也就是说，$\sum_{i=1}^{n}X_i$收敛到正态分布$N(n\mu,n\sigma^2)$&lt;/p>
&lt;h3 id="样本的统计量">样本的统计量&lt;/h3>
&lt;p>设$X_1,X_2,\dots X_n \dots$为一组样本，则样本均值：&lt;/p>
&lt;p>$\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i$&lt;/p>
&lt;p>样本方差：&lt;/p>
&lt;p>$S^2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2$&lt;/p>
&lt;p>样本方差的分母使用$n-1$而非$n$，是为了无偏。&lt;/p>
&lt;h2 id="参数估计-1">参数估计&lt;/h2>
&lt;h3 id="矩估计">矩估计&lt;/h3>
&lt;p>设总体的均值为$\mu$，方差为$\sigma^2$，其中$\mu$与$\sigma$未知，则有原点距表达式：&lt;/p>
&lt;p>$$
\left{
\begin{array}{c}
E(X)=\mu \
E(X^2)=Var(X)+[E(X)]^2=\sigma^2+\mu^2
\end{array}
\right.
$$&lt;/p>
&lt;p>根据该总体的一组样本，求得原点距：&lt;/p>
&lt;p>$$
\left{
\begin{array}{c}
\hat{\mu} = \bar{X} \
\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(X_i-\bar{X})^2
\end{array}
\right.
$$&lt;/p>
&lt;h3 id="极大似然估计mle">极大似然估计(MLE)&lt;/h3>
&lt;p>极大似然估计来源于对于贝叶斯公式的抽象，给定某些样本$D$，在这些样本中计算某结论$A_1、A_2、\dots A_n$出现的概率，即$P(A_i|D)$，则有：&lt;/p>
&lt;p>$maxP(A_i|D) \to maxP(D|A_i)$&lt;/p>
&lt;p>假设总体分布为$f(x,\theta)$,$X_1,X_2,\dots X_n \dots$为独立同分布，于是，它们的联合密度函数为：&lt;/p>
&lt;p>$L(x&lt;em>1,x_2,\dots,x_n;\theta_1,\theta_2,\dots,\theta_k)=\prod&lt;/em>{i=1}{n}f(x_i;\theta_1,\theta_2,\dots,\theta_k)$&lt;/p>
&lt;p>这里$\theta$是一个未知量，即$L(x,\theta)$是关于$\theta$的函数，即似然函数。而求取$\theta$值的过程，就是所谓的极大似然估计。&lt;/p>
&lt;p>$logL(\theta&lt;em>1,\theta_2,\dots,\theta_k)=\sum&lt;/em>{i=1}^{n}f(x_i;\theta_1,\theta_2,\dots,\theta_k)$&lt;/p>
&lt;p>$\frac{\partial L(\theta)}{\partial \theta_i}=0,i=1,2,\dots,k$&lt;/p>
&lt;h4 id="二项分布的极大似然估计">二项分布的极大似然估计&lt;/h4>
&lt;p>投硬币实验中，进行$N$次的独立实验，$n$次朝上，$N-n$次朝下。假定朝上的概率为$p$，使用对数似然函数作为目标函数：&lt;/p>
&lt;p>$f(n|p)=log(p^n(1-p)^{N-n}) {\to}h(p)$&lt;/p>
&lt;p>$\frac{\partial h(p)}{\partial p}=\frac{n}{p}-\frac{N-n}{1-p} \to 0$&lt;/p>
&lt;p>$p=\frac{n}{N}$&lt;/p>
&lt;h4 id="正态分布的极大似然估计">正态分布的极大似然估计&lt;/h4>
&lt;p>$$
l(x)=log\prod_i \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} \
=\sum_i log \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} \
= -\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_i{(x_i-\mu)^2}
$$&lt;/p>
&lt;p>得到目标函数之后，将目标函数对于参数$\mu$，$\sigma$分别求偏导，很容易得到$\mu$，$\sigma$的式子：&lt;/p>
&lt;p>$$
\mu = \frac{1}{n}\sum_ix_i \
\sigma^2=\frac{1}{n}\sum_i(x_i-\mu)^2
$$&lt;/p>
&lt;p>这个结论和矩估计的结果是一致的，并且意义非常直观：样本的均值即高斯分布的均值，样本的伪方差即高斯分布的方差。注意，经典意义下的方差，分母是$n-1$；在似然估计的方法中，求得的方差是$n$。&lt;/p>
&lt;h3 id="参数估计的无偏性">参数估计的无偏性&lt;/h3>
&lt;p>利用已知样本$X_1,X_2,\dots X_n \dots$能够得到参数的一个估计$\hat \theta$，因此$\hat \theta$可以写成$\hat \theta(X_1,X_2,\dots,X_n)$，对于不同的样本，$\hat \theta$的值一般不同。因此，可以看做是关于样本的随机变量。它是可以求均值的：$E(\hat \theta)$。如果该值等于总体的实际分布$\theta$，就说这个估计是无偏估计，即$E(\hat \theta)=\theta$。&lt;/p>
&lt;p>一般来说，样本均值和方差都是总体的无偏估计。假设总体均值为$\mu$，方差为$\sigma^2$，$X_1,X_2,\dots X_n \dots$为来自该总体的样本，即：&lt;/p>
&lt;p>$$
\bar X=\frac{1}{n}\sum*{i=1}^nX_i \
S^2=\frac{1}{n-1}\sum*{i=1}^{n}(X_i-\bar X^2) \
E(\bar X)=\mu \
$$&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://7xlgth.com1.z0.glb.clouddn.com/33106155-CB3C-4458-AE27-4E8E0D8B5811.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>上面是论述了当求样本方差为$n$时候的情况，而如果样本方差为$n-1$时：&lt;/p>
&lt;p>$$
E(\bar X^2)=Var(\bar X) + [E(\bar X)]^2=\frac{\sigma^2}{n}+\mu^2 \
E(X&lt;em>i^2)=Var(X_i)+[E(X_i)]^2=\sigma^2 + \mu^2 \
E(S^2)=\frac{1}{n-1}[E(\sum&lt;/em>{i=1}^{n}X_i^2)-nE(\bar X^2)]= \
\frac{1}{n-1}[(n\sigma^2 + n\mu^2)-n(\frac{\sigma^2}{n} + \mu^2)] \
= \sigma^2
$$&lt;/p>
&lt;h1 id="线性代数">线性代数&lt;/h1>
&lt;p>方阵的行列式定义如下：&lt;/p>
&lt;ul>
&lt;li>$1$阶方阵的行列式为该元素本身&lt;/li>
&lt;li>$n$阶方阵的行列式等于它的任一行或者列的各元素与其对应的代数余子式乘积之和。&lt;/li>
&lt;/ul>
&lt;h2 id="矩阵">矩阵&lt;/h2>
&lt;h2 id="矩阵运算">矩阵运算&lt;/h2>
&lt;h3 id="矩阵模型">矩阵模型&lt;/h3>
&lt;p>考虑某个随机过程$\pi$，它的状态有$n$个，用$1 \sim n$来表示。假设当前时刻$t$处于$i$状态下，那么它在$t+1$时刻位于$j$状态的概率为$P(i,j)=P(j|i)$，即状态转移的概率只会依赖于前一个状态。&lt;/p>
&lt;p>任何一个矩阵模型，即转移概率模型都会达到一种平稳分布的状态，即&lt;/p>
&lt;p>$$
lim*{n \to \infty}P*{ij}^{n}=\pi(j) \
lim_{n \to \infty}P^n =
\begin{bmatrix}
\pi(1) &amp;amp; \pi(2) &amp;amp; \dots &amp;amp; \pi(n)\
\pi(1) &amp;amp; \pi(2) &amp;amp; \dots &amp;amp; \pi(n)\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots
\end{bmatrix} \quad
$$&lt;/p>
&lt;p>线性方程$xP=x$的非负且唯一解即是$\pi$。&lt;/p>
&lt;h3 id="矩阵乘法">矩阵乘法&lt;/h3>
&lt;p>$A$为$m&lt;em>s$阶的矩阵，$B$为$s&lt;/em>n$阶的矩阵，那么，$C=A&lt;em>B$是$m&lt;/em>n$阶的矩阵，其中：&lt;/p>
&lt;p>$c*{ij}=\sum*{k=1}^sa*{ik}b*{kj}$&lt;/p>
&lt;p>$A$为$m&lt;em>s$阶的矩阵，$B$为$s&lt;/em>1$的列向量，则$Ax$为$m&lt;em>1$的列向量，则$Ax$为$m&lt;/em>1$的列向量，记$\vec {y} = A* \vec x$。由于$n$维列向量和$n$维空间的点一一对应，上式实际给出了从$n$维空间的点到$m$维空间点的线性变换。&lt;/p>
&lt;h3 id="矩阵的秩">矩阵的秩&lt;/h3>
&lt;p>在$m&lt;em>n$矩阵$A$中，任取$k$行$k$列，不改变这$k^2$个元素在$A$中的次序，得到$k$阶方阵，称为矩阵$A$的$k$阶子式。显然，$m&lt;/em>n$矩阵$A$的$k$阶子式有$C_m^kC_n^k$个。假设在矩阵$A$中有一个不等于 0 的$r$阶子式$D$，并且所有$r+1$阶子式(如果存在的话)全部等于 0，那么$D$称为矩阵$A$的最高阶非零子式，$r$称为矩阵$A$的秩，记作$R(A)=r$。&lt;/p>
&lt;p>$$
\left{
\begin{array}{c}
a*{11}x_1+a*{12}x&lt;em>2+ \dots + +a&lt;/em>{1n}x&lt;em>n=b_1\
a&lt;/em>{11}x&lt;em>1+a&lt;/em>{12}x&lt;em>2+ \dots + +a&lt;/em>{1n}x&lt;em>n=b_1\
\dots \
a&lt;/em>{11}x&lt;em>1+a&lt;/em>{12}x&lt;em>2+ \dots + +a&lt;/em>{1n}x_n=b_1\
\end{array}
\right.
\to Ax=b
$$&lt;/p>
&lt;p>对于$n$元线性方程组$Ax=b$：&lt;/p>
&lt;ul>
&lt;li>无解的充要条件是$R(A)&amp;lt;R(A,b)$&lt;/li>
&lt;li>有唯一解的充要条件是$R(A)=R(A,b)=n$&lt;/li>
&lt;/ul>
&lt;ul>
&lt;li>有无限多解的充要条件是$R(A)=R(A,b)&amp;lt;n$&lt;/li>
&lt;/ul>
&lt;h2 id="向量组">向量组&lt;/h2>
&lt;h3 id="向量组等价">向量组等价&lt;/h3>
&lt;p>向量$b$能由向量组$A:a_1,a_2,\dots,a_m$线性表示的充要条件是矩阵$A=(a_1,a_2,\dots,a_m)$的秩等于矩阵$B=(a_1,a_2,\dots,a_m,b)$的秩。设有两个向量组$A:a_1,a_2,\dots,a_m$以及$B:b_1,b_2,\dots,b_n$，若$B$组的向量都能由向量组$A$线性表示，则称向量组$B$能由向量组$A$线性表示。若向量组$A$与向量组$B$能够相互线性表示，则称两个向量组等价。如果两个向量组等价，则存在矩阵$K$:系数&lt;/p>
&lt;p>$$
(b&lt;em>1 b_2 \dots b_n) = (a_1 a_2 \dots a_n)
\begin{bmatrix}
k&lt;/em>{11} &amp;amp; k*{12} &amp;amp; \dots &amp;amp; k*{1n}\
k*{21} &amp;amp; k*{22} &amp;amp; \dots &amp;amp; k_{2n}\
\vdots &amp;amp; \vdots &amp;amp; \ddots &amp;amp; \vdots
\end{bmatrix} \quad
$$&lt;/p>
&lt;p>向量组$B$能由向量组$A$线性表示的充要条件是矩阵$A=(a_1,a_2,\dots,a_m)$的秩等于矩阵$(A,B)=(a_1,a_2,\dots,a_m,b_1,b_2,\dots,b_n)$的秩，即：$R(A)=R(A,B)$。&lt;/p>
&lt;h2 id="特征值与特征变换">特征值与特征变换&lt;/h2>
&lt;h3 id="正交阵">正交阵&lt;/h3>
&lt;p>如果$n$阶矩阵$A$满足$A^TA=I$，则称$A$为正交矩阵，简称为正交阵。$A$成为正交阵的充要条件是$A$的列(行)向量都是单位向量，且两两正交。$Ax$称作正交变换，注意，正交变换不改变向量长度。&lt;/p>
&lt;h3 id="特征值与特征向量">特征值与特征向量&lt;/h3>
&lt;p>$A$是$n$阶矩阵，如果存在数$\lambda$和$n$维非 0 列向量$x$满足$Ax=\lambda x$，那么，数$\lambda$称为$A$的特征值，$x$称为$A$的对应于特征值$\lambda$的特征向量。假设$n$阶矩阵$A=(a_{ij})$的特征值为$\lambda_1,\lambda_2,\dots,\lambda_n$，则&lt;/p>
&lt;p>$$
\lambda&lt;em>1+\lambda_2+,\dots,+\lambda_n=a&lt;/em>{11}+a*{22}+\dots+a*{nn} \
\lambda_1 \lambda_2 \dots \lambda_n = |A|
$$&lt;/p>
&lt;p>矩阵$A$主行列式的元素和，称作矩阵$A$的迹。&lt;/p>
&lt;h1 id="信息论">信息论&lt;/h1></description></item><item><title>Symbol-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/symbol-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/symbol-cheatsheet/</guid><description>&lt;h1 id="符号">符号&lt;/h1>
&lt;h1 id="希腊字母">希腊字母&lt;/h1>
&lt;h1 id="运算符号">运算符号&lt;/h1>
&lt;h2 id="逻辑运算符">逻辑运算符&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>$\because$（&lt;code>\because&lt;/code>）&lt;/p>
&lt;p>∴：\therefore
∀：\forall
∃：\exists
≠：\not=
≯：\not&amp;gt;
⊄：\not\subset&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="运算">运算&lt;/h1>
&lt;p>^表示上标，
_表示下标，
如果上（下）标内容多于一个字符就需要使用{}，注意不是( ), 因为( )经常是公式本身组成部分，为避免冲突，所以选用了{ } 将其包起来。&lt;/p>
&lt;p>示例：$x^{y^z}=(1+e^x)^{-2xy^w}$&lt;/p>
&lt;p>效果：xyz=(1+ex)−2xyw
上面输入的上下标都是在字符的右侧，要想在左侧或者两侧都写上下标，那么需要使用\sideset 语法。&lt;/p>
&lt;p>示例：$\sideset{^1_2}{^3_4}\bigotimes$&lt;/p>
&lt;p>效果：12⨂34&lt;/p>
&lt;p>3.3 括号和分隔符&lt;/p>
&lt;p>( )和[ ]就是自身了，由于{ } 是 Tex 的元字符，所以表示它自身时需要转义。&lt;/p>
&lt;p>示例：$f(x,y) = x^2 + y^2, x\epsilon[0,100]$&lt;/p>
&lt;p>效果：f(x,y)=x2+y2,xϵ[0,100]
有时候括号需要大号的，普通括号不好看，此时需要使用\left 和\right 加大括号的大小。&lt;/p>
&lt;p>示例：$(\frac{x}{y})^8，\left(\frac{x}{y}\right)^8$&lt;/p>
&lt;p>效果：(xy)8，(xy)8
\left 和\right 必须成对出现，对于不显示的一边可以使用 . 代替。&lt;/p>
&lt;p>示例：$\left.\frac{{\rm d}u}{{\rm d}x} \right| _{x=0}$&lt;/p>
&lt;p>效果：dudx∣∣x=0
3.4 分数&lt;/p>
&lt;p>使用\frac{分子}{分母}格式，或者 分子\over 分母。&lt;/p>
&lt;p>示例：$\frac{1}{2x+1}或者1\over{2x+1}$&lt;/p>
&lt;p>效果：12x+1 或者 12x+1
3.5 开方&lt;/p>
&lt;p>示例：$\sqrt[9]{3}和\sqrt{3}$&lt;/p>
&lt;p>效果：3‾‾√9 和 3‾‾√
3.6 省略号&lt;/p>
&lt;p>有两种省略号，\ldots 表示语文本底线对其的省略号，\cdots 表示与文本中线对其的省略号。&lt;/p>
&lt;p>示例：$f(x_1, x_2, \ldots, x_n)=x_1^2 + x_2^2+ \cdots + x_n^2$&lt;/p>
&lt;p>效果：f(x1,x2,…,xn)=x21+x22+⋯+x2n
3.7 矢量&lt;/p>
&lt;p>示例：$\vec{a} \cdot \vec{b}=0$&lt;/p>
&lt;p>效果: a⃗ ⋅b⃗ =0
3.8 积分&lt;/p>
&lt;p>示例：$\int_0^1x^2{\rm d}x $&lt;/p>
&lt;p>效果：∫10x2dx
3.9 极限&lt;/p>
&lt;p>示例：$\lim_{n\rightarrow+\infty}\frac{1}{n(n+1)}$&lt;/p>
&lt;p>效果：limn→+∞1n(n+1)
3.10 累加、累乘&lt;/p>
&lt;p>示例：$\sum_1^n\frac{1}{x^2}，\prod_{i=0}^n\frac{1}{x^2}$&lt;/p>
&lt;p>效果：∑n11x2，∏ni=01x2
3.11 希腊字母&lt;/p>
&lt;p>希腊字符示例：$$\alpha　A　\beta　B　\gamma　\Gamma　\delta　\Delta　\epsilon　E \varepsilon　　\zeta　Z　\eta　H　\theta　\Theta　\vartheta \iota　I　\kappa　K　\lambda　\Lambda　\mu　M　\nu　N \xi　\Xi　o　O　\pi　\Pi　\varpi　　\rho　P \varrho　　\sigma　\Sigma　\varsigma　　\tau　T　\upsilon　\Upsilon \phi　\Phi　\varphi　　\chi　X　\psi　\Psi　\omega　\Omega$$&lt;/p>
&lt;p>效果：&lt;/p>
&lt;p>α 　 A 　 β 　 B 　 γ 　 Γ 　 δ 　 Δ 　 ϵ 　 Eε 　　 ζ 　 Z 　 η 　 H 　 θ 　 Θ 　 ϑι 　 I 　 κ 　 K 　 λ 　 Λ 　 μ 　 M 　 ν 　 Nξ 　 Ξ 　 o 　 O 　 π 　 Π 　 ϖ 　　 ρ 　 Pϱ 　　 σ 　 Σ 　 ς 　　 τ 　 T 　 υ 　 Υϕ 　 Φ 　 φ 　　 χ 　 X 　 ψ 　 Ψ 　 ω 　 Ω&lt;/p>
&lt;p>3.12 数学符号大汇总
±：\pm
×：\times
÷：\div
∣：\mid&lt;/p>
&lt;p>⋅：\cdot
∘：\circ
∗: \ast
⨀：\bigodot
⨂：\bigotimes
⨁：\bigoplus
≤：\leq
≥：\geq
≠：\neq
≈：\approx
≡：\equiv
∑：\sum
∏：\prod
∐：\coprod&lt;/p>
&lt;p>集合运算符：
∅：\emptyset
∈：\in
∉：\notin
⊂：\subset
⊃：\supset
⊆：\subseteq
⊇：\supseteq
⋂：\bigcap
⋃：\bigcup
⋁：\bigvee
⋀：\bigwedge
⨄：\biguplus
⨆：\bigsqcup&lt;/p>
&lt;p>对数运算符：
log：\log
lg：\lg
ln：\ln&lt;/p>
&lt;p>三角运算符：
⊥：\bot
∠：\angle
30∘：30^\circ
sin：\sin
cos：\cos
tan：\tan
cot：\cot
sec：\sec
csc：\csc&lt;/p>
&lt;p>微积分运算符：
y′x：\prime
∫：\int
∬：\iint
∭：\iiint
⨌：\iiiint
∮：\oint
lim：\lim
∞：\infty
∇：\nabla&lt;/p>
&lt;p>戴帽符号：
ŷ：\hat{y}
yˇ：\check{y}
y˘：\breve{y}&lt;/p>
&lt;p>连线符号：
a+b+c+d⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯：\overline{a+b+c+d}
a+b+c+d⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯：\underline{a+b+c+d}
a+b+c⏟1.0+d2.0：\overbrace{a+\underbrace{b+c}_{1.0}+d}^{2.0}&lt;/p>
&lt;p>箭头符号：
↑：\uparrow
↓：\downarrow
⇑：\Uparrow
⇓：\Downarrow
→：\rightarrow
←：\leftarrow
⇒：\Rightarrow
⇐：\Leftarrow
⟶：\longrightarrow
⟵：\longleftarrow
⟹：\Longrightarrow
⟸：\Longleftarrow&lt;/p>
&lt;p>3.13 需要转义的字符&lt;/p>
&lt;p>要输出字符　空格　#　$　%　&amp;amp;　_　{　}　，用命令：　\空格　#　$　%　&amp;amp;　_　{　}&lt;/p>
&lt;p>3.14 使用指定字体&lt;/p>
&lt;p>{\rm text}如：
使用罗马字体：text text
其他的字体还有：
\rm 　　罗马体　　　　　　　\it 　　意大利体
\bf 　　黑体　　　　　　　　\cal 　花体
\sl 　　倾斜体　　　　　　　\sf 　　等线体
\mit 　数学斜体　　　　　　\tt 　　打字机字体
\sc 　　小体大写字母&lt;/p>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://wenku.baidu.com/view/fd741028453610661ed9f458.html?from=search" target="_blank" rel="noopener">https://wenku.baidu.com/view/fd741028453610661ed9f458.html?from=search&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Tensorflow-CheatSheet</title><link>https://ng-tech.icu/books/awesome-cheatsheets/ai/tensorflow-cheatsheet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cheatsheets/ai/tensorflow-cheatsheet/</guid><description>&lt;h1 id="tensorflow-cheatsheet--tensorflow-基础概念与实践清单">TensorFlow CheatSheet | TensorFlow 基础概念与实践清单&lt;/h1>
&lt;p>TensorFlow 是一个使用数据流图进行数值计算的开放源代码软件库。图中的节点代表数学运算，而图中的边则代表在这些节点之间传递的多维数组（张量）。借助这种灵活的架构，您可以通过一个 API 将计算工作部署到桌面设备、服务器或移动设备中的一个或多个 CPU 或 GPU。TensorFlow 最初是由 Google Brain 团队（隶属于 Google 机器智能研究部门）中的研究人员和工程师开发的，旨在用于进行机器学习和深度神经网络研究。但该系统具有很好的通用性，还可以应用于众多其他领域。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://ww1.sinaimg.cn/large/007rAy9hgy1g05814b4nwj30u00gngvs.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>TensorFlow 是用于表示某种类型的计算抽象（称为计算图）的框架。在使用 TensorFlow 库时，我们首先需要构建计算图，然后初始化 Session，最后填充数据，并且获取结果。&lt;/p>
&lt;p>计算图本质上是有向图，也是用于捕获有关如何计算的指令的全局数据结构。&lt;/p>
&lt;h1 id="基础概念">基础概念&lt;/h1>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">tensorflow&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">tf&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 每次我们调用 tf.constant 时，都会在计算图中创建一个新的节点&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">two_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="n">two_node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Tensor(&amp;#34;Const:0&amp;#34;, shape=(), dtype=int32)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>会话的作用是处理内存分配和优化，使我们能够实际执行由计算图指定的计算。你可以将计算图想象为我们想要执行的计算的模版：它列出了所有步骤。为了使用计算图，我们需要启动一个会话，它使我们能够实际地完成任务；例如，遍历模版的所有节点来分配一堆用于存储计算输出的存储器。&lt;/p>
&lt;p>会话包含一个指向全局图的指针，该指针通过指向所有节点的指针不断更新。这意味着在创建节点之前还是之后创建会话都无所谓。创建会话对象后，可以使用 sess.run(node) 返回节点的值，并且 TensorFlow 将执行确定该值所需的所有计算。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">two_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">three_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sum_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">two_node&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">three_node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">two_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">sum_node&lt;/span>&lt;span class="p">]))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># [2, 5]&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>如果我们需要在计算图中动态地传入数据，则可以使用占位符和 feed_dict，占位符是一种用于接受外部输入的节点:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 如果不传入占位符值则会抛出异常&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">input_placeholder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">int32&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">input_placeholder&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">input_placeholder&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">})&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>TensorFlow 会自动地帮我们构建计算路径，当我们在依赖于图中其他节点的节点上调用 sess.run() 时，我们也需要计算那些节点的值。如果这些节点具有依赖关系，那么我们需要计算这些值（依此类推……），直到达到计算图的顶端，即节点没有父节点时。TensorFlow 仅通过必需的节点自动进行计算这一事实是该框架的一个巨大优势。如果计算图非常大并且有许多不必要的节点，那么它可以节省大量调用的运行时间。它允许我们构建大型的多用途计算图，这些计算图使用单个共享的核心节点集合，并根据所采取的不同计算路径去做不同的事情。对于几乎所有应用而言，根据所采取的计算路径考虑 sess.run() 的调用是很重要的。&lt;/p>
&lt;p>tf.constant 与 tf.placeholder 都是所谓的无祖先(no-ancestor node)节点，我们还需要变量，其值会随着模型的训练而不断变化。需要使用 tf.get_variable(name，shape) 来创建变量，如果要创建一个标量，就需要使用形状为 [] 的空列表。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">count_variable&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">count_variable&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1">&amp;#39;&amp;#39;&amp;#39;
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1">Traceback (most recent call last):
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1">...
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1">tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value count
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1"> [[Node: _retval_count_0_0 = _Retval[T=DT_FLOAT, index=0, _device=&amp;#34;/job:localhost/replica:0/task:0/device:CPU:0&amp;#34;](count)]]
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s1">&amp;#39;&amp;#39;&amp;#39;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>当我们操作一个尚未赋值的变量时，其会抛出异常；此时我们需要理由 &lt;code>tf.assign()&lt;/code> 或者初始化函数来初始化变量:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">count_variable&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">zero_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">assign_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">assign&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">count_variable&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">zero_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">assign_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">count_variable&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>另一种则是利用全局的初始化函数:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">const_init_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant_initializer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">count_variable&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;count&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="n">initializer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">const_init_node&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">global_variables_initializer&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">init&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">count_variable&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>最后，我们需要为计算图添加优化器(Optimizer)模块，以让模型能够真正地学习。在深度学习中，典型的内循环训练如下：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>获取输入和 true_output&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据输入和参数计算推测值&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据推测与 true_output 之间的差异计算损失&lt;/p>
&lt;/li>
&lt;li>
&lt;p>根据损失的梯度更新参数&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这里以简单的线性回归问题为例，描述如何构建 TensorFlow 模型:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">tensorflow&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">tf&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">### build the graph&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## first set up the parameters&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">m&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;m&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="n">initializer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant_initializer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">get_variable&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;b&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[],&lt;/span> &lt;span class="n">initializer&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant_initializer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">init&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">global_variables_initializer&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## then set up the computations&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">input_placeholder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">output_placeholder&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">placeholder&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">input_placeholder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">output_placeholder&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">y_guess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">m&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">square&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">y&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">y_guess&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## finally, set up the optimizer and minimization node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">optimizer&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">train&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">GradientDescentOptimizer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">1e-3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_op&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">optimizer&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">minimize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">### start the session&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">init&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">### perform the training loop&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">random&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">## set up problem&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">true_m&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">true_b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">for&lt;/span> &lt;span class="n">update_i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">100000&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">## (1) get the input and output&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">input_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">output_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">true_m&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">input_data&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">true_b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">## (2), (3), and (4) all take place within a single call to sess.run()!&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">_loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">loss&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">train_op&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">feed_dict&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="p">{&lt;/span>&lt;span class="n">input_placeholder&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">input_data&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">output_placeholder&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">output_data&lt;/span>&lt;span class="p">})&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span> &lt;span class="n">update_i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">_loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">### finally, print out the values we learned for our two variables&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="s2">&amp;#34;True parameters: m=&lt;/span>&lt;span class="si">%.4f&lt;/span>&lt;span class="s2">, b=&lt;/span>&lt;span class="si">%.4f&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">true_m&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">true_b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span> &lt;span class="s2">&amp;#34;Learned parameters: m=&lt;/span>&lt;span class="si">%.4f&lt;/span>&lt;span class="s2">, b=&lt;/span>&lt;span class="si">%.4f&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="o">%&lt;/span> &lt;span class="nb">tuple&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">m&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">]))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>optimizer = tf.train.GradientDescentOptimizer(1e-3)&lt;/code> 不会向计算图中添加节点，它只是创建一个包含有用的帮助函数的 Python 对象。而 &lt;code>train_op = optimizer.minimize(loss)&lt;/code> 将一个节点添加到图中，并将一个指针存储在变量 train_op 中。train_op 节点没有输出，但是有一个十分复杂的副作用：train_op 回溯输入和损失的计算路径，寻找变量节点。对于它找到的每个变量节点，计算该变量对于损失的梯度。然后计算该变量的新值：当前值减去梯度乘以学习率的积。最后，它执行赋值操作更新变量的值。&lt;/p>
&lt;p>因此基本上，当我们调用 sess.run(train_op) 时，它对我们的所有变量做了一个梯度下降的步骤。当然，我们也需要使用 feed_dict 填充输入和输出占位符，并且我们还希望打印损失的值，因为这样方便调试。tf.Print 实际上是一种具有输出和副作用的 Tensorflow 节点，它有两个必需参数：要复制的节点和要打印的内容列表。要复制的节点可以是图中的任何节点；tf.Print 是一个与要复制的节点相关的恒等操作，意味着输出的是输入的副本。但是，它的副作用是打印出打印列表里的所有当前值。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">two_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">three_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sum_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">two_node&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">three_node&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">print_sum_node&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sum_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">two_node&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">three_node&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">sess&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">print_sum_node&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># [2][3]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 5&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 数据获取&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 数据处理与划分&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 模型/神经网络构建&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 损失函数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 变量初始化&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 模型训练&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 模型预测&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 模型导出&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="损失函数">损失函数&lt;/h1>
&lt;h2 id="交叉熵">交叉熵&lt;/h2>
&lt;p>交叉熵也可以用作损失函数值，其公式定义如下，其中 $q(x)$ 为模型的预估，$p(x)$ 为机器学习中样本的 label:&lt;/p>
&lt;p>$$
H(p,q) = -\sum_xp(x)logq(x)
$$&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">tensorflow&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">tf&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">random&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">randint&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dims&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">8&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">pos&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dims&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random_uniform&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">dims&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">maxval&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">float32&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">labels&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">one_hot&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dims&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">res1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax_cross_entropy_with_logits&lt;/span>&lt;span class="p">(&lt;/span> &lt;span class="n">logits&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">labels&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">res2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sparse_softmax_cross_entropy_with_logits&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logits&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">labels&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">constant&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">pos&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">sess&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">res1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">res2&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="辅助函数">辅助函数&lt;/h1>
&lt;p>tf.argmax 返回的是 vector 中的最大值的索引号，如果 vector 是一个向量，那就返回一个值，如果是一个矩阵，那就返回一个向量，这个向量的每一个维度都是相对应矩阵行的最大值元素的索引号。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">tensorflow&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">tf&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">6&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">]]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">with&lt;/span> &lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Session&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">sess&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">A&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">sess&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">run&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">tf&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">argmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://www.jiqizhixin.com/articles/2018-07-02-6" target="_blank" rel="noopener">https://www.jiqizhixin.com/articles/2018-07-02-6&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>