<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>10.实践案例 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/</link><atom:link href="https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/index.xml" rel="self" type="application/rss+xml"/><description>10.实践案例</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>10.实践案例</title><link>https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/</link></image><item><title>2021-爱奇艺-基于 Netty 的长连接网关</title><link>https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/2021-%E7%88%B1%E5%A5%87%E8%89%BA-%E5%9F%BA%E4%BA%8E-netty-%E7%9A%84%E9%95%BF%E8%BF%9E%E6%8E%A5%E7%BD%91%E5%85%B3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/2021-%E7%88%B1%E5%A5%87%E8%89%BA-%E5%9F%BA%E4%BA%8E-netty-%E7%9A%84%E9%95%BF%E8%BF%9E%E6%8E%A5%E7%BD%91%E5%85%B3/</guid><description>&lt;h1 id="2021-基于-netty-的长连接网关">2021-基于 Netty 的长连接网关&lt;/h1>
&lt;p>WebSocket 是目前实现服务端推送的主流技术，恰当使用能够有效提供系统响应能力，提升用户体验。通过 WebSocket 长连接网关可以快速为系统增加数据推送能力，有效减少运维成本，提高开发效率。长连接网关的价值在于它封装了 WebSocket 通信细节，与业务系统解耦，使得长连接网关与业务系统可独立优化迭代，避免重复开发，便于开发与维护。其次，网关提供了简单易用的 HTTP 推送通道，支持多种开发语言接入，便于系统集成和使用。另外，网关采用了分布式架构，可以实现服务的水平扩容、负载均衡与高可用。最后，网关集成了监控与报警，当系统异常时能及时预警，保证服务的健康和稳定。&lt;/p>
&lt;p>在众多的 WebSocket 实现中，从性能、扩展性、社区支持等方面考虑，最终选择了 Netty。Netty 是一个高性能、事件驱动、异步非阻塞的网络通信框架，在许多知名的开源软件中被广泛使用。WebSocket 是有状态的，无法像直接 HTTP 以集群方式实现负载均衡，长连接建立后即与服务端某个节点保持着会话，因此集群下想要得知会话属于哪个节点，有两种方案，一种是使用类似微服务的注册中心来维护全局的会话映射关系，一种是使用事件广播由各节点自行判断是否持有会话，两种方案对比如下表所示。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>方案&lt;/th>
&lt;th>优点&lt;/th>
&lt;th>缺点&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>注册中心&lt;/td>
&lt;td>会话映射关系清晰，集群规模较大时更合适&lt;/td>
&lt;td>实现复杂，强依赖注册中心，有额外运维成本&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>事件广播&lt;/td>
&lt;td>实现简单更加轻量&lt;/td>
&lt;td>节点较多时，所有节点均被广播，资源浪费&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>综合考虑实现成本与集群规模，选择了轻量级的事件广播方案。实现广播可以选择基于 RocketMQ 的消息广播、基于 Redis 的 Publish/Subscribe、基于 ZooKeeper 的通知等方案，其优缺点对比如下表所示。从吞吐量、实时性、持久化、实现难易等方面考虑，最终选择了 RocketMQ。&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>方案&lt;/th>
&lt;th>优点&lt;/th>
&lt;th>缺点&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>基于 RocketMQ&lt;/td>
&lt;td>吞吐量高、高可用、保证可靠&lt;/td>
&lt;td>实时性不如 Redis&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>基于 Redis&lt;/td>
&lt;td>实时性高、实现简单&lt;/td>
&lt;td>不保证可靠&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>基于 ZooKeeper&lt;/td>
&lt;td>实现简单&lt;/td>
&lt;td>写入性能较差，不适合频繁写入场景&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="系统架构">系统架构&lt;/h2>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://img.imgdb.cn/item/6051b557524f85ce299b8bce.jpg" alt="Netty &amp;amp;amp; WebSocket 架构" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>网关的整体流程如下：&lt;/p>
&lt;ul>
&lt;li>客户端与网关任一节点握手建立起长连接，节点将其加入到内存维护的长连接队列。客户端定时向服务端发送心跳消息，如果超过设定的时间仍没有收到心跳，则认为客户端与服务端的长连接已断开，服务端会关闭连接，清理内存中的会话。&lt;/li>
&lt;li>当业务系统需要向客户端推送数据时，通过网关提供的 HTTP 接口将数据发向网关。&lt;/li>
&lt;li>网关在接收到推送请求后，将消息写入 RocketMQ。&lt;/li>
&lt;li>网关作为消费者，以广播模式消费消息，所有节点都会接收到消息。&lt;/li>
&lt;li>节点接收到消息后判断推送的消息目标是否在自己内存中维护的长连接队列里，如果存在则通过长连接推送数据，否则直接忽略。&lt;/li>
&lt;/ul>
&lt;p>网关以多节点方式构成集群，每节点负责一部分长连接，可实现负载均衡，当面对海量连接时，也可以通过增加节点的方式分担压力，实现水平扩展。同时，当节点出现宕机时，客户端会尝试重新与其他节点握手建立长连接，保证服务整体的可用性。&lt;/p>
&lt;h2 id="会话管理">会话管理&lt;/h2>
&lt;p>长连接建立起来后，会话维护在各节点的内存中。SessionManager 组件负责管理会话，内部使用了哈希表维护了 UID 与 UserSession 的关系；UserSession 代表用户维度的会话，一个用户可能会同时建立多个长连接，因此 UserSession 内部同样使用了一个哈希表维护 Channel 与 ChannelSession 的关系。为了避免用户无限制的创建长连接，UserSession 在内部的 ChannelSession 超过一定数量后，会将最早建立的 ChannelSession 关闭，减少服务器资源占用。SessionManager、UserSession、ChannelSession 的关系如下图所示。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://img.imgdb.cn/item/6051b608524f85ce299bf4a7.jpg" alt="SessionManager 组件" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p></description></item><item><title>2021-喜马拉雅-自研网关架构演进过程</title><link>https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/2021-%E5%96%9C%E9%A9%AC%E6%8B%89%E9%9B%85-%E8%87%AA%E7%A0%94%E7%BD%91%E5%85%B3%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E8%BF%87%E7%A8%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/2021-%E5%96%9C%E9%A9%AC%E6%8B%89%E9%9B%85-%E8%87%AA%E7%A0%94%E7%BD%91%E5%85%B3%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E8%BF%87%E7%A8%8B/</guid><description>&lt;h1 id="喜马拉雅自研网关架构演进过程">喜马拉雅自研网关架构演进过程&lt;/h1>
&lt;p>网关除了要实现最基本的功能反向代理外，还有公有特性，比如黑白名单，流控，鉴权，熔断，API 发布，监控和报警等，我们还根据业务方的需求实现了流量调度，流量 Copy，预发布，智能化升降级，流量预热等相关功能，下面就我们网关在这些方便的一些实践经验以及发展历程，下面是喜马拉雅网关的演化过程：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6054476c524f85ce29082b65.jpg" alt="喜马拉雅网关演化" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h1 id="tomcat-nio--asyncservlet">Tomcat NIO + AsyncServlet&lt;/h1>
&lt;p>网关在架构设计时最为关键点，就是网关在接收到请求，调用后端服务时不能阻塞 Block，否则网关的吞吐量很难上去，因为最耗时的就是调用后端服务这个远程调用过程，如果这里是阻塞的，Tomcat 的工作线程都 block 了，在等待后端服务响应的过程中，不能去处理其他的请求,这个地方一定要异步：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/60544795524f85ce2908423e.jpg" alt="架构图" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>这版我们实现单独的 Push 层，作为网关收到响应后，响应客户端时，通过这层实现，和后端服务的通信是 HttpNioClient，对业务的支持黑白名单，流控，鉴权，API 发布等功能。&lt;/p>
&lt;p>但是这版只是功能上达到网关的要求，处理能力很快就成了瓶颈，单机 qps 到 5k 的时候，就会不停的 full gc，后面通过 dump 线上的堆分析，发现全是 Tomcat 缓存了很多 HTTP 的请求，因为 Tomcat 默认会缓存 200 个 requestProcessor，每个 prcessor 都关联了一个 request，还有就是 Servlet 3.0 Tomcat 的异步实现会出现内存泄漏，后面通过减少这个配置，效果明显。但性能肯定就下降了，总结了下，基于 Tomcat 做为接入端，有如下几个问题：&lt;/p>
&lt;h2 id="tomcat-自身的问题">Tomcat 自身的问题：&lt;/h2>
&lt;ul>
&lt;li>缓存太多，Tomcat 用了很多对象池技术，内存有限的情况下，流量一高很容易触发 gc。&lt;/li>
&lt;li>内存 copy，Tomcat 的默认是用堆内存，所以数据需要读到堆内，而我们后端服务是 Netty，有堆外内存，需要通过数次 copy。&lt;/li>
&lt;li>Tomcat 还有个问题是读 body 是阻塞的, Tomcat 的 NIO 模型和 reactor 模型不一样，读 body 是 block 的。&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6054497d524f85ce2909aa86.jpg" alt="Tomcat buffer 的关系图" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>通过上面的图，我们可以看出，Tomcat 对外封装的很好，内部默认的情况下会有三次 copy&lt;/p>
&lt;h2 id="httpnioclient-的问题">HttpNioClient 的问题&lt;/h2>
&lt;p>获取和释放连接都需要加锁，对应网关这样的代理服务场景，会频繁的建连和关闭连接，势必会影响性能。&lt;/p>
&lt;h1 id="第二版-netty--全异步">第二版 Netty + 全异步&lt;/h1>
&lt;p>基于 Netty 的优势，我们实现了全异步，无锁，分层的架构，先看下我们基于 Netty 做接入端的架构图：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/605449d5524f85ce2909e176.jpg" alt="Netty 架构" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="接入层">接入层&lt;/h2>
&lt;p>Netty 的 IO 线程，负责 HTTP 协议的编解码工作，同时对协议层面的异常做监控报警&lt;/p>
&lt;p>对 HTTP 协议的编解码做了优化，对异常，攻击性请求监控可视化。比如我们对 HTTP 的请求行和请求头大小是有限制的，Tomcat 是请求行和请求加在一起，不超过 8k，Netty 是分别有大小限制。假如客户端发送了超过阀值的请求，带 cookie 的请求很容易超过，正常情况下，Netty 就直接响应 400 给客户端。&lt;/p>
&lt;p>经过改造后，我们只取正常大小的部分，同时标记协议解析失败，到业务层后，就可以判断出是那个服务出现这类问题，其他的一些攻击性的请求，比如只发请求头，不发 body 或者发部分这些都需要监控和报警。&lt;/p>
&lt;h2 id="业务逻辑层">业务逻辑层&lt;/h2>
&lt;p>负责对 API 路由，流量调度等一序列的支持业务的公有逻辑，都在这层实现，采样责任链模式，这层不会有 IO 操作。在业界和一些大厂的网关设计中，业务逻辑层基本都是设计成责任链模式，公有的业务逻辑也在这层实现，我们在这层也是相同的套路，支持了：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>用户鉴权和登陆校验&lt;/strong>，支持接口级别配置&lt;/li>
&lt;li>&lt;strong>黑白名单&lt;/strong>，分全局和应用，以及 ip 维度，参数级别&lt;/li>
&lt;li>&lt;strong>流量控制&lt;/strong>，支持自动和手动，自动是对超大流量自动拦截，通过令牌桶算法实现&lt;/li>
&lt;li>&lt;strong>智能熔断&lt;/strong>，在 histrix 的基础上做了改进，支持自动升降级，我们是全部自动的，也支持手动配置立即熔断，就是发现服务异常比例达到阀值，就自动触发熔断&lt;/li>
&lt;li>&lt;strong>灰度发布&lt;/strong>，我对新启动的机器的流量支持类似 tcp 的慢启动机制，给机器一个预热的时间窗口&lt;/li>
&lt;li>&lt;strong>统一降级&lt;/strong>，我们对所有转发失败的请求都会找统一降级的逻辑，只要业务方配了降级规则，都会降级，我们对降级规则是支持到参数级别的，包含请求头里的值，是非常细粒度的，另外我们还会和 varnish 打通，支持 varnish 的优雅降级&lt;/li>
&lt;li>&lt;strong>流量调度&lt;/strong>，支持业务根据筛选规则，对流量筛选到对应的机器，也支持只让筛选的流量访问这台机器，这在查问题/新功能发布验证时非常用，可以先通过小部分流量验证再大面积发布上线。&lt;/li>
&lt;li>&lt;strong>流量 copy&lt;/strong>，我们支持对线上的原始请求根据规则 copy 一份，写入到 mq 或者其他的 upstream，来做线上跨机房验证和压力测试。&lt;/li>
&lt;li>&lt;strong>请求日志采样&lt;/strong>，我们对所有的失败的请求都会采样落盘，提供业务方排查问题支持，也支持业务方根据规则进行个性化采样，我们采样了整个生命周期的数据，包含请求和响应相关的所有数据。&lt;/li>
&lt;/ul>
&lt;p>上面提到的这么多都是对流量的治理，我们每个功能都是一个 filter，处理失败都不影响转发流程，而且所有的这些规则的元数据在网关启动时就会全部初始化好。在执行的过程中，不会有 IO 操作，目前有些设计会对多个 filter 做并发执行，由于我们的都是内存操作，开销并不大，所以我们目前并没有支持并发执行。&lt;/p>
&lt;p>还有个就是规则会修改，我们修改规则时，会通知网关服务，做实时刷新，我们对内部自己的这种元数据更新的请求，通过独立的线程处理，防止 IO 在操作时影响业务线程。&lt;/p>
&lt;h2 id="服务调用层">服务调用层&lt;/h2>
&lt;p>服务调用对于代理网关服务是关键的地方，一定需要异步，我们通过 Netty 实现,同时也很好的利用了 Netty 提供的连接池，做到了获取和释放都是无锁操作。&lt;/p>
&lt;h2 id="异步-push">异步 Push&lt;/h2>
&lt;p>网关在发起服务调用后，让工作线程继续处理其他的请求，而不需要等待服务端返回，这里的设计是我们为每个请求都会创建一个上下文，我们在发完请求后，把该请求的 context 绑定到对应的连接上，等 Netty 收到服务端响应时，就会在给连接上执行 read 操作。&lt;/p>
&lt;p>解码完后，再从给连接上获取对应的 context，通过 context 可以获取到接入端的 session，这样 push 就通过 session 把响应写回客户端了，这样设计也是基于 HTTP 的连接是独占的，即连接和请求上下文绑定。&lt;/p>
&lt;h2 id="连接池">连接池&lt;/h2>
&lt;p>连接池的原理如下图：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/60544a45524f85ce290a2bed.jpg" alt="连接池的原理" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>服务调用层除了异步发起远程调用外，还需要对后端服务的连接进行管理，HTTP 不同于 RPC，HTTP 的连接是独占的，所以在释放的时候要特别小心，一定要等服务端响应完了才能释放，还有就是连接关闭的处理也要小心，总结如下几点：&lt;/p>
&lt;ul>
&lt;li>Connection:close&lt;/li>
&lt;li>空闲超时，关闭连接&lt;/li>
&lt;li>读超时关闭连接&lt;/li>
&lt;li>写超时，关闭连接&lt;/li>
&lt;li>Fin,Reset&lt;/li>
&lt;/ul>
&lt;p>上面几种需要关闭连接的场景，下面主要说下 Connection:close 和空闲写超时两种，其他的应该是比较常见的比如读超时，连接空闲超时，收到 fin，reset 码这几个。&lt;/p>
&lt;ul>
&lt;li>Connection:close&lt;/li>
&lt;/ul>
&lt;p>后端服务是 Tomcat，Tomcat 对连接重用的次数是有限制的，默认是 100 次，当达到 100 次后，Tomcat 会通过在响应头里添加 Connection:close，让客户端关闭该连接，否则如果再用该连接发送的话，会出现 400。&lt;/p>
&lt;p>还有就是如果端上的请求带了 connection:close,那 Tomcat 就不等这个连接重用到 100 次，即一次就关闭，通过在响应头里添加 Connection:close，即成了短连接，这个在和 Tomcat 保持长连接时，需要注意的，如果要利用，就要主动 remove 掉这个 close 头。&lt;/p>
&lt;ul>
&lt;li>写超时&lt;/li>
&lt;/ul>
&lt;p>首先网关什么时候开始计算服务的超时时间，如果从调用 writeAndFlush 开始就计算，这其实是包含了 Netty 对 HTTP 的 encode 时间和从队列里把请求发出去即 flush 的时间，这样是对后端服务不公平的，所以需要在真正 flush 成功后开始计时，这样是和服务端最接近的，当然还包含了网络往返时间和内核协议栈处理的时间，这个不可避免，但基本不变。&lt;/p>
&lt;p>所以我们是 flush 成功回调后开始启动超时任务，这里就有个注意的地方，如果 flush 不能快速回调，比如来了一个大的 post 请求，body 部分比较大，而 Netty 发送的时候第一次默认是发 1k 的大小，如果还没有发完，则增大发送的大小继续发，如果在 Netty 在 16 次后还没有发送完成，则不会再继续发送，而是提交一个 flushTask 到任务队列，待下次执行到后再发送。这时 flush 回调的时间就比较大，导致这样的请求不能及时关闭，而且后端服务 Tomcat 会一直阻塞在读 body 的地方，基于上面的分析，所以我们需要一个写超时，对大的 body 请求，通过写超时来及时关闭。&lt;/p>
&lt;h2 id="全链路超时机制">全链路超时机制&lt;/h2>
&lt;p>下面是我们在整个链路超时处理的机制。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/60544af1524f85ce290ab657.jpg" alt="超时处理机制" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;ul>
&lt;li>协议解析超时&lt;/li>
&lt;li>等待队列超时&lt;/li>
&lt;li>建连超时&lt;/li>
&lt;li>等待连接超时&lt;/li>
&lt;li>写前检查是否超时&lt;/li>
&lt;li>写超时&lt;/li>
&lt;li>响应超时&lt;/li>
&lt;/ul>
&lt;h2 id="监控报警">监控报警&lt;/h2>
&lt;p>网关业务方能看到的是监控和报警，我们是实现秒级别报警和秒级别的监控，监控数据定时上报给我们的管理系统，由管理系统负责聚合统计，落盘到 influxdb。我们对 HTTP 协议做了全面的监控和报警，无论是协议层的还是服务层的：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>协议层&lt;/p>
&lt;ul>
&lt;li>攻击性请求，只发头，不发/发部分 body，采样落盘，还原现场，并报警&lt;/li>
&lt;li>Line or Head or Body 过大的请求，采样落盘，还原现场，并报警&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>应用层&lt;/p>
&lt;ul>
&lt;li>耗时监控，有慢请求，超时请求，以及 tp99，tp999 等&lt;/li>
&lt;li>qps 监控和报警&lt;/li>
&lt;li>带宽监控和报警，支持对请求和响应的行，头，body 单独监控。&lt;/li>
&lt;li>响应码监控，特别是 400，和 404&lt;/li>
&lt;li>连接监控，我们对接入端的连接，以及和后端服务的连接，后端服务连接上待发送字节大小也都做了监控&lt;/li>
&lt;li>失败请求监控&lt;/li>
&lt;li>流量抖动报警，这是非常有必要的，流量抖动要么是出了问题，要么就是出问题的前兆。&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="性能优化实践">性能优化实践&lt;/h2>
&lt;h3 id="对象池技术">对象池技术&lt;/h3>
&lt;p>对于高并发系统，频繁的创建对象不仅有分配内存的开销外，还有对 GC 会造成压力，我们在实现时会对频繁使用的比如线程池的任务 task，StringBuffer 等会做写重用，减少频繁的申请内存的开销。&lt;/p>
&lt;h3 id="上下文切换">上下文切换&lt;/h3>
&lt;p>高并发系统，通常都采用异步设计，异步化后，不得不考虑线程上下文切换的问题，我们的线程模型如下：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/60544b99524f85ce290b1cdf.jpg" alt="上下文切换" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>我们整个网关没有涉及到 IO 操作，但我们在业务逻辑这块还是和 Netty 的 IO 编解码线程异步，是有两个原因，1）是防止开发写的代码有阻塞，2）是业务逻辑打日志可能会比较多，在突发的情况下，在 push 线程时，支持用 Netty 的 IO 线程替代，这里做的工作比较少，这里有异步修改为同步后(通过修改配置调整)，CPU 的上下文切换减少 20%，进而提高了整体的吞吐量，就是不能为了异步而异步，zull2 的设计和我们的类似。&lt;/p>
&lt;h3 id="gc-优化">GC 优化&lt;/h3>
&lt;p>在高并发系统，GC 的优化不可避免，我们在用了对象池技术和堆外内存时，对象很少进入老年代，另外我们年轻代会设置的比较大，而且 SurvivorRatio=2，晋升年龄设置最大 15，尽量对象在年轻代就回收掉， 但监控发现老年代的内存还是会缓慢增长。&lt;/p>
&lt;p>通过 dump 分析，我们每个后端服务创建一个连接，都时有一个 socket，socket 的 AbstractPlainSocketImpl，而 AbstractPlainSocketImpl 就重写了 Object 类的 finalize 方法，实现如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="cm">/**
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm">* Cleans up if the user forgets to close it.
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cm">*/&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kd">protected&lt;/span> &lt;span class="kt">void&lt;/span> &lt;span class="nf">finalize&lt;/span>&lt;span class="o">()&lt;/span> &lt;span class="kd">throws&lt;/span> &lt;span class="n">IOException&lt;/span> &lt;span class="o">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">close&lt;/span>&lt;span class="o">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>是为了我们没有主动关闭连接，做的一个兜底，在 GC 回收的时候，先把对应的连接资源给释放了，由于 finalize 的机制是通过 JVM 的 Finalizer 线程来处理的，而且 Finalizer 线程的优先级不高，默认是 8，需要等到 Finalizer 线程把 ReferenceQueue 的对象对于的 finalize 方法执行完，还要等到下次 GC 时，才能把该对象回收，导致创建连接的这些对象在年轻代不能立即回收，从而进入了老年代，这也是为啥老年代会一直缓慢增长的问题。&lt;/p>
&lt;h3 id="日志">日志&lt;/h3>
&lt;p>高并发下，特别是 Netty 的 IO 线程除了要执行该线程上的 IO 读写操作，还有执行异步任务和定时任务，如果 IO 线程处理不过来队列里的任务，很有可能导致新进来异步任务出现被拒绝的情况。&lt;/p>
&lt;p>那什么情况下可能呢，IO 是异步读写的问题不大，就是多耗点 CPU，最有可能 block 住 IO 线程的是我们打的日志，目前 Log4j 的 ConsoleAppender 日志 immediateFlush 属性默认为 true，即每次打 log 都是同步写 flush 到磁盘的，这个对于内存操作来说，慢了很多。&lt;/p>
&lt;p>同时 AsyncAppender 的日志队列满了也会 block 住线程,log4j 默认的 buffer 大小是 128，而且是 block 的，即如果 buffer 的大小达到 128，就阻塞了写日志的线程，在并发写日志量大的的情况下，特别是堆栈很多时，log4j 的 Dispatcher 线程会出现变慢要刷盘，这样 buffer 就不能快速消费，很容易写满日志事件，导致 Netty IO 线程 block 住，所以我们在打日志时，也要注意精简。&lt;/p></description></item><item><title>2022-How Tinder Built Their Own API Gateway</title><link>https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/2022-how-tinder-built-their-own-api-gateway/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/microcn-series/4.%E6%8E%A5%E5%85%A5%E7%BD%91%E5%85%B3/10.%E5%AE%9E%E8%B7%B5%E6%A1%88%E4%BE%8B/2022-how-tinder-built-their-own-api-gateway/</guid><description>&lt;h1 id="how-tinder-built-their-own-api-gateway">How Tinder Built Their Own API Gateway&lt;/h1>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://blog.quastor.org/p/tinder-built-api-gateway" target="_blank" rel="noopener">https://blog.quastor.org/p/tinder-built-api-gateway&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>