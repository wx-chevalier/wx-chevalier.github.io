<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>99.参考资料 | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</link>
      <atom:link href="https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/index.xml" rel="self" type="application/rss+xml" />
    <description>99.参考资料</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>99.参考资料</title>
      <link>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/</link>
    </image>
    
    <item>
      <title>2019-NLP 中的 RNN、Seq2Seq 与 Attention 注意力机制</title>
      <link>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-nlp-%E4%B8%AD%E7%9A%84-rnnseq2seq-%E4%B8%8E-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-nlp-%E4%B8%AD%E7%9A%84-rnnseq2seq-%E4%B8%8E-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/52119092&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>2020-完全解析 RNN, Seq2Seq, Attention 注意力机制</title>
      <link>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2020-%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90-rnn-seq2seq-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2020-%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90-rnn-seq2seq-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/51383402&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>2021-Transformer模型详解（图解最完整版）</title>
      <link>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%AE%8C%E6%95%B4%E7%89%88/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%AE%8C%E6%95%B4%E7%89%88/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/338817680&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;transformer-模型详解图解最完整版&#34;&gt;Transformer 模型详解（图解最完整版）&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>2021-超详细图解 Self-Attention</title>
      <link>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E8%B6%85%E8%AF%A6%E7%BB%86%E5%9B%BE%E8%A7%A3-self-attention/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E8%B6%85%E8%AF%A6%E7%BB%86%E5%9B%BE%E8%A7%A3-self-attention/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/410776234&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;原文地址&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>2023-Transformers from Scratch</title>
      <link>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-transformers-from-scratch/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-transformers-from-scratch/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://e2eml.school/transformers.html#one_hot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Original&lt;/a&gt;，&lt;a href=&#34;https://blog.csdn.net/weixin_44355919/article/details/126785625&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;中文翻译&lt;/a&gt; TODO!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;transformers-from-scratch&#34;&gt;Transformers from Scratch&lt;/h1&gt;
&lt;p&gt;“我拖延了几年时间后才开始深入研究 Transformers 模型。最后因为不了解他们是如何实现而产生的不适感变得对我来说过于强烈（才终于开始着手其中），以下便是我深入研究的内容。&lt;/p&gt;
&lt;p&gt;Transformers 在 2017 年的这篇论文中被介绍为序列转换的工具——将一个符号序列转换为另一个符号。该领域最流行的应用是语义翻译，如从英语翻译到德语。它同时也被拓展出实现序列补全的功能-给定一个起始提示，以相同的脉络和风格进行下去。Transformers 已迅速成为自然语言处理的研究和产品开发的一个不可或缺的工具。&lt;/p&gt;
&lt;p&gt;在我们开始之前先提个醒。我们将讨论很多关于矩阵乘法的内容，并涉及反向传播（用于训练模型的算法），但你不需要事先了解任何内容。我们将逐一添加我们需要的概念，并加以解释。&lt;/p&gt;
&lt;p&gt;这将不是一段短暂的学习旅程，但我希望你能乐在其中。”&lt;/p&gt;
&lt;h1 id=&#34;one-hot-encoding独热编码&#34;&gt;One-hot encoding（独热编码）&lt;/h1&gt;
&lt;p&gt;首先是文字处理部分。对于海量的文字，我们的第一步是把所有的字符转换成数据，这样就可以对它们进行数学运算。&lt;/p&gt;
&lt;p&gt;假设我们的目标是实现可以响应我们语音命令的计算机。我们的工作是建立一个 Transformers 工具，将一连串的声音转换为一连串的文字。&lt;/p&gt;
&lt;p&gt;首先需要选择我们的词汇表，即我们在每个序列中要使用的符号集合。在我们的案例中，将有两类不同的符号集合，一类代表声音输入序列，另一类代表输出的文字序列。&lt;/p&gt;
&lt;p&gt;现在，假设我们使用英语工作。英语中有数万个单词，也许还有几千个用以涵盖计算机专用术语。这将使我们的词汇量级达到十万之多。将单词转换为数字的一种方法是，从一开始计数，给每个单词分配自己的数字。这样一来，一连串的单词就可以被表示为一串数字。&lt;/p&gt;
&lt;p&gt;例如，假设有一种极小的语言，其词汇量只有三个：files, find, and my。每个词都可以换成一个数字，也许 files=1，find=2，my=3。那么，由单词序列[find, my, files]组成的句子 &amp;ldquo;Find my files &amp;ldquo;就可以表示为数字序列[2, 3, 1]。&lt;/p&gt;
&lt;p&gt;这是一种相当有效的将符号转换为数字的方法，但事实证明，还有一种格式对计算机来说更容易操作，那就是 One-hot 编码。在单次编码中，一个符号由一个大部分为零的数组表示，与词汇的长度相同，只有一个元素的值为 1。数组中的每个元素都对应于一个单独的符号。&lt;/p&gt;
&lt;p&gt;另一种认识 One-hot 编码的方式是，每个词仍然被分配自己的数字，但现在这个数字是一个数组的索引。下图是我们上面的例子，用 One-hot 编码表示。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://assets.ng-tech.icu/item/20230426223555.png&#34; alt=&#34;One-hot&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;因此，&amp;ldquo;Find my files &amp;ldquo;这句话变成了一连串的一维数组，在你把它们压缩在一起后，开始看起来像一个二维数组了。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://assets.ng-tech.icu/item/20230426223629.png&#34; alt=&#34;二维数组&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;注意，我将交替使用 &amp;ldquo;一维数组 &amp;ldquo;和 &amp;ldquo;矢量 &amp;ldquo;这两个术语。同样，&amp;ldquo;二维数组 &amp;ldquo;和 &amp;ldquo;矩阵 &amp;ldquo;也是如此。&lt;/p&gt;
&lt;h1 id=&#34;点积&#34;&gt;点积&lt;/h1&gt;
&lt;p&gt;One-hot 表示法的一个真正有用的地方是，它让我们可以计算点积。它们也以其他令人生畏的名字著称，如内积和标量积。要得到两个向量的点积，需要将它们相应的元素相乘，然后将结果相加。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://assets.ng-tech.icu/item/20230426223712.png&#34; alt=&#34;点积&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;当我们处理 One-hot 词的表示时，点积相当有用。任何 One-hot 向量与自身的点积都是 1。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
