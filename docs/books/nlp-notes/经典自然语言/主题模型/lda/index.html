<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="Latent Dirichlet Dirichlet Distribution & Dirichlet Process:狄利克雷分布于狄利克雷过程 作者：肉很多链接：https://www.zhihu.com/question/26751755/answer/80931791来源：知乎著作权归"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/lda/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/lda/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/lda/"><meta property="og:title" content="LDA | Next-gen Tech Edu"><meta property="og:description" content="Latent Dirichlet Dirichlet Distribution & Dirichlet Process:狄利克雷分布于狄利克雷过程 作者：肉很多链接：https://www.zhihu.com/question/26751755/answer/80931791来源：知乎著作权归"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>LDA | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=fd1975933eb077c572f2144ee09600db><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">主题模型</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idf62814d1aec92e82cc474855cb3e8826")' href=#idf62814d1aec92e82cc474855cb3e8826 aria-expanded=false aria-controls=idf62814d1aec92e82cc474855cb3e8826 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/>经典自然语言</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idf62814d1aec92e82cc474855cb3e8826 aria-expanded=false aria-controls=idf62814d1aec92e82cc474855cb3e8826><i class="fa-solid fa-angle-down" id=caret-idf62814d1aec92e82cc474855cb3e8826></i></a></div><ul class="nav docs-sidenav collapse show" id=idf62814d1aec92e82cc474855cb3e8826><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id10e5d5fc96ec9fa77ffb2be69018cedd")' href=#id10e5d5fc96ec9fa77ffb2be69018cedd aria-expanded=false aria-controls=id10e5d5fc96ec9fa77ffb2be69018cedd aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/>词嵌入</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id10e5d5fc96ec9fa77ffb2be69018cedd aria-expanded=false aria-controls=id10e5d5fc96ec9fa77ffb2be69018cedd><i class="fa-solid fa-angle-right" id=caret-id10e5d5fc96ec9fa77ffb2be69018cedd></i></a></div><ul class="nav docs-sidenav collapse" id=id10e5d5fc96ec9fa77ffb2be69018cedd><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id4d70266e5bd6c0d62687b5e324c924c9")' href=#id4d70266e5bd6c0d62687b5e324c924c9 aria-expanded=false aria-controls=id4d70266e5bd6c0d62687b5e324c924c9 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/>词向量</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id4d70266e5bd6c0d62687b5e324c924c9 aria-expanded=false aria-controls=id4d70266e5bd6c0d62687b5e324c924c9><i class="fa-solid fa-angle-right" id=caret-id4d70266e5bd6c0d62687b5e324c924c9></i></a></div><ul class="nav docs-sidenav collapse" id=id4d70266e5bd6c0d62687b5e324c924c9><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/>基于 Gensim 的 Word2Vec 实践</a></li></ul></div><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E6%A6%82%E8%BF%B0/>概述</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id79d4aecf00d90487dca71cf83b24a12c")' href=#id79d4aecf00d90487dca71cf83b24a12c aria-expanded=false aria-controls=id79d4aecf00d90487dca71cf83b24a12c aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>统计语言模型</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id79d4aecf00d90487dca71cf83b24a12c aria-expanded=false aria-controls=id79d4aecf00d90487dca71cf83b24a12c><i class="fa-solid fa-angle-right" id=caret-id79d4aecf00d90487dca71cf83b24a12c></i></a></div><ul class="nav docs-sidenav collapse" id=id79d4aecf00d90487dca71cf83b24a12c><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id774bc18c9c918aa32379eb740312e0a2")' href=#id774bc18c9c918aa32379eb740312e0a2 aria-expanded=false aria-controls=id774bc18c9c918aa32379eb740312e0a2 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/>BERT</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id774bc18c9c918aa32379eb740312e0a2 aria-expanded=false aria-controls=id774bc18c9c918aa32379eb740312e0a2><i class="fa-solid fa-angle-right" id=caret-id774bc18c9c918aa32379eb740312e0a2></i></a></div><ul class="nav docs-sidenav collapse" id=id774bc18c9c918aa32379eb740312e0a2><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/>目标函数</a></li><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA/>输入表示</a></li></ul></div><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/>Word2Vec</a></li><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E8%AF%8D%E8%A1%A8%E7%A4%BA/>词表示</a></li><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/>基础文本处理</a></li><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>统计语言模型</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-ided3ef90acbcf67915b5da74475b05bff")' href=#ided3ef90acbcf67915b5da74475b05bff aria-expanded=false aria-controls=ided3ef90acbcf67915b5da74475b05bff aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%AD%E6%B3%95%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/>语法语义分析</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#ided3ef90acbcf67915b5da74475b05bff aria-expanded=false aria-controls=ided3ef90acbcf67915b5da74475b05bff><i class="fa-solid fa-angle-right" id=caret-ided3ef90acbcf67915b5da74475b05bff></i></a></div><ul class="nav docs-sidenav collapse" id=ided3ef90acbcf67915b5da74475b05bff><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%AD%E6%B3%95%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/>命名实体识别</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idde19769b6cee9616d93198fec51b721f")' href=#idde19769b6cee9616d93198fec51b721f aria-expanded=false aria-controls=idde19769b6cee9616d93198fec51b721f aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/>主题模型</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idde19769b6cee9616d93198fec51b721f aria-expanded=false aria-controls=idde19769b6cee9616d93198fec51b721f><i class="fa-solid fa-angle-down" id=caret-idde19769b6cee9616d93198fec51b721f></i></a></div><ul class="nav docs-sidenav collapse show" id=idde19769b6cee9616d93198fec51b721f><li class="child level active"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/lda/>LDA</a></li></ul></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#beta-分布dirichlet-分布的基础>Beta 分布:Dirichlet 分布的基础</a></li><li><a href=#dirichlet-分布多项分布的共轭分布>Dirichlet 分布:多项分布的共轭分布</a><ul><li><a href=#symmetric-dirichlet-distribution对称-dirichlet-分布>Symmetric Dirichlet Distribution(对称 Dirichlet 分布)</a></li></ul></li></ul><ul><li><a href=#terminology>Terminology</a></li><li><a href=#模型过程>模型过程</a></li><li><a href=#参数学习>参数学习</a><ul><li><a href=#似然概率>似然概率</a></li></ul></li></ul><ul><li><a href=#gibbs-updating-rule>Gibbs Updating Rule</a></li><li><a href=#词分布和主题分布总结>词分布和主题分布总结</a></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>LDA</h1><div class=article-style><h1 id=latent-dirichlet>Latent Dirichlet</h1><h1 id=dirichlet-distribution--dirichlet-process狄利克雷分布于狄利克雷过程>Dirichlet Distribution & Dirichlet Process:狄利克雷分布于狄利克雷过程</h1><p>作者：肉很多链接：https://www.zhihu.com/question/26751755/answer/80931791来源：知乎著作权归作者所有，转载请联系作者获得授权。要想易懂地理解dirichlet distribution，首先先得知道它的特殊版本 beta distribution 干了什么。而要理解 beta distribution 有什么用，还得了解 Bernoulli process。</p><p>首先先看<strong>Bernoulli process</strong>。要理解什么是 Bernoulli process，首先先看什么 Bernoulli trial。Bernoulli trial 简单地说就是一个只有两个结果的简单 trial，比如***抛硬币*<strong>。
那我们就用</strong>抛一个(不均匀)硬币**来说好了，X = 1 就是头，X = 0 就是字，我们设定 q 是抛出字的概率。
那什么是 bernoulli process？就是从 Bernoulli population 里随机抽样，或者说就是重复的独立 Bernoulli trials，再或者说就是狂抛这枚硬币 n 次记结果吧(汗=_=)。好吧，我们就一直抛吧，我们记下 X=0 的次数 k.</p><p>现在问题来了。
Q：<strong>我们如何知道这枚硬币抛出字的概率？<strong>我们知道，如果可以一直抛下去，最后 k/n 一定会趋近于 q；可是现实中有很多场合不允许我们总抛硬币，比如</strong>我只允许你抛 4 次</strong>。你该怎么回答这个问题？显然你在只抛 4 次的情况下，k/n 基本不靠谱；那你只能"<strong>猜一下 q 大致分布在[0,1]中间的哪些值里会比较合理</strong>",但绝不可能得到一个准确的结果比如 q 就是等于 k/n。</p><p>举个例子，比如：4 次抛掷出现“头头字字”，你肯定觉得 q 在 0.5 附近比较合理，q 在 0.2 和 0.8 附近的硬币抛出这个结果应该有点不太可能，q = 0.05 和 0.95 那是有点扯淡了。
你如果把这些值画出来，你会发现 q 在[0,1]区间内呈现的就是一个中间最高，两边低的情况。从感性上说，这样应当是比较符合常理的。</p><p>那我们如果有个什么工具能描述一下这个 q 可能的分布就好了，比如用一个概率密度函数来描述一下? 这当然可以，可是我们还需要注意另一个问题，那就是随着 n 增长观测变多，<strong>你每次的概率密度函数该怎么计算</strong>？该怎么利用以前的结果更新(这个在形式上和计算上都很重要)？</p><p>到这里，其实很自然地会想到把 bayes theorem 引进来，因为 Bayes 能随着不断的观测而更新概率；而且每次只需要前一次的 prior 等等…在这先不多说 bayes 有什么好，接下来用更形式化语言来讲其实说得更清楚。</p><p><strong>我们现在用更正规的语言重新整理一下思路。<strong>现在有个硬币得到 random sample X = (x1,x2,&mldr;xn)，我们需要基于这 n 次观察的结果来估算一下</strong>q 在[0,1]中取哪个值比较靠谱</strong>，由于我们不能再用单一一个确定的值描述 q，所以我们用一个分布函数来描述：有关 q 的概率密度函数(说得再简单点，即是 q 在[0,1]“分布律”)。当然，这应当写成一个条件密度：f(q|X)，因为我们总是观测到 X 的情况下，来猜的 q。</p><p>现在我们来看看 Bayes theorem，看看它能带来什么不同：<figure><div class="d-flex justify-content-center"><div class=w-100><img src="//zhihu.com/equation?tex=P%28q%7Cx%29+P%28x%29+%3D+P%28X%3Dx%7Cq%29P%28q%29" alt="P(q|x) P(x) = P(X=x|q)P(q)" loading=lazy data-zoomable></div></div></figure></p><p>在这里 P(q)就是关于 q 的先验概率(所谓先验，就是在得到观察 X 之前，我们设定的关于 q 的概率密度函数)。P(q|x)是观测到 x 之后得到的关于 q 的后验概率。注意，到这里公式里出现的都是"概率"，并没有在[0,1]上的概率密度函数出现。为了让贝叶斯定理和密度函数结合到一块。我们可以从方程两边由 P(q)得到 f(q)，而由 P(q|x)得到 f(q|x)。
又注意到 P(x)可以认定为是个常量(Q：why？)，可以在分析这类问题时不用管。<strong>那么，这里就有个简单的结论——**<strong>关于 q 的后验概率密度 f(q|x)就和“关于 q 的*</strong>*先验概率密度乘以一个条件概率"成比例，即：</strong><figure><div class="d-flex justify-content-center"><div class=w-100><img src="//zhihu.com/equation?tex=f%28q%7Cx%29%5Csim+P%28X%3Dx%7Cq%29f%28q%29" alt="f(q|x)\sim P(X=x|q)f(q)" loading=lazy data-zoomable></div></div></figure></p><p>带着以上这个结论，我们再来看这个抛硬币问题：
连续抛 n 次，即为一个 bernoulli process，则在 q 确定时，n 次抛掷结果确定时，又观察得到 k 次字的概率可以描述为：<figure><div class="d-flex justify-content-center"><div class=w-100><img src="//zhihu.com/equation?tex=P%28X%3Dx%7Cp%29+%3D+q%5E%7Bk%7D%281-q%29%5E%7Bn-k%7D+" alt="P(X=x|p) = q^{k}(1-q)^{n-k} " loading=lazy data-zoomable></div></div></figure>那么 f(q|x)就和先验概率密度乘以以上的条件概率是成比例的：<figure><div class="d-flex justify-content-center"><div class=w-100><img src="//zhihu.com/equation?tex=f%28q%7Cx%29+%5Csim+q%5E%7Bk%7D%281-q%29%5E%7Bn-k%7Df%28q%29+" alt="f(q|x) \sim q^{k}(1-q)^{n-k}f(q) " loading=lazy data-zoomable></div></div></figure>虽然我们不知道，也求不出那个 P(x)，但我们知道它是固定的，我们这时其实已经得到了一个求 f(q|x)的公式(只要在 n 次观测下确定了，f(q)确定了，那么 f(q|x)也确定了)。</p><p>现在在来看 f(q)。显然，在我们对硬币一无所知的时候，我们应当认为硬币抛出字的概率 q 有可能在[0,1]上任意处取值。f(q)在这里取个均匀分布的密度函数是比较合适的，即 f(q) = 1 (for q in [0,1])。
有些同学可能发现了，这里面<figure><div class="d-flex justify-content-center"><div class=w-100><img src="//zhihu.com/equation?tex=f%28q%7Cx%29+%5Csim+q%5E%7Bk%7D%281-q%29%5E%7Bn-k%7D" alt="f(q|x) \sim q^{k}(1-q)^{n-k}" loading=lazy data-zoomable></div></div></figure>，<strong>那个<figure><div class="d-flex justify-content-center"><div class=w-100><img src="//zhihu.com/equation?tex=q%5E%7Bk%7D%281-q%29%5E%7Bn-k%7D" alt=q^{k}(1-q)^{n-k} loading=lazy data-zoomable></div></div></figure>乘上[0,1]的均匀分布不就是一个 Beta distribution 么</strong>？
对，它就是一个 Beta distribution。Beta distribution 由两个参数 alpha、beta 确定；在这里对应的 alpha 等于 k+1，beta 等于 n+1-k。而<strong>均匀分布的先验密度函数，就是那个 f(q)也可以被 beta distribution 描述</strong>，这时 alpha 等于 1，beta 也等于 1。</p><p>更有意思的是，当我们每多抛一次硬币，出现字时，我们只需要 alpha = alpha + 1；出现头只需要 beta = beta + 1。这样就能得到需要估计的概率密度 f(q|x)…</p><p>其实之所以计算会变得这么简单，是因为被 beta distribution 描述的 prior 经过 bayes formula 前后还是一个 beta distribution；这种不改变函数本身所属 family 的特性，叫<strong>共轭(conjugate)</strong>。</p><p>ok。讲到这你应该明白，对于有两个结果的重复 Bernoulli trial，我们用 beta prior/distribution 就能解决。那么加入我们有 n 个结果呢？比如抛的是骰子？
这时候上面的 Bernoulli trial 就要变成有一次 trial 有 k 个可能的结果；Bernoulli distribution 就变成 multinomial distribution。而 beta distribution 所表述的先验分布，也要改写成一个多结果版本的先验分布。那就是 dirichlet distribution。
均匀的先验分布 Beta(1,1)也要变成 k 个结果的 Dir(alpha/K)。dirichlet prior 也有共轭的性质，所以也是非常好计算的。
简而言之，就是由 2 种外推到 k 种，而看待它们的视角并没有什么不同。
他们有着非常非常非常相似的形式。</p><p><strong>结论 1：dirichlet distribution 就是由 2 种结果 bernoulli trial 导出的 beta distribution 外推到 k 种的 generalization</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.stats</span> <span class=kn>import</span> <span class=n>dirichlet</span><span class=p>,</span> <span class=n>poisson</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>numpy.random</span> <span class=kn>import</span> <span class=n>choice</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>collections</span> <span class=kn>import</span> <span class=n>defaultdict</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>num_documents</span> <span class=o>=</span> <span class=mi>5</span>
</span></span><span class=line><span class=cl><span class=n>num_topics</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl><span class=n>topic_dirichlet_parameter</span> <span class=o>=</span> <span class=mi>1</span> <span class=c1># beta</span>
</span></span><span class=line><span class=cl><span class=n>term_dirichlet_parameter</span> <span class=o>=</span> <span class=mi>1</span> <span class=c1># alpha</span>
</span></span><span class=line><span class=cl><span class=n>vocabulary</span> <span class=o>=</span> <span class=p>[</span><span class=s2>&#34;see&#34;</span><span class=p>,</span> <span class=s2>&#34;spot&#34;</span><span class=p>,</span> <span class=s2>&#34;run&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>num_terms</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>vocabulary</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>length_param</span> <span class=o>=</span> <span class=mi>10</span> <span class=c1># xi</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>term_distribution_by_topic</span> <span class=o>=</span> <span class=p>{}</span> <span class=c1># Phi</span>
</span></span><span class=line><span class=cl><span class=n>topic_distribution_by_document</span> <span class=o>=</span> <span class=p>{}</span> <span class=c1># Theta</span>
</span></span><span class=line><span class=cl><span class=n>document_length</span> <span class=o>=</span> <span class=p>{}</span>
</span></span><span class=line><span class=cl><span class=n>topic_index</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>list</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>word_index</span> <span class=o>=</span> <span class=n>defaultdict</span><span class=p>(</span><span class=nb>list</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>term_distribution</span> <span class=o>=</span> <span class=n>dirichlet</span><span class=p>(</span><span class=n>num_terms</span> <span class=o>-</span> <span class=p>[</span><span class=n>term_dirichlet_parameter</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>topic_distribution</span> <span class=o>=</span> <span class=n>dirichlet</span><span class=p>(</span><span class=n>num_topics</span> <span class=o>-</span> <span class=p>[</span><span class=n>topic_dirichlet_parameter</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 遍历每个主题</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>topic</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_topics</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 采样得出每个主题对应的词分布</span>
</span></span><span class=line><span class=cl>    <span class=n>term_distribution_by_topic</span><span class=p>[</span><span class=n>topic</span><span class=p>]</span> <span class=o>=</span> <span class=n>term_distribution</span><span class=o>.</span><span class=n>rvs</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 遍历所有的文档</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>document</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_documents</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># 采样出该文档对应的主题分布</span>
</span></span><span class=line><span class=cl>    <span class=n>topic_distribution_by_document</span><span class=p>[</span><span class=n>document</span><span class=p>]</span> <span class=o>=</span> <span class=n>topic_distribution</span><span class=o>.</span><span class=n>rvs</span><span class=p>()[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>topic_distribution_param</span> <span class=o>=</span> <span class=n>topic_distribution_by_document</span><span class=p>[</span><span class=n>document</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># 从泊松分布中采样出文档长度</span>
</span></span><span class=line><span class=cl>    <span class=n>document_length</span><span class=p>[</span><span class=n>document</span><span class=p>]</span> <span class=o>=</span> <span class=n>poisson</span><span class=p>(</span><span class=n>length_param</span><span class=p>)</span><span class=o>.</span><span class=n>rvs</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># 遍历整个文档中的所有词</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>word</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>document_length</span><span class=p>[</span><span class=n>document</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=n>topics</span> <span class=o>=</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_topics</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 采样出某个生成主题</span>
</span></span><span class=line><span class=cl>        <span class=n>topic</span> <span class=o>=</span> <span class=n>choice</span><span class=p>(</span><span class=n>topics</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=n>topic_distribution_param</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>topic_index</span><span class=p>[</span><span class=n>document</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>topic</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 采样出某个生成词</span>
</span></span><span class=line><span class=cl>        <span class=n>term_distribution_param</span> <span class=o>=</span> <span class=n>term_distribution_by_topic</span><span class=p>[</span><span class=n>topic</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>word_index</span><span class=p>[</span><span class=n>document</span><span class=p>]</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>choice</span><span class=p>(</span><span class=n>vocabulary</span><span class=p>,</span> <span class=n>p</span><span class=o>=</span><span class=n>term_distribution_param</span><span class=p>))</span>
</span></span></code></pre></div><p>如果还有困惑的同学可以参考如下 Python 代码：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>def</span> <span class=nf>perplexity</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>docs</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>docs</span> <span class=o>==</span> <span class=kc>None</span><span class=p>:</span> <span class=n>docs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>docs</span>
</span></span><span class=line><span class=cl>    <span class=c1># 单词在主题上的分布矩阵</span>
</span></span><span class=line><span class=cl>    <span class=n>phi</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>worddist</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>log_per</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>N</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>Kalpha</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>K</span> <span class=o>*</span> <span class=bp>self</span><span class=o>.</span><span class=n>alpha</span>
</span></span><span class=line><span class=cl>    <span class=o>//</span><span class=n>遍历语料集中的所有文档</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>m</span><span class=p>,</span> <span class=n>doc</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>docs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=o>//</span> <span class=n>n_m_z</span> <span class=n>为每个文档中每个主题的单词数</span><span class=err>，</span><span class=n>theta</span> <span class=n>即是每个单词出现的频次占比</span>
</span></span><span class=line><span class=cl>        <span class=n>theta</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_m_z</span><span class=p>[</span><span class=n>m</span><span class=p>]</span> <span class=o>/</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>docs</span><span class=p>[</span><span class=n>m</span><span class=p>])</span> <span class=o>+</span> <span class=n>Kalpha</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>w</span> <span class=ow>in</span> <span class=n>doc</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=o>//</span> <span class=n>numpy</span><span class=o>.</span><span class=n>inner</span><span class=p>(</span><span class=n>phi</span><span class=p>[:,</span><span class=n>w</span><span class=p>],</span> <span class=n>theta</span><span class=p>)</span> <span class=n>即是某个出现的概率统计值</span>
</span></span><span class=line><span class=cl>            <span class=n>log_per</span> <span class=o>-=</span> <span class=n>numpy</span><span class=o>.</span><span class=n>log</span><span class=p>(</span><span class=n>numpy</span><span class=o>.</span><span class=n>inner</span><span class=p>(</span><span class=n>phi</span><span class=p>[:,</span><span class=n>w</span><span class=p>],</span> <span class=n>theta</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>N</span> <span class=o>+=</span> <span class=nb>len</span><span class=p>(</span><span class=n>doc</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>numpy</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>log_per</span> <span class=o>/</span> <span class=n>N</span><span class=p>)</span>
</span></span></code></pre></div><h1 id=introduction>Introduction</h1><blockquote><p>LDA has been widely used in textual analysis,</p></blockquote><p>LDA 是标准的词袋模型。</p><ul><li><a href=http://blog.csdn.net/v_july_v/article/details/41209515 target=_blank rel=noopener>通俗理解 LDA 主题模型</a></li></ul><p>LDA 主要涉及的问题包括共轭先验分布、Dirichlet 分布以及 Gibbs 采样算法学习参数。LDA 的输入为文档数目$M$，词数目$V$(非重复的 term)，主题数目$K$。<figure><div class="d-flex justify-content-center"><div class=w-100><img src=http://7xlgth.com1.z0.glb.clouddn.com/5C724613-24AC-4782-B1DB-E890B87885FF.png alt loading=lazy data-zoomable></div></div></figure></p><h1 id=mathematics>Mathematics</h1><h2 id=beta-分布dirichlet-分布的基础>Beta 分布:Dirichlet 分布的基础</h2><p>Beta 分布的概率密度为：</p><p>$$
f(x) =
\left {
\begin{aligned}
\frac{1}{B(\alpha,\beta)}x^{\alpha-1}(1-x)^{\beta-1}, x \in (0,1) \
0,其他
\end{aligned}
\right.
$$</p><p>其中$$B(\alpha,\beta) = \int_0^1 x^{\alpha - 1}(1-x)^{\beta-1}dx=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}$$
其中 Gamma 函数可以看做阶乘的实数域的推广：</p><p>$$
\Gamma(x) = \int_0^{\infty}t^{x-1}e^{-t}dt \Rightarrow \Gamma(n) = (n-1)! \Rightarrow B(\alpha,\beta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}
$$</p><p>Beta 分布的期望为：
$$E(X) = \frac{\alpha + \beta}{\alpha}$$</p><h2 id=dirichlet-分布多项分布的共轭分布>Dirichlet 分布:多项分布的共轭分布</h2><p>Dirichlet 分布实际上就是把:</p><p>$$
\alpha = \alpha_1 ,
\beta = \alpha_2 ,
x = x_1 ,
x - 1 = x_2
$$</p><p>$$
f(\vec{p} | \vec{\alpha}) = \left {
\begin{aligned}
\frac{1}{\Delta(\vec{\alpha})} \prod_{k=1}^{K} p_k^{\alpha_k - 1} ,p_k \in (0,1) \
0,其他
\end{aligned}
\right.
$$</p><p>可以简记为：</p><p>$$
Dir(\vec{p} | \vec{\alpha}) = \frac{1}{\Delta(\vec{\alpha})} \prod_{k=1}^{K} p_k^{\alpha_k - 1}
$$</p><p>其中</p><p>$$
\Delta(\vec{\alpha}) = \frac{ \prod_{k=1}^K \Gamma(\alpha_k)}{ \Gamma(\sum_{k=1}^{K}\alpha_k)}
$$</p><p>该部分在给定的$\vec{\alpha}$情况下是可以计算出来值的。假设给定的一篇文档有 50 个主题，那么$\vec{\alpha}$就是维度为 50 的向量。在没有任何先验知识的情况下，最方便的也是最稳妥的初始化就是将这个 50 个值设置为同一个值。</p><h3 id=symmetric-dirichlet-distribution对称-dirichlet-分布>Symmetric Dirichlet Distribution(对称 Dirichlet 分布)</h3><p>一旦采取了对称的 Dirichlet 分布，因为参数向量中的所有值都一样，公式可以改变为：</p><p>$$
Dir(\vec{p} | \alpha,K) = \frac{1}{\Delta_K(\alpha)} \prod_{k=1}^{K} p_k^{\alpha - 1} \
\Delta_K(\vec{\alpha}) = \Gamma^K(\alpha){ \Gamma(K * \alpha)}
$$</p><p>而不同的$\alpha$取值，当$\alpha=1$时候，退化为均匀分布。当$\alpha>1$时候，$p_1 = p_2 = \dots = p_k$的概率增大。当$\alpha&lt;1$时候，$p_1 = 1, p_{非i} = 0$的概率增大。映射到具体的文档分类中，$\alpha$取值越小，说明各个主题之间的离差越大。而$\alpha$值越大，说明该文档中各个主题出现的概率约接近。</p><p>在实际的应用中，一般会选用$1/K$作为$\alpha$的初始值。</p><h1 id=模型解释>模型解释</h1><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=http://7xlgth.com1.z0.glb.clouddn.com/D73D69FE-BA28-4E66-871F-B594B4BEFC29.png alt loading=lazy data-zoomable></div></div></figure>上图的箭头指向即是条件依赖。</p><h2 id=terminology>Terminology</h2><ul><li><p>字典中共有$V$个不可重复的 term，如果这些 term 出现在了具体的文章中，就是 word。在具体的某文章中的 word 当然是可能重复的。</p></li><li><p>语料库(Corpus)中共有$m$篇文档，分别是$d_1,d_2,\dots,d_m$，每篇文章长度为$N_m$，即由$N_i$个 word 组成。每篇文章都有各自的主题分布，主题分布服从多项式分布，该多项式分布的参数服从 Dirichlet 分布，该 Dirichlet 分布的参数为$\vec{ \alpha }$。注意，多项分布的共轭先验分布为 Dirichlet 分布。</p><blockquote><p>怎么来看待所谓的文章主题服从多项分布呢。你每一个文章等于多一次实验，$m$篇文档就等于做了$m$次实验。而每次实验中有$K$个结果，每个结果以一定概率出现。</p></blockquote></li><li><p>一共涉及到$K$(值给定)个主题，$T_1,T_2,\dots,T_k$。每个主题都有各自的词分布，词分布为多项式分布，该多项式分布的参数服从 Dirichlet 分布，该 Diriclet 分布的参数为$\vec{\beta}$。注意，一个词可能从属于多个主题。</p></li></ul><h2 id=模型过程>模型过程</h2><p>$\vec{\alpha}$与$\vec{\beta}$为先验分布的参数，一般会实现给定。如取 0.1 的对称 Dirichlet 分布，表示在参数学习结束后，期望每个文档的主题不会十分集中。</p><p>(1)选定文档主题</p><p>(2)根据主题选定词</p><h2 id=参数学习>参数学习</h2><p>给定一个文档集合，$w_{mn}$是可以观察到的已知变量，$\vec{\alpha}$与$\vec{\beta}$是根据经验给定的先验参数，其他的变量$z_{mn}$、$\vec{\theta}$、$\vec{\varphi}$都是未知的隐含变量，需要根据观察到的变量来学习估计的。根据上图，可以写出所有变量的联合分布：</p><h3 id=似然概率>似然概率</h3><p>一个词$w_{mn}$(即 word，可重复的词)初始化为一个词$t$(term/token，不重复的词汇)的概率是：</p><p>$$
p(w_{m,n}=t | \vec{\theta_m},\Phi) = \sum_{k=1}^K p(w_{m,n}=t | \vec{\phi_k})p(z_{m,n}=k|\vec{\theta}_m)
$$</p><p>上式即给定某个主题的情况下能够看到某个词的概率的总和。每个文档中出现主题$k$的概率乘以主题$k$下出现词$t$的概率，然后枚举所有主题求和得到。整个文档集合的似然函数为：</p><p>$$
p(W | \Theta,\Phi) = \prod_{m=1}^{M}p(\vec{w_m} | \vec{\theta_m},\Phi) = \prod_{m=1}^M \prod_{n=1}^{N_m}p(w_{m,n}|\vec{\theta_m},\Phi)
$$</p><h1 id=gibbs-sampling>Gibbs Sampling</h1><blockquote><p>首先通俗理解一下，在某篇文档中存在着$N_m$个词，依次根据其他的词推算某个词来自于某个主题的概率，从而达到收敛。最开始的时候，某个词属于某个主题是随机分配的。Gibbs Sampling 的核心在于找出某个词到底属于哪个主题。</p></blockquote><p>Gibbs Sampling 算法的运行方式是每次选取概率向量的一个维度，给定其他维度的变量值采样当前度的值，不断迭代直到收敛输出待估计的参数。初始时随机给文本中的每个词分配主题$z^{(0)}$，然后统计每个主题$z$下出现词$t$的数量以及每个文档$m$下出现主题$z$的数量，每一轮计算$p(z_i|z_{\neq i},d,w)$，即排除当前词的主题分布。
这里的联合分布：</p><p>$$
p(\vec{w},\vec{z} | \vec{\alpha},\vec{\beta}) = p(\vec{w} | \vec{z},\vec{\beta})p(\vec{z} | \vec{\alpha})
$$</p><p>第一项因子是给定主题采样词的过程。后面的因此计算，$n_z^{(t)}$表示词$t$被观察到分配给主题$z$的次数，$n_m^{(k)}$表示主题$k$分配给文档$m$的次数。</p><p>$$
p(\vec{w} | ,\vec{z},\vec{\beta})
= \int p(\vec{w} | \vec{z},\vec{\Phi})p(\Phi | \vec{\beta})d \Phi \
= \int \prod_{z=1}^{K} \frac{1}{\Delta(\vec{\beta})}\prod_{t=1}^V \phi_{z,t}^{n_z^{(t)} + \beta_t - 1}d\vec{\phi_z} \
= \prod_{z=1}^{K}\frac{\Delta(\vec{n_z} + \vec{\beta})}{\Delta(\vec{ \beta })} ,
\vec{n_z} = { n_z^{(t)} }_{t=1}^V
$$</p><p>$$
p(\vec{z} | \vec{\alpha}) \
= \int p(\vec{z} | \Theta) p(\Theta|\vec{\alpha}) d\Theta \
= \int \prod_{m=1}^{M} \frac{1}{\Delta(\vec\alpha)} \prod_{k=1}^K\theta_{m,k}^{ n_m^{(k)} + \alpha_k - 1 }d\vec{\theta_m} \
= \prod_{m=1}^M \frac{ \Delta(\vec{n_m} + \vec\alpha) }{ \Delta(\vec\alpha) }, \vec{n_m}={ n_m^{(k)} }_{k=1}^K
$$</p><h2 id=gibbs-updating-rule>Gibbs Updating Rule</h2><h2 id=词分布和主题分布总结>词分布和主题分布总结</h2><p>经过上面的 Gibbs 采样，各个词所被分配到的主题已经完成了收敛，在这里就可以计算出文档属于主题的概率以及词属于文档的概率了。</p><p>$$
\phi_{k,t} = \frac{ n_k^{(t)} + \beta_t }{ \sum^V_{t=1}n_k^{(t)} + \beta_t } \
\theta_{m,k} = \frac{ n_m^{(k)} + \alpha_k }{ \sum^K_{k=1}n_m^{(k)} + \alpha_k } \
$$</p><p>$$
p(\vec{\theta_m} | \vec{z_m}, \vec{\alpha} )
= \frac{1}{Z_{\theta_m}} \prod_{n=1}^{N_m} p(z_{m,n} | \vec{\theta_m} * p(\vec{\theta_m} | \vec{alpha} ))
= Dir(\vec{\theta_m} | \vec{n_m} + \vec{\alpha})
\
p(\vec{\phi_k} | \vec{z}, \vec{w}, \vec{\beta} ) =
\frac{1}{Z_{\phi_k}} \prod_{i:z_i=k} p(w_i | \vec{\phi_k}) * p(\vec{\phi_k} | \vec{\beta})
= Dir(\vec{\phi_k} | \vec{n_k} + \vec{\beta})
$$</p><h1 id=代码实现>代码实现</h1><p>代码的输入有文档数目$M$、词的数目$V$(非重复的 term)、主题数目$K$，以及用$d$表示第几个文档，$k$表示主题，$w$表示词汇(term)，$n$表示词(word)。
$z[d][w]$:第$d$篇文档的第$w$个词来自哪个主题。$M$行，$X$列，$X$为对应的文档长度：即词(可重复)的数目。
$nw[w][t]$:第 w 个词是第 t 个主题的次数。word-topic 矩阵，列向量$nw[][t]$表示主题 t 的词频数分布；V 行 K 列。
$nd[d][t]$:第 d 篇文档中第 t 个主题出现的次数，doc-topic 矩阵，行向量$nd[d]$表示文档$d$的主题频数分布。M 行，K 列。
辅助向量：
$ntSum[t]$:第 t 个主题在所有语料出现的次数，K 维
$ndSum[d]$:第 d 篇文档中词的数目(可重复)，M 维
$P[t]$:对于当前计算的某词属于主题 t 的概率，K 维</p><h1 id=超参数的确定>超参数的确定</h1><ul><li>交叉验证</li><li>$\alpha$表达了不同文档间主题是否鲜明，$\beta$度量了有多少近义词能够属于同一个类别。</li><li>给定主题数目$K$，可以使用：</li></ul><p>$$
\alpha = 50 / K \
\beta = 0.01
$$</p></div><div class=article-widget><div class="container-xl row post-nav"></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div><script type=text/javascript id=clstr_globe async src="//clustrmaps.com/globe.js?d=kgpJG5sWZQpKujBmD-uW1B54-WBPol-DuDtrB2KFjKs"></script></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>