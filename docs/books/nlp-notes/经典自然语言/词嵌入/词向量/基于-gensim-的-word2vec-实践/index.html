<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="Word2Vec 基于 Gensim 的 Word2Vec 实践，从属于笔者的程序猿的数据科学与机器学习实战手册，代码参考gensim.ipynb。推荐前置阅读Python 语法速览与机器学习开发环境搭建，Scikit-Learn 备忘录。 Word2Vec Tutorial Getting Started with Word2Vec"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.fab3cd1900ae35687457073b2d518207.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/"><meta property="og:title" content="基于 Gensim 的 Word2Vec 实践 | Next-gen Tech Edu"><meta property="og:description" content="Word2Vec 基于 Gensim 的 Word2Vec 实践，从属于笔者的程序猿的数据科学与机器学习实战手册，代码参考gensim.ipynb。推荐前置阅读Python 语法速览与机器学习开发环境搭建，Scikit-Learn 备忘录。 Word2Vec Tutorial Getting Started with Word2Vec"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>基于 Gensim 的 Word2Vec 实践 | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=286a6b18053ee3d7758a2a08e1fa4de2><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">词向量</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id10e5d5fc96ec9fa77ffb2be69018cedd")' href=#id10e5d5fc96ec9fa77ffb2be69018cedd aria-expanded=false aria-controls=id10e5d5fc96ec9fa77ffb2be69018cedd aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/>词嵌入</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id10e5d5fc96ec9fa77ffb2be69018cedd aria-expanded=false aria-controls=id10e5d5fc96ec9fa77ffb2be69018cedd><i class="fa-solid fa-angle-down" id=caret-id10e5d5fc96ec9fa77ffb2be69018cedd></i></a></div><ul class="nav docs-sidenav collapse show" id=id10e5d5fc96ec9fa77ffb2be69018cedd><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id4d70266e5bd6c0d62687b5e324c924c9")' href=#id4d70266e5bd6c0d62687b5e324c924c9 aria-expanded=false aria-controls=id4d70266e5bd6c0d62687b5e324c924c9 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/>词向量</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id4d70266e5bd6c0d62687b5e324c924c9 aria-expanded=false aria-controls=id4d70266e5bd6c0d62687b5e324c924c9><i class="fa-solid fa-angle-down" id=caret-id4d70266e5bd6c0d62687b5e324c924c9></i></a></div><ul class="nav docs-sidenav collapse show" id=id4d70266e5bd6c0d62687b5e324c924c9><li class="child level active"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/>基于 Gensim 的 Word2Vec 实践</a></li></ul></div><li class="child level"><a href=/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E6%A6%82%E8%BF%B0/>概述</a></li></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#模型创建>模型创建</a><ul><li><a href=#word2vec-参数>Word2Vec 参数</a></li></ul></li><li><a href=#外部语料集>外部语料集</a></li><li><a href=#模型保存与读取>模型保存与读取</a></li><li><a href=#模型预测>模型预测</a><ul><li><a href=#模型评估>模型评估</a></li></ul></li></ul><ul><li><a href=#简单语料集>简单语料集</a></li><li><a href=#外部语料集-1>外部语料集</a></li><li><a href=#中文语料集>中文语料集</a></li></ul><ul><li><a href=#可视化预览>可视化预览</a></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>基于 Gensim 的 Word2Vec 实践</h1><div class=article-style><h1 id=word2vec>Word2Vec</h1><ul><li><a href=https://zhuanlan.zhihu.com/p/24961011 target=_blank rel=noopener>基于 Gensim 的 Word2Vec 实践</a>，从属于笔者的<a href=https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders target=_blank rel=noopener>程序猿的数据科学与机器学习实战手册</a>，代码参考<a href=https://github.com/wx-chevalier/DataScience-And-MachineLearning-Handbook-For-Coders/blob/master/code/python/nlp/genism/gensim.ipynb target=_blank rel=noopener>gensim.ipynb</a>。推荐前置阅读<a href=https://zhuanlan.zhihu.com/p/24536868 target=_blank rel=noopener>Python 语法速览与机器学习开发环境搭建</a>，<a href=https://zhuanlan.zhihu.com/p/24770526 target=_blank rel=noopener>Scikit-Learn 备忘录</a>。</li></ul><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://i.ytimg.com/vi/xMwx2A_o5r4/maxresdefault.jpg alt loading=lazy data-zoomable></div></div></figure></p><blockquote><ul><li><a href=https://rare-technologies.com/word2vec-tutorial/ target=_blank rel=noopener>Word2Vec Tutorial</a></li><li><a href=http://textminingonline.com/getting-started-with-word2vec-and-glove-in-python target=_blank rel=noopener>Getting Started with Word2Vec and GloVe in Python</a></li></ul></blockquote><h2 id=模型创建>模型创建</h2><p><a href=http://radimrehurek.com/gensim/models/word2vec.html target=_blank rel=noopener>Gensim</a>中 Word2Vec 模型的期望输入是进过分词的句子列表，即是某个二维数组。这里我们暂时使用 Python 内置的数组，不过其在输入数据集较大的情况下会占用大量的 RAM。Gensim 本身只是要求能够迭代的有序句子列表，因此在工程实践中我们可以使用自定义的生成器，只在内存中保存单条语句。</p><pre tabindex=0><code># 引入 word2vec
from gensim.models import word2vec

# 引入日志配置
import logging

logging.basicConfig(format=&#39;%(asctime)s : %(levelname)s : %(message)s&#39;, level=logging.INFO)

# 引入数据集
raw_sentences = [&#34;the quick brown fox jumps over the lazy dogs&#34;,&#34;yoyoyo you go home now to sleep&#34;]

# 切分词汇
sentences= [s.encode(&#39;utf-8&#39;).split() for s in sentences]

# 构建模型
model = word2vec.Word2Vec(sentences, min_count=1)

# 进行相关性比较
model.similarity(&#39;dogs&#39;,&#39;you&#39;)
</code></pre><p>这里我们调用<code>Word2Vec</code>创建模型实际上会对数据执行两次迭代操作，第一轮操作会统计词频来构建内部的词典数结构，第二轮操作会进行神经网络训练，而这两个步骤是可以分步进行的，这样对于某些不可重复的流(譬如 Kafka 等流式数据中)可以手动控制：</p><pre tabindex=0><code>model = gensim.models.Word2Vec(iter=1)  # an empty model, no training yet
model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator
model.train(other_sentences)  # can be a non-repeatable, 1-pass generator
</code></pre><h3 id=word2vec-参数>Word2Vec 参数</h3><ul><li>min_count</li></ul><pre tabindex=0><code>model = Word2Vec(sentences, min_count=10)  # default value is 5
</code></pre><p>在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置<code>min_count</code>参数进行控制。一般而言，合理的参数值会设置在 0~100 之间。</p><ul><li>size</li></ul><p><code>size</code>参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为 100 层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。</p><pre tabindex=0><code>model = Word2Vec(sentences, size=200)  # default value is 100
</code></pre><ul><li>workers</li></ul><p><code>workers</code>参数用于设置并发训练时候的线程数，不过仅当<code>Cython</code>安装的情况下才会起作用：</p><pre tabindex=0><code>model = Word2Vec(sentences, workers=4) # default = 1 worker = no parallelization
</code></pre><h2 id=外部语料集>外部语料集</h2><p>在真实的训练场景中我们往往会使用较大的语料集进行训练，譬如这里以 Word2Vec 官方的<a href=http://mattmahoney.net/dc/text8.zip target=_blank rel=noopener>text8</a>为例，只要改变模型中的语料集开源即可：</p><pre tabindex=0><code>sentences = word2vec.Text8Corpus(&#39;text8&#39;)
model = word2vec.Word2Vec(sentences, size=200)
</code></pre><p>这里语料集中的语句是经过分词的，因此可以直接使用。笔者在第一次使用该类时报错了，因此把 Gensim 中的源代码贴一下，也方便以后自定义处理其他语料集：</p><pre tabindex=0><code>class Text8Corpus(object):
    &#34;&#34;&#34;Iterate over sentences from the &#34;text8&#34; corpus, unzipped from http://mattmahoney.net/dc/text8.zip .&#34;&#34;&#34;
    def __init__(self, fname, max_sentence_length=MAX_WORDS_IN_BATCH):
        self.fname = fname
        self.max_sentence_length = max_sentence_length

    def __iter__(self):
        # the entire corpus is one gigantic line -- there are no sentence marks at all
        # so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens
        sentence, rest = [], b&#39;&#39;
        with utils.smart_open(self.fname) as fin:
            while True:
                text = rest + fin.read(8192)  # avoid loading the entire file (=1 line) into RAM
                if text == rest:  # EOF
                    words = utils.to_unicode(text).split()
                    sentence.extend(words)  # return the last chunk of words, too (may be shorter/longer)
                    if sentence:
                        yield sentence
                    break
                last_token = text.rfind(b&#39; &#39;)  # last token may have been split in two... keep for next iteration
                words, rest = (utils.to_unicode(text[:last_token]).split(),
                               text[last_token:].strip()) if last_token &gt;= 0 else ([], text)
                sentence.extend(words)
                while len(sentence) &gt;= self.max_sentence_length:
                    yield sentence[:self.max_sentence_length]
                    sentence = sentence[self.max_sentence_length:]
</code></pre><p>我们在上文中也提及，如果是对于大量的输入语料集或者需要整合磁盘上多个文件夹下的数据，我们可以以迭代器的方式而不是一次性将全部内容读取到内存中来节省 RAM 空间：</p><pre tabindex=0><code>class MySentences(object):
    def __init__(self, dirname):
        self.dirname = dirname

    def __iter__(self):
        for fname in os.listdir(self.dirname):
            for line in open(os.path.join(self.dirname, fname)):
                yield line.split()

sentences = MySentences(&#39;/some/directory&#39;) # a memory-friendly iterator
model = gensim.models.Word2Vec(sentences)
</code></pre><h2 id=模型保存与读取>模型保存与读取</h2><pre tabindex=0><code>model.save(&#39;text8.model&#39;)
2015-02-24 11:19:26,059 : INFO : saving Word2Vec object under text8.model, separately None
2015-02-24 11:19:26,060 : INFO : not storing attribute syn0norm
2015-02-24 11:19:26,060 : INFO : storing numpy array &#39;syn0&#39; to text8.model.syn0.npy
2015-02-24 11:19:26,742 : INFO : storing numpy array &#39;syn1&#39; to text8.model.syn1.npy

model1 = Word2Vec.load(&#39;text8.model&#39;)

model.save_word2vec_format(&#39;text.model.bin&#39;, binary=True)
2015-02-24 11:19:52,341 : INFO : storing 71290x200 projection weights into text.model.bin

model1 = word2vec.Word2Vec.load_word2vec_format(&#39;text.model.bin&#39;, binary=True)
2015-02-24 11:22:08,185 : INFO : loading projection weights from text.model.bin
2015-02-24 11:22:10,322 : INFO : loaded (71290, 200) matrix from text.model.bin
2015-02-24 11:22:10,322 : INFO : precomputing L2-norms of word weight vectors
</code></pre><h2 id=模型预测>模型预测</h2><p>Word2Vec 最著名的效果即是以语义化的方式推断出相似词汇：</p><pre tabindex=0><code>model.most_similar(positive=[&#39;woman&#39;, &#39;king&#39;], negative=[&#39;man&#39;], topn=1)
[(&#39;queen&#39;, 0.50882536)]
model.doesnt_match(&#34;breakfast cereal dinner lunch&#34;;.split())
&#39;cereal&#39;
model.similarity(&#39;woman&#39;, &#39;man&#39;)
0.73723527
model.most_similar([&#39;man&#39;])
[(u&#39;woman&#39;, 0.5686948895454407),
 (u&#39;girl&#39;, 0.4957364797592163),
 (u&#39;young&#39;, 0.4457539916038513),
 (u&#39;luckiest&#39;, 0.4420626759529114),
 (u&#39;serpent&#39;, 0.42716869711875916),
 (u&#39;girls&#39;, 0.42680859565734863),
 (u&#39;smokes&#39;, 0.4265017509460449),
 (u&#39;creature&#39;, 0.4227582812309265),
 (u&#39;robot&#39;, 0.417464017868042),
 (u&#39;mortal&#39;, 0.41728296875953674)]
</code></pre><p>如果我们希望直接获取某个单词的向量表示，直接以下标方式访问即可：</p><pre tabindex=0><code>model[&#39;computer&#39;]  # raw NumPy vector of a word
array([-0.00449447, -0.00310097,  0.02421786, ...], dtype=float32)
</code></pre><h3 id=模型评估>模型评估</h3><p>Word2Vec 的训练属于无监督模型，并没有太多的类似于监督学习里面的客观评判方式，更多的依赖于端应用。Google 之前公开了 20000 条左右的语法与语义化训练样本，每一条遵循<code>A is to B as C is to D</code>这个格式，地址在<a href=https://word2vec.googlecode.com/svn/trunk/questions-words.txt target=_blank rel=noopener>这里</a>:</p><pre tabindex=0><code>model.accuracy(&#39;/tmp/questions-words.txt&#39;)
2014-02-01 22:14:28,387 : INFO : family: 88.9% (304/342)
2014-02-01 22:29:24,006 : INFO : gram1-adjective-to-adverb: 32.4% (263/812)
2014-02-01 22:36:26,528 : INFO : gram2-opposite: 50.3% (191/380)
2014-02-01 23:00:52,406 : INFO : gram3-comparative: 91.7% (1222/1332)
2014-02-01 23:13:48,243 : INFO : gram4-superlative: 87.9% (617/702)
2014-02-01 23:29:52,268 : INFO : gram5-present-participle: 79.4% (691/870)
2014-02-01 23:57:04,965 : INFO : gram7-past-tense: 67.1% (995/1482)
2014-02-02 00:15:18,525 : INFO : gram8-plural: 89.6% (889/992)
2014-02-02 00:28:18,140 : INFO : gram9-plural-verbs: 68.7% (482/702)
2014-02-02 00:28:18,140 : INFO : total: 74.3% (5654/7614)
</code></pre><p>还是需要强调下，训练集上表现的好也不意味着 Word2Vec 在真实应用中就会表现的很好，还是需要因地制宜。</p><h1 id=模型训练>模型训练</h1><h2 id=简单语料集>简单语料集</h2><h2 id=外部语料集-1>外部语料集</h2><h2 id=中文语料集>中文语料集</h2><p>约三十余万篇文章</p><h1 id=模型应用>模型应用</h1><h2 id=可视化预览>可视化预览</h2><pre tabindex=0><code>from sklearn.decomposition import PCA
import matplotlib.pyplot as plt


def wv_visualizer(model, word = [&#34;man&#34;]):

    # 寻找出最相似的十个词
    words = [wp[0] for wp in model.most_similar(word,20)]

    # 提取出词对应的词向量
    wordsInVector = [model[word] for word in words]

    # 进行 PCA 降维
    pca = PCA(n_components=2)
    pca.fit(wordsInVector)
    X = pca.transform(wordsInVector)

    # 绘制图形
    xs = X[:, 0]
    ys = X[:, 1]

    # draw
    plt.figure(figsize=(12,8))
    plt.scatter(xs, ys, marker = &#39;o&#39;)
    for i, w in enumerate(words):
        plt.annotate(
            w,
            xy = (xs[i], ys[i]), xytext = (6, 6),
            textcoords = &#39;offset points&#39;, ha = &#39;left&#39;, va = &#39;top&#39;,
            **dict(fontsize=10)
        )

    plt.show()

# 调用时传入目标词组即可
wv_visualizer(model,[&#34;China&#34;,&#34;Airline&#34;])
</code></pre></div><div class=article-widget><div class="container-xl row post-nav"></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div><script type=text/javascript id=clstr_globe async src="//clustrmaps.com/globe.js?d=kgpJG5sWZQpKujBmD-uW1B54-WBPol-DuDtrB2KFjKs"></script></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>