<!DOCTYPE html><html lang="zh" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
    <meta name="google-site-verification" content="google69a5cccb61297807" />
    <meta name="baidu-site-verification" content="cqmZHEleVh" />
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="Numbers every LLM Developer should know When I was at Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage.
Notes on the Github version Last updates: 2023-05-17
If you feel there&rsquo;s an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc?" />

  
  <link rel="alternate" hreflang="zh" href="https://ng-tech.icu/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-numbers-every-llm-developer-should-know/" />

  
  
  
    <meta name="theme-color" content="#0a55a7" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css" integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin="anonymous">
    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.0d97305106da5efa530e28b021b4c580.css" />

  




<script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>

<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', "G-40NYXJ8823");
</script>


  


  


  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://ng-tech.icu/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-numbers-every-llm-developer-should-know/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@wx-chevalier" />
    <meta property="twitter:creator" content="@wx-chevalier" />
  
  <meta property="og:site_name" content="Next-gen Tech Edu" />
  <meta property="og:url" content="https://ng-tech.icu/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-numbers-every-llm-developer-should-know/" />
  <meta property="og:title" content="2023-Numbers every LLM Developer should know | Next-gen Tech Edu" />
  <meta property="og:description" content="Numbers every LLM Developer should know When I was at Google, there was a document put together by Jeff Dean, the legendary engineer, called Numbers every Engineer should know. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage.
Notes on the Github version Last updates: 2023-05-17
If you feel there&rsquo;s an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc?" /><meta property="og:image" content="https://ng-tech.icu/media/sharing.png" />
    <meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png" /><meta property="og:locale" content="zh" />
  
    
    
  

  



  

  

  





  <title>2023-Numbers every LLM Developer should know | Next-gen Tech Edu</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="ad4ac636a7fdc759fa0ba9897a242dd1" >
  <button onclick="topFunction()" id="backTopBtn" title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden="true"></i></button>
  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6 search-title">
          <p>搜索</p>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="关闭"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

      
      

      
    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

      <div id="search-common-queries">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Next-gen Tech Edu</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="切换导航">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Next-gen Tech Edu</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/books-gallery"><span>笔记（万篇）</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#knowledge-map"><span>知识图谱</span></a>
          </li>

          
          

          
          <style>
            .dropdown-item{
              display: inline-flex;
            }
          </style>
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>实验室</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="/galaxy-home/gh-craft"><span>Craft 方块世界</span></a>
              
                <a class="dropdown-item" href="/galaxy-home/glossary-cards"><span>3D 知识卡牌</span></a>
              
            </div>
          </li>

          
          

          
          <style>
            .dropdown-item{
              display: inline-flex;
            }
          </style>
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>其他阅读渠道</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="https://zhuanlan.zhihu.com/wxyyxc1992"><img style="width:16px;height:16px;display:inline-block;margin-right:8px" src="https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png"></img><span>知乎</span></a>
              
                <a class="dropdown-item" href="https://segmentfault.com/blog/wxyyxc1992"><img style="width:16px;height:16px;display:inline-block;margin-right:8px" src="https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png"></img><span>SegmentFault</span></a>
              
                <a class="dropdown-item" href="https://zhuanlan.zhihu.com/wxyyxc1992"><img style="width:16px;height:16px;display:inline-block;margin-right:8px" src="https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png"></img><span>掘金</span></a>
              
            </div>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="搜索"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        <li class="nav-item">
            <a class="nav-link" href="https://github.com/wx-chevalier" aria-label="GitHub"><i class="fa-brands fa-github" aria-hidden="true"></i></a>
        </li>
        

        
        
        

        
        
        
        <div></div>
        
        <style>
        @media only screen and (max-width: 600px) {
          .jimmysong-template {
            display: none!important;
          }
        }
        </style>
        
        <li class="jimmysong-template" style="color: white;font-size: 12px;">
          <a href="https://jimmysong.io" style="color: white">By Jimmy Song's Template</a>
        </li>
      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    




<link rel="stylesheet" href="//unpkg.com/heti/umd/heti.min.css">
<div class="container-xl docs">
  <div class="row flex-xl-nowrap">
    <div class="docs-sidebar">
      <form class="docs-search d-flex align-items-center">
  <button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type="button" data-toggle="collapse" data-target="#docs-nav" aria-controls="docs-nav" aria-expanded="false" aria-label="Toggle section navigation">
    <div class="d-flex">
      <span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">
        
          99.参考资料
        
      </span>
      <span><i class="fas fa-chevron-down"></i></span>
    </div>
  </button>

  
  <button class="form-control sidebar-search js-search d-none d-md-flex">
    <i class="fas fa-search pr-2"></i>
    <span class="sidebar-search-text">搜索...</span>
    <span class="sidebar-search-shortcut">/</span>
  </button>
  
</form>

<nav class="collapse docs-links" id="docs-nav">
  
  
  
  
  
  

  
  
    

    
      

      <ul class="nav docs-sidenav">
        <li style="display: inline-flex">
          <a style="cursor: pointer;" onclick="window.history.back()">
            <i class="fas fa-arrow-left pr-1"></i>
            Back
          </a>
          <span>|</span>
          <a href="/books/">
            <i class="fa-solid fa-house" style="margin-right: 4px"></i>
            Books
          </a>
        </li>
      </ul>

      
      
        
          
        
      



  
    
    
    
    
      
    
    

    
    
    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-ideb51962189c6be538ff99d277f5ab939&#34;)" href="#ideb51962189c6be538ff99d277f5ab939" aria-expanded="false" aria-controls="ideb51962189c6be538ff99d277f5ab939" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/">NLP-Notes</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#ideb51962189c6be538ff99d277f5ab939" aria-expanded="false" aria-controls="ideb51962189c6be538ff99d277f5ab939">
    
    <i class="fa-solid fa-angle-down" id="caret-ideb51962189c6be538ff99d277f5ab939"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  show " id="ideb51962189c6be538ff99d277f5ab939">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id36dede421b4d80d75ccab27fc8e79946&#34;)" href="#id36dede421b4d80d75ccab27fc8e79946" aria-expanded="false" aria-controls="id36dede421b4d80d75ccab27fc8e79946" aria-hidden="false" data-toggle="collapse">
    
    </div>
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id39ca9f459fbc8f7d10896a0da3596cf3&#34;)" href="#id39ca9f459fbc8f7d10896a0da3596cf3" aria-expanded="false" aria-controls="id39ca9f459fbc8f7d10896a0da3596cf3" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/">99.参考资料</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id39ca9f459fbc8f7d10896a0da3596cf3" aria-expanded="false" aria-controls="id39ca9f459fbc8f7d10896a0da3596cf3">
    
    <i class="fa-solid fa-angle-down" id="caret-id39ca9f459fbc8f7d10896a0da3596cf3"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  show " id="id39ca9f459fbc8f7d10896a0da3596cf3">
      



  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-building-systems-with-the-chatgpt-api/1.introduction/">1.Introduction</a></li>




  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-langchain-for-llm-application-development/1.%E5%BC%80%E7%AF%87%E4%BB%8B%E7%BB%8D/">1.开篇介绍</a></li>




  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-building-systems-with-the-chatgpt-api/11.conclusion/">11.conclusion</a></li>




  <li class="child level active"><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-numbers-every-llm-developer-should-know/">2023-Numbers every LLM Developer should know</a></li>




  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E9%99%86%E5%A5%87-%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82/">2023-陆奇-我的大模型世界观</a></li>




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id6c0b62831af535413eac6f9134388f52&#34;)" href="#id6c0b62831af535413eac6f9134388f52" aria-expanded="false" aria-controls="id6c0b62831af535413eac6f9134388f52" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-chatgpt-prompt-engineering-for-developers/">2023-吴恩达-《ChatGPT Prompt Engineering for Developers》</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id6c0b62831af535413eac6f9134388f52" aria-expanded="false" aria-controls="id6c0b62831af535413eac6f9134388f52">
    
        <i class="fa-solid fa-angle-right" id="caret-id6c0b62831af535413eac6f9134388f52"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id6c0b62831af535413eac6f9134388f52">
      



  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-chatgpt-prompt-engineering-for-developers/00._index/">00.README</a></li>




  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-chatgpt-prompt-engineering-for-developers/01.-%E7%AE%80%E4%BB%8B/">01. 简介</a></li>




  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-chatgpt-prompt-engineering-for-developers/09.-%E6%80%BB%E7%BB%93/">09. 总结</a></li>

      
        </ul>
      
    

    
      </div>
    




  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-langchain-for-llm-application-development/8.%E8%AF%BE%E7%A8%8B%E6%80%BB%E7%BB%93/">8.课程总结</a></li>




  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-building-systems-with-the-chatgpt-api/readme/">readme</a></li>




  <li class="child level "><a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-langchain-for-llm-application-development/readme/">readme</a></li>

      
        </ul>
      
    

    
      </div>
    




  <li class="child level "><a href="/books/nlp-notes/introduction/">INTRODUCTION</a></li>




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id0f3a2f251393fa53f2b45f59f11ba1dd&#34;)" href="#id0f3a2f251393fa53f2b45f59f11ba1dd" aria-expanded="false" aria-controls="id0f3a2f251393fa53f2b45f59f11ba1dd" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/">LLM</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id0f3a2f251393fa53f2b45f59f11ba1dd" aria-expanded="false" aria-controls="id0f3a2f251393fa53f2b45f59f11ba1dd">
    
        <i class="fa-solid fa-angle-right" id="caret-id0f3a2f251393fa53f2b45f59f11ba1dd"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id0f3a2f251393fa53f2b45f59f11ba1dd">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-ide1ffcfcd52c4caed741dce5e8746afd5&#34;)" href="#ide1ffcfcd52c4caed741dce5e8746afd5" aria-expanded="false" aria-controls="ide1ffcfcd52c4caed741dce5e8746afd5" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/">99.参考资料</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#ide1ffcfcd52c4caed741dce5e8746afd5" aria-expanded="false" aria-controls="ide1ffcfcd52c4caed741dce5e8746afd5">
    
        <i class="fa-solid fa-angle-right" id="caret-ide1ffcfcd52c4caed741dce5e8746afd5"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="ide1ffcfcd52c4caed741dce5e8746afd5">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-ida85c57d2b21e050d986bbfab7e370c48&#34;)" href="#ida85c57d2b21e050d986bbfab7e370c48" aria-expanded="false" aria-controls="ida85c57d2b21e050d986bbfab7e370c48" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/cohere-llm-university/">cohere-LLM University</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#ida85c57d2b21e050d986bbfab7e370c48" aria-expanded="false" aria-controls="ida85c57d2b21e050d986bbfab7e370c48">
    
        <i class="fa-solid fa-angle-right" id="caret-ida85c57d2b21e050d986bbfab7e370c48"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="ida85c57d2b21e050d986bbfab7e370c48">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-idaabcff728053a5c115fdedb4bca268be&#34;)" href="#idaabcff728053a5c115fdedb4bca268be" aria-expanded="false" aria-controls="idaabcff728053a5c115fdedb4bca268be" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/cohere-llm-university/01.what-are-large-language-models/">01.What are Large Language Models?</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#idaabcff728053a5c115fdedb4bca268be" aria-expanded="false" aria-controls="idaabcff728053a5c115fdedb4bca268be">
    
        <i class="fa-solid fa-angle-right" id="caret-idaabcff728053a5c115fdedb4bca268be"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="idaabcff728053a5c115fdedb4bca268be">
      



  <li class="child level "><a href="/books/nlp-notes/llm/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/cohere-llm-university/01.what-are-large-language-models/01.text-embeddings/">01.Text Embeddings</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id149c7b950ae4d982118ed83ec14d8b82&#34;)" href="#id149c7b950ae4d982118ed83ec14d8b82" aria-expanded="false" aria-controls="id149c7b950ae4d982118ed83ec14d8b82" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/gpt/">GPT</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id149c7b950ae4d982118ed83ec14d8b82" aria-expanded="false" aria-controls="id149c7b950ae4d982118ed83ec14d8b82">
    
        <i class="fa-solid fa-angle-right" id="caret-id149c7b950ae4d982118ed83ec14d8b82"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id149c7b950ae4d982118ed83ec14d8b82">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id42fecf3714ff279be906fb6a592e86f3&#34;)" href="#id42fecf3714ff279be906fb6a592e86f3" aria-expanded="false" aria-controls="id42fecf3714ff279be906fb6a592e86f3" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/gpt/chatgpt/">ChatGPT</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id42fecf3714ff279be906fb6a592e86f3" aria-expanded="false" aria-controls="id42fecf3714ff279be906fb6a592e86f3">
    
        <i class="fa-solid fa-angle-right" id="caret-id42fecf3714ff279be906fb6a592e86f3"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id42fecf3714ff279be906fb6a592e86f3">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id60444ffd5de140725f64e2ab6c082b4e&#34;)" href="#id60444ffd5de140725f64e2ab6c082b4e" aria-expanded="false" aria-controls="id60444ffd5de140725f64e2ab6c082b4e" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/gpt/chatgpt/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/">99.参考资料</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id60444ffd5de140725f64e2ab6c082b4e" aria-expanded="false" aria-controls="id60444ffd5de140725f64e2ab6c082b4e">
    
        <i class="fa-solid fa-angle-right" id="caret-id60444ffd5de140725f64e2ab6c082b4e"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id60444ffd5de140725f64e2ab6c082b4e">
      



  <li class="child level "><a href="/books/nlp-notes/llm/gpt/chatgpt/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-gpt-4-%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%A1%AC%E6%A0%B8%E8%A7%A3%E8%AF%BB/">2023-GPT-4 大模型硬核解读</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id687f58da131bb60cc891dad07da525ea&#34;)" href="#id687f58da131bb60cc891dad07da525ea" aria-expanded="false" aria-controls="id687f58da131bb60cc891dad07da525ea" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/langchain/">LangChain</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id687f58da131bb60cc891dad07da525ea" aria-expanded="false" aria-controls="id687f58da131bb60cc891dad07da525ea">
    
        <i class="fa-solid fa-angle-right" id="caret-id687f58da131bb60cc891dad07da525ea"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id687f58da131bb60cc891dad07da525ea">
      



  <li class="child level "><a href="/books/nlp-notes/llm/langchain/2023-langchain-%E4%B8%AD%E6%96%87%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/">2023-LangChain 中文入门教程</a></li>

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-ideb9cad914aac4b49afcfd297516b2f26&#34;)" href="#ideb9cad914aac4b49afcfd297516b2f26" aria-expanded="false" aria-controls="ideb9cad914aac4b49afcfd297516b2f26" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">大模型微调</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#ideb9cad914aac4b49afcfd297516b2f26" aria-expanded="false" aria-controls="ideb9cad914aac4b49afcfd297516b2f26">
    
        <i class="fa-solid fa-angle-right" id="caret-ideb9cad914aac4b49afcfd297516b2f26"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="ideb9cad914aac4b49afcfd297516b2f26">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id9eb302593d7ed15f5e29932c77718a83&#34;)" href="#id9eb302593d7ed15f5e29932c77718a83" aria-expanded="false" aria-controls="id9eb302593d7ed15f5e29932c77718a83" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/">99.参考资料</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id9eb302593d7ed15f5e29932c77718a83" aria-expanded="false" aria-controls="id9eb302593d7ed15f5e29932c77718a83">
    
        <i class="fa-solid fa-angle-right" id="caret-id9eb302593d7ed15f5e29932c77718a83"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id9eb302593d7ed15f5e29932c77718a83">
      



  <li class="child level "><a href="/books/nlp-notes/llm/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-finetuning-large-language-models/">2023-Finetuning Large Language Models</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id13afe656bd21386067d1467721d4b42c&#34;)" href="#id13afe656bd21386067d1467721d4b42c" aria-expanded="false" aria-controls="id13afe656bd21386067d1467721d4b42c" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/">代码生成</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id13afe656bd21386067d1467721d4b42c" aria-expanded="false" aria-controls="id13afe656bd21386067d1467721d4b42c">
    
        <i class="fa-solid fa-angle-right" id="caret-id13afe656bd21386067d1467721d4b42c"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id13afe656bd21386067d1467721d4b42c">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id89d7dca74667bd0d84eab271e9702b9c&#34;)" href="#id89d7dca74667bd0d84eab271e9702b9c" aria-expanded="false" aria-controls="id89d7dca74667bd0d84eab271e9702b9c" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/llm/%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/">99.参考资料</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id89d7dca74667bd0d84eab271e9702b9c" aria-expanded="false" aria-controls="id89d7dca74667bd0d84eab271e9702b9c">
    
        <i class="fa-solid fa-angle-right" id="caret-id89d7dca74667bd0d84eab271e9702b9c"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id89d7dca74667bd0d84eab271e9702b9c">
      



  <li class="child level "><a href="/books/nlp-notes/llm/%E4%BB%A3%E7%A0%81%E7%94%9F%E6%88%90/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-an-example-of-llm-prompting-for-programming/">2023-An example of LLM prompting for programming</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-idea56f1f3c4966507565ef4f6cf98782a&#34;)" href="#idea56f1f3c4966507565ef4f6cf98782a" aria-expanded="false" aria-controls="idea56f1f3c4966507565ef4f6cf98782a" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/transformer/">Transformer</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#idea56f1f3c4966507565ef4f6cf98782a" aria-expanded="false" aria-controls="idea56f1f3c4966507565ef4f6cf98782a">
    
        <i class="fa-solid fa-angle-right" id="caret-idea56f1f3c4966507565ef4f6cf98782a"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="idea56f1f3c4966507565ef4f6cf98782a">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id9c48c832e38aec39fa33e56e2d207563&#34;)" href="#id9c48c832e38aec39fa33e56e2d207563" aria-expanded="false" aria-controls="id9c48c832e38aec39fa33e56e2d207563" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/">99.参考资料</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id9c48c832e38aec39fa33e56e2d207563" aria-expanded="false" aria-controls="id9c48c832e38aec39fa33e56e2d207563">
    
        <i class="fa-solid fa-angle-right" id="caret-id9c48c832e38aec39fa33e56e2d207563"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id9c48c832e38aec39fa33e56e2d207563">
      



  <li class="child level "><a href="/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-nlp-%E4%B8%AD%E7%9A%84-rnnseq2seq-%E4%B8%8E-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">2019-NLP 中的 RNN、Seq2Seq 与 Attention 注意力机制</a></li>




  <li class="child level "><a href="/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2020-%E5%AE%8C%E5%85%A8%E8%A7%A3%E6%9E%90-rnn-seq2seq-attention-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">2020-完全解析 RNN, Seq2Seq, Attention 注意力机制</a></li>




  <li class="child level "><a href="/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-transformer%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%E5%9B%BE%E8%A7%A3%E6%9C%80%E5%AE%8C%E6%95%B4%E7%89%88/">2021-Transformer模型详解（图解最完整版）</a></li>




  <li class="child level "><a href="/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E8%B6%85%E8%AF%A6%E7%BB%86%E5%9B%BE%E8%A7%A3-self-attention/">2021-超详细图解 Self-Attention</a></li>




  <li class="child level "><a href="/books/nlp-notes/transformer/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-transformers-from-scratch/">2023-Transformers from Scratch</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-idf62814d1aec92e82cc474855cb3e8826&#34;)" href="#idf62814d1aec92e82cc474855cb3e8826" aria-expanded="false" aria-controls="idf62814d1aec92e82cc474855cb3e8826" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/">经典自然语言</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#idf62814d1aec92e82cc474855cb3e8826" aria-expanded="false" aria-controls="idf62814d1aec92e82cc474855cb3e8826">
    
        <i class="fa-solid fa-angle-right" id="caret-idf62814d1aec92e82cc474855cb3e8826"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="idf62814d1aec92e82cc474855cb3e8826">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id10e5d5fc96ec9fa77ffb2be69018cedd&#34;)" href="#id10e5d5fc96ec9fa77ffb2be69018cedd" aria-expanded="false" aria-controls="id10e5d5fc96ec9fa77ffb2be69018cedd" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/">词嵌入</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id10e5d5fc96ec9fa77ffb2be69018cedd" aria-expanded="false" aria-controls="id10e5d5fc96ec9fa77ffb2be69018cedd">
    
        <i class="fa-solid fa-angle-right" id="caret-id10e5d5fc96ec9fa77ffb2be69018cedd"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id10e5d5fc96ec9fa77ffb2be69018cedd">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id4d70266e5bd6c0d62687b5e324c924c9&#34;)" href="#id4d70266e5bd6c0d62687b5e324c924c9" aria-expanded="false" aria-controls="id4d70266e5bd6c0d62687b5e324c924c9" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/">词向量</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id4d70266e5bd6c0d62687b5e324c924c9" aria-expanded="false" aria-controls="id4d70266e5bd6c0d62687b5e324c924c9">
    
        <i class="fa-solid fa-angle-right" id="caret-id4d70266e5bd6c0d62687b5e324c924c9"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id4d70266e5bd6c0d62687b5e324c924c9">
      



  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E8%AF%8D%E5%90%91%E9%87%8F/%E5%9F%BA%E4%BA%8E-gensim-%E7%9A%84-word2vec-%E5%AE%9E%E8%B7%B5/">基于 Gensim 的 Word2Vec 实践</a></li>

      
        </ul>
      
    

    
      </div>
    




  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%8D%E5%B5%8C%E5%85%A5/%E6%A6%82%E8%BF%B0/">概述</a></li>

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id79d4aecf00d90487dca71cf83b24a12c&#34;)" href="#id79d4aecf00d90487dca71cf83b24a12c" aria-expanded="false" aria-controls="id79d4aecf00d90487dca71cf83b24a12c" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">统计语言模型</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id79d4aecf00d90487dca71cf83b24a12c" aria-expanded="false" aria-controls="id79d4aecf00d90487dca71cf83b24a12c">
    
        <i class="fa-solid fa-angle-right" id="caret-id79d4aecf00d90487dca71cf83b24a12c"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id79d4aecf00d90487dca71cf83b24a12c">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id774bc18c9c918aa32379eb740312e0a2&#34;)" href="#id774bc18c9c918aa32379eb740312e0a2" aria-expanded="false" aria-controls="id774bc18c9c918aa32379eb740312e0a2" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/">BERT</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#id774bc18c9c918aa32379eb740312e0a2" aria-expanded="false" aria-controls="id774bc18c9c918aa32379eb740312e0a2">
    
        <i class="fa-solid fa-angle-right" id="caret-id774bc18c9c918aa32379eb740312e0a2"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="id774bc18c9c918aa32379eb740312e0a2">
      



  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E7%9B%AE%E6%A0%87%E5%87%BD%E6%95%B0/">目标函数</a></li>




  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/bert/%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA/">输入表示</a></li>

      
        </ul>
      
    

    
      </div>
    




  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/word2vec/">Word2Vec</a></li>




  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E8%AF%8D%E8%A1%A8%E7%A4%BA/">词表示</a></li>




  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E5%9F%BA%E7%A1%80%E6%96%87%E6%9C%AC%E5%A4%84%E7%90%86/">基础文本处理</a></li>




  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/%E7%BB%9F%E8%AE%A1%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">统计语言模型</a></li>

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-ided3ef90acbcf67915b5da74475b05bff&#34;)" href="#ided3ef90acbcf67915b5da74475b05bff" aria-expanded="false" aria-controls="ided3ef90acbcf67915b5da74475b05bff" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%AD%E6%B3%95%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/">语法语义分析</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#ided3ef90acbcf67915b5da74475b05bff" aria-expanded="false" aria-controls="ided3ef90acbcf67915b5da74475b05bff">
    
        <i class="fa-solid fa-angle-right" id="caret-ided3ef90acbcf67915b5da74475b05bff"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="ided3ef90acbcf67915b5da74475b05bff">
      



  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E8%AF%AD%E6%B3%95%E8%AF%AD%E4%B9%89%E5%88%86%E6%9E%90/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/">命名实体识别</a></li>

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-idde19769b6cee9616d93198fec51b721f&#34;)" href="#idde19769b6cee9616d93198fec51b721f" aria-expanded="false" aria-controls="idde19769b6cee9616d93198fec51b721f" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/">主题模型</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#idde19769b6cee9616d93198fec51b721f" aria-expanded="false" aria-controls="idde19769b6cee9616d93198fec51b721f">
    
        <i class="fa-solid fa-angle-right" id="caret-idde19769b6cee9616d93198fec51b721f"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="idde19769b6cee9616d93198fec51b721f">
      



  <li class="child level "><a href="/books/nlp-notes/%E7%BB%8F%E5%85%B8%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B/lda/">LDA</a></li>

      
        </ul>
      
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-idf9eb0737774bfcb6f7721173244544fe&#34;)" href="#idf9eb0737774bfcb6f7721173244544fe" aria-expanded="false" aria-controls="idf9eb0737774bfcb6f7721173244544fe" aria-hidden="false" data-toggle="collapse">
    
    <a class="d-inline docs-toc-link " href="/books/nlp-notes/%E8%A1%8C%E4%B8%9A%E5%BA%94%E7%94%A8/">行业应用</a>
    <a class="nav-toogle d-inline level" aria-hidden="false" data-toggle="collapse" href="#idf9eb0737774bfcb6f7721173244544fe" aria-expanded="false" aria-controls="idf9eb0737774bfcb6f7721173244544fe">
    
        <i class="fa-solid fa-angle-right" id="caret-idf9eb0737774bfcb6f7721173244544fe"></i>
    
    </a>
    
    </div>
    
      
      <ul class="nav docs-sidenav collapse  " id="idf9eb0737774bfcb6f7721173244544fe">
      



  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id42c2fb26f13483c39a87b8fa72178ac3&#34;)" href="#id42c2fb26f13483c39a87b8fa72178ac3" aria-expanded="false" aria-controls="id42c2fb26f13483c39a87b8fa72178ac3" aria-hidden="false" data-toggle="collapse">
    
    </div>
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-id30c032f307d628645469a49eeaac7057&#34;)" href="#id30c032f307d628645469a49eeaac7057" aria-expanded="false" aria-controls="id30c032f307d628645469a49eeaac7057" aria-hidden="false" data-toggle="collapse">
    
    </div>
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    




  
    
    
    
    
      
    
    

    
    
    
    <div class="docs-toc-item has-child">
    <div class="parent-node d-flex justify-content-between" onClick="Collapse(&#34;caret-ida884c7392190e51dee8c0112022a5c30&#34;)" href="#ida884c7392190e51dee8c0112022a5c30" aria-expanded="false" aria-controls="ida884c7392190e51dee8c0112022a5c30" aria-hidden="false" data-toggle="collapse">
    
    </div>
    

    
      </div>
    

      
        </ul>
      
    

    
      </div>
    

    
  
</nav>

    </div>

    
    
    <div class="d-none d-xl-block col-xl-2 docs-toc">
      
     
      <ul class="nav toc-top">
        <li><a href="#" id="back_to_top" class="docs-toc-title">目录</a></li>
      </ul>
     

      <nav id="TableOfContents">
  <ul>
    <li><a href="#notes-on-the-github-version">Notes on the Github version</a></li>
    <li><a href="#prompts">Prompts</a>
      <ul>
        <li><a href="#40-901-amount-saved-by-appending-be-concise-to-your-prompt">40-90%: Amount saved by appending “Be Concise” to your prompt</a></li>
        <li><a href="#13-average-tokens-per-word">1.3: Average tokens per word</a></li>
      </ul>
    </li>
    <li><a href="#prices2">Prices</a>
      <ul>
        <li><a href="#50-cost-ratio-of-gpt-4-to-gpt-35-turbo3">~50: Cost Ratio of GPT-4 to GPT-3.5 Turbo</a></li>
        <li><a href="#5-cost-ratio-of-generation-of-text-using-gpt-35-turbo-vs-openai-embedding">5: Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding</a></li>
        <li><a href="#10-cost-ratio-of-openai-embedding-to-self-hosted-embedding">10: Cost Ratio of OpenAI embedding to Self-Hosted embedding</a></li>
        <li><a href="#6-cost-ratio-of-openai-base-vs-fine-tuned-model-queries">6: Cost Ratio of OpenAI base vs fine tuned model queries</a></li>
        <li><a href="#1-cost-ratio-of-self-hosted-base-vs-fine-tuned-model-queries">1: Cost Ratio of Self-Hosted base vs fine-tuned model queries</a></li>
      </ul>
    </li>
    <li><a href="#training-and-fine-tuning">Training and Fine Tuning</a>
      <ul>
        <li><a href="#1-million-cost-to-train-a-13-billion-parameter-model-on-14-trillion-tokens">~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens</a></li>
        <li><a href="#lt-0001-cost-ratio-of-fine-tuning-vs-training-from-scratch">&lt; 0.001: Cost ratio of fine tuning vs training from scratch</a></li>
      </ul>
    </li>
    <li><a href="#gpu-memory">GPU Memory</a>
      <ul>
        <li><a href="#v100-16gb-a10g-24gb-a100-4080gb-gpu-memory-capacities">V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities</a></li>
        <li><a href="#2x-number-of-parameters-typical-gpu-memory-requirements-of-an-llm-for-serving">2x number of parameters: Typical GPU memory requirements of an LLM for serving</a></li>
        <li><a href="#1gb-typical-gpu-memory-requirements-of-an-embedding-model">~1GB: Typical GPU memory requirements of an embedding model</a></li>
        <li><a href="#10x-throughput-improvement-from-batching-llm-requests">&gt;10x: Throughput improvement from batching LLM requests</a></li>
        <li><a href="#1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model">~1 MB: GPU Memory required for 1 token of output with a 13B parameter model</a></li>
      </ul>
    </li>
  </ul>

  <ul>
    <li><a href="#notes">Notes</a></li>
  </ul>
</nav>

      
<div class="subscribe-module col-24 mt-1">
    <img src="https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png" alt="image" title="王下邀月熊的微信公众号"/>
</div>



    </div>
    

    <main class="py-md-3 pl-md-3 docs-content col-xl-8" role="main">

      <article class="article">

          

          <h1>2023-Numbers every LLM Developer should know</h1>

          <div class="article-style">
            <!-----

Yay, no errors, warnings, or alerts!

Conversion time: 0.472 seconds.


Using this Markdown file:

1. Paste this output into your source file.
2. See the notes and action items below regarding this conversion run.
3. Check the rendered output (headings, lists, code blocks, tables) for proper
   formatting and use a linkchecker before you publish this page.

Conversion notes:

* Docs to Markdown version 1.0β34
* Wed May 17 2023 09:47:33 GMT-0700 (PDT)
* Source doc: Numbers every LLM developer should know
----->
<h1 id="numbers-every-llm-developer-should-know">Numbers every LLM Developer should know</h1>
<p>When I was at Google, there was a document put together by <a href="https://en.wikipedia.org/wiki/Jeff_Dean" target="_blank" rel="noopener">Jeff Dean</a>, the legendary engineer, called <a href="http://brenocon.com/dean_perf.html" target="_blank" rel="noopener">Numbers every Engineer should know</a>. It’s really useful to have a similar set of numbers for LLM developers to know that are useful for back-of-the envelope calculations. Here we share particular numbers we at Anyscale use, why the number is important and how to use it to your advantage.</p>
<h2 id="notes-on-the-github-version">Notes on the Github version</h2>
<p>Last updates: 2023-05-17</p>
<p>If you feel there&rsquo;s an issue with the accuracy of the numbers, please file an issue. Think there are more numbers that should be in this doc? Let us know or file a PR.</p>
<p>We are thinking the next thing we should add here is some stats on tokens per second of different models.</p>
<h2 id="prompts">Prompts</h2>
<h3 id="40-901-amount-saved-by-appending-be-concise-to-your-prompt">40-90%<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>: Amount saved by appending “Be Concise” to your prompt</h3>
<p>It’s important to remember that you pay by the token for responses. This means that asking an LLM to be concise can save you a lot of money. This can be broadened beyond simply appending “be concise” to your prompt: if you are using GPT-4 to come up with 10 alternatives, maybe ask it for 5 and keep the other half of the money.</p>
<h3 id="13-average-tokens-per-word">1.3: Average tokens per word</h3>
<p>LLMs operate on tokens. Tokens are words or sub-parts of words, so “eating” might be broken into two tokens “eat” and “ing”. A 750 word document in English will be about 1000 tokens. For languages other than English, the tokens per word increases depending on their commonality in the LLM&rsquo;s embedding corpus.</p>
<p>Knowing this ratio is important because most billing is done in tokens, and the LLM’s context window size is also defined in tokens.</p>
<h2 id="prices2">Prices<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></h2>
<p>Prices are of course subject to change, but given how expensive LLMs are to operate, the numbers in this section are critical. We use OpenAI for the numbers here, but prices from other providers you should check out (<a href="https://cdn2.assets-servd.host/anthropic-website/production/images/model_pricing_may2023.pdf" target="_blank" rel="noopener">Anthropic</a>, <a href="https://cohere.com/pricing" target="_blank" rel="noopener">Cohere</a>) are in the same ballpark.</p>
<h3 id="50-cost-ratio-of-gpt-4-to-gpt-35-turbo3">~50: Cost Ratio of GPT-4 to GPT-3.5 Turbo<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></h3>
<p>What this means is that for many practical applications, it’s much better to use GPT-4 for things like generation and then use that data to fine tune a smaller model. It is roughly 50 times cheaper to use GPT-3.5-Turbo than GPT-4 (the “roughly” is because GPT-4 charges differently for the prompt and the generated output) – so you really need to check on how far you can get with GPT-3.5-Turbo. GPT-3.5-Turbo is more than enough for tasks like summarization for example.</p>
<h3 id="5-cost-ratio-of-generation-of-text-using-gpt-35-turbo-vs-openai-embedding">5: Cost Ratio of generation of text using GPT-3.5-Turbo vs OpenAI embedding</h3>
<p>This means it is way cheaper to look something up in a vector store than to ask an LLM to generate it. E.g. “What is the capital of Delaware?” when looked up in an neural information retrieval system costs about 5x<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> less than if you asked GPT-3.5-Turbo. The cost difference compared to GPT-4 is a whopping 250x!</p>
<h3 id="10-cost-ratio-of-openai-embedding-to-self-hosted-embedding">10: Cost Ratio of OpenAI embedding to Self-Hosted embedding</h3>
<blockquote>
<p>Note: this number is sensitive to load and embedding batch size, so please consider this approximate.</p>
</blockquote>
<p>In our blog post, we noted that using a g4dn.4xlarge (on-demand price: $1.20/hr) we were able to embed at about 9000 tokens per second using HuggingFace’s SentenceTransformers (which are pretty much as good as OpenAI’s embeddings). Doing some basic math of that rate and that node type indicates it is considerably cheaper (factor of 10 cheaper) to self-host embeddings (and that is before you start to think about things like ingress and egress fees).</p>
<h3 id="6-cost-ratio-of-openai-base-vs-fine-tuned-model-queries">6: Cost Ratio of OpenAI base vs fine tuned model queries</h3>
<p>It costs you 6 times as much to serve a fine tuned model as it does the base model on OpenAI. This is pretty exorbitant, but might make sense because of the possible multi-tenancy of base models. It also means it is far more cost effective to tweak the prompt for a base model than to fine tune a customized model.</p>
<h3 id="1-cost-ratio-of-self-hosted-base-vs-fine-tuned-model-queries">1: Cost Ratio of Self-Hosted base vs fine-tuned model queries</h3>
<p>If you’re self hosting a model, then it more or less costs the same amount to serve a fine tuned model as it does to serve a base one: the models have the same number of parameters.</p>
<h2 id="training-and-fine-tuning">Training and Fine Tuning</h2>
<h3 id="1-million-cost-to-train-a-13-billion-parameter-model-on-14-trillion-tokens">~$1 million: Cost to train a 13 billion parameter model on 1.4 trillion tokens</h3>
<p>The <a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="noopener">LLaMa paper</a> mentions it took them 21 days to train LLaMa using 2048 GPUs A100 80GB GPUs. We considered training our own model on the Red Pajama training set, then we ran the numbers. The above is assuming everything goes right, nothing crashes, and the calculation succeeds on the first time, etc. Plus it involves the coordination of 2048 GPUs. That’s not something most companies can do (shameless plug time: of course, we at Anyscale can – that’s our <a href="https://www.anyscale.com/blog/training-175b-parameter-language-models-at-1000-gpu-scale-with-alpa-and-ray" target="_blank" rel="noopener">bread and butter</a>! Contact us if you’d like to learn more). The point is that training your own LLM is possible, but it’s not cheap. And it will literally take days to complete each run. Much cheaper to use a pre-trained model.</p>
<h3 id="lt-0001-cost-ratio-of-fine-tuning-vs-training-from-scratch">&lt; 0.001: Cost ratio of fine tuning vs training from scratch</h3>
<p>This is a bit of a generalization, but the cost of fine tuning is negligible. We showed for example that you can fine tune a <a href="https://www.anyscale.com/blog/how-to-fine-tune-and-serve-llms-simply-quickly-and-cost-effectively-using" target="_blank" rel="noopener">6B parameter model for about 7</a>. Even at OpenAI’s rate for its most expensive fine-tunable model, Davinci, it is 3c per 1000 tokens. That means to fine tune on the entire works of Shakespeare (about 1 million words), you’re looking at $40<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>. However, fine tuning is one thing and training from scratch is another …</p>
<h2 id="gpu-memory">GPU Memory</h2>
<p>If you’re self-hosting a model, it’s really important to understand GPU memory because LLMs push your GPU’s memory to the limit. The following statistics are specifically about inference. You need considerably more memory for training or fine tuning.</p>
<h3 id="v100-16gb-a10g-24gb-a100-4080gb-gpu-memory-capacities">V100: 16GB, A10G: 24GB, A100: 40/80GB: GPU Memory Capacities</h3>
<p>It may seem strange, but it’s important to know the amount of memory different types of GPUs have. This will cap the number of parameters your LLM can have. Generally, we like to use A10Gs because they cost $1.50 to $2 per hour each at AWS on-demand prices and have 24G of GPU memory, vs the A100s which will run you about $5 each at AWS on-demand prices.</p>
<h3 id="2x-number-of-parameters-typical-gpu-memory-requirements-of-an-llm-for-serving">2x number of parameters: Typical GPU memory requirements of an LLM for serving</h3>
<p>For example, if you have a 7 billion parameter model, it takes about 14GB of GPU space. This is because most of the time, one 16-bit float (or 2 bytes) is required per parameter. There’s usually no need to go beyond 16-bit accuracy, and most of the time when you go to 8-bit accuracy you start to lose resolution (though that may be acceptable in some cases). Of course there are efforts to reduce this, notably llama.cpp which runs a 13 billion parameter model on a 6GB GPU by quantizing aggressively down to 4 bits (and 8 bits without too much impact), but that’s atypical.</p>
<h3 id="1gb-typical-gpu-memory-requirements-of-an-embedding-model">~1GB: Typical GPU memory requirements of an embedding model</h3>
<p>Whenever you are doing sentence embedding (a very typical thing you do for clustering, semantic search and classification tasks), you need an embedding model like <a href="https://www.sbert.net/docs/pretrained_models.html#sentence-embedding-models/" target="_blank" rel="noopener">sentence transformers</a>. OpenAI also has its own embeddings that they provide commercially.</p>
<p>You typically don’t have to worry about how much memory embeddings take on the GPU, they’re fairly small. We’ve even had the embedding and the LLM on the same GPU.</p>
<h3 id="10x-throughput-improvement-from-batching-llm-requests">&gt;10x: Throughput improvement from batching LLM requests</h3>
<p>Running an LLM query through a GPU is very high latency: it may take, say, 5 seconds, with a throughput of 0.2 queries per second. The funny thing is, though, if you run two tasks, it might only take 5.2 seconds. This means that if you can bundle 25 queries together, it would take about 10 seconds, and our throughput has improved to 2.5 queries per second. However, see the next point.</p>
<h3 id="1-mb-gpu-memory-required-for-1-token-of-output-with-a-13b-parameter-model">~1 MB: GPU Memory required for 1 token of output with a 13B parameter model</h3>
<p>The amount of memory you need is directly proportional to the maximum number of tokens you want to generate. So for example, if you want to generate outputs of up to 512 tokens (about 380 words), you need 512MB. No big deal you might say – I have 24GB to spare, what’s 512MB? Well, if you want to run bigger batches it starts to add up. So if you want to do batches of 16, you need 8GB of space. There are some techniques being developed that overcome this, but it’s still a real issue.</p>
<h1 id="cheatsheet">Cheatsheet</h1>
<img width="1097" alt="Screenshot 2023-05-17 at 1 46 09 PM" src="https://github.com/ray-project/llm-numbers/assets/9677264/5d40c6a3-84d7-436a-8fc4-a8d58008765d">
<h1 id="next-steps">Next Steps</h1>
<p>See our earlier <a href="https://www.anyscale.com/blog/ray-common-production-challenges-for-generative-ai-infrastructure" target="_blank" rel="noopener">blog series on solving Generative AI infrastructure</a> and <a href="https://www.anyscale.com/blog/llm-open-source-search-engine-langchain-ray" target="_blank" rel="noopener">using LangChain with Ray</a>. <br>
<br>
If you are interested in learning more about Ray, see <a href="http://ray.io/" target="_blank" rel="noopener">Ray.io</a> and <a href="http://docs.ray.io/" target="_blank" rel="noopener">Docs.Ray.io</a>. <br>
<br>
To connect with the Ray community join #LLM on the <a href="https://docs.google.com/forms/d/e/1FAIpQLSfAcoiLCHOguOm8e7Jnn-JJdZaCxPGjgVCvFijHB5PLaQLeig/viewform" target="_blank" rel="noopener">Ray Slack</a> or our <a href="https://discuss.ray.io/" target="_blank" rel="noopener">Discuss forum</a>. <br>
<br>
If you are interested in our Ray hosted service for ML Training and Serving, see <a href="http://www.anyscale.com/platform" target="_blank" rel="noopener">Anyscale.com/Platform </a>and click the &lsquo;Try it now&rsquo; button</p>
<p><strong>Ray Summit 2023:</strong> If you are interested to learn much more about how Ray can be used to build performant and scalable LLM applications and fine-tune/train/serve LLMs on Ray, join <a href="https://raysummit.anyscale.com/" target="_blank" rel="noopener">Ray Summit</a> on September 18-20th! We have a set of great keynote speakers including John Schulman from OpenAI and Aidan Gomez from Cohere, community and tech talks about Ray as well as <a href="https://github.com/ray-project/ray-educational-materials/blob/main/NLP_workloads/Text_generation/LLM_finetuning_and_batch_inference.ipynb" target="_blank" rel="noopener">practical training focused on LLMs</a>.</p>
<!-- Footnotes themselves at the bottom. -->
<h2 id="notes">Notes</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Based on experimentation with GPT-3.5-Turbo using a suite of prompts on 2023-05-08.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Retrieved from <a href="http://openai.com/pricing" target="_blank" rel="noopener">http://openai.com/pricing</a> on 2023-05-08.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><strong>GPT-4</strong>: 6c/1k tokens for the prompt, 12c/1k tokens for the generation (32,000 window version, 8,000 window version is half that). <strong>GPT-3.5 Turbo</strong>: 0.2c/1k tokens.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>This assumes the vector lookup is “free.” It’s not, but it uses CPUs (much cheaper) and is fairly fast.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>1 million words / 0.75 tokens/word / 1000*0.03 = $40.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

          </div>

          



          
          
          <div class="article-widget">
            
<div class="container-xl row post-nav">
  
  
  
  <div class="col-6 post-nav-item">
    <div class="meta-nav">上一页</div>
    <a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E5%90%B4%E6%81%A9%E8%BE%BE-building-systems-with-the-chatgpt-api/11.conclusion/" rel="next">11.conclusion</a>
  </div>
  
  
  
  <div class="col-6 post-nav-item">
    <div class="meta-nav">下一页</div>
    <a href="/books/nlp-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2023-%E9%99%86%E5%A5%87-%E6%88%91%E7%9A%84%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%B8%96%E7%95%8C%E8%A7%82/" rel="prev">2023-陆奇-我的大模型世界观</a>
  </div>
  
</div>

          </div>
          

        <div class="body-footer">
          <p>最近更新于 0001-01-01</p>

          



          


  
  
  

  

  
  <section id="comments" class="mb-3 pt-0">
    
<div id="disqus_thread"></div>
<script>
  var disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); 
    s.async = true;
    s.src = 'https://' + "ngte" + '.disqus.com/embed.js';
    
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  </section>
  



          


        </div>

      </article>

      <footer class="site-footer">

  



  

  
  <div class="copyright py-4 bg-footer">
      <div class="row justify-content-center">
        <div class="text-center footer-color">
          <p class="mb-0">© 2017-2022 NGTE all rights reserved</p>
        </div>
    </div>
  </div>

  <script type="text/javascript" id="clstr_globe" async src="//clustrmaps.com/globe.js?d=kgpJG5sWZQpKujBmD-uW1B54-WBPol-DuDtrB2KFjKs"></script>
  
</footer>


    </main>
  </div>
</div>
<script src="//unpkg.com/heti/umd/heti-addon.min.js"></script>
<script>
  const heti = new Heti('.article');
  heti.autoSpacing();
</script>
<script type="text/javascript">
  window.$crisp = [];
  window.CRISP_WEBSITE_ID = "12adcc35-9621-4313-8262-62dc654b29d8";
  (function () {
    setTimeout(function() {
      d = document;
      s = d.createElement("script");
      s.src = "https://client.crisp.chat/l.js";
      s.async = 1;
      d.getElementsByTagName("head")[0].appendChild(s);
    }, 2500);
  })();
</script>
  </div>

  <div class="page-footer">
    
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>

    
    
    
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
      

    

    
    
    

    
    
    
      
      <script id="search-hit-algolia-template" type="text/html">
        <div class="search-hit">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a>
            </div>
            <div class="article-metadata search-hit-type">{{type}}</div>
            <p class="search-hit-description">{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js" crossorigin="anonymous"></script>
      
      
    

    
    

    
    
    
    
      <script id="dsq-count-scr" src="https://ngte.disqus.com/count.js" async></script>
      
    

    
    
      
      
      
      
      
      
      
    

    
    <script src="/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js" type="module"></script>
    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":false}</script>

    
    
    
    
    
    
    
    
    
    
    <script src="/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js"></script>

    
    
    
    
    
    
    <script>

var mybutton = document.getElementById("backTopBtn");


window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}


function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>


    

    
    
    <script src="https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
    <script>
      anchors.add();
    </script>
    

    
    <script>



(function() {
  'use strict';

  if(!document.queryCommandSupported('copy')) {
    return;
  }

  function flashCopyMessage(el, msg) {
    el.className = "highlight-copy-btn";
    el.textContent = msg;
    setTimeout(function() {
      el.textContent = "";
      el.className = "highlight-copy-btn fa fa-copy";
    }, 1000);
  }

  function selectText(node) {
    var selection = window.getSelection();
    var range = document.createRange();
    range.selectNodeContents(node);
    selection.removeAllRanges();
    selection.addRange(range);
    return selection;
  }

  function addCopyButton(containerEl) {
    var copyBtn = document.createElement("button");
    copyBtn.className = "highlight-copy-btn fa fa-copy";
    copyBtn.textContent = "";

    var codeEl = containerEl.firstElementChild;
    copyBtn.addEventListener('click', function() {
      try {
        var selection = selectText(codeEl);
        document.execCommand('copy');
        selection.removeAllRanges();
        
        flashCopyMessage(copyBtn, '已复制')
        
      } catch(e) {
        console && console.log(e);
        flashCopyMessage(copyBtn, 'Failed :\'(')
      }
    });

    containerEl.appendChild(copyBtn);
  }

  
  var highlightBlocks = document.getElementsByClassName('highlight');
  Array.prototype.forEach.call(highlightBlocks, addCopyButton);
})();
</script>

    


</body>
</html>
