<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week1 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/</link><atom:link href="https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/index.xml" rel="self" type="application/rss+xml"/><description>Week1</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>Week1</title><link>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/</link></image><item><title>二元分类与 Logistic 回归</title><link>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E4%B8%8E-logistic-%E5%9B%9E%E5%BD%92/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E4%B8%8E-logistic-%E5%9B%9E%E5%BD%92/</guid><description>&lt;h1 id="二元分类与-logistic-回归">二元分类与 Logistic 回归&lt;/h1>
&lt;p>本部分将会介绍神经网格构建与训练的基础知识；一般来说，网络的计算过程由正向传播(Forward Propagation)与反向传播(Back Propagation)两部分组成。这里我们将会以简单的 Logistic 回归为例，讲解如何解决常见的二元分类(Binary Classification)问题。这里我们将会尝试训练出简单的神经网络以自动识别某个图片是否为猫，为猫则输出 1，否则输出 0。计算机中的图片往往表示为红、绿、蓝三个通道的像素值；如果我们的图像是 64 _ 64 像素值大小，我们的单张图片的特征维度即为 64 _ 64 * 3 = 12288，即可以使用 $n_x = 12288$ 来表示特征向量的维度。&lt;/p>
&lt;h1 id="深度学习的标准术语约定">深度学习的标准术语约定&lt;/h1>
&lt;h2 id="神经网络的符号">神经网络的符号&lt;/h2>
&lt;p>上标 $^{(i)}$ 表示第 $i$ 个训练用例，而上标 $^{[l]}$ 则表示第 $l$ 层。&lt;/p>
&lt;h3 id="尺寸">尺寸&lt;/h3>
&lt;ul>
&lt;li>$m$：数据集中用例的数目。&lt;/li>
&lt;li>$n_x$：输入的单个用例的特征向量维度。&lt;/li>
&lt;li>$n_y$：输出的维度(待分类的数目)。&lt;/li>
&lt;li>$n_h^{[l]}$：第 $l$ 个隐层的单元数目，在循环中，我们可能定义 $n_x = n_h^{[0]}$ 以及 $n_y = n_h^{number , of , layers + 1}$。&lt;/li>
&lt;li>$L$：神经网络的层数。&lt;/li>
&lt;/ul>
&lt;h3 id="对象">对象&lt;/h3>
&lt;ul>
&lt;li>$X \in R^{n_x \times m}$：输入的矩阵，即包含了 $m$ 个用例，每个用例的特征向量维度为 $n_x$。&lt;/li>
&lt;li>$x^{(i)} \in R^{n_x}$：第 $i$ 个用例的特征向量，表示为列向量。&lt;/li>
&lt;li>$Y \in R^{n_y \times m}$：标签矩阵。&lt;/li>
&lt;li>$y^{(i)} \in R^{n_y}$：第 $i$ 个用例的输出标签。&lt;/li>
&lt;li>$W^{[l]} \in R^{number , of , units , in , next , layer \times number , of , unites , in , the , previous , layer}$：第 $l$ 层与第 $l+1$ 层之间的权重矩阵，在简单的二元分类且仅有输入层与输出层的情况下，其维度就是 $ 1 \times n_x$。&lt;/li>
&lt;li>$b^{[l]} \in R^{number , of , units , in , next , layer}$：第 $l$ 层的偏差矩阵。&lt;/li>
&lt;li>$\hat{y} \in R^{n_y}$：输出的预测向量，也可以表示为 $a^{[L]}$，其中 $L$ 表示网络中的总层数。&lt;/li>
&lt;/ul>
&lt;h3 id="通用前向传播等式">通用前向传播等式&lt;/h3>
&lt;ul>
&lt;li>$ a = g^{[l]}(W_xx^{(i)} + b_1) = g^{[l]}(z_1) $，其中 $g^{[l]}$ 表示第 $l$ 层的激活函数。&lt;/li>
&lt;li>$\hat{y}^{(i)} = softmax(W_hh + b_2)$。&lt;/li>
&lt;li>通用激活公式：$a&lt;em>j^{[l]} = g^{[l]}(\sum_kw&lt;/em>{jk}^{[l]}a_k^{[l-1]} + b_j^{[l]}) = g^{[l]}(z_j^{[l]})$。&lt;/li>
&lt;li>$J(x, W, b, y)$ 或者 $J(\hat{y}, y)$ 表示损失函数。&lt;/li>
&lt;/ul>
&lt;h3 id="损失函数">损失函数&lt;/h3>
&lt;ul>
&lt;li>$J*{CE(\hat{y},y)} = - \sum*{i=0}^m y^{(i)}log \hat{y}^{(i)}$&lt;/li>
&lt;li>$J*{1(\hat{y},y)} = \sum*{i=0}^m | y^{(i)} - \hat{y}^{(i)} |$&lt;/li>
&lt;/ul>
&lt;h2 id="深度学习的表示">深度学习的表示&lt;/h2>
&lt;p>在深度学习中，使用结点代表输入、激活函数或者数据，边代表权重或者偏差，下图即是两个典型的神经网络：&lt;/p>
&lt;h1 id="logistic-回归">Logistic 回归&lt;/h1>
&lt;h2 id="基础模型">基础模型&lt;/h2>
&lt;p>在猫咪识别问题中，我们给定了未知的图片，可以将其表示为 $X \in R^{n_x}$ 的特征向量；我们的任务就是寻找合适的算法，来根据特征向量推导出该图片是猫咪的概率。在上面的介绍中我们假设了 Logistic 函数的参数为 $w \in R^{n_x} $ 以及 $b \in R$，则输出的计算公式可以表示为：&lt;/p>
&lt;p>$$
\hat{y} = \sigma(w^Tx + b)
$$&lt;/p>
&lt;p>这里的 $\sigma$ 表示 Sigmoid 函数，该函数的表达式与线型如下：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Sigmoid-function-2.svg/2000px-Sigmoid-function-2.svg.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>上图中可以发现，当 $t$ 非常大时，$e^{-t}$ 趋近于 0，整体的函数值趋近于 1；反之，如果 $t$ 非常小的时候，整体的函数值趋近于 0。&lt;/p>
&lt;h2 id="损失函数与代价函数">损失函数与代价函数&lt;/h2>
&lt;p>我们的训练目标是在给定训练数据 ${(x^{(1)}, y^{(1)}),&amp;hellip;,(x^{(m)},y^{(m)})}$ 的情况下使得 $\hat{y}^{(i)}$ 尽可能接近 $y^{(i)}$，而所谓的损失函数即是用于衡量预测结果与真实值之间的误差。最简单的损失函数定义方式为平方差损失：&lt;/p>
&lt;p>$$
L(\hat{y},y) = \frac{1}{2} (\hat{y} - y)^2
$$&lt;/p>
&lt;p>不过 Logistic 回归中我们并不倾向于使用这样的损失函数，因为其对于梯度下降并不友好，很多情况下会陷入非凸状态而只能得到局部最优解。这里我们将会使用如下的损失函数：&lt;/p>
&lt;p>$$
L(\hat{y},y) = -(ylog\hat{y} + (1-y)log(1-\hat{y}))
$$&lt;/p>
&lt;p>我们的优化目标是希望损失函数值越小越好，这里我们考虑两个极端情况，当 $y = 1$ 时，损失函数值为 $-log\hat{y}$；此时如果 $\hat{y} = 1$，则损失函数为 0。反之如果 $\hat{y} = 0$，则损失函数值趋近于无穷大。当 $y = 0$ 时，损失函数值为 $-log(1-\hat{y})$；如果 $\hat{y} = 1$，则损失函数值也变得无穷大。这样我们可以将 Logistic 回归中总的代价函数定义为：&lt;/p>
&lt;p>$$
J(w,b) =
\frac{1}{m}\sum*{i=1}^mL(\hat{y}^{(i)} - y^{(i)}) =
-\frac{1}{m} \sum*{i=1}^m [y^{(i)}log\hat{y}^{(i)} + (1-y^{(i)})log(1-\hat{y}^{(i)})]
$$&lt;/p>
&lt;p>在深度学习的模型训练中我们常常会接触到损失函数(Loss Function)与代价函数(Cost Function)的概念，其中损失函数代指单个训练用例的错误程度，而代价函数往往是整个训练集中所有训练用例的损失函数值的平均。&lt;/p></description></item><item><title>基于 Logistic 回归的图像分类实践</title><link>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E5%9F%BA%E4%BA%8E-logistic-%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%AE%9E%E8%B7%B5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E5%9F%BA%E4%BA%8E-logistic-%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%AE%9E%E8%B7%B5/</guid><description>&lt;h1 id="基于-logistic-回归的图像分类实践">基于 Logistic 回归的图像分类实践&lt;/h1>
&lt;h1 id="延伸阅读">延伸阅读&lt;/h1></description></item><item><title>基于 Numpy 的 Python 向量操作</title><link>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E5%9F%BA%E4%BA%8E-numpy-%E7%9A%84-python-%E5%90%91%E9%87%8F%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E5%9F%BA%E4%BA%8E-numpy-%E7%9A%84-python-%E5%90%91%E9%87%8F%E6%93%8D%E4%BD%9C/</guid><description>&lt;h1 id="broadcasting">BroadCasting&lt;/h1>
&lt;p>Numpy 会自动进行矩阵扩展操作以适应指定的矩阵运算&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">5&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">15&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">3&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">A&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">8&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">6&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">B&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">7&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">1&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">Result&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="n">d&lt;/span> &lt;span class="n">array&lt;/span>&lt;span class="p">):&lt;/span> &lt;span class="mi">8&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">7&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">6&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">a&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># a.shape = (4, 3)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># b.shape = (3, 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>No! In numpy the &amp;ldquo;*&amp;rdquo; operator indicates element-wise multiplication. It is different from &amp;ldquo;np.dot()&amp;rdquo;. If you would try &amp;ldquo;c = np.dot(a,b)&amp;rdquo; you would get c.shape = (4, 2).&lt;/p>
&lt;p>Also, the broadcasting cannot happen because of the shape of b. b should have been something like (4, 1) or (1, 3) to broadcast properly. So a*b leads to an error!&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-py" data-lang="py">&lt;span class="line">&lt;span class="cl">&lt;span class="n">a&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># a.shape = (4, 3)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randn&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="c1"># b.shape = (3, 2)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">b&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">//&lt;/span> &lt;span class="ne">ValueError&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">operands&lt;/span> &lt;span class="n">could&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="n">be&lt;/span> &lt;span class="n">broadcast&lt;/span> &lt;span class="n">together&lt;/span> &lt;span class="k">with&lt;/span> &lt;span class="n">shapes&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">4&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h1 id="延伸阅读">延伸阅读&lt;/h1></description></item><item><title>浅层神经网络</title><link>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid><description>&lt;h1 id="浅层神经网络">浅层神经网络&lt;/h1>
&lt;h1 id="单隐层计算">单隐层计算&lt;/h1>
&lt;p>我们可以得到结构相似的输入数据与一层隐层输出&lt;/p>
&lt;h1 id="activation-function">Activation Function&lt;/h1>
&lt;p>构建神经网络中非常重要的一个环节就是选择合适的激活函数(Activation Function)，激活函数是为了增加神经网络模型的非线性，也可以看做从数据空间到最终表达空间的一种映射。仅就 Sigmod 与 tahn 相比时，在大部分情况下我们应该优先使用 tahn 函数；除了在最终的输出层，因为输出层我们需要得到的是 0~1 范围内的概率表示。譬如在上面介绍的浅层神经网络中，我们就可以使用 Sigmod 作为隐层的激活函数，而使用 tahn 作为输出层的激活函数。&lt;/p>
&lt;p>不过 Sigmod 与 tahn 同样都存在在极大值或者极小值处梯度较小、收敛缓慢的问题。并且采用 Sigmoid 等函数，算激活函数时(指数运算)，计算量大，反向传播求误差梯度时，求导涉及除法，计算量相对大；而采用 ReLU(rectified linear unit) 激活函数，整个过程的计算量节省很多。此外，ReLU 会使一部分神经元的输出为 0，这样就造成了网络的稀疏性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://raw.githubusercontent.com/wx-chevalier/OSS/master/2017/8/1/activation_function.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="为什么需要非线性激活函数">为什么需要非线性激活函数？&lt;/h2>
&lt;p>如果我们选择了所谓的 Identity Activation Function，即直接将输入值作为输出返回，这样我们最终的输出值也就自然与输入值存在线性相关性。&lt;/p>
&lt;h2 id="激活函数的导数">激活函数的导数&lt;/h2>
&lt;h1 id="gradient-descent--梯度下降">Gradient Descent | 梯度下降&lt;/h1>
&lt;h2 id="反向传播的直观解释">反向传播的直观解释&lt;/h2>
&lt;h2 id="随机初始化">随机初始化&lt;/h2>
&lt;h1 id="延伸阅读">延伸阅读&lt;/h1></description></item><item><title>神经网络、有监督学习与深度学习</title><link>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/</guid><description>&lt;h1 id="神经网络有监督学习与深度学习">神经网络、有监督学习与深度学习&lt;/h1>
&lt;p>深度学习正在逐步地改变世界，从网络搜索、广告推荐这样传统的互联网业务到健康医疗、自动驾驶等不同的行业领域。百年前的电气革命为社会带来了新的支柱产业，而如今 AI 正是新时代的电力基础，驱动社会技术的快速发展。本课程的第一部分，关注于如何构建包括深度神经网络在内的神经网络以及如何利用数据训练该网络，课程的最后将会构建出可识别动物的深度神经网络。本课程的第二部分将会讲解更多有关于深度学习实践的相关资料，包括超参数调优、正则化、如何从 Momentum Armrest Prop 以及 AD Authorization 等算法中选择合适的优化算法。第三部分将会学习如何自定义机器学习项目，如何进行数据预处理、将数据应用到模型训练、提取交叉校验的训练集与验证集等内容。第四部分将会着眼于卷积神经网络 CNN，如何构建经典的 CNN 模型。在最后的第五部分，将会学习构建序列模型(Seq2Seq 等)以解决自然语言处理相关的任务；典型的序列模型包括了 RNNs、LSTM 等。&lt;/p>
&lt;h1 id="神经网络">神经网络&lt;/h1>
&lt;p>深度学习往往关联于大规模神经网络的训练，本章我们即来了解下何谓神经网络。以经典的房屋价格预测为例，假设我们拥有六组房屋数据，每组包含了房屋的面积以及价格；我们希望寻找合适的函数来根据房屋的尺寸预测房屋价格。如果我们采用线性回归来解决这个问题，我们会画出一条 &lt;code>y = kx + b&lt;/code> 这样的函数线，其形式如下黑色线所示：&lt;/p>
&lt;p>我们知道房屋的价格不可能为负数，因此我们可以将使用 ReLU(Rectified Linear Unit)函数来描述尺寸与价格之间的关系，如上图蓝色线型所示。我们可以将该问题抽象为输入为房间的尺寸 x，输出为房间的价格 y，而某个神经元即为接受输入并且进行合适的运算之后输出目标值的函数：&lt;/p>
&lt;p>如上图所示即是最简单的单元神经网络，而复杂的神经网络即是由无数的神经元分层连接堆叠而成。譬如实际的房屋价格会由尺寸、卧室数目、所属区域(Zip Code)以及社区的富裕程度影响。我们理想的神经网络即是能够自动帮我们构建隐层(Hidden Units)，即输入单元之间的关系以进行最好地预测：&lt;/p>
&lt;p>给定输入之后，神经网络的任务之一即是为我们自动构建隐层；每个隐层单元都会输入输入层的全部特征作为输入值。&lt;/p>
&lt;h1 id="有监督学习">有监督学习&lt;/h1>
&lt;p>神经网络的分类很多，不过截止到目前大多数的有价值的神经网络都还是基于机器学习中所谓的有监督学习(Supervised Learning)。在有监督学习中，我们的训练数据集中已知了特征与结果输出之间的对应关系，而目标就是寻找正确的输入与输出之间的关系表示。譬如目前最赚钱的深度学习应用之一，在线广告中就是输入有关于网站展示的信息以及部分用户的信息，神经网络会预测用户是否会点击该广告；通过为不同的用户展示他们最感兴趣的广告，来增加用户的实际点击率。下表即列举了几种常见的领域应用与它们的输入输出：&lt;/p>
&lt;p>计算机视觉近年来也发展迅速，典型的应用之一即是图片标注；我们可能随机输入一张图片来寻找与它最相近的图片。语音识别则是能够将用户输入的语音数据转化为文字表述；机器翻译则是能将不同语言间的语句进行自由转化，譬如将某个英文段落转化为对应的中文表述。而在自动驾驶中，我们可能会输入某张从雷达中获取的车前图片，来判断路上其他车的相对位置。而对于这些不同的行业领域我们也需要应用不同类型的神经网络，譬如对上文提及的房价预测，我们就可以使用标准的神经网络；而对于图片应用则会优先考虑使用卷积神经网络(CNN)。&lt;/p>
&lt;p>而对于序列数据，譬如随时间播放的音频流，其可以表示为一维的时间序列，我们通常会使用 RNN 来处理这个类型的数据。而在文本处理中，我们也常常将文本表示为字符序列，因此也往往会使用 RNN 来处理这个类型的数据。对于自动驾驶这样更为复杂的应用，我们可能会需要同时处理图片、文本等多种类别的数据，因此会使用混合网络架构。&lt;/p>
&lt;p>模型训练中我们常见的另一组概念就是结构化数据与非结构化数据，结构化数据有点类似于关系型数据库中存放的数据；譬如上面介绍的房屋价格预测中，我们会拥有包含了尺寸、卧室数等列的数据表，这种形式的就是所谓结构化数据。结构化数据中每个特征，譬如房间尺寸、卧室数目、用户年龄等都有可解释的意义；而非结构化数据的典型代表，语音、文本或者图片，往往会使用像素值或者单个词作为特征向量的组成，这些特征值往往很难有实际意义的解释。人类经过长时间的进化之后能够较好地辨析非结构化数据，而利用深度学习技术，现在机器也在不断提升自身对于非结构化数据的辨析能力。&lt;/p>
&lt;h1 id="深度学习">深度学习&lt;/h1>
&lt;p>深度学习背后的理论基础与技术概念已经出现了有数十年，本部分我们即来讨论为何直到近些年深度学习才得到了爆炸性的发展。我们可以用下图来描述数据集的大小与算法性能(准确率、推准率等)之间的关系：&lt;/p>
&lt;p>对于支持向量机、Logistics 回归这样经典的机器学习算法而言，在数据量从零递增的初始阶段，其性能会不断提升；不过很快就会触碰到天花板，此时性能很难再随着数据集的增长而提升。而伴随着移动互联网时代的到来，我们能够从网站、移动应用或者其他安装在电子终端设备上的传感器中获取到海量的数据；这些数据在开启大数据时代的同时也为深度学习的发展提供了坚实的基础。我们在上图中也可以看出，越是大型的神经网络随着数据量的增加，其性能提升的越快，并且其性能天花板也是越高。&lt;/p>
&lt;p>深度学习崛起的另一个重要基石就是计算能力的提升，这里不仅指新一代 CPU 或者 GPU 设备，还有是在许多基础优化算法上的革新，都使得我们能够更快地训练出神经网络。譬如早期我们会使用 Sigmod 函数作为神经网络的激活函数，随着 x 的增大其梯度会逐渐趋近于零，这就导致了模型收敛变得相对缓慢；而 ReLU 则能较好地避免这个问题，其在正无穷大时梯度值依然保持恒定。简单地从 Sigmod 函数迁移到 ReLU 即能够为模型训练带来极大的效率提升，这也方便了我们构建出更复杂的神经网络。&lt;/p></description></item><item><title>梯度下降与向量化操作</title><link>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E5%90%91%E9%87%8F%E5%8C%96%E6%93%8D%E4%BD%9C/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/ai-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E5%90%91%E9%87%8F%E5%8C%96%E6%93%8D%E4%BD%9C/</guid><description>&lt;h1 id="梯度下降与向量化操作">梯度下降与向量化操作&lt;/h1>
&lt;p>我们在前文&lt;a href="https://zhuanlan.zhihu.com/p/28530027" target="_blank" rel="noopener">二元分类与 Logistic 回归&lt;/a>中建立了 Logistic 回归的预测公式：&lt;/p>
&lt;p>$$
\hat{y} = \sigma(w^Tx + b), , \sigma(z) = \frac{1}{1+e^{-z}}
$$&lt;/p>
&lt;p>整个训练集的损失函数为：&lt;/p>
&lt;p>$$
J(w,b) =
\frac{1}{m}\sum*{i=1}^mL(\hat{y}^{(i)} - y^{(i)}) = \
-\frac{1}{m} \sum*{i=1}^m [y^{(i)}log\hat{y}^{(i)} + (1-y^{(i)})log(1-\hat{y}^{(i)})]
$$&lt;/p>
&lt;p>模型的训练目标即是寻找合适的 $w$ 与 $b$ 以最小化代价函数值；简单起见我们先假设 $w$ 与 $b$ 都是一维实数，那么可以得到如下的 $J$ 关于 $w$ 与 $b$ 的图：&lt;/p>
&lt;p>上图所示的函数 $J$ 即是典型的凸函数，与非凸函数的区别在于其不含有多个局部最低点；选择这样的代价函数就保证了无论我们初始化模型参数如何，都能够寻找到合适的最优解。如果我们仅考虑对于 $w$ 参数进行更新，则可以得到如下的一维图形：&lt;/p>
&lt;p>参数 $w$ 的更新公式为：&lt;/p>
&lt;p>$$
w := w - \alpha \frac{dJ(w)}{dw}
$$&lt;/p>
&lt;p>其中 $\alpha$ 表示学习速率，即每次更新的 $w$ 的步伐长度；当 $w$ 大于最优解 $w&amp;rsquo;$ 时，导数大于 0；即 $\frac{dJ(w)}{dw}$ 的值大于 0，那么 $w$ 就会向更小的方向更新。反之当 $w$ 小于最优解 $w&amp;rsquo;$ 时，导数小于 0，那么 $w$ 就会向更大的方向更新。&lt;/p>
&lt;h1 id="导数">导数&lt;/h1>
&lt;p>本部分是对于微积分中导数(Derivative)相关理论进行简单讲解，熟悉的同学可以跳过。&lt;/p>
&lt;p>上图中，$a = 2$ 时，$f(a) = 6$；$a = 2.001$ 时，$f(a) = 6.003$，导数为 $\frac{6.003 - 6}{2.001 - 2} = 3$；在某个直线型函数中，其导数值是恒定不变的。我们继续考虑下述二次函数：&lt;/p>
&lt;p>上图中，$a = 2$ 时，$f(a) = 4$；$a = 2.001$ 时，$f(a) \approx 4.004$，此处的导数即为 4。而当 $a = 5$ 时，此处的导数为 10；可以发现二次函数的导数值随着 $x$ 值的变化而变化。下表列举出了常见的导数：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="http://durofy.com/wp-content/uploads/2012/10/basic_derivatives.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>下表列举了常用的导数复合运算公式：&lt;/p>
&lt;h1 id="计算图computation-graph">计算图(Computation Graph)&lt;/h1>
&lt;p>神经网络中的计算即是由多个计算网络输出的前向传播与计算梯度的后向传播构成，我们可以将复杂的代价计算函数切割为多个子过程：&lt;/p>
&lt;p>$$
J(a, b, c) = 3 \times (a + bc)
$$&lt;/p>
&lt;p>定义 $u = bc$ 以及 $v = a + u$ 和 $J = 3v$，那么整个计算图可以定义如下：&lt;/p>
&lt;p>根据导数计算公式，我们可知：&lt;/p>
&lt;p>$$
\frac{dJ}{dv} = 3, ,
\frac{dJ}{da} = \frac{dJ}{dv} \frac{dv}{da} = 3
$$&lt;/p>
&lt;p>在复杂的计算图中，我们往往需要经过大量的中间计算才能得到最终输出值与原始参数的导数 $dvar = \frac{dFinalOutputVar}{d{var}&amp;rsquo;}$，这里的 $dvar$ 即表示了最终输出值相对于任意中间变量的导数。而所谓的反向传播(Back Propagation)即是当我们需要计算最终值相对于某个特征变量的导数时，我们需要利用计算图中上一步的结点定义。&lt;/p>
&lt;h1 id="logistic-回归中的导数计算">Logistic 回归中的导数计算&lt;/h1>
&lt;p>我们在上文中讨论过 Logistic 回归的损失函数计算公式为：&lt;/p>
&lt;p>$$
z = w^Tx + b \
\hat{y} = a = \sigma(z) \
L(a,y) = -( ylog(a) + (1-y)log(1-a) )
$$&lt;/p>
&lt;p>这里我们假设输入的特征向量维度为 2，即输入参数共有 $x_1, w_1, x_2, w_2, b$ 这五个；可以推导出如下的计算图：&lt;/p>
&lt;p>首先我们反向求出 $L$ 对于 $a$ 的导数：&lt;/p>
&lt;p>$$
da = \frac{dL(a,y)}{da} = -\frac{y}{a} + \frac{1-y}{1-a}
$$&lt;/p>
&lt;p>然后继续反向求出 $L$ 对于 $z$ 的导数：&lt;/p>
&lt;p>$$
dz = \frac{dL}{dz}
=\frac{dL(a,y)}{dz}
= \frac{dL}{da} \frac{da}{dz}
= a-y
$$&lt;/p>
&lt;p>依次类推求出最终的损失函数相较于原始参数的导数之后，根据如下公式进行参数更新：&lt;/p>
&lt;p>$$
w_1 := w_1 - \alpha dw_1 \
w_2 := w_2 - \alpha dw_2 \&lt;/p>
&lt;p>b := b - \alpha db
$$&lt;/p>
&lt;p>接下来我们需要将对于单个用例的损失函数扩展到整个训练集的代价函数：&lt;/p>
&lt;p>$$
J(w,b) = \frac{1}{m} \sum*{i=1}^m L(a^{(i)},y) \
a^{(i)} = \hat{y}^{(i)} = \sigma(z^{(i)}) = \sigma(w^Tx^{(i)} + b)
$$&lt;/p>
&lt;p>我们可以对于某个权重参数 $w_1$，其导数计算为：&lt;/p>
&lt;p>$$
\frac{\partial J(w,b)}{\partial w_1} = \frac{1}{m} \sum*{i=1}^m \frac{\partial}{\partial w_1}L(a^{(i)},y^{(i)})
$$&lt;/p>
&lt;p>完整的 Logistic 回归中某次训练的流程如下，这里仅假设特征向量的维度为 2：&lt;/p>
&lt;h1 id="向量化操作">向量化操作&lt;/h1>
&lt;p>在上述的 $m$ 个训练用例的 Logistic 回归中，每次训练我们需要进行两层循环，外层循环遍历所有的特征，内层循环遍历所有的训练用例；如果特征向量的维度或者训练用例非常多时，多层循环无疑会大大降低运行效率，因此我们使用向量化(Vectorization)操作来进行实际的训练。我们首先来讨论下何谓向量化操作。在 Logistic 回归中，我们需要计算 $z = w^Tx + b$，如果是非向量化的循环方式操作，我们可能会写出如下的代码：&lt;/p>
&lt;pre tabindex="0">&lt;code>z = 0;
for i in range(n_x):
z += w[i] * x[i]
z += b
&lt;/code>&lt;/pre>&lt;p>而如果是向量化的操作，我们的代码则会简洁很多：&lt;/p>
&lt;pre tabindex="0">&lt;code>z = np.dot(w, x) + b
&lt;/code>&lt;/pre>&lt;p>在&lt;a href="https://parg.co/bjz" target="_blank" rel="noopener">未来的章节&lt;/a>中我们会实际比较循环操作与向量化操作二者的性能差异，可以发现向量化操作能够带来近百倍的性能提升；目前无论 GPU 还是 CPU 环境都内置了并行指令集，SIMD(Single Instruction Multiple Data)，因此无论何时我们都应该尽可能避免使用显式的循环。Numpy 还为我们提供了很多便捷的向量转化操作，譬如 &lt;code>np.exp(v)&lt;/code> 用于进行指数计算，&lt;code>np.log(v)&lt;/code> 用于进行对数计算，&lt;code>np.abs(v)&lt;/code> 用于进行绝对值计算。&lt;/p>
&lt;p>下面我们将上述的 Logistic 回归流程转化为向量化操作，其中输入数据可以变为 $n*x \times m$ 的矩阵，即共有 $m$ 个训练用例，每个用例的维度为 $n_x$：&lt;/p>
&lt;p>$$
Z = np.dot(W^TX) + b \
A = [a^{(1)},a^{(2)},&amp;hellip;,a^{(m)}] = \sigma(z)
$$&lt;/p>
&lt;p>我们可以得到各个变量梯度计算公式为：&lt;/p>
&lt;p>$$
dZ = A - Y = [a^{(1)} y^{(1)}&amp;hellip;] \
db = \frac{1}{m}\sum*{i=1}^mdz^{(i)}=\frac{1}{m}np.sum(dZ) \
dW = \frac{1}{m} X dZ^{T}=
\frac{1}{m}
\begin{bmatrix}
\vdots \&lt;/p>
&lt;p>x^{(i1)} &amp;hellip; x^{(im)} \&lt;/p>
&lt;p>\vdots \
\end{bmatrix}
\begin{bmatrix}
\vdots \&lt;/p>
&lt;p>dz^{(i)} \&lt;/p>
&lt;p>\vdots \
\end{bmatrix} \
= \frac{1}{m}
\begin{bmatrix}
\vdots \&lt;/p>
&lt;p>x^{(1)}dz^{(1)} + &amp;hellip; + x^{(m)}dz^{(m)} \&lt;/p>
&lt;p>\vdots \
\end{bmatrix} \
$$&lt;/p>
&lt;h1 id="延伸阅读">延伸阅读&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/25612011" target="_blank" rel="noopener">机器学习、深度学习与自然语言处理领域推荐的书籍列表&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/28488349" target="_blank" rel="noopener">Andrew NG 深度学习课程笔记：神经网络、有监督学习与深度学习&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/26249110" target="_blank" rel="noopener">基于 Python 的简单自然语言处理实践&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul></description></item></channel></rss>