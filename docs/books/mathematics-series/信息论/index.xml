<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>信息论 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/mathematics-series/%E4%BF%A1%E6%81%AF%E8%AE%BA/</link><atom:link href="https://ng-tech.icu/books/mathematics-series/%E4%BF%A1%E6%81%AF%E8%AE%BA/index.xml" rel="self" type="application/rss+xml"/><description>信息论</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>信息论</title><link>https://ng-tech.icu/books/mathematics-series/%E4%BF%A1%E6%81%AF%E8%AE%BA/</link></image><item><title>KL 散度</title><link>https://ng-tech.icu/books/mathematics-series/%E4%BF%A1%E6%81%AF%E8%AE%BA/kl-%E6%95%A3%E5%BA%A6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/mathematics-series/%E4%BF%A1%E6%81%AF%E8%AE%BA/kl-%E6%95%A3%E5%BA%A6/</guid><description>&lt;h1 id="kl-散度--相对熵">KL 散度 &amp;amp; 相对熵&lt;/h1>
&lt;p>相对熵又称互熵，交叉熵，鉴别信息，Kullback 熵，Kullback-Leible 散度(即 KL 散度)等。设$p(x)$和$q(x)$是$x$取值的两个概率概率分布，则$p$对$q$的相对熵为&lt;/p>
&lt;p>$$
D(p||q) = \sum_{i=1}^{n}p(x_i)log\frac{p(x_i)}{q(x_i)}
$$&lt;/p>
&lt;p>在一定程度上，熵可以度量两个随机变量的距离。KL 散度是两个概率分布 P 和 Q 差别的非对称性的度量。KL 散度是用来度量使用基于 Q 的编码来编码来自 P 的样本平均所需的额外的位元数。典型情况下，P 表示数据的真实分布，Q 表示数据的理论分布，模型分布，或 P 的近似分布。
相对熵(KL 散度)有两个主要的性质。如下
(1)尽管 KL 散度从直观上是个度量或距离函数，但它并不是一个真正的度量或者距离，因为它不具有对称性，即$$D(p||q) \neq D(q||p)$$
(2)相对熵的值为非负值，即&lt;/p>
&lt;p>$$
D(p||q) &amp;gt; 0
$$&lt;/p>
&lt;h1 id="相对熵的应用">相对熵的应用&lt;/h1>
&lt;p>相对熵可以衡量两个随机分布之间的距离，当两个随机分布相同时，它们的相对熵为零，当两个随机分布的差别增大时，它们的相对熵也会增大。所以相对熵(KL 散度)可以用于比较文本的相似度，先统计出词的频率，然后计算 KL 散度就行了。另外，在多指标系统评估中，指标权重分配是一个重点和难点，通过相对熵可以处理。&lt;/p>
&lt;h1 id="互信息">互信息&lt;/h1>
&lt;p>两个随机变量$X$，$Y$的互信息，定义为$X$，$Y$的联合分布和独立分布乘积的相对熵。
$$I(X,Y)=D(P(X,Y)||P(X)P(Y))$$
$$I(X,Y)=\sum_{x,y}log\frac{p(x,y)}{p(x)p(y)}$$&lt;/p>
&lt;h1 id="信息增益">信息增益&lt;/h1>
&lt;p>信息增益表示得知特征 A 的信息而使得类$X$的信息的不确定性减少的程度。信息增益的定义为特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D|A)$之差：&lt;/p>
&lt;p>$$g(D,A) = H(D) - H(D|A)$$&lt;/p></description></item><item><title>熵</title><link>https://ng-tech.icu/books/mathematics-series/%E4%BF%A1%E6%81%AF%E8%AE%BA/%E7%86%B5/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/mathematics-series/%E4%BF%A1%E6%81%AF%E8%AE%BA/%E7%86%B5/</guid><description>&lt;h1 id="熵">熵&lt;/h1>
&lt;p>信息熵反应了一个系统的有序化程度，一个系统越是有序，那么它的信息熵就越低，反之就越高。对于事件 $X=x$，定义自信息 Self-Information 为：$I(x)=-\log P(x)$。自信息仅仅处理单个输出，而熵就是为自信息的期望：&lt;/p>
&lt;p>$$
H(X)=\mathbb{E}&lt;em>{X \sim P(X)}[I(x)]=-\mathbb{E}&lt;/em>{X \sim P(X)}[\log P(x)]
$$&lt;/p>
&lt;p>熵一般记作 $H(P)$。熵刻画了按照真实分布 来识别一个样本所需要的编码长度的期望（即平均编码长度），譬如含有 4 个字母 &lt;code>(A,B,C,D)&lt;/code> 的样本集中，真实分布 $P=\left(\frac{1}{2}, \frac{1}{2}, 0,0\right)$，则只需要 1 位编码即可识别样本。对于离散型随机变量 $X$，假设其取值集合大小为 $K$，则可以证明：$0 \leq H(X) \leq \log K$。&lt;/p>
&lt;h1 id="条件熵">条件熵&lt;/h1>
&lt;p>对于随机变量 $Y$ 和 $X$，条件熵 $H(Y|X)$ 表示：已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。条件熵的定义为：$X$ 给定条件下 $Y$ 的条件概率分布的熵对 $X$ 的期望：&lt;/p>
&lt;p>$$
H(Y | X)=\mathbb{E}&lt;em>{X \sim P(X)}[H(Y | X=x)]=-\mathbb{E}&lt;/em>{(X, Y) \sim P(X, Y)} \log P(Y | X)
$$&lt;/p>
&lt;p>对于离散型随机变量，存在：&lt;/p>
&lt;p>$$
H(Y | X)=\sum_{x} p(x) H(Y | X=x)=-\sum_{x} \sum_{y} p(x, y) \log p(y | x)
$$&lt;/p>
&lt;p>对于连续型随机变量，则存在：&lt;/p>
&lt;p>$$
H(Y | X)=\int p(x) H(Y | X=x) d x=-\iint p(x, y) \log p(y | x) d x d y
$$&lt;/p>
&lt;p>根据定义可以证明：&lt;/p>
&lt;p>$$
H(X, Y)=H(Y | X)+H(X)
$$&lt;/p>
&lt;p>即：描述 $X$ 和 $Y$ 所需要的信息是：描述 $X$ 所需要的信息加上给定 $X$ 条件下描述 $Y$ 所需的额外信息。&lt;/p>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://www.zhihu.com/question/65288314/answer/244557337" target="_blank" rel="noopener">https://www.zhihu.com/question/65288314/answer/244557337&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>