<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>贝叶斯理论 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/</link><atom:link href="https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/index.xml" rel="self" type="application/rss+xml"/><description>贝叶斯理论</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>贝叶斯理论</title><link>https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/</link></image><item><title>贝叶斯推导</title><link>https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E5%AF%BC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E5%AF%BC/</guid><description>&lt;h1 id="bayesian-inference">Bayesian Inference&lt;/h1>
&lt;p>Let X1X1 be the vector of observable random variables. Let X2X2 be the vector of latent random variables. Let ΘΘ be the vector of parameters. f(x2,θ|x1)=f(x1|x2,θ)f(x2|θ)f(θ)f(x1)&lt;/p>
&lt;h2 id="极大似然估计-maximum-likelihood-estimation">极大似然估计 Maximum Likelihood Estimation&lt;/h2>
&lt;p>假设有一堆独立同分布数据 X1,…,Xn，其 PDF 为 p(x;θ)，其中 θ 为模型参数，则其似然函数为: $$L&lt;em>n(\theta)=\prod\limits&lt;/em>{i=1}^n p(X&lt;em>i; \theta)$$ 而极大似然估计就是要找到参数 $\theta$，使得似然函数的值最大。这意思就是找到一个参数 $\theta$，使得使用分布 p(x;θ) 来估计这一堆数据 Xi 的效果最好。为啥捏，因为假设 X 都是离散值的情况下，Ln(Xi;θ) 表达的含义是：从参数 θ 通过模型 p(x;θ) 产生这一堆数据的概率(把所有单个数据产生的概率乘起来就是产生这一堆数据的概率)。所以 p(x;θ)=Pθ(x={Xi})，那么如果当有两个参数 θ1 和 θ2 时，$P&lt;/em>{\theta&lt;em>1}(x={X_i})&amp;gt;P&lt;/em>{\theta&lt;em>2}(x={X_i})$，则说明 θ1 更好的描述了这组数据，因此要找到一个 θ 使得整似然函数的值最大！所以只要将似然函数对 θ 求导，就可以找到这样的 θ。例子：求 N 次伯努利分布的最大似然估计: $$ Bern(x|\mu)=\mu^x(1-\mu)^{1-x} \ L(X|\mu)=\prod\limits&lt;/em>{i=1}^N \mu^{X*i}(1-\mu)^{1-X_i}=\mu^S(1-\mu)^{N-S} 其中 S=\sum\limits*{i=1}^N X_i $$ 将 $log L(X|\mu)$ 对 $\mu$ 求导得到 $\frac{S}{\mu}-\frac{N-S}{1-\mu}=0$，最终得到 : $$ \hat{\mu}_N=\frac{1}{N} S=\bar{X}_N $$&lt;/p>
&lt;h2 id="maximum-a-posterior-estimation--极大后验估计">Maximum A Posterior Estimation | 极大后验估计&lt;/h2>
&lt;p>极大后验估计中加入了一些先验知识，它最大化的是一个后验函数。具体来说，因为贝叶斯定律:&lt;/p>
&lt;p>$$
p(\theta|x)=\frac{p(x|\theta)p(\theta)}{p(x)}
$$&lt;/p>
&lt;p>那么极大后验估计就是要求&lt;/p>
&lt;p>$$
\hat{\theta}&lt;em>{MAP}=\underset{\theta}{argmax}~ p(x|\theta)p(\theta)=\underset{\theta}{argmax}{\sum\limits&lt;/em>{X_i} log ~p(X_i|\theta) + log~p(\theta)}
$$&lt;/p>
&lt;p>可见，极大后验估计中相对于最大似然估计，多了 log p(θ)，也就是先验的影响。&lt;/p>
&lt;h2 id="贝叶斯推断-bayesian-inference">贝叶斯推断 :Bayesian Inference&lt;/h2>
&lt;p>前面的 MAP 是一个点估计，只估计似然函数达到最大点的情况下，参数 θ 的值。而贝叶斯推断通过假设关于参数 $\theta$ 的一个分布而不是直接求单个值来扩展了 MAP 算法。&lt;/p>
&lt;blockquote>
&lt;p>Bayesian inference extends the MAP approach by allowing a distribution over the parameter set θ instead of making a direct estimate. Not only encodes this the maximum(a posteriori) value of the data-generated parameters, but it also incorporates expectation as another parameter estimate as well as variance information as a measure of estimation quality or confidence.&lt;/p>
&lt;/blockquote>
&lt;p>具体来说，给定数据 X 和需要求的参数 θ，贝叶斯推断需要求出一个具体的分布: $$ p(\theta|X)=P(X|\theta)P(\theta)/P(X) $$&lt;/p>
&lt;ul>
&lt;li>$X$: 观测数据&lt;/li>
&lt;li>$\theta$: 潜变量&lt;/li>
&lt;/ul>
&lt;p>这里和 MAP 的区别就在于，MAP 忽略了 P(X) 因为它是常量，对于 MAP 的过程：求导后再求等于 0 来获得最好的 θ，这个常量是没有用的。但是贝叶斯推断要的是整个 p(θ|X) 的分布，所以 P(X) 这个 normalisation term 是需要被求出来的。在获得具体的分布之后，所要求的参数值可以通过估计期望或方差得到。&lt;/p></description></item><item><title>变分贝叶斯推导</title><link>https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/%E5%8F%98%E5%88%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E5%AF%BC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/%E5%8F%98%E5%88%86%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%8E%A8%E5%AF%BC/</guid><description>&lt;h1 id="variation-bayesian-inference--变分贝叶斯推导">Variation Bayesian Inference | 变分贝叶斯推导&lt;/h1>
&lt;p>变分贝叶斯方法最早由 Matthew J.Beal 在他的博士论文《Variational Algorithms for Approximate Bayesian Inference 》中提出，作者将其应用于隐马尔科夫模型，混合因子分析，非线性动力学，图模型等。变分贝叶斯是一类用于贝叶斯估计和机器学习领域中近似计算复杂(intractable )积分的技术。它主要应用于复杂的统计模型中，这种模型一般包括三类变量：观测变量 (observed variables, data)，未知参数(parameters )和潜变量(latent variables )。在贝叶斯推断中，参数和潜变量统称为不可观测变量 (unobserved variables)。变分贝叶斯方法主要是两个目的&lt;/p>
&lt;ul>
&lt;li>近似不可观测变量的后验概率，以便通过这些变量作出统计推断。&lt;/li>
&lt;li>对一个特定的模型，给出观测变量的边缘似然函数(或称为证据，evidence )的下界。主要用于模型的选择，认为模型的边缘似然值越高，则模型对数据拟合程度越好，该模型产生 Data 的概率也越高。&lt;/li>
&lt;/ul>
&lt;p>对于第一个目的，蒙特卡洛模拟，特别是用 Gibbs 取样的 MCMC 方法，可以近似计算复杂的后验分布，能很好地应用到贝叶斯统计推断。此方法通过大量的样本估计真实的后验，因而近似结果带有一定的随机性。与此不同的是，变分贝叶斯方法提供一种局部最优，但具有确定解的近似后验方法。从某种角度看，变分贝叶斯可以看做是 EM 算法的扩展，因为它也是采用极大后验估计 (MAP)，即用单个最有可能的参数值来代替完全贝叶斯估计。另外，变分贝叶斯也通过一组相互依然(mutually dependent )的等式进行不断的迭代来获得最优解。&lt;/p></description></item><item><title>朴素贝叶斯</title><link>https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/mathematics-series/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%90%86%E8%AE%BA/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</guid><description>&lt;h1 id="likelihood似然">Likelihood：似然&lt;/h1>
&lt;p>我们首先来讨论下为什么当我们观测到 $D={16,8,2,64}$ 时更倾向于认为假设空间是所有 2 的方幂值的集合，而不是笃定假设空间是所有偶数的集合。虽然两个假设空间都符合我们的观测结果，但是归纳的过程中我们会尽量避免可疑的巧合(Suspicious Coincidences )。如果我们认为假设空间是所有偶数的集合，那么又该如何说服自己这些数字都是 2 的方幂值呢？为了更方便的形式化讨论这个现象，我们假设从某个假设空间中随机取值的概率分布为均匀分布，可以推导出从假设空间中进行 N 次取值得到观测集合的概率为: $$ p(D|h) = [\frac{1}{size(h)}]^N = [\frac{1}{|h|}]^N $$ 对于这个等式最形象化的解释就是奥卡姆剃刀原则(Occam’s razor )，我们倾向于选择符合观测值的最小 / 最简的假设空间。在 $D={16}$ 的情况下，如果假设空间为 2 的方幂值，则仅有 6 个符合条件的数字，推导出 $p(D|h*{two}) = 1/6$。而如果是所有的偶数集合，$p(D|h*{even}) = 1/50$。显而易见 $h&lt;em>two &amp;gt; h_even$，如果观测序列中有 4 个数值，则 $h&lt;/em>{two} = (1/6)^4 = 7.7 * 10^{-4}$，然而 $h_{even} = (1/50)^4 = 1.6*10^{-7}$，不同的假设空间的概率值差异越发的大了。因此我们会认为 $D = {16，8，2，64}$ 这个观测序列是来自于 2 的方幂值这个假设空间而不是所有的偶数集合这个假设空间。&lt;/p>
&lt;h1 id="prior先验">Prior：先验&lt;/h1>
&lt;p>前一节我们讨论了所谓似然的概念，当观测到 $D = {16，8，2，64}$ 时我们会倾向于认为其采样于 2 的方幂值这个集合，不过为啥不是 $h&amp;rsquo; = 除了 32 之外的 2 的方幂值 $ 这个似然概率更大的集合呢？直观来看就是 $h&amp;rsquo; = 除了 32 之外的 2 的方幂值 $ 这个假设与常规思维不符，而对于这样奇特的思维我们可以赋予其较低的先验概率值来降低其最终得到的后验概率。总计而言，贝叶斯理论中概率并不需要频率解释，先验分布也可以称为主观概率，是根据经验对随机现象的发生可能性的一种看法或者信念。统计学家萨维奇曾给出过一个著名的女士品茶的例子：一位常喝牛奶加茶的女士说她可以分辨在杯中先加入的是茶还是奶。连续做了十次实验，她都说对了。显然这来自于她的经验而非猜测。我们在日常生活中也经常使用基于经验或者信念的主观的概率陈述。比如说，天气预报里说明天(8 月 3 日)降水概率 30%，就是关于 “ 明日降水 ” 这个事件的一种信念，因为作为 8 月 3 日的明天是不可重复的，自然也就没有频率意义。再比如说，医生认为对某位病人进行手术的成功可能性为 80%，也是根据自己的经验而具有的的信念，而非在这位病人身上反复进行试验的频率结果。把 θ 看做随机变量，进而提出先验分布，在许多情况下是合理的。比如工厂产品的合格率每一天都有波动，可以看做随机变量；明天的降水概率虽然是几乎不动的，但这是基于经验和规律提出来的概率陈述，也可以看做随机变量。尽管我们使用后验分布来进行推理，但先验分布的选取也是很重要的。常见的先验分布类型包括：&lt;/p>
&lt;ul>
&lt;li>无信息先验(Noninformative Priors ) 无信息先验只包含了参数的模糊的或者一般的信息，是对后验分布影响最小的先验分布。很多人愿意选取无信息先验，因为这种先验与其它 “ 主观 ” 的先验相比更接近 “ 客观 ”。通常，我们把均匀分布作为无信息先验来使用，这相当于在参数所有的可能值上边指派了相同的似然。但是无先验信息的使用也要慎重，比如有些情况下会导致不恰当的后验分布(如不可积分的后验概率密度)。&lt;/li>
&lt;li>Jeffreys 先验 (Jeffreys’ Prior) Jeffreys 提出的选取先验分布的原则是一种不变原理，采用 Fisher 信息阵的平方根作为 θ 的无信息先验分布。较好地解决了无信息先验中的一个矛盾，即若对参数 θ 选用均匀分布，则其函数 g(θ) 往往不是均匀分布。&lt;/li>
&lt;li>信息先验(Informative Priors ) 根据以前的经验、研究或专家经验得到的先验分布。&lt;/li>
&lt;li>共轭先验(Conjugate Priors ) 共轭先验是指先验分布和后验分布来自同一个分布族的情况，就是说先验和后验有相同的分布形式(当然，参数是不同的)。这些共轭先验是结合似然的形式推导出来的。共轭先验是经常被使用的一种先验分布形式，原因在于数学处理和计算上的方便性，同时后验分布的一些参数也可以有很好的解释。&lt;/li>
&lt;/ul>
&lt;h1 id="posterior后验">Posterior：后验&lt;/h1>
&lt;p>后验值即为似然乘以先验再进行归一化，对于这里的数字游戏: $$ p(h|D) = \frac{p(D|h)p(h)}{\sum*{h&amp;rsquo; \in H}p(D,h&amp;rsquo;)} = \frac {p(h)\amalg(D\in h) / |h|^N} {\sum*{h&amp;rsquo; \in H}p(h&amp;rsquo;)\amalg(D \in h&amp;rsquo;) / |h&amp;rsquo;|^N} $$ 其中 $\amalg(D\in h) $ 当且仅当 $D$ 中所有数据都属于假设空间 $h$ 时取 1，其他情况下取 0。。&lt;/p>
&lt;h1 id="共轭先验">共轭先验&lt;/h1></description></item></channel></rss>