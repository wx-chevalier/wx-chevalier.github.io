<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>bigdata | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/</link><atom:link href="https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/index.xml" rel="self" type="application/rss+xml"/><description>bigdata</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>bigdata</title><link>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/</link></image><item><title>cardinality-estimation</title><link>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/cardinality-estimation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/cardinality-estimation/</guid><description>&lt;p>如何计算数据流中不同元素的个数？例如，独立访客(Unique Visitor，简称UV)统计。这个问题称为基数估计(Cardinality Estimation)，也是一个很经典的题目。&lt;/p>
&lt;h3 id="方案1-hashset">方案1: HashSet&lt;/h3>
&lt;p>首先最容易想到的办法是用HashSet，每来一个元素，就往里面塞，HashSet的大小就所求答案。但是在大数据的场景下，HashSet在单机内存中存不下。&lt;/p>
&lt;h3 id="方案2-bitmap">方案2: bitmap&lt;/h3>
&lt;p>HashSet耗内存主要是由于存储了元素的真实值，可不可以不存储元素本身呢？bitmap就是这样一个方案，假设已经知道不同元素的个数的上限，即基数的最大值，设为N，则开一个长度为N的bit数组，地位跟HashSet一样。每个元素与bit数组的某一位一一对应，该位为1，表示此元素在集合中，为0表示不在集合中。那么bitmap中1的个数就是所求答案。&lt;/p>
&lt;p>这个方案的缺点是，bitmap的长度与实际的基数无关，而是与基数的上限有关。假如要计算上限为1亿的基数，则需要12.5MB的bitmap，十个网站就需要125M。关键在于，这个内存使用与集合元素数量无关，即使一个网站仅仅有一个1UV，也要为其分配12.5MB内存。该算法的空间复杂度是$$O(N_{max})$$。&lt;/p>
&lt;p>实际上目前还没有发现在大数据场景中准确计算基数的高效算法，因此在不追求绝对准确的情况下，使用近似算法算是一个不错的解决方案。&lt;/p>
&lt;h3 id="方案3-linear-counting">方案3: Linear Counting&lt;/h3>
&lt;p>Linear Counting的基本思路是：&lt;/p>
&lt;ul>
&lt;li>选择一个哈希函数h，其结果服从均匀分布&lt;/li>
&lt;li>开一个长度为m的bitmap，均初始化为0(m设为多大后面有讨论)&lt;/li>
&lt;li>数据流每来一个元素，计算其哈希值并对m取模，然后将该位置为1&lt;/li>
&lt;li>查询时，设bitmap中还有u个bit为0，则不同元素的总数近似为 $$-m\log\dfrac{u}{m}$$&lt;/li>
&lt;/ul>
&lt;p>在使用Linear Counting算法时，主要需要考虑的是bitmap长度&lt;code>m&lt;/code>。m主要由两个因素决定，基数大小以及容许的误差。假设基数大约为n，允许的误差为ϵ，则m需要满足如下约束，&lt;/p>
&lt;p>$$m &amp;gt; \dfrac{\epsilon^t-t-1}{(\epsilon t)^2}$$, 其中 $$t=\dfrac{n}{m}$$&lt;/p>
&lt;p>精度越高，需要的m越大。&lt;/p>
&lt;p>Linear Counting 与方案1中的bitmap很类似，只是改善了bitmap的内存占用，但并没有完全解决，例如一个网站只有一个UV，依然要为其分配m位的bitmap。该算法的空间复杂度与方案2一样，依然是$$O(N_{max})$$。&lt;/p>
&lt;h3 id="方案4-loglog-counting">方案4: LogLog Counting&lt;/h3>
&lt;p>LogLog Counting的算法流程：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>均匀随机化。选取一个哈希函数h应用于所有元素，然后对哈希后的值进行基数估计。哈希函数h必须满足如下条件，&lt;/p>
&lt;ol>
&lt;li>哈希碰撞可以忽略不计。哈希函数h要尽可能的减少冲突&lt;/li>
&lt;li>h的结果是均匀分布的。也就是说无论原始数据的分布如何，其哈希后的结果几乎服从均匀分布（完全服从均匀分布是不可能的，D. Knuth已经证明不可能通过一个哈希函数将一组不服从均匀分布的数据映射为绝对均匀分布，但是很多哈希函数可以生成几乎服从均匀分布的结果，这里我们忽略这种理论上的差异，认为哈希结果就是服从均匀分布）。&lt;/li>
&lt;li>哈希后的结果是固定长度的&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>
&lt;p>对于元素计算出哈希值，由于每个哈希值是等长的，令长度为L&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对每个哈希值，从高到低找到第一个1出现的位置，取这个位置的最大值，设为p，则基数约等于$$2^p$$&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>如果直接使用上面的单一估计量进行基数估计会由于偶然性而存在较大误差。因此，LLC采用了分桶平均的思想来降低误差。具体来说，就是将哈希空间平均分成m份，每份称之为一个桶（bucket）。对于每一个元素，其哈希值的前k比特作为桶编号，其中$$2^k=m$$，而后L-k个比特作为真正用于基数估计的比特串。桶编号相同的元素被分配到同一个桶，在进行基数估计时，首先计算每个桶内最大的第一个1的位置，设为&lt;code>p[i]&lt;/code>，然后对这m个值取平均后再进行估计，即基数的估计值为$$2^{\frac{1}{m}\Sigma_{i=0}^{m-1} p[i]}$$。这相当于做多次实验然后去平均值，可以有效降低因偶然因素带来的误差。&lt;/p>
&lt;p>LogLog Counting 的空间复杂度仅有$$O(\log_2(\log_2(N_{max})))$$，内存占用极少，这是它的优点。不过LLC也有自己的缺点，当基数不是很大的时候，误差比较大。&lt;/p>
&lt;p>关于该算法的数学证明，请阅读原始论文和参考资料里的链接，这里不再赘述。&lt;/p>
&lt;h3 id="方案5-hyperloglog-counting">方案5: HyperLogLog Counting&lt;/h3>
&lt;p>HyperLogLog Counting（以下简称HLLC）的基本思想是在LLC的基础上做改进，&lt;/p>
&lt;ul>
&lt;li>
&lt;p>第1个改进是使用调和平均数替代几何平均数，调和平均数可以有效抵抗离群值的扰。注意LLC是对各个桶取算术平均数，而算术平均数最终被应用到2的指数上，所以总体来看LLC取的是几何平均数。由于几何平均数对于离群值（例如0）特别敏感，因此当存在离群值时，LLC的偏差就会很大，这也从另一个角度解释了为什么基数n不太大时LLC的效果不太好。这是因为n较小时，可能存在较多空桶，这些特殊的离群值干扰了几何平均数的稳定性。使用调和平均数后，估计公式变为 $$\hat{n}=\frac{\alpha_mm^2}{\Sigma_{i=0}^{m-1} p[i]}$$，其中$$\alpha_m=(m\int_0^{\infty}(log_2(\frac{2+u}{1+u}))^mdu)^{-1}$$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>第2个改进是加入了分段偏差修正。具体来说，设e为基数的估计值，&lt;/p>
&lt;ul>
&lt;li>当 $$e \leq \frac{5}{2}m$$时，使用 Linear Counting&lt;/li>
&lt;li>当 $$\frac{5}{2}m&amp;lt;e\leq \frac{1}{30}2^{32}$$时，使用 HyperLogLog Counting&lt;/li>
&lt;li>当 $$e&amp;gt;\frac{1}{30}2^{32}$$时，修改估计公式为$$\hat{n}=-2^{32}\log(1-e/2^{32})$$&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>关于分段偏差修正的效果分析也可以在原论文中找到。&lt;/p>
&lt;h3 id="参考资料">参考资料&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-i.html" target="_blank" rel="noopener">解读Cardinality Estimation算法（第一部分：基本概念）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-ii.html" target="_blank" rel="noopener">解读Cardinality Estimation算法（第二部分：Linear Counting）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iii.html" target="_blank" rel="noopener">解读Cardinality Estimation算法（第三部分：LogLog Counting）&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://blog.codinglabs.org/articles/algorithms-for-cardinality-estimation-part-iv.html" target="_blank" rel="noopener">解读Cardinality Estimation算法（第四部分：HyperLogLog Counting及Adaptive Counting）&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>data-stream-sampling</title><link>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/data-stream-sampling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/data-stream-sampling/</guid><description>&lt;p>有一个无限的整数数据流，如何从中随机地抽取k个整数出来？&lt;/p>
&lt;p>这是一个经典的数据流采样问题，我们一步一步来分析。&lt;/p>
&lt;h3 id="当k1时">当k=1时&lt;/h3>
&lt;p>我们先考虑最简单的情况，k=1，即只需要随机抽取一个样本出来。抽样方法如下：&lt;/p>
&lt;ol>
&lt;li>当第一个整数到达时，保存该整数&lt;/li>
&lt;li>当第2个整数到达时，以1/2的概率使用该整数替换第1个整数，以1/2的概率丢弃改整数&lt;/li>
&lt;li>当第i个整数到达时，以$$\dfrac{1}{i}$$的概率使用第i个整数替换被选中的整数，以$$1-\dfrac{1}{i}$$的概率丢弃第i个整数&lt;/li>
&lt;/ol>
&lt;p>假设数据流目前已经流出共n个整数，这个方法能保证每个元素被选中的概率是$$\dfrac{1}{n}$$吗？用数学归纳法，证明如下：&lt;/p>
&lt;ol>
&lt;li>当n=1时，由于是第1个数，被选中的概率是100%，命题成立&lt;/li>
&lt;li>假设当n=m(m&amp;gt;=1)时，命题成立，即前m个数，每一个被选中的概率是 $$\dfrac{1}{m}$$&lt;/li>
&lt;li>当n=m+1时，第m+1个数被选中的概率是 $$\dfrac{1}{m+1}$$, 前m个数被选中的概率是$$\dfrac{1}{m} \cdot (1-\dfrac{1}{m+1})=\dfrac{1}{m+1}$$，命题依然成立&lt;/li>
&lt;/ol>
&lt;p>由1，2，3知n&amp;gt;=1时命题成立，证毕。&lt;/p>
&lt;h3 id="当k1时-1">当k&amp;gt;1时&lt;/h3>
&lt;p>当 k &amp;gt; 1，需要随机采样多个样本时，方法跟上面很类似，&lt;/p>
&lt;ol>
&lt;li>前k个整数到达时，全部保留，即被选中的概率是 100%，&lt;/li>
&lt;li>第i个整数到达时，以$$k/i$$的概率替换k个数中的某一个，以$$1-\dfrac{k}{i}$$的概率丢弃，保留k个数不变&lt;/li>
&lt;/ol>
&lt;p>假设数据流目前已经流出共N个整数，这个方法能保证每个元素被选中的概率是$$\dfrac{k}{N}$$吗？用数学归纳法，证明如下：&lt;/p>
&lt;ol>
&lt;li>当n=m(m&amp;lt;=k)时，被选中的概率是100%，命题成立&lt;/li>
&lt;li>假设当n=m(m&amp;gt;k)时，命题成立，即前m个数，每一个被选中的概率是 $$\dfrac{1}{m}$$&lt;/li>
&lt;li>当n=m+1时，第m+1个数被选中的概率是 $$\dfrac{k}{m+1}$$, 前m个数被选中的概率是$$\dfrac{1}{m} \cdot [\dfrac{k}{m+1} \cdot (1-\dfrac{1}{k})+1-\dfrac{k}{m+1}]=\dfrac{1}{m+1}$$，命题依然成立&lt;/li>
&lt;/ol>
&lt;p>由1，2，3知n&amp;gt;=1时命题成立，证毕。&lt;/p>
&lt;h3 id="参考资料">参考资料&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://rosona.github.io/post/20151223/" target="_blank" rel="noopener">浅谈流处理算法 (1) – 蓄水池采样&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.glassdoor.com/Interview/Given-a-stream-of-integers-of-unknown-possibly-large-length-how-would-you-pick-one-at-random-Now-prove-its-random-QTN_36764.htm" target="_blank" rel="noopener">Google Interview Question: Given a stream of integers of&amp;hellip; | Glassdoor&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>frequency-estimation</title><link>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/frequency-estimation/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/frequency-estimation/</guid><description>&lt;p>如何计算数据流中任意元素的频率？&lt;/p>
&lt;p>这个问题也是大数据场景下的一个经典问题，称为频率估计(Frequency Estimation)问题。&lt;/p>
&lt;h3 id="方案-1-hashmap">方案 1: HashMap&lt;/h3>
&lt;p>用一个 HashMap 记录每个元素的出现次数，每来一个元素，就把相应的计数器增 1。这个方法在大数据的场景下不可行，因为元素太多，单机内存无法存下这个巨大的 HashMap。&lt;/p>
&lt;h3 id="方案-2-数据分片--hashmap">方案 2: 数据分片 + HashMap&lt;/h3>
&lt;p>既然单机内存存不下所有元素，一个很自然的改进就是使用多台机器。假设有 8 台机器，每台机器都有一个 HashMap，第 1 台机器只处理&lt;code>hash(elem)%8==0&lt;/code>的元素，第 2 台机器只处理&lt;code>hash(elem)%8==1&lt;/code>的元素，以此类推。查询的时候，先计算这个元素在哪台机器上，然后去那台机器上的 HashMap 里取出计数器。&lt;/p>
&lt;p>方案 2 能够 scale, 但是依旧是把所有元素都存了下来，代价比较高。&lt;/p>
&lt;p>如果允许近似计算，那么有很多高效的近似算法，单机就可以处理海量的数据。下面讲几个经典的近似算法。&lt;/p>
&lt;h3 id="方案-3-count-min-sketch">方案 3: Count-Min Sketch&lt;/h3>
&lt;p>Count-Min Sketch 算法流程：&lt;/p>
&lt;ol>
&lt;li>选定 d 个 hash 函数，开一个 dxm 的二维整数数组作为哈希表&lt;/li>
&lt;li>对于每个元素，分别使用 d 个 hash 函数计算相应的哈希值，并对 m 取余，然后在对应的位置上增 1，二维数组中的每个整数称为 sketch&lt;/li>
&lt;li>要查询某个元素的频率时，只需要取出 d 个 sketch, 返回最小的那一个（其实 d 个 sketch 都是该元素的近似频率，返回任意一个都可以，该算法选择最小的那个）&lt;/li>
&lt;/ol>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../https://assets.ng-tech.icu/book/Andrew-Ng-DeepLearning-AI/count-min-sketch.jpg" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>这个方法的思路和 Bloom Filter 比较类似，都是用多个 hash 函数来降低冲突。&lt;/p>
&lt;ul>
&lt;li>空间复杂度&lt;code>O(dm)&lt;/code>。Count-Min Sketch 需要开一个 &lt;code>dxm&lt;/code> 大小的二位数组，所以空间复杂度是&lt;code>O(dm)&lt;/code>&lt;/li>
&lt;li>时间复杂度&lt;code>O(n)&lt;/code>。Count-Min Sketch 只需要一遍扫描，所以时间复杂度是&lt;code>O(n)&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Count-Min Sketch 算法的优点是省内存，缺点是对于出现次数比较少的元素，准确性很差，因为二维数组相比于原始数据来说还是太小，hash 冲突比较严重，导致结果偏差比较大。&lt;/p>
&lt;h3 id="方案-4-count-mean-min-sketch">方案 4: Count-Mean-Min Sketch&lt;/h3>
&lt;p>Count-Min Sketch 算法对于低频的元素，结果不太准确，主要是因为 hash 冲突比较严重，产生了噪音，例如当 m=20 时，有 1000 个数 hash 到这个 20 桶，平均每个桶会收到 50 个数，这 50 个数的频率重叠在一块了。Count-Mean-Min Sketch 算法做了如下改进：&lt;/p>
&lt;ul>
&lt;li>来了一个查询，按照 Count-Min Sketch 的正常流程，取出它的 d 个 sketch&lt;/li>
&lt;li>对于每个 hash 函数，估算出一个噪音，噪音等于该行所有整数(除了被查询的这个元素)的平均值&lt;/li>
&lt;li>用该行的 sketch 减去该行的噪音，作为真正的 sketch&lt;/li>
&lt;li>返回 d 个 sketch 的中位数&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-java" data-lang="java">&lt;span class="line">&lt;span class="cl">&lt;span class="kd">class&lt;/span> &lt;span class="nc">CountMeanMinSketch&lt;/span> &lt;span class="o">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1">// initialization and addition procedures as in CountMinSketch
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="c1">// n is total number of added elements
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1">&lt;/span> &lt;span class="kt">long&lt;/span> &lt;span class="nf">estimateFrequency&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kt">long&lt;/span> &lt;span class="n">e&lt;/span>&lt;span class="o">[]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">new&lt;/span> &lt;span class="kt">long&lt;/span>&lt;span class="o">[&lt;/span>&lt;span class="n">d&lt;/span>&lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nf">for&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="o">;&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="o">&amp;lt;&lt;/span> &lt;span class="n">d&lt;/span>&lt;span class="o">;&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">++)&lt;/span> &lt;span class="o">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">sketchCounter&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">estimators&lt;/span>&lt;span class="o">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">][&lt;/span> &lt;span class="nf">hash&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="o">,&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">noiseEstimation&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">n&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">sketchCounter&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="o">(&lt;/span>&lt;span class="n">m&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">e&lt;/span>&lt;span class="o">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">sketchCounter&lt;/span> &lt;span class="err">–&lt;/span> &lt;span class="n">noiseEstimator&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">median&lt;/span>&lt;span class="o">(&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="o">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="o">}&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Count-Mean-Min Sketch 算法能够显著的改善在长尾数据上的精确度。&lt;/p>
&lt;h3 id="参考资料">参考资料&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="http://chuansong.me/n/2035207" target="_blank" rel="noopener">数据流处理—摘要的艺术&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://www.cnblogs.com/fxjwind/p/3289221.html" target="_blank" rel="noopener">大数据处理中基于概率的数据结构 - fxjwind - 博客园&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dirtysalt.github.io/probabilistic-data-structures-for-web-analytics-and-data-mining.html" target="_blank" rel="noopener">Probabilistic Data Structures for Web Analytics and Data Mining&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>heavy-hitters</title><link>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/heavy-hitters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/heavy-hitters/</guid><description>&lt;p>寻找数据流中出现最频繁的k个元素(find top k frequent items in a data stream)。这个问题也称为 Heavy Hitters.&lt;/p>
&lt;p>这题也是从实践中提炼而来的，例如搜索引擎的热搜榜，找出访问网站次数最多的前10个IP地址，等等。&lt;/p>
&lt;h3 id="方案1-hashmap--heap">方案1: HashMap + Heap&lt;/h3>
&lt;p>用一个 &lt;code>HashMap&amp;lt;String, Long&amp;gt;&lt;/code>，存放所有元素出现的次数，用一个小根堆，容量为k，存放目前出现过的最频繁的k个元素，&lt;/p>
&lt;ol>
&lt;li>每次从数据流来一个元素，如果在HashMap里已存在，则把对应的计数器增1，如果不存在，则插入，计数器初始化为1&lt;/li>
&lt;li>在堆里查找该元素，如果找到，把堆里的计数器也增1，并调整堆；如果没有找到，把这个元素的次数跟堆顶元素比较，如果大于堆丁元素的出现次数，则把堆丁元素替换为该元素，并调整堆&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>空间复杂度&lt;code>O(n)&lt;/code>。HashMap需要存放下所有元素，需要&lt;code>O(n)&lt;/code>的空间，堆需要存放k个元素，需要&lt;code>O(k)&lt;/code>的空间，跟&lt;code>O(n)&lt;/code>相比可以忽略不急，总的时间复杂度是&lt;code>O(n)&lt;/code>&lt;/li>
&lt;li>时间复杂度&lt;code>O(n)&lt;/code>。每次来一个新元素，需要在HashMap里查找一下，需要&lt;code>O(1)&lt;/code>的时间；然后要在堆里查找一下，&lt;code>O(k)&lt;/code>的时间，有可能需要调堆，又需要&lt;code>O(logk)&lt;/code>的时间，总的时间复杂度是&lt;code>O(n(k+logk))&lt;/code>，k是常量，所以可以看做是O(n)。&lt;/li>
&lt;/ul>
&lt;p>如果元素数量巨大，单机内存存不下，怎么办？ 有两个办法，见方案2和3。&lt;/p>
&lt;h3 id="方案2-多机hashmap--heap">方案2: 多机HashMap + Heap&lt;/h3>
&lt;ul>
&lt;li>可以把数据进行分片。假设有8台机器，第1台机器只处理&lt;code>hash(elem)%8==0&lt;/code>的元素，第2台机器只处理&lt;code>hash(elem)%8==1&lt;/code>的元素，以此类推。&lt;/li>
&lt;li>每台机器都有一个HashMap和一个 Heap, 各自独立计算出 top k 的元素&lt;/li>
&lt;li>把每台机器的Heap，通过网络汇总到一台机器上，将多个Heap合并成一个Heap，就可以计算出总的 top k 个元素了&lt;/li>
&lt;/ul>
&lt;h3 id="方案3-count-min-sketch--heap">方案3: Count-Min Sketch + Heap&lt;/h3>
&lt;p>既然方案1中的HashMap太大，内存装不小，那么可以用&lt;a href="frequency-estimation.md">Count-Min Sketch算法&lt;/a>代替HashMap，&lt;/p>
&lt;ul>
&lt;li>在数据流不断流入的过程中，维护一个标准的Count-Min Sketch 二维数组&lt;/li>
&lt;li>维护一个小根堆，容量为k&lt;/li>
&lt;li>每次来一个新元素，
&lt;ul>
&lt;li>将相应的sketch增1&lt;/li>
&lt;li>在堆中查找该元素，如果找到，把堆里的计数器也增1，并调整堆；如果没有找到，把这个元素的sketch作为钙元素的频率的近似值，跟堆顶元素比较，如果大于堆丁元素的频率，则把堆丁元素替换为该元素，并调整堆&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>这个方法的时间复杂度和空间复杂度如下：&lt;/p>
&lt;ul>
&lt;li>空间复杂度&lt;code>O(dm)&lt;/code>。m是二维数组的列数，d是二维数组的行数，堆需要&lt;code>O(k)&lt;/code>的空间，不过k通常很小，堆的空间可以忽略不计&lt;/li>
&lt;li>时间复杂度&lt;code>O(nlogk)&lt;/code>。每次来一个新元素，需要在二维数组里查找一下，需要&lt;code>O(1)&lt;/code>的时间；然后要在堆里查找一下，&lt;code>O(logk)&lt;/code>的时间，有可能需要调堆，又需要&lt;code>O(logk)&lt;/code>的时间，总的时间复杂度是&lt;code>O(nlogk)&lt;/code>。&lt;/li>
&lt;/ul>
&lt;h3 id="方案4-lossy-counting">方案4: Lossy Counting&lt;/h3>
&lt;p>Lossy Couting 算法流程：&lt;/p>
&lt;ol>
&lt;li>建立一个HashMap&amp;lt;String, Long&amp;gt;，用于存放每个元素的出现次数&lt;/li>
&lt;li>建立一个窗口（窗口的大小由错误率决定，后面具体讨论）&lt;/li>
&lt;li>等待数据流不断流进这个窗口，直到窗口满了，开始统计每个元素出现的频率，统计结束后，每个元素的频率减1，然后将出现次数为0的元素从HashMap中删除&lt;/li>
&lt;li>返回第2步，不断循环&lt;/li>
&lt;/ol>
&lt;p>Lossy Counting 背后朴素的思想是，出现频率高的元素，不太可能减一后变成0，如果某个元素在某个窗口内降到了0，说明它不太可能是高频元素，可以不再跟踪它的计数器了。随着处理的窗口越来越多，HashMap也会不断增长，同时HashMap里的低频元素会被清理出去，这样内存占用会保持在一个很低的水平。&lt;/p>
&lt;p>很显然，Lossy Counting 算法是个近似算法，但它的错误率是可以在数学上证明它的边界的。假设要求错误率不大于ε，那么窗口大小为1/ε，对于长度为N的流，有N／（1/ε）＝εN 个窗口，由于每个窗口结束时减一了，那么频率最多被少计数了窗口个数εN。&lt;/p>
&lt;p>该算法只需要一遍扫描，所以时间复杂度是&lt;code>O(n)&lt;/code>。&lt;/p>
&lt;p>该算法的内存占用，主要在于那个HashMap, Gurmeet Singh Manku 在他的论文里，证明了HashMap里最多有 &lt;code>1/ε log (εN)&lt;/code>个元素，所以空间复杂度是&lt;code>O(1/ε log (εN))&lt;/code>。&lt;/p>
&lt;h3 id="方案5-spacesaving">方案5: SpaceSaving&lt;/h3>
&lt;p>TODO, 原始论文 &amp;ldquo;Efficient Computation of Frequent and Top-k Elements in Data Streams&amp;rdquo;&lt;/p>
&lt;h3 id="参考资料">参考资料&lt;/h3>
&lt;ol>
&lt;li>&lt;a href="http://vaffanculo.twiki.di.uniroma1.it/pub/Ing_algo/WebHome/p14_Cormode_JAl_05.pdf" target="_blank" rel="noopener">An improved data stream summary:the count-min sketch and its applications&lt;/a> by Graham Cormode&lt;/li>
&lt;li>&lt;a href="http://delab.csd.auth.gr/courses/c_dm_pms/afc.pdf" target="_blank" rel="noopener">Approximate Frequency Counts over Data Streams&lt;/a> by Gurmeet Singh Manku&lt;/li>
&lt;li>A.Metwally, D.Agrawal, A.El Abbadi. Efficient Computation of Frequent and Top-k Elements in Data Streams. In Proceeding of the 10th International Conference on Database Theory(ICDT), pp 398-412,2005.&lt;/li>
&lt;li>Massimo Cafaro, et al. “A parallel space saving algorithm for frequent items and the Hurwitz zeta distribution”. Proceeding arXiv: 1401.0702v12 [cs.DS] 19 Setp 2015.&lt;/li>
&lt;li>&lt;a href="http://www.cse.ust.hk/~raywong/comp5331/References/EfficientComputationOfFrequentAndTop-kElementsInDataStreams.pdf" target="_blank" rel="noopener">Efficient Computation of Frequent and Top-k Elements in Data Streams&lt;/a> by Ahmed Metwally&lt;/li>
&lt;li>&lt;a href="http://dmac.rutgers.edu/Workshops/WGUnifyingTheory/Slides/cormode.pdf" target="_blank" rel="noopener">Finding Frequent Items in Data Streams &lt;/a>&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>&lt;a href="http://www.wdiandi.com/p/b3779f.shtml" target="_blank" rel="noopener">实时大数据流上的频率统计：Lossy Counting Algorithm - 待字闺中&lt;/a>&lt;/li>
&lt;li>&lt;a href="http://stackoverflow.com/a/8033083/381712" target="_blank" rel="noopener">What is Lossy Counting? - Stack Overflow&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>membership-query</title><link>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/membership-query/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/membership-query/</guid><description>&lt;p>给定一个无限的数据流和一个有限集合，如何判断数据流中的元素是否在这个集合中？&lt;/p>
&lt;p>在实践中，我们经常需要判断一个元素是否在一个集合中，例如垃圾邮件过滤，爬虫的网址去重，等等。这题也是一道很经典的题目，称为成员查询(Membership Query)。&lt;/p>
&lt;p>答案: Bloom Filter&lt;/p></description></item><item><title>range-query</title><link>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/range-query/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E4%B8%8E%E8%BF%90%E7%BB%B4/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/range-query/</guid><description>&lt;p>给定一个无限的整数数据流，如何查询在某个范围内的元素出现的总次数？例如数据库常常需要 SELECT count(v) WHERE v &amp;gt;= l AND v &amp;lt; u。这个经典的问题称为范围查询(Range Query)。&lt;/p>
&lt;h3 id="方案-1-array-of-count-min-sketches">方案 1: Array of Count-Min Sketches&lt;/h3>
&lt;p>有一个简单方法，既然&lt;a href="https://soulmachine.gitbooks.io/system-design/content/cn/bigdata/frequency-estimation.html" target="_blank" rel="noopener">Count-Min Sketch&lt;/a>可以计算每个元素的频率，那么我们把指定范围内所有元素的 sketch 加起来，不就是这个范围内元素出现的总数了吗？要注意，由于每个 sketch 都是近似值，多个近似值相加，误差会被放大，所以这个方法不可行。&lt;/p>
&lt;p>解决的办法就是使用多个“分辨率”不同的 Count-Min Sketch。第 1 个 sketch 每个格子存放单个元素的频率，第 2 个 sketch 每个格子存放 2 个元素的频率（做法很简答，把该元素的哈希值的最低位 bit 去掉，即右移一位，等价于除以 2，再继续后续流程），第 3 个 sketch 每个格子存放 4 个元素的频率（哈希值右移 2 位即可），以此类推，最后一个 sketch 有 2 个格子，每个格子存放一半元素的频率总数，即第 1 个格子存放最高 bit 为 0 的元素的总次数，第 2 个格子存放最高 bit 为 1 的元素的总次数。Sketch 的个数约等于&lt;code>log(不同元素的总数)&lt;/code>。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>插入元素时，算法伪代码如下，&lt;/p>
&lt;pre>&lt;code>def insert(x):
for i in range(1, d+1):
M1[i][h[i](x)] += 1
M2[i][h[i](x)/2] += 1
M3[i][h[i](x)/4] += 1
M4[i][h[i](x)/8] += 1
# ...
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>查询范围[l, u)时，从粗粒度到细粒度，找到多个区间，能够不重不漏完整覆盖区间[l, u)，将这些 sketch 的值加起来，就是该范围内的元素总数。举个例子，给定某个范围，如下图所示，最粗粒度的那个 sketch 里找不到一个格子，就往细粒度找，最后找到第 1 个 sketch 的 2 个格子，第 2 个 sketch 的 1 个格子和第 3 个 sketch 的 1 个格子，共 4 个格子，能够不重不漏的覆盖整个范围，把 4 个红线部分的值加起来就是所求结果&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="../https://assets.ng-tech.icu/book/Andrew-Ng-DeepLearning-AI/array-of-count-min-sketch.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="参考资料">参考资料&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="http://www.cnblogs.com/fxjwind/p/3289221.html" target="_blank" rel="noopener">大数据处理中基于概率的数据结构 - fxjwind - 博客园&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://dirtysalt.github.io/probabilistic-data-structures-for-web-analytics-and-data-mining.html" target="_blank" rel="noopener">Probabilistic Data Structures for Web Analytics and Data Mining&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>