<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="å‚è€ƒç­”æ¡ˆè¯·æŸ¥é˜…ã€ŠMachineLearning-Notesã€‹ Supervised machine learning What is supervised machine learning? ğŸ‘¶ A case when we have both features (the matrix X) and the labels (the vector y) Linear regression What is regression? Which models can you use to solve a regression problem? ğŸ‘¶ Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor). Here are some common regression models: Linear Regression"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/awesome-interviews/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/awesome-interviews/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/awesome-interviews/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/"><meta property="og:title" content="æœºå™¨å­¦ä¹ é¢è¯•é¢˜ | Next-gen Tech Edu"><meta property="og:description" content="å‚è€ƒç­”æ¡ˆè¯·æŸ¥é˜…ã€ŠMachineLearning-Notesã€‹ Supervised machine learning What is supervised machine learning? ğŸ‘¶ A case when we have both features (the matrix X) and the labels (the vector y) Linear regression What is regression? Which models can you use to solve a regression problem? ğŸ‘¶ Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor). Here are some common regression models: Linear Regression"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>æœºå™¨å­¦ä¹ é¢è¯•é¢˜ | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=8f9681540a3ee31211938268bdd56ffc><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>æœç´¢</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=å…³é—­><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=åˆ‡æ¢å¯¼èˆª>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>ç¬”è®°ï¼ˆä¸‡ç¯‡ï¼‰</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>çŸ¥è¯†å›¾è°±</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>å®éªŒå®¤</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft æ–¹å—ä¸–ç•Œ</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D çŸ¥è¯†å¡ç‰Œ</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>å…¶ä»–é˜…è¯»æ¸ é“</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>çŸ¥ä¹</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>æ˜é‡‘</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=æœç´¢><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">ç®—æ³•å·¥ç¨‹å¸ˆ</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>æœç´¢...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id68452c494a5aadff0bcc5625153e87b9")' href=#id68452c494a5aadff0bcc5625153e87b9 aria-expanded=false aria-controls=id68452c494a5aadff0bcc5625153e87b9 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/>Awesome-Interviews</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id68452c494a5aadff0bcc5625153e87b9 aria-expanded=false aria-controls=id68452c494a5aadff0bcc5625153e87b9><i class="fa-solid fa-angle-down" id=caret-id68452c494a5aadff0bcc5625153e87b9></i></a></div><ul class="nav docs-sidenav collapse show" id=id68452c494a5aadff0bcc5625153e87b9><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id3f0a6a2854743697c9ef2d0c32fcbb2b")' href=#id3f0a6a2854743697c9ef2d0c32fcbb2b aria-expanded=false aria-controls=id3f0a6a2854743697c9ef2d0c32fcbb2b aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/00.%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80/>00.é¢è¯•åŸºç¡€</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id3f0a6a2854743697c9ef2d0c32fcbb2b aria-expanded=false aria-controls=id3f0a6a2854743697c9ef2d0c32fcbb2b><i class="fa-solid fa-angle-right" id=caret-id3f0a6a2854743697c9ef2d0c32fcbb2b></i></a></div><ul class="nav docs-sidenav collapse" id=id3f0a6a2854743697c9ef2d0c32fcbb2b><li class="child level"><a href=/books/awesome-interviews/00.%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80/%E5%8F%8D%E5%90%91%E9%9D%A2%E8%AF%95/>åå‘é¢è¯•</a></li><li class="child level"><a href=/books/awesome-interviews/00.%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80/%E7%AE%80%E5%8E%86%E6%8F%90%E5%8D%87/>ç®€å†æå‡</a></li><li class="child level"><a href=/books/awesome-interviews/00.%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80/%E9%9D%A2%E8%AF%95%E6%8A%80%E5%B7%A7/>é¢è¯•æŠ€å·§</a></li><li class="child level"><a href=/books/awesome-interviews/00.%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80/%E9%9D%A2%E8%AF%95%E6%B5%81%E7%A8%8B/>é¢è¯•æµç¨‹</a></li><li class="child level"><a href=/books/awesome-interviews/00.%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80/%E6%99%BA%E5%8A%9B%E9%A2%98/>æ™ºåŠ›é¢˜</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id7e34d13813926cfbfcc1a6f0a9eda490")' href=#id7e34d13813926cfbfcc1a6f0a9eda490 aria-expanded=false aria-controls=id7e34d13813926cfbfcc1a6f0a9eda490 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/01.cs-%E9%80%9A%E8%AF%86/>01.CS é€šè¯†</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id7e34d13813926cfbfcc1a6f0a9eda490 aria-expanded=false aria-controls=id7e34d13813926cfbfcc1a6f0a9eda490><i class="fa-solid fa-angle-right" id=caret-id7e34d13813926cfbfcc1a6f0a9eda490></i></a></div><ul class="nav docs-sidenav collapse" id=id7e34d13813926cfbfcc1a6f0a9eda490><li class="child level"><a href=/books/awesome-interviews/01.cs-%E9%80%9A%E8%AF%86/git-%E9%9D%A2%E8%AF%95%E9%A2%98/>Git é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/01.cs-%E9%80%9A%E8%AF%86/go-%E9%9D%A2%E8%AF%95%E9%A2%98/>Go é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/01.cs-%E9%80%9A%E8%AF%86/java-%E9%9D%A2%E8%AF%95%E9%A2%98/>Java é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/01.cs-%E9%80%9A%E8%AF%86/linux-%E9%9D%A2%E8%AF%95%E9%A2%98/>Linux é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/01.cs-%E9%80%9A%E8%AF%86/rust-%E9%9D%A2%E8%AF%95%E9%A2%98/>Rust é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/01.cs-%E9%80%9A%E8%AF%86/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%A2%98/>è®¾è®¡æ¨¡å¼é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/01.cs-%E9%80%9A%E8%AF%86/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E9%9D%A2%E8%AF%95%E9%A2%98/>æ•°æ®ç»“æ„ä¸ç®—æ³•é¢è¯•é¢˜</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id753b90c43256eaef77815b84deba37b8")' href=#id753b90c43256eaef77815b84deba37b8 aria-expanded=false aria-controls=id753b90c43256eaef77815b84deba37b8 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/10.%E4%BA%A7%E5%93%81%E4%B8%8E%E8%BF%90%E8%90%A5/>10.äº§å“ä¸è¿è¥</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id753b90c43256eaef77815b84deba37b8 aria-expanded=false aria-controls=id753b90c43256eaef77815b84deba37b8><i class="fa-solid fa-angle-right" id=caret-id753b90c43256eaef77815b84deba37b8></i></a></div><ul class="nav docs-sidenav collapse" id=id753b90c43256eaef77815b84deba37b8><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idd1d672a321666fd1934692e647db3061")' href=#idd1d672a321666fd1934692e647db3061 aria-expanded=false aria-controls=idd1d672a321666fd1934692e647db3061 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/10.%E4%BA%A7%E5%93%81%E4%B8%8E%E8%BF%90%E8%90%A5/pd-%E8%83%BD%E5%8A%9B%E6%A8%A1%E5%9E%8B/>PD èƒ½åŠ›æ¨¡å‹</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idd1d672a321666fd1934692e647db3061 aria-expanded=false aria-controls=idd1d672a321666fd1934692e647db3061><i class="fa-solid fa-angle-right" id=caret-idd1d672a321666fd1934692e647db3061></i></a></div><ul class="nav docs-sidenav collapse" id=idd1d672a321666fd1934692e647db3061><li class="child level"><a href=/books/awesome-interviews/10.%E4%BA%A7%E5%93%81%E4%B8%8E%E8%BF%90%E8%90%A5/pd-%E8%83%BD%E5%8A%9B%E6%A8%A1%E5%9E%8B/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E7%9A%84%E9%81%93%E6%9C%AF%E5%99%A8/>äº§å“ç»ç†çš„é“ã€æœ¯ã€å™¨</a></li></ul></div><li class="child level"><a href=/books/awesome-interviews/10.%E4%BA%A7%E5%93%81%E4%B8%8E%E8%BF%90%E8%90%A5/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86%E9%9D%A2%E8%AF%95%E9%A2%98/>äº§å“ç»ç†é¢è¯•é¢˜</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idd907d5d1ed2292a9a62ddc38ea3c12fd")' href=#idd907d5d1ed2292a9a62ddc38ea3c12fd aria-expanded=false aria-controls=idd907d5d1ed2292a9a62ddc38ea3c12fd aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/20.%E5%85%B6%E4%BB%96%E8%81%8C%E4%BD%8D/>20.å…¶ä»–èŒä½</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idd907d5d1ed2292a9a62ddc38ea3c12fd aria-expanded=false aria-controls=idd907d5d1ed2292a9a62ddc38ea3c12fd><i class="fa-solid fa-angle-right" id=caret-idd907d5d1ed2292a9a62ddc38ea3c12fd></i></a></div><ul class="nav docs-sidenav collapse" id=idd907d5d1ed2292a9a62ddc38ea3c12fd><li class="child level"><a href=/books/awesome-interviews/20.%E5%85%B6%E4%BB%96%E8%81%8C%E4%BD%8D/%E6%80%BB%E7%BB%8F%E7%90%86%E5%8A%A9%E7%90%86/>æ€»ç»ç†åŠ©ç†</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idd63ebac372fabf15c4ef78b914d2bd6a")' href=#idd63ebac372fabf15c4ef78b914d2bd6a aria-expanded=false aria-controls=idd63ebac372fabf15c4ef78b914d2bd6a aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/98.%E5%A4%A7%E5%8E%82%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/>98.å¤§å‚é¢è¯•é›†é”¦</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idd63ebac372fabf15c4ef78b914d2bd6a aria-expanded=false aria-controls=idd63ebac372fabf15c4ef78b914d2bd6a><i class="fa-solid fa-angle-right" id=caret-idd63ebac372fabf15c4ef78b914d2bd6a></i></a></div><ul class="nav docs-sidenav collapse" id=idd63ebac372fabf15c4ef78b914d2bd6a><li class="child level"><a href=/books/awesome-interviews/98.%E5%A4%A7%E5%8E%82%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4/>é˜¿é‡Œå·´å·´</a></li><li class="child level"><a href=/books/awesome-interviews/98.%E5%A4%A7%E5%8E%82%E9%9D%A2%E8%AF%95%E9%9B%86%E9%94%A6/%E8%9A%82%E8%9A%81%E9%87%91%E6%9C%8D/>èš‚èšé‡‘æœ</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6e62644470b076adeaa2968f981f7f55")' href=#id6e62644470b076adeaa2968f981f7f55 aria-expanded=false aria-controls=id6e62644470b076adeaa2968f981f7f55 aria-hidden=false data-toggle=collapse></div></div><li class="child level"><a href=/books/awesome-interviews/interviews-list/>Interviews-List</a></li><li class="child level"><a href=/books/awesome-interviews/introduction/>INTRODUCTION</a></li><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id44d38d4f7218837f45352c8f09579bf6")' href=#id44d38d4f7218837f45352c8f09579bf6 aria-expanded=false aria-controls=id44d38d4f7218837f45352c8f09579bf6 aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id1a4c1829a83cc509e0de8f8e04473c73")' href=#id1a4c1829a83cc509e0de8f8e04473c73 aria-expanded=false aria-controls=id1a4c1829a83cc509e0de8f8e04473c73 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/>åç«¯å¼€å‘å·¥ç¨‹å¸ˆ</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id1a4c1829a83cc509e0de8f8e04473c73 aria-expanded=false aria-controls=id1a4c1829a83cc509e0de8f8e04473c73><i class="fa-solid fa-angle-right" id=caret-id1a4c1829a83cc509e0de8f8e04473c73></i></a></div><ul class="nav docs-sidenav collapse" id=id1a4c1829a83cc509e0de8f8e04473c73><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-ida5f5326f90562235d1d044abe0239a57")' href=#ida5f5326f90562235d1d044abe0239a57 aria-expanded=false aria-controls=ida5f5326f90562235d1d044abe0239a57 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/>99.å‚è€ƒèµ„æ–™</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#ida5f5326f90562235d1d044abe0239a57 aria-expanded=false aria-controls=ida5f5326f90562235d1d044abe0239a57><i class="fa-solid fa-angle-right" id=caret-ida5f5326f90562235d1d044abe0239a57></i></a></div><ul class="nav docs-sidenav collapse" id=ida5f5326f90562235d1d044abe0239a57><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id8782c59bcea6cc3fd94a195089192fc4")' href=#id8782c59bcea6cc3fd94a195089192fc4 aria-expanded=false aria-controls=id8782c59bcea6cc3fd94a195089192fc4 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/>2017-ç³»ç»Ÿè®¾è®¡é¢è¯•é¢˜ç²¾é€‰</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id8782c59bcea6cc3fd94a195089192fc4 aria-expanded=false aria-controls=id8782c59bcea6cc3fd94a195089192fc4><i class="fa-solid fa-angle-right" id=caret-id8782c59bcea6cc3fd94a195089192fc4></i></a></div><ul class="nav docs-sidenav collapse" id=id8782c59bcea6cc3fd94a195089192fc4><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id4baff1c53bb57a7b8fb6b76cd1f6dedd")' href=#id4baff1c53bb57a7b8fb6b76cd1f6dedd aria-expanded=false aria-controls=id4baff1c53bb57a7b8fb6b76cd1f6dedd aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/>cn</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id4baff1c53bb57a7b8fb6b76cd1f6dedd aria-expanded=false aria-controls=id4baff1c53bb57a7b8fb6b76cd1f6dedd><i class="fa-solid fa-angle-right" id=caret-id4baff1c53bb57a7b8fb6b76cd1f6dedd></i></a></div><ul class="nav docs-sidenav collapse" id=id4baff1c53bb57a7b8fb6b76cd1f6dedd><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/api-rate-limiting/>api-rate-limiting</a></li><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idcf04fce9b5de4d9beaae6f74f389d567")' href=#idcf04fce9b5de4d9beaae6f74f389d567 aria-expanded=false aria-controls=idcf04fce9b5de4d9beaae6f74f389d567 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/appendix/>appendix</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idcf04fce9b5de4d9beaae6f74f389d567 aria-expanded=false aria-controls=idcf04fce9b5de4d9beaae6f74f389d567><i class="fa-solid fa-angle-right" id=caret-idcf04fce9b5de4d9beaae6f74f389d567></i></a></div><ul class="nav docs-sidenav collapse" id=idcf04fce9b5de4d9beaae6f74f389d567><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/appendix/raft/>raft</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/appendix/skip-list/>skip-list</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id64d10dbcef6ecd1cac5424eec6a7e984")' href=#id64d10dbcef6ecd1cac5424eec6a7e984 aria-expanded=false aria-controls=id64d10dbcef6ecd1cac5424eec6a7e984 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/>bigdata</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id64d10dbcef6ecd1cac5424eec6a7e984 aria-expanded=false aria-controls=id64d10dbcef6ecd1cac5424eec6a7e984><i class="fa-solid fa-angle-right" id=caret-id64d10dbcef6ecd1cac5424eec6a7e984></i></a></div><ul class="nav docs-sidenav collapse" id=id64d10dbcef6ecd1cac5424eec6a7e984><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/cardinality-estimation/>cardinality-estimation</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/data-stream-sampling/>data-stream-sampling</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/frequency-estimation/>frequency-estimation</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/heavy-hitters/>heavy-hitters</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/membership-query/>membership-query</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/bigdata/range-query/>range-query</a></li></ul></div><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/crawler/>crawler</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/distributed-id-generator/>distributed-id-generator</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/filesystem-cache/>filesystem-cache</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/hashmap/>hashmap</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/key-value-store/>key-value-store</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/load-balancer/>load-balancer</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/method/>method</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/news-feed/>news-feed</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/pagerank/>pagerank</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/search-engine/>search-engine</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/summary/>SUMMARY</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/task-scheduler/>task-scheduler</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/tinyurl/>tinyurl</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/cn/top-k-frequent-ip-in-one-hour/>top-k-frequent-ip-in-one-hour</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6ad66e74f6cb10a3c55e1af3a5b3ca8e")' href=#id6ad66e74f6cb10a3c55e1af3a5b3ca8e aria-expanded=false aria-controls=id6ad66e74f6cb10a3c55e1af3a5b3ca8e aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/images/>images</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id6ad66e74f6cb10a3c55e1af3a5b3ca8e aria-expanded=false aria-controls=id6ad66e74f6cb10a3c55e1af3a5b3ca8e><i class="fa-solid fa-angle-right" id=caret-id6ad66e74f6cb10a3c55e1af3a5b3ca8e></i></a></div><ul class="nav docs-sidenav collapse" id=id6ad66e74f6cb10a3c55e1af3a5b3ca8e><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id28582f4adb2c68443924e07cf3113f46")' href=#id28582f4adb2c68443924e07cf3113f46 aria-expanded=false aria-controls=id28582f4adb2c68443924e07cf3113f46 aria-hidden=false data-toggle=collapse></div></div></ul></div><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2017-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98%E7%B2%BE%E9%80%89/langs/>LANGS</a></li></ul></div><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2018-%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%A2%98/>2018-åç«¯å¼€å‘é¢è¯•é¢˜</a></li></ul></div><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/mybatis-%E9%9D%A2%E8%AF%95%E9%A2%98/>Mybatis é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E9%AB%98%E5%8F%AF%E7%94%A8%E6%9E%B6%E6%9E%84%E9%9D%A2%E8%AF%95%E9%A2%98/>é«˜å¯ç”¨æ¶æ„é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E9%9D%A2%E8%AF%95%E9%A2%98/>è½¯ä»¶æ¶æ„é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%8E%E4%BA%91%E5%8E%9F%E7%94%9F%E9%9D%A2%E8%AF%95%E9%A2%98/>å¾®æœåŠ¡ä¸äº‘åŸç”Ÿé¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E9%9D%A2%E8%AF%95%E9%A2%98/>æ¶ˆæ¯é˜Ÿåˆ—é¢è¯•é¢˜</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idfa9174f2505020ef4326ac4d70dd4d1e")' href=#idfa9174f2505020ef4326ac4d70dd4d1e aria-expanded=false aria-controls=idfa9174f2505020ef4326ac4d70dd4d1e aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/>å‰ç«¯å·¥ç¨‹å¸ˆ</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idfa9174f2505020ef4326ac4d70dd4d1e aria-expanded=false aria-controls=idfa9174f2505020ef4326ac4d70dd4d1e><i class="fa-solid fa-angle-right" id=caret-idfa9174f2505020ef4326ac4d70dd4d1e></i></a></div><ul class="nav docs-sidenav collapse" id=idfa9174f2505020ef4326ac4d70dd4d1e><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idbd0ecb09a7cc23a7532134e5322fb5f6")' href=#idbd0ecb09a7cc23a7532134e5322fb5f6 aria-expanded=false aria-controls=idbd0ecb09a7cc23a7532134e5322fb5f6 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/javascript-typescript/>JavaScript & TypeScript</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idbd0ecb09a7cc23a7532134e5322fb5f6 aria-expanded=false aria-controls=idbd0ecb09a7cc23a7532134e5322fb5f6><i class="fa-solid fa-angle-right" id=caret-idbd0ecb09a7cc23a7532134e5322fb5f6></i></a></div><ul class="nav docs-sidenav collapse" id=idbd0ecb09a7cc23a7532134e5322fb5f6><li class="child level"><a href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/javascript-typescript/javascript-%E7%BC%96%E7%A0%81%E9%A2%98/>JavaScript ç¼–ç é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/javascript-typescript/javascript-%E9%9D%A2%E8%AF%95%E9%A2%98/>JavaScript é¢è¯•é¢˜</a></li></ul></div><li class="child level"><a href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/node-%E9%9D%A2%E8%AF%95%E9%A2%98/>Node é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/react-%E9%9D%A2%E8%AF%95%E9%A2%98/>React é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/vue-%E9%9D%A2%E8%AF%95%E9%A2%98/>Vue é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/web-%E9%9D%A2%E8%AF%95%E9%A2%98/>Web é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E5%89%8D%E7%AB%AF%E5%B7%A5%E7%A8%8B%E5%B8%88/%E7%A7%BB%E5%8A%A8%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%A2%98/>ç§»åŠ¨å¼€å‘é¢è¯•é¢˜</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id1667b169eb5339f00bc4f0cc67ba5008")' href=#id1667b169eb5339f00bc4f0cc67ba5008 aria-expanded=false aria-controls=id1667b169eb5339f00bc4f0cc67ba5008 aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id356fc03af39cf08c7d052622cf092fca")' href=#id356fc03af39cf08c7d052622cf092fca aria-expanded=false aria-controls=id356fc03af39cf08c7d052622cf092fca aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idbc75248101b1ecdac18a56e440afec7e")' href=#idbc75248101b1ecdac18a56e440afec7e aria-expanded=false aria-controls=idbc75248101b1ecdac18a56e440afec7e aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88/>ç®—æ³•å·¥ç¨‹å¸ˆ</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idbc75248101b1ecdac18a56e440afec7e aria-expanded=false aria-controls=idbc75248101b1ecdac18a56e440afec7e><i class="fa-solid fa-angle-down" id=caret-idbc75248101b1ecdac18a56e440afec7e></i></a></div><ul class="nav docs-sidenav collapse show" id=idbc75248101b1ecdac18a56e440afec7e><li class="child level active"><a href=/books/awesome-interviews/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E9%9D%A2%E8%AF%95%E9%A2%98/>æœºå™¨å­¦ä¹ é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98/>æ•°ç†ç»Ÿè®¡é¢è¯•é¢˜</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id24d88287faa41d30a2350713ce6d0bf7")' href=#id24d88287faa41d30a2350713ce6d0bf7 aria-expanded=false aria-controls=id24d88287faa41d30a2350713ce6d0bf7 aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id9578455543c6a8e14e3a34402116f855")' href=#id9578455543c6a8e14e3a34402116f855 aria-expanded=false aria-controls=id9578455543c6a8e14e3a34402116f855 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/>ç³»ç»Ÿæ¶æ„å¸ˆ</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id9578455543c6a8e14e3a34402116f855 aria-expanded=false aria-controls=id9578455543c6a8e14e3a34402116f855><i class="fa-solid fa-angle-right" id=caret-id9578455543c6a8e14e3a34402116f855></i></a></div><ul class="nav docs-sidenav collapse" id=id9578455543c6a8e14e3a34402116f855><li class="child level"><a href=/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/docker-k8s-%E9%9D%A2%E8%AF%95%E9%A2%98/>Docker & K8s é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/>å¹¶å‘ç¼–ç¨‹</a></li><li class="child level"><a href=/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%9D%A2%E8%AF%95%E9%A2%98/>æ“ä½œç³»ç»Ÿé¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E9%9D%A2%E8%AF%95%E9%A2%98/>åˆ†å¸ƒå¼è®¡ç®—é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E9%9D%A2%E8%AF%95%E9%A2%98/>åˆ†å¸ƒå¼ç³»ç»Ÿé¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9D%A2%E8%AF%95%E9%A2%98/>æ•°æ®åº“é¢è¯•é¢˜</a></li><li class="child level"><a href=/books/awesome-interviews/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%B8%88/%E7%BD%91%E7%BB%9C%E9%9D%A2%E8%AF%95%E9%A2%98/>ç½‘ç»œé¢è¯•é¢˜</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idbc5648e2c3142df2cf485160bfafdc18")' href=#idbc5648e2c3142df2cf485160bfafdc18 aria-expanded=false aria-controls=idbc5648e2c3142df2cf485160bfafdc18 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/awesome-interviews/%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%88/>è¿ç»´å·¥ç¨‹å¸ˆ</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#idbc5648e2c3142df2cf485160bfafdc18 aria-expanded=false aria-controls=idbc5648e2c3142df2cf485160bfafdc18><i class="fa-solid fa-angle-right" id=caret-idbc5648e2c3142df2cf485160bfafdc18></i></a></div><ul class="nav docs-sidenav collapse" id=idbc5648e2c3142df2cf485160bfafdc18><li class="child level"><a href=/books/awesome-interviews/%E8%BF%90%E7%BB%B4%E5%B7%A5%E7%A8%8B%E5%B8%88/sre-%E9%9D%A2%E8%AF%95%E9%A2%98/>SRE é¢è¯•é¢˜</a></li></ul></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>ç›®å½•</a></li></ul><nav id=TableOfContents><ul><li><a href=#supervised-machine-learning>Supervised machine learning</a></li><li><a href=#linear-regression>Linear regression</a></li><li><a href=#validation>Validation</a></li><li><a href=#classification>Classification</a></li><li><a href=#regularization>Regularization</a></li><li><a href=#feature-selection>Feature selection</a></li><li><a href=#decision-trees>Decision trees</a></li><li><a href=#random-forest>Random forest</a></li><li><a href=#gradient-boosting>Gradient boosting</a></li><li><a href=#parameter-tuning>Parameter tuning</a></li><li><a href=#neural-networks>Neural networks</a></li><li><a href=#optimization-in-neural-networks>Optimization in neural networks</a></li><li><a href=#neural-networks-for-computer-vision>Neural networks for computer vision</a></li><li><a href=#text-classification>Text classification</a></li><li><a href=#clustering>Clustering</a></li><li><a href=#dimensionality-reduction>Dimensionality reduction</a></li><li><a href=#ranking-and-search>Ranking and search</a></li><li><a href=#recommender-systems>Recommender systems</a></li><li><a href=#time-series>Time series</a></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=ç‹ä¸‹é‚€æœˆç†Šçš„å¾®ä¿¡å…¬ä¼—å·></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>æœºå™¨å­¦ä¹ é¢è¯•é¢˜</h1><div class=article-style><blockquote><p>å‚è€ƒç­”æ¡ˆè¯·æŸ¥é˜…ã€Š<a href="https://github.com/wx-chevalier/MachineLearning-Notes?q=" target=_blank rel=noopener>MachineLearning-Notes</a>ã€‹</p></blockquote><h2 id=supervised-machine-learning>Supervised machine learning</h2><p><strong>What is supervised machine learning? ğŸ‘¶</strong></p><p>A case when we have both features (the matrix X) and the labels (the vector y)</p><br><h2 id=linear-regression>Linear regression</h2><p><strong>What is regression? Which models can you use to solve a regression problem? ğŸ‘¶</strong></p><p>Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor).
Here are some common regression models:</p><ul><li><em>Linear Regression</em> establishes a linear relationship between target and predictor (s). It predicts a numeric value and has a shape of a straight line.</li><li><em>Logistic Regression</em> solves classification problem when target is categorical. It searches for the probability of an event, predicts a value from 0 to 1.</li><li><em>Polynomial Regression</em> has a regression equation with the power of independent variable more than 1. It is a curve that fits into the data points.</li><li><em>Ridge Regression</em> helps when predictors are highly correlated (multicollinearity problem). It penalizes the squares of regression coefficients but doesnâ€™t allow to reach zeros (uses l2 regularization).</li><li><em>Lasso Regression</em> penalizes the absolute values of regression coefficients and allow reach absolute zero for some coefficient (allow feature selection).</li></ul><br><p><strong>What is linear regression? When do we use it? ğŸ‘¶</strong></p><p>Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).</p><p>With a simple equation:</p><pre tabindex=0><code>y = B0 + B1*x1 + ... + Bn * xN
</code></pre><p>B is regression coefficients, x values are the independent (explanatory) variables and y is dependent variable.</p><p>The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.</p><p>Simple linear regression:</p><pre tabindex=0><code>y = B0 + B1*x1
</code></pre><p>Multiple linear regression:</p><pre tabindex=0><code>y = B0 + B1*x1 + ... + Bn * xN
</code></pre><br><p><strong>Whatâ€™s the normal distribution? Why do we care about it? ğŸ‘¶</strong></p><p>The normal distribution is a continuous probability distribution whose probability density function takes the following formula:<figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif alt=formula loading=lazy data-zoomable></div></div></figure></p><p>where Î¼ is the mean and Ïƒ is the standard deviation of the distribution.</p><p>The normal distribution derives its importance from the <strong>Central Limit Theorem</strong>, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e <strong>the distribution of the mean of the samples is normal</strong>. It is important that each sample is independent from the other.</p><p>This is powerful because it helps us study processes whose population distribution is unknown to us.</p><br><p><strong>How do we check if a variable follows the normal distribution? â€â­ï¸</strong></p><ol><li>Plot a histogram out of the sampled data. If you can fit the bell-shaped &ldquo;normal&rdquo; curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.</li><li>Check Skewness and Kurtosis of the sampled data. Zero-skewness and zero-kurtosis are typical for a normal distribution, so the farther away from 0, the more non-normal the distribution.</li><li>Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously.</li><li>Check for Quantile-Quantile plot. It is a scatterplot created by plotting two sets of quantiles against one another. Normal Q-Q plot place the data points in a roughly straight line.</li></ol><br><p><strong>What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What are the methods for solving linear regression do you know? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is gradient descent? How does it work? â€â­ï¸</strong></p><p>Gradient descent is an algorithm that uses calculus concept of gradient to try and reach local or global minima. It works by taking the negative of the gradient in a point of a given function, and updating that point repeatedly using the calculated negative gradient, until the algorithm reaches a local or global minimum, which will cause future iterations of the algorithm to return values that are equal or too close to the current point. It is widely used in machine learning applications.</p><br><p><strong>What is the normal equation? â€â­ï¸</strong></p><p>Normal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression.</p><br><p><strong>What is SGD â€Šâ€”â€Š stochastic gradient descent? Whatâ€™s the difference with the usual gradient descent? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Which metrics for evaluating regression models do you know? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>What are MSE and RMSE? ğŸ‘¶</strong></p><p>MSE stands for <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror while RMSE stands for <strong>R</strong>oot <strong>M</strong>ean <strong>S</strong>quare <strong>E</strong>rror. They are metrics with which we can evaluate models.</p><br><h2 id=validation>Validation</h2><p><strong>What is overfitting? ğŸ‘¶</strong></p><p>When your model perform very well on your training set but can&rsquo;t generalize the test set, because it adjusted a lot to the training set.</p><br><p><strong>How to validate your models? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>Why do we need to split our data into three parts: train, validation, and test? ğŸ‘¶</strong></p><p>The training set is used to fit the model, i.e. to train the model with the data. The validation set is then used to provide an unbiased evaluation of a model while fine-tuning hyperparameters. This improves the generalization of the model. Finally, a test data set which the model has never &ldquo;seen&rdquo; before should be used for the final evaluation of the model. This allows for an unbiased evaluation of the model. The evaluation should never be performed on the same data that is used for training. Otherwise the model performance would not be representative.</p><br><p><strong>Can you explain how cross-validation works? ğŸ‘¶</strong></p><p>Cross-validation is the process to separate your total training set into two subsets: training and validation set, and evaluate your model to choose the hyperparameters. But you do this process iteratively, selecting differents training and validation set, in order to reduce the bias that you would have by selecting only one validation set.</p><br><p><strong>What is K-fold cross-validation? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>How do we choose K in K-fold cross-validation? Whatâ€™s your favorite K? ğŸ‘¶</strong></p><p>Answer here</p><br><h2 id=classification>Classification</h2><p><strong>What is classification? Which models would you use to solve a classification problem? ğŸ‘¶</strong></p><p>Classification problems are problems in which our prediction space is discrete, i.e. there is a finite number of values the output variable can be. Some models which can be used to solve classification problems are: logistic regression, decision tree, random forests, multi-layer perceptron, one-vs-all, amongst others.</p><br><p><strong>What is logistic regression? When do we need to use it? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>Is logistic regression a linear model? Why? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>What is sigmoid? What does it do? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>How do we evaluate classification models? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>What is accuracy? ğŸ‘¶</strong></p><p>Accuracy is a metric for evaluating classification models. It is calculated by dividing the number of correct predictions by the number of total predictions.</p><br><p><strong>Is accuracy always a good metric? ğŸ‘¶</strong></p><p>Accuracy is not a good performance metric when there is imbalance in the dataset. For example, in binary classification with 95% of A class and 5% of B class, prediction accuracy can be 95%. In case of imbalance dataset, we need to choose Precision, recall, or F1 Score depending on the problem we are trying to solve.</p><br><p><strong>What is the confusion table? What are the cells in this table? ğŸ‘¶</strong></p><p>Confusion table (or confusion matrix) shows how many True positives (TP), True Negative (TN), False Positive (FP) and False Negative (FN) model has made.</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center>Actual</th><th style=text-align:center>Actual</th></tr></thead><tbody><tr><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>Positive (1)</td><td style=text-align:center>Negative (0)</td></tr><tr><td style=text-align:center>Predicted</td><td style=text-align:center>Positive (1)</td><td style=text-align:center>TP</td><td style=text-align:center>FP</td></tr><tr><td style=text-align:center>Predicted</td><td style=text-align:center>Negative (0)</td><td style=text-align:center>FN</td><td style=text-align:center>TN</td></tr></tbody></table><ul><li>True Positives (TP): When the actual class of the observation is 1 (True) and the prediction is 1 (True)</li><li>True Negative (TN): When the actual class of the observation is 0 (False) and the prediction is 0 (False)</li><li>False Positive (FP): When the actual class of the observation is 0 (False) and the prediction is 1 (True)</li><li>False Negative (FN): When the actual class of the observation is 1 (True) and the prediction is 0 (False)</li></ul><p>Most of the performance metrics for classification models are based on the values of the confusion matrix.</p><br><p><strong>What are precision, recall, and F1-score? ğŸ‘¶</strong></p><ul><li>Precision and recall are classification evaluation metrics:</li><li>P = TP / (TP + FP) and R = TP / (TP + FN).</li><li>Where TP is true positives, FP is false positives and FN is false negatives</li><li>In both cases the score of 1 is the best: we get no false positives or false negatives and only true positives.</li><li>F1 is a combination of both precision and recall in one score:</li><li>F1 = 2 * PR / (P + R).</li><li>Max F score is 1 and min is 0, with 1 being the best.</li></ul><br><p><strong>Precision-recall trade-off â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is the ROC curve? When to use it? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is AUC (AU ROC)? When to use it? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How to interpret the AU ROC score? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is the PR (precision-recall) curve? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is the area under the PR curve? Is it a useful metric? â€â­ï¸I</strong></p><p>Answer here</p><br><p><strong>In which cases AU PR is better than AU ROC? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What do we do with categorical variables? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Why do we need one-hot encoding? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=regularization>Regularization</h2><p><strong>What happens to our linear regression model if we have three columns in our data: x, y, z â€Šâ€”â€Š and z is a sum of x and y? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What happens to our linear regression model if the column z in the data is a sum of columns x and y and some random noise? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is regularization? Why do we need it? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>Which regularization techniques do you know? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What kind of regularization techniques are applicable to linear models? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How does L2 regularization look like in a linear model? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How do we select the right regularization parameters? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>Whatâ€™s the effect of L2 regularization on the weights of a linear model? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How L1 regularization looks like in a linear model? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Whatâ€™s the difference between L2 and L1 regularization? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Can we have both L1 and L2 regularization components in a linear model? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Whatâ€™s the interpretation of the bias term in linear models? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How do we interpret weights in linear models? â€â­ï¸</strong></p><p>If the variables are normalized, we can interpret weights in linear models like the importance of this variable in the predicted result.</p><br><p><strong>If a weight for one variable is higher than for another â€Šâ€”â€Š can we say that this variable is more important? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>When do we need to perform feature normalization for linear models? When itâ€™s okay not to do it? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=feature-selection>Feature selection</h2><p><strong>What is feature selection? Why do we need it? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>Is feature selection important for linear models? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Which feature selection techniques do you know? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Can we use L1 regularization for feature selection? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Can we use L2 regularization for feature selection? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=decision-trees>Decision trees</h2><p><strong>What are the decision trees? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>How do we train decision trees? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What are the main parameters of the decision tree model? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>How do we handle categorical variables in decision trees? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What are the benefits of a single decision tree compared to more complex models? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How can we know which features are more important for the decision tree model? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=random-forest>Random forest</h2><p><strong>What is random forest? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>Why do we need randomization in random forest? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What are the main parameters of the random forest model? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How do we select the depth of the trees in random forest? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How do we know how many trees we need in random forest? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Is it easy to parallelize training of a random forest model? How can we do it? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What are the potential problems with many large trees? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What if instead of finding the best split, we randomly select a few splits and just select the best from them. Will it work? ğŸš€</strong></p><p>Answer here</p><br><p><strong>What happens when we have correlated features in our data? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=gradient-boosting>Gradient boosting</h2><p><strong>What is gradient boosting trees? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Whatâ€™s the difference between random forest and gradient boosting? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Is it possible to parallelize training of a gradient boosting model? How to do it? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Feature importance in gradient boosting trees â€Šâ€”â€Š what are possible options? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Are there any differences between continuous and discrete variables when it comes to feature importance of gradient boosting models? ğŸš€</strong></p><p>Answer here</p><br><p><strong>What are the main parameters in the gradient boosting model? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How do you approach tuning parameters in XGBoost or LightGBM? ğŸš€</strong></p><p>Answer here</p><br><p><strong>How do you select the number of trees in the gradient boosting model? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=parameter-tuning>Parameter tuning</h2><p><strong>Which parameter tuning strategies (in general) do you know? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Whatâ€™s the difference between grid search parameter tuning strategy and random search? When to use one or another? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=neural-networks>Neural networks</h2><p><strong>What kind of problems neural nets can solve? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>How does a usual fully-connected feed-forward neural network work? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Why do we need activation functions? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>What are the problems with sigmoid as an activation function? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is ReLU? How is it better than sigmoid or tanh? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How we can initialize the weights of a neural network? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What if we set all the weights of a neural network to 0? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What regularization techniques for neural nets do you know? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is dropout? Why is it useful? How does it work? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=optimization-in-neural-networks>Optimization in neural networks</h2><p><strong>What is backpropagation? How does it work? Why do we need it? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Which optimization techniques for training neural nets do you know? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How do we use SGD (stochastic gradient descent) for training a neural net? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Whatâ€™s the learning rate? ğŸ‘¶</strong></p><p>The learning rate is an important hyperparameter that controls how quickly the model is adapted to the problem during the training. It can be seen as the &ldquo;step width&rdquo; during the parameter updates, i.e. how far the weights are moved into the direction of the minimum of our optimization problem.</p><br><p><strong>What happens when the learning rate is too large? Too small? ğŸ‘¶</strong></p><p>A large learning rate can accelerate the training. However, it is possible that we &ldquo;shoot&rdquo; too far and miss the minimum of the function that we want to optimize, which will not result in the best solution. On the other hand, training with a small learning rate takes more time but it is possible to find a more precise minimum. The downside can be that the solution is stuck in a local minimum, and the weights won&rsquo;t update even if it is not the best possible global solution.</p><br><p><strong>How to set the learning rate? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is Adam? Whatâ€™s the main difference between Adam and SGD? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>When would you use Adam and when SGD? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Do we want to have a constant learning rate or we better change it throughout training? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How do we decide when to stop training a neural net? ğŸ‘¶</strong></p><p>Answer here</p><br><p><strong>What is model checkpointing? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Can you tell us how you approach the model training process? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=neural-networks-for-computer-vision>Neural networks for computer vision</h2><p><strong>How we can use neural nets for computer vision? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Whatâ€™s a convolutional layer? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Why do we actually need convolutions? Canâ€™t we use fully-connected layers for that? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Whatâ€™s pooling in CNN? Why do we need it? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How does max pooling work? Are there other pooling techniques? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Are CNNs resistant to rotations? What happens to the predictions of a CNN if an image is rotated? ğŸš€</strong></p><p>Answer here</p><br><p><strong>What are augmentations? Why do we need them? ğŸ‘¶What kind of augmentations do you know? ğŸ‘¶How to choose which augmentations to use? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What kind of CNN architectures for classification do you know? ğŸš€</strong></p><p>Answer here</p><br><p><strong>What is transfer learning? How does it work? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is object detection? Do you know any architectures for that? ğŸš€</strong></p><p>Answer here</p><br><p><strong>What is object segmentation? Do you know any architectures for that? ğŸš€</strong></p><p>Answer here</p><br><h2 id=text-classification>Text classification</h2><p><strong>How can we use machine learning for text classification? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is bag of words? How we can use it for text classification? â€â­ï¸</strong></p><p>Bag of Words is a representation of text that describes the occurrence of words within a document. The order or structure of the words is not considered. For text classification, we look at the histogram of the words within the text and consider each word count as a feature.</p><br><p><strong>What are the advantages and disadvantages of bag of words? â€â­ï¸</strong></p><p>Advantages:</p><ol><li>Simple to understand and implement.</li></ol><p>Disadvantages:</p><ol><li>The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.</li><li>Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons</li><li>Discarding word order ignores the context, and in turn meaning of words in the document. Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (â€œthis is interestingâ€ vs â€œis this interestingâ€), synonyms (â€œold bikeâ€ vs â€œused bikeâ€).</li></ol><br><p><strong>What are N-grams? How can we use them? â€â­ï¸</strong></p><p>The function to tokenize into consecutive sequences of words is called n-grams. It can be used to find out N most co-occurring words (how often word X is followed by word Y) in a given sentence.</p><br><p><strong>How large should be N for our bag of words when using N-grams? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is TF-IDF? How is it useful for text classification? â€â­ï¸</strong></p><p>Term Frequency (TF) is a scoring of the frequency of the word in the current document. Inverse Document Frequency(IDF) is a scoring of how rare the word is across documents. It is used in scenario where highy recurring words may not contain as much informational content
as the domain specific words. For example, words like â€œtheâ€ that are frequent across all documents therefore need to be less weighted. The Tf-IDF score highlights words that are distinct (contain useful information) in a given document.</p><br><p><strong>Which model would you use for text classification with bag of words features? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Would you prefer gradient boosting trees model or logistic regression when doing text classification with bag of words? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What are word embeddings? Why are they useful? Do you know Word2Vec? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Do you know any other ways to get word embeddings? ğŸš€</strong></p><p>Answer here</p><br><p><strong>If you have a sentence with multiple words, you may need to combine multiple word embeddings into one. How would you do it? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Would you prefer gradient boosting trees model or logistic regression when doing text classification with embeddings? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How can you use neural nets for text classification? ğŸš€</strong></p><p>Answer here</p><br><p><strong>How can we use CNN for text classification? ğŸš€</strong></p><p>Answer here</p><br><h2 id=clustering>Clustering</h2><p><strong>What is unsupervised learning? ğŸ‘¶</strong></p><p>Unsupervised learning aims to detect paterns in data where no labels are given.</p><br><p><strong>What is clustering? When do we need it? ğŸ‘¶</strong></p><p>Clustering algorithms group objects such that similar feature points are put into the same groups (clusters) and dissimilar feature points are put into different clusters.</p><br><p><strong>Do you know how K-means works? â€â­ï¸</strong></p><ol><li>Partition points into k subsets.</li><li>Compute the seed points as the new centroids of the clusters of the current partitioning.</li><li>Assign each point to the cluster with the nearest seed point.</li><li>Go back to step 2 or stop when the assignment does not change.</li></ol><br><p><strong>How to select K for K-means? â€â­ï¸</strong></p><ul><li>Domain knowledge, i.e. an expert knows the value of k</li><li>Elbow method: compute the clusters for different values of k, for each k, calculate the total within-cluster sum of square, plot the sum according to the number of clusters and use the band as the number of clusters.</li><li>Average silhouette method: compute the clusters for different values of k, for each k, calculate the average silhouette of observations, plot the silhouette according to the number of clusters and select the maximum as the number of clusters.</li></ul><br><p><strong>What are the other clustering algorithms do you know? â€â­ï¸</strong></p><ul><li>k-medoids: Takes the most central point instead of the mean value as the center of the cluster. This makes it more robust to noise.</li><li>Agglomerative Hierarchical Clustering (AHC): hierarchical clusters combining the nearest clusters starting with each point as its own cluster.</li><li>DIvisive ANAlysis Clustering (DIANA): hierarchical clustering starting with one cluster containing all points and splitting the clusters until each point describes its own cluster.</li><li>Density-Based Spatial Clustering of Applications with Noise (DBSCAN): Cluster defined as maximum set of density-connected points.</li></ul><br><p><strong>Do you know how DBScan works? â€â­ï¸</strong></p><ul><li>Two input parameters epsilon (neighborhood radius) and minPts (minimum number of points in an epsilon-neighborhood)</li><li>Cluster defined as maximum set of density-connected points.</li><li>Points p_j and p_i are density-connected w.r.t. epsilon and minPts if there is a point o such that both, i and j are density-reachable from o w.r.t. epsilon and minPts.</li><li>p_j is density-reachable from p_i w.r.t. epsilon, minPts if there is a chain of points p_i -> p_i+1 -> p_i+x = p_j such that p_i+x is directly density-reachable from p_i+x-1.</li><li>p_j is a directly density-reachable point of the neighborhood of p_i if dist(p_i,p_j) &lt;= epsilon.</li></ul><br><p><strong>When would you choose K-means and when DBScan? â€â­ï¸</strong></p><ul><li>DBScan is more robust to noise.</li><li>DBScan is better when the amount of clusters is difficult to guess.</li><li>K-means has a lower complexity, i.e. it will be much faster, especially with a larger amount of points.</li></ul><br><h2 id=dimensionality-reduction>Dimensionality reduction</h2><p><strong>What is the curse of dimensionality? Why do we care about it? â€â­ï¸</strong></p><p>Data in only one dimension is relatively tightly packed. Adding a dimension stretches the points across that dimension, pushing them further apart. Additional dimensions spread the data even further making high dimensional data extremely sparse. We care about it, because it is difficult to use machine learning in sparse spaces.</p><br><p><strong>Do you know any dimensionality reduction techniques? â€â­ï¸</strong></p><ul><li>Singular Value Decomposition (SVD)</li><li>Principal Component Analysis (PCA)</li><li>Linear Discriminant Analysis (LDA)</li><li>T-distributed Stochastic Neighbor Embedding (t-SNE)</li><li>Autoencoders</li><li>Fourier and Wavelet Transforms</li></ul><br><p><strong>Whatâ€™s singular value decomposition? How is it typically used for machine learning? â€â­ï¸</strong></p><ul><li>Singular Value Decomposition (SVD) is a general matrix decomposition method that factors a matrix X into three matrices L (left singular values), Î£ (diagonal matrix) and R^T (right singular values).</li><li>For machine learning, Principal Component Analysis (PCA) is typically used. It is a special type of SVD where the singular values correspond to the eigenvectors and the values of the diagonal matrix are the squares of the eigenvalues. We use these features as they are statistically descriptive.</li><li>Having calculated the eigenvectors and eigenvalues, we can use the Kaiser-Guttman criterion, a scree plot or the proportion of explained variance to determine the principal components (i.e. the final dimensionality) that are useful for dimensionality reduction.</li></ul><br><h2 id=ranking-and-search>Ranking and search</h2><p><strong>What is the ranking problem? Which models can you use to solve them? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What are good unsupervised baselines for text information retrieval? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How would you evaluate your ranking algorithms? Which offline metrics would you use? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is precision and recall at k? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>What is mean average precision at k? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How can we use machine learning for search? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How can we get training data for our ranking algorithms? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>Can we formulate the search problem as a classification problem? How? â€â­ï¸</strong></p><p>Answer here</p><br><p><strong>How can we use clicks data as the training data for ranking algorithms? ğŸš€</strong></p><p>Answer here</p><br><p><strong>Do you know how to use gradient boosting trees for ranking? ğŸš€</strong></p><p>Answer here</p><br><p><strong>How do you do an online evaluation of a new ranking algorithm? â€â­ï¸</strong></p><p>Answer here</p><br><h2 id=recommender-systems>Recommender systems</h2><p><strong>What is a recommender system? ğŸ‘¶</strong></p><p>Recommender systems are software tools and techniques that provide suggestions for items that are most likely of interest to a particular user.</p><br><p><strong>What are good baselines when building a recommender system? â€â­ï¸</strong></p><ul><li>A good recommer system should give relevant and personalized information.</li><li>It should not recommend items the user knows well or finds easily.</li><li>It should make diverse suggestions.</li><li>A user should explore new items.</li></ul><br><p><strong>What is collaborative filtering? â€â­ï¸</strong></p><ul><li>Collaborative filtering is the most prominent approach to generate recommendations.</li><li>It uses the wisdom of the crowd, i.e. it gives recommendations based on the experience of others.</li><li>A recommendation is calculated as the average of other experiences.</li><li>Say we want to give a score that indicates how much user u will like an item i. Then we can calculate it with the experience of N other users U as r_ui = 1/N * sum(v in U) r_vi.</li><li>In order to rate similar experiences with a higher weight, we can introduce a similarity between users that we use as a multiplier for each rating.</li><li>Also, as users have an individual profile, one user may have an average rating much larger than another user, so we use normalization techniques (e.g. centering or Z-score normalization) to remove the users&rsquo; biases.</li><li>Collaborative filtering does only need a rating matrix as input and improves over time. However, it does not work well on sparse data, does not work for cold starts (see below) and usually tends to overfit.</li></ul><br><p><strong>How we can incorporate implicit feedback (clicks, etc) into our recommender systems? â€â­ï¸</strong></p><p>In comparison to explicit feedback, implicit feedback datasets lack negative examples. For example, explicit feedback can be a positive or a negative rating, but implicit feedback may be the number of purchases or clicks. One popular approach to solve this problem is named weighted alternating least squares (wALS) [Hu, Y., Koren, Y., & Volinsky, C. (2008, December). Collaborative filtering for implicit feedback datasets. In Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on (pp. 263-272). IEEE.]. Instead of modeling the rating matrix directly, the numbers (e.g. amount of clicks) describe the strength in observations of user actions. The model tries to find latent factors that can be used to predict the expected preference of a user for an item.</p><br><p><strong>What is the cold start problem? â€â­ï¸</strong></p><p>Collaborative filterung incorporates crowd knowledge to give recommendations for certain items. Say we want to recommend how much a user will like an item, we then will calculate the score using the recommendations of other users for this certain item. We can distinguish between two different ways of a cold start problem now. First, if there is a new item that has not been rated yet, we cannot give any recommendation. Also, when there is a new user, we cannot calculate a similarity to any other user.</p><br><p><strong>Possible approaches to solving the cold start problem? â€â­ï¸ğŸš€</strong></p><ul><li>Content-based filtering incorporates features about items to calculate a similarity between them. In this way, we can recommend items that have a high similarity to items that a user liked already. In this way, we are not dependant on the ratings of other users for a given item anymore and solve the cold start problem for new items.</li><li>Demographic filtering incorporates user profiles to calculate a similarity between them and solves the cold start problem for new users.</li></ul><br><h2 id=time-series>Time series</h2><p><strong>What is a time series? ğŸ‘¶</strong></p><p>A time series is a set of observations ordered in time usually collected at regular intervals.</p><br><p><strong>How is time series different from the usual regression problem? ğŸ‘¶</strong></p><p>The principle behind causal forecasting is that the value that has to be predicted is dependant on the input features (causal factors). In time series forecasting, the to be predicted value is expected to follow a certain pattern over time.</p><br><p><strong>Which models do you know for solving time series problems? â€â­ï¸</strong></p><ul><li>Simple Exponential Smoothing: approximate the time series with an exponentional function</li><li>Trend-Corrected Exponential Smoothing (Holtâ€˜s Method): exponential smoothing that also models the trend</li><li>Trend- and Seasonality-Corrected Exponential Smoothing (Holt-Winterâ€˜s Method): exponential smoothing that also models trend and seasonality</li><li>Time Series Decomposition: decomposed a time series into the four components trend, seasonal variation, cycling varation and irregular component</li><li>Autoregressive models: similar to multiple linear regression, except that the dependent variable y_t depends on its own previous values rather than other independent variables.</li><li>Deep learning approaches (RNN, LSTM, etc.)</li></ul><br><p><strong>If thereâ€™s a trend in our series, how we can remove it? And why would we want to do it? â€â­ï¸</strong></p><p>We can explicitly model the trend (and/or seasonality) with approaches such as Holt&rsquo;s Method or Holt-Winter&rsquo;s Method. We want to explicitly model the trend to reach the stationarity property for the data. Many time series approaches require stationarity. Without stationarity,the interpretation of the results of these analyses is problematic [Manuca, Radu & Savit, Robert. (1996). Stationarity and nonstationarity in time series analysis. Physica D: Nonlinear Phenomena. 99. 134-161. 10.1016/S0167-2789(96)00139-X. ].</p><br><p><strong>You have a series with only one variable â€œyâ€ measured at time t. How do predict â€œyâ€ at time t+1? Which approaches would you use? â€â­ï¸</strong></p><p>We want to look at the correlation between different observations of y. This measure of correlation is called autocorrelation. Autoregressive models are multiple regression models where the time-lag series of the original time series are treated like multiple independent variables.</p><br><p><strong>You have a series with a variable â€œyâ€ and a set of features. How do you predict â€œyâ€ at t+1? Which approaches would you use? â€â­ï¸</strong></p><p>Given the assumption that the set of features gives a meaningful causation to y, a causal forecasting approach such as linear regression or multiple nonlinear regression might be useful. In case there is a lot of data and the explanability of the results is not a high priority, we can also consider deep learning approaches.</p><br><p><strong>What are the problems with using trees for solving time series problems? â€â­ï¸</strong></p><p>Answer here</p><br></div><div class=article-widget><div class="container-xl row post-nav"><div class="col-6 post-nav-item"><div class=meta-nav>ä¸‹ä¸€é¡µ</div><a href=/books/awesome-interviews/%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E9%9D%A2%E8%AF%95%E9%A2%98/ rel=prev>æ•°ç†ç»Ÿè®¡é¢è¯•é¢˜</a></div></div></div><div class=body-footer><p>æœ€è¿‘æ›´æ–°äº 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>Â© 2017-2022 NGTE all rights reserved</p></div></div></div><script type=text/javascript id=clstr_globe async src="//clustrmaps.com/globe.js?d=kgpJG5sWZQpKujBmD-uW1B54-WBPol-DuDtrB2KFjKs"></script></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"å·²å¤åˆ¶")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>