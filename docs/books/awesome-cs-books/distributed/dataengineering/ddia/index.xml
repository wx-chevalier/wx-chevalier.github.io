<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DDIA | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/awesome-cs-books/distributed/dataengineering/ddia/</link><atom:link href="https://ng-tech.icu/books/awesome-cs-books/distributed/dataengineering/ddia/index.xml" rel="self" type="application/rss+xml"/><description>DDIA</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>DDIA</title><link>https://ng-tech.icu/books/awesome-cs-books/distributed/dataengineering/ddia/</link></image><item><title>2017-Kleppmann-《Designing Data Intensive Applications》</title><link>https://ng-tech.icu/books/awesome-cs-books/distributed/dataengineering/ddia/2017-kleppmann-designing-data-intensive-applications/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cs-books/distributed/dataengineering/ddia/2017-kleppmann-designing-data-intensive-applications/</guid><description>&lt;h1 id="designing-data-intensive-applicationshttpswwwgoodreadscombookshow23463279-designing-data-intensive-applications">&lt;a href="https://www.goodreads.com/book/show/23463279-designing-data-intensive-applications" target="_blank" rel="noopener">Designing Data-Intensive Applications&lt;/a>&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="#reliable-scalable-and-maintainable-applications">Reliable, scalable, and maintainable applications&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#reliability">Reliability&lt;/a>&lt;/li>
&lt;li>&lt;a href="#scalability">Scalability&lt;/a>&lt;/li>
&lt;li>&lt;a href="#maintainability">Maintainability&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#data-models-and-query-language">Data models and query language&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#relational-model-vs-document-model">Relational model vs document model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#query-languages-for-data">Query languages for data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#graph-like-data-models">Graph-like data models&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#storage-and-retrieval">Storage and retrieval&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#data-structures-that-power-up-your-database">Data structures that power up your database&lt;/a>&lt;/li>
&lt;li>&lt;a href="#transaction-processing-or-analytics">Transaction processing or analytics?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#column-oriented-storage">Column-oriented storage&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#encoding-and-evolution">Encoding and evolution&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#formats-for-encoding-data">Formats for encoding data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#modes-of-dataflow">Modes of dataflow&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#replication">Replication&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#leaders-and-followers">Leaders and followers&lt;/a>&lt;/li>
&lt;li>&lt;a href="#problems-with-replication-lag">Problems with replication lag&lt;/a>&lt;/li>
&lt;li>&lt;a href="#multi-leader-replication">Multi-leader replication&lt;/a>&lt;/li>
&lt;li>&lt;a href="#leaderless-replication">Leaderless replication&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#partitioning">Partitioning&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#partitioning-and-replication">Partitioning and replication&lt;/a>&lt;/li>
&lt;li>&lt;a href="#partition-of-key-value-data">Partition of key-value data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#partitioning-and-secondary-indexes">Partitioning and secondary indexes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#rebalancing-partitions">Rebalancing partitions&lt;/a>&lt;/li>
&lt;li>&lt;a href="#request-routing">Request routing&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#transactions">Transactions&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#the-slippery-concept-of-a-transaction">The slippery concept of a transaction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#weak-isolation-levels">Weak isolation levels&lt;/a>&lt;/li>
&lt;li>&lt;a href="#serializability">Serializability&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#the-trouble-with-distributed-systems">The trouble with distributed systems&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#faults-and-partial-failures">Faults and partial failures&lt;/a>&lt;/li>
&lt;li>&lt;a href="#unreliable-networks">Unreliable networks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#unreliable-clocks">Unreliable clocks&lt;/a>&lt;/li>
&lt;li>&lt;a href="#knowledge-truth-and-lies">Knowledge, truth and lies&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#consistency-and-consensus">Consistency and consensus&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#consistency-guarantees">Consistency guarantees&lt;/a>&lt;/li>
&lt;li>&lt;a href="#linearizability">Linearizability&lt;/a>&lt;/li>
&lt;li>&lt;a href="#ordering-guarantees">Ordering guarantees&lt;/a>&lt;/li>
&lt;li>&lt;a href="#distributed-transactions-and-consensus">Distributed transactions and consensus&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#batch-processing">Batch processing&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#batch-processing-with-unix-tools">Batch processing with Unix tools&lt;/a>&lt;/li>
&lt;li>&lt;a href="#map-reduce-and-distributed-filesystems">Map reduce and distributed filesystems&lt;/a>&lt;/li>
&lt;li>&lt;a href="#beyond-mapreduce">Beyond MapReduce&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#stream-processing">Stream processing&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#transmitting-event-streams">Transmitting event streams&lt;/a>&lt;/li>
&lt;li>&lt;a href="#databases-and-streams">Databases and streams&lt;/a>&lt;/li>
&lt;li>&lt;a href="#processing-streams">Processing Streams&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#the-future-of-data-systems">The future of data systems&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#data-integration">Data integration&lt;/a>&lt;/li>
&lt;li>&lt;a href="#unbundling-databases">Unbundling databases&lt;/a>&lt;/li>
&lt;li>&lt;a href="#aiming-for-correctness">Aiming for correctness&lt;/a>&lt;/li>
&lt;li>&lt;a href="#doing-the-right-thing">Doing the right thing&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="reliable-scalable-and-maintainable-applications">Reliable, scalable, and maintainable applications&lt;/h2>
&lt;p>A data-intensive application is typically built from standard building blocks. They usually need to:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Store data (&lt;em>databases&lt;/em>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Speed up reads (&lt;em>caches&lt;/em>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Search data (&lt;em>search indexes&lt;/em>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Send a message to another process asynchronously (&lt;em>stream processing&lt;/em>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Periodically crunch data (&lt;em>batch processing&lt;/em>)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Reliability&lt;/strong>. To work &lt;em>correctly&lt;/em> even in the face of &lt;em>adversity&lt;/em>.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Scalability&lt;/strong>. Reasonable ways of dealing with growth.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Maintainability&lt;/strong>. Be able to work on it &lt;em>productively&lt;/em>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h3 id="reliability">Reliability&lt;/h3>
&lt;p>Typical expectations:&lt;/p>
&lt;ul>
&lt;li>Application performs the function the user expected&lt;/li>
&lt;li>Tolerate the user making mistakes&lt;/li>
&lt;li>Its performance is good&lt;/li>
&lt;li>The system prevents abuse&lt;/li>
&lt;/ul>
&lt;p>Systems that anticipate faults and can cope with them are called &lt;em>fault-tolerant&lt;/em> or &lt;em>resilient&lt;/em>.&lt;/p>
&lt;p>&lt;strong>A fault is usually defined as one component of the system deviating from its spec&lt;/strong>, whereas &lt;em>failure&lt;/em> is when the system as a whole stops providing the required service to the user.&lt;/p>
&lt;p>You should generally &lt;strong>prefer tolerating faults over preventing faults&lt;/strong>.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Hardware faults&lt;/strong>. Until recently redundancy of hardware components was sufficient for most applications. As data volumes increase, more applications use a larger number of machines, proportionally increasing the rate of hardware faults. &lt;strong>There is a move towards systems that tolerate the loss of entire machines&lt;/strong>. A system that tolerates machine failure can be patched one node at a time, without downtime of the entire system (&lt;em>rolling upgrade&lt;/em>).&lt;/li>
&lt;li>&lt;strong>Software errors&lt;/strong>. It is unlikely that a large number of hardware components will fail at the same time. Software errors are a systematic error within the system, they tend to cause many more system failures than uncorrelated hardware faults.&lt;/li>
&lt;li>&lt;strong>Human errors&lt;/strong>. Humans are known to be unreliable. Configuration errors by operators are a leading cause of outages. You can make systems more reliable:
&lt;ul>
&lt;li>Minimising the opportunities for error, peg: with admin interfaces that make easy to do the &amp;ldquo;right thing&amp;rdquo; and discourage the &amp;ldquo;wrong thing&amp;rdquo;.&lt;/li>
&lt;li>Provide fully featured non-production &lt;em>sandbox&lt;/em> environments where people can explore and experiment safely.&lt;/li>
&lt;li>Automated testing.&lt;/li>
&lt;li>Quick and easy recovery from human error, fast to rollback configuration changes, roll out new code gradually and tools to recompute data.&lt;/li>
&lt;li>Set up detailed and clear monitoring, such as performance metrics and error rates (&lt;em>telemetry&lt;/em>).&lt;/li>
&lt;li>Implement good management practices and training.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="scalability">Scalability&lt;/h3>
&lt;p>This is how do we cope with increased load. We need to succinctly describe the current load on the system; only then we can discuss growth questions.&lt;/p>
&lt;hr>
&lt;h4 id="twitter-example">Twitter example&lt;/h4>
&lt;p>Twitter main operations&lt;/p>
&lt;ul>
&lt;li>Post tweet: a user can publish a new message to their followers (4.6k req/sec, over 12k req/sec peak)&lt;/li>
&lt;li>Home timeline: a user can view tweets posted by the people they follow (300k req/sec)&lt;/li>
&lt;/ul>
&lt;p>Two ways of implementing those operations:&lt;/p>
&lt;ol>
&lt;li>Posting a tweet simply inserts the new tweet into a global collection of tweets. When a user requests their home timeline, look up all the people they follow, find all the tweets for those users, and merge them (sorted by time). This could be done with a SQL &lt;code>JOIN&lt;/code>.&lt;/li>
&lt;li>Maintain a cache for each user&amp;rsquo;s home timeline. When a user &lt;em>posts a tweet&lt;/em>, look up all the people who follow that user, and insert the new tweet into each of their home timeline caches.&lt;/li>
&lt;/ol>
&lt;p>Approach 1, systems struggle to keep up with the load of home timeline queries. So the company switched to approach 2. The average rate of published tweets is almost two orders of magnitude lower than the rate of home timeline reads.&lt;/p>
&lt;p>Downside of approach 2 is that posting a tweet now requires a lot of extra work. Some users have over 30 million followers. A single tweet may result in over 30 million writes to home timelines.&lt;/p>
&lt;p>Twitter moved to an hybrid of both approaches. Tweets continue to be fanned out to home timelines but a small number of users with a very large number of followers are fetched separately and merged with that user&amp;rsquo;s home timeline when it is read, like in approach 1.&lt;/p>
&lt;hr>
&lt;h4 id="describing-performance">Describing performance&lt;/h4>
&lt;p>What happens when the load increases:&lt;/p>
&lt;ul>
&lt;li>How is the performance affected?&lt;/li>
&lt;li>How much do you need to increase your resources?&lt;/li>
&lt;/ul>
&lt;p>In a batch processing system such as Hadoop, we usually care about &lt;em>throughput&lt;/em>, or the number of records we can process per second.&lt;/p>
&lt;blockquote>
&lt;h5 id="latency-and-response-time">Latency and response time&lt;/h5>
&lt;p>The response time is what the client sees. Latency is the duration that a request is waiting to be handled.&lt;/p>
&lt;/blockquote>
&lt;p>It&amp;rsquo;s common to see the &lt;em>average&lt;/em> response time of a service reported. However, the mean is not very good metric if you want to know your &amp;ldquo;typical&amp;rdquo; response time, it does not tell you how many users actually experienced that delay.&lt;/p>
&lt;p>&lt;strong>Better to use percentiles.&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;em>Median&lt;/em> (&lt;em>50th percentile&lt;/em> or &lt;em>p50&lt;/em>). Half of user requests are served in less than the median response time, and the other half take longer than the median&lt;/li>
&lt;li>Percentiles &lt;em>95th&lt;/em>, &lt;em>99th&lt;/em> and &lt;em>99.9th&lt;/em> (&lt;em>p95&lt;/em>, &lt;em>p99&lt;/em> and &lt;em>p999&lt;/em>) are good to figure out how bad your outliners are.&lt;/li>
&lt;/ul>
&lt;p>Amazon describes response time requirements for internal services in terms of the 99.9th percentile because the customers with the slowest requests are often those who have the most data. The most valuable customers.&lt;/p>
&lt;p>On the other hand, optimising for the 99.99th percentile would be too expensive.&lt;/p>
&lt;p>&lt;em>Service level objectives&lt;/em> (SLOs) and &lt;em>service level agreements&lt;/em> (SLAs) are contracts that define the expected performance and availability of a service.
An SLA may state the median response time to be less than 200ms and a 99th percentile under 1s. &lt;strong>These metrics set expectations for clients of the service and allow customers to demand a refund if the SLA is not met.&lt;/strong>&lt;/p>
&lt;p>Queueing delays often account for large part of the response times at high percentiles. &lt;strong>It is important to measure times on the client side.&lt;/strong>&lt;/p>
&lt;p>When generating load artificially, the client needs to keep sending requests independently of the response time.&lt;/p>
&lt;blockquote>
&lt;h5 id="percentiles-in-practice">Percentiles in practice&lt;/h5>
&lt;p>Calls in parallel, the end-user request still needs to wait for the slowest of the parallel calls to complete.
The chance of getting a slow call increases if an end-user request requires multiple backend calls.&lt;/p>
&lt;/blockquote>
&lt;h4 id="approaches-for-coping-with-load">Approaches for coping with load&lt;/h4>
&lt;ul>
&lt;li>&lt;em>Scaling up&lt;/em> or &lt;em>vertical scaling&lt;/em>: Moving to a more powerful machine&lt;/li>
&lt;li>&lt;em>Scaling out&lt;/em> or &lt;em>horizontal scaling&lt;/em>: Distributing the load across multiple smaller machines.&lt;/li>
&lt;li>&lt;em>Elastic&lt;/em> systems: Automatically add computing resources when detected load increase. Quite useful if load is unpredictable.&lt;/li>
&lt;/ul>
&lt;p>Distributing stateless services across multiple machines is fairly straightforward. Taking stateful data systems from a single node to a distributed setup can introduce a lot of complexity. Until recently it was common wisdom to keep your database on a single node.&lt;/p>
&lt;h3 id="maintainability">Maintainability&lt;/h3>
&lt;p>The majority of the cost of software is in its ongoing maintenance. There are three design principles for software systems:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Operability&lt;/strong>. Make it easy for operation teams to keep the system running.&lt;/li>
&lt;li>&lt;strong>Simplicity&lt;/strong>. Easy for new engineers to understand the system by removing as much complexity as possible.&lt;/li>
&lt;li>&lt;strong>Evolvability&lt;/strong>. Make it easy for engineers to make changes to the system in the future.&lt;/li>
&lt;/ul>
&lt;h4 id="operability-making-life-easy-for-operations">Operability: making life easy for operations&lt;/h4>
&lt;p>A good operations team is responsible for&lt;/p>
&lt;ul>
&lt;li>Monitoring and quickly restoring service if it goes into bad state&lt;/li>
&lt;li>Tracking down the cause of problems&lt;/li>
&lt;li>Keeping software and platforms up to date&lt;/li>
&lt;li>Keeping tabs on how different systems affect each other&lt;/li>
&lt;li>Anticipating future problems&lt;/li>
&lt;li>Establishing good practices and tools for development&lt;/li>
&lt;li>Perform complex maintenance tasks, like platform migration&lt;/li>
&lt;li>Maintaining the security of the system&lt;/li>
&lt;li>Defining processes that make operations predictable&lt;/li>
&lt;li>Preserving the organisation&amp;rsquo;s knowledge about the system&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Good operability means making routine tasks easy.&lt;/strong>&lt;/p>
&lt;h4 id="simplicity-managing-complexity">Simplicity: managing complexity&lt;/h4>
&lt;p>When complexity makes maintenance hard, budget and schedules are often overrun. There is a greater risk of introducing bugs.&lt;/p>
&lt;p>Making a system simpler means removing &lt;em>accidental&lt;/em> complexity, as non inherent in the problem that the software solves (as seen by users).&lt;/p>
&lt;p>One of the best tools we have for removing accidental complexity is &lt;em>abstraction&lt;/em> that hides the implementation details behind clean and simple to understand APIs and facades.&lt;/p>
&lt;h4 id="evolvability-making-change-easy">Evolvability: making change easy&lt;/h4>
&lt;p>&lt;em>Agile&lt;/em> working patterns provide a framework for adapting to change.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;em>Functional requirements&lt;/em>: what the application should do&lt;/li>
&lt;li>&lt;em>Nonfunctional requirements&lt;/em>: general properties like security, reliability, compliance, scalability, compatibility and maintainability.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="data-models-and-query-language">Data models and query language&lt;/h2>
&lt;p>Most applications are built by layering one data model on top of another. Each layer hides the complexity of the layers below by providing a clean data model. These abstractions allow different groups of people to work effectively.&lt;/p>
&lt;h3 id="relational-model-vs-document-model">Relational model vs document model&lt;/h3>
&lt;p>The roots of relational databases lie in &lt;em>business data processing&lt;/em>, &lt;em>transaction processing&lt;/em> and &lt;em>batch processing&lt;/em>.&lt;/p>
&lt;p>The goal was to hide the implementation details behind a cleaner interface.&lt;/p>
&lt;p>&lt;em>Not Only SQL&lt;/em> has a few driving forces:&lt;/p>
&lt;ul>
&lt;li>Greater scalability&lt;/li>
&lt;li>preference for free and open source software&lt;/li>
&lt;li>Specialised query optimisations&lt;/li>
&lt;li>Desire for a more dynamic and expressive data model&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>With a SQL model, if data is stored in a relational tables, an awkward translation layer is translated, this is called &lt;em>impedance mismatch&lt;/em>.&lt;/strong>&lt;/p>
&lt;p>JSON model reduces the impedance mismatch and the lack of schema is often cited as an advantage.&lt;/p>
&lt;p>JSON representation has better &lt;em>locality&lt;/em> than the multi-table SQL schema. All the relevant information is in one place, and one query is sufficient.&lt;/p>
&lt;p>In relational databases, it&amp;rsquo;s normal to refer to rows in other tables by ID, because joins are easy. In document databases, joins are not needed for one-to-many tree structures, and support for joins is often weak.&lt;/p>
&lt;p>If the database itself does not support joins, you have to emulate a join in application code by making multiple queries.&lt;/p>
&lt;p>The most popular database for business data processing in the 1970s was the IBM&amp;rsquo;s &lt;em>Information Management System&lt;/em> (IMS).&lt;/p>
&lt;p>IMS used a &lt;em>hierarchical model&lt;/em> and like document databases worked well for one-to-many relationships, but it made many-to-,any relationships difficult, and it didn&amp;rsquo;t support joins.&lt;/p>
&lt;h4 id="the-network-model">The network model&lt;/h4>
&lt;p>Standardised by a committee called the Conference on Data Systems Languages (CODASYL) model was a generalisation of the hierarchical model. In the tree structure of the hierarchical model, every record has exactly one parent, while in the network model, a record could have multiple parents.&lt;/p>
&lt;p>The links between records are like pointers in a programming language. The only way of accessing a record was to follow a path from a root record called &lt;em>access path&lt;/em>.&lt;/p>
&lt;p>A query in CODASYL was performed by moving a cursor through the database by iterating over a list of records. If you didn&amp;rsquo;t have a path to the data you wanted, you were in a difficult situation as it was difficult to make changes to an application&amp;rsquo;s data model.&lt;/p>
&lt;h4 id="the-relational-model">The relational model&lt;/h4>
&lt;p>By contrast, the relational model was a way to lay out all the data in the open&amp;quot; a relation (table) is simply a collection of tuples (rows), and that&amp;rsquo;s it.&lt;/p>
&lt;p>The query optimiser automatically decides which parts of the query to execute in which order, and which indexes to use (the access path).&lt;/p>
&lt;p>The relational model thus made it much easier to add new features to applications.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>The main arguments in favour of the document data model are schema flexibility, better performance due to locality, and sometimes closer data structures to the ones used by the applications. The relation model counters by providing better support for joins, and many-to-one and many-to-many relationships.&lt;/strong>&lt;/p>
&lt;p>If the data in your application has a document-like structure, then it&amp;rsquo;s probably a good idea to use a document model. The relational technique of &lt;em>shredding&lt;/em> can lead unnecessary complicated application code.&lt;/p>
&lt;p>The poor support for joins in document databases may or may not be a problem.&lt;/p>
&lt;p>If you application does use many-to-many relationships, the document model becomes less appealing. Joins can be emulated in application code by making multiple requests. Using the document model can lead to significantly more complex application code and worse performance.&lt;/p>
&lt;h4 id="schema-flexibility">Schema flexibility&lt;/h4>
&lt;p>Most document databases do not enforce any schema on the data in documents. Arbitrary keys and values can be added to a document, when reading, &lt;strong>clients have no guarantees as to what fields the documents may contain.&lt;/strong>&lt;/p>
&lt;p>Document databases are sometimes called &lt;em>schemaless&lt;/em>, but maybe a more appropriate term is &lt;em>schema-on-read&lt;/em>, in contrast to &lt;em>schema-on-write&lt;/em>.&lt;/p>
&lt;p>Schema-on-read is similar to dynamic (runtime) type checking, whereas schema-on-write is similar to static (compile-time) type checking.&lt;/p>
&lt;p>The schema-on-read approach if the items on the collection don&amp;rsquo;t have all the same structure (heterogeneous)&lt;/p>
&lt;ul>
&lt;li>Many different types of objects&lt;/li>
&lt;li>Data determined by external systems&lt;/li>
&lt;/ul>
&lt;h4 id="data-locality-for-queries">Data locality for queries&lt;/h4>
&lt;p>If your application often needs to access the entire document, there is a performance advantage to this &lt;em>storage locality&lt;/em>.&lt;/p>
&lt;p>The database typically needs to load the entire document, even if you access only a small portion of it. On updates, the entire document usually needs to be rewritten, it is recommended that you keep documents fairly small.&lt;/p>
&lt;h4 id="convergence-of-document-and-relational-databases">Convergence of document and relational databases&lt;/h4>
&lt;p>PostgreSQL does support JSON documents. RethinkDB supports relational-like joins in its query language and some MongoDB drivers automatically resolve database references. Relational and document databases are becoming more similar over time.&lt;/p>
&lt;h3 id="query-languages-for-data">Query languages for data&lt;/h3>
&lt;p>SQL is a &lt;em>declarative&lt;/em> query language. In an &lt;em>imperative language&lt;/em>, you tell the computer to perform certain operations in order.&lt;/p>
&lt;p>In a declarative query language you just specify the pattern of the data you want, but not &lt;em>how&lt;/em> to achieve that goal.&lt;/p>
&lt;p>A declarative query language hides implementation details of the database engine, making it possible for the database system to introduce performance improvements without requiring any changes to queries.&lt;/p>
&lt;p>Declarative languages often lend themselves to parallel execution while imperative code is very hard to parallelise across multiple cores because it specifies instructions that must be performed in a particular order. Declarative languages specify only the pattern of the results, not the algorithm that is used to determine results.&lt;/p>
&lt;h4 id="declarative-queries-on-the-web">Declarative queries on the web&lt;/h4>
&lt;p>In a web browser, using declarative CSS styling is much better than manipulating styles imperatively in JavaScript. Declarative languages like SQL turned out to be much better than imperative query APIs.&lt;/p>
&lt;h4 id="mapreduce-querying">MapReduce querying&lt;/h4>
&lt;p>&lt;em>MapReduce&lt;/em> is a programming model for processing large amounts of data in bulk across many machines, popularised by Google.&lt;/p>
&lt;p>Mongo offers a MapReduce solution.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="nx">db&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">observations&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">mapReduce&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">function&lt;/span> &lt;span class="nx">map&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="mi">2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">year&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">this&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">observationTimestamp&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">getFullYear&lt;/span>&lt;span class="p">();&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">var&lt;/span> &lt;span class="nx">month&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">this&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">observationTimestamp&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">getMonth&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">emit&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">year&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="s2">&amp;#34;-&amp;#34;&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="nx">month&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="k">this&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">numAnimals&lt;/span>&lt;span class="p">);&lt;/span> &lt;span class="mi">3&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="kd">function&lt;/span> &lt;span class="nx">reduce&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">key&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nx">values&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="nb">Array&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nx">values&lt;/span>&lt;span class="p">);&lt;/span> &lt;span class="mi">5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">query&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="nx">family&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s2">&amp;#34;Sharks&amp;#34;&lt;/span> &lt;span class="p">},&lt;/span> &lt;span class="mi">1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">out&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s2">&amp;#34;monthlySharkReport&amp;#34;&lt;/span> &lt;span class="mi">6&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;code>map&lt;/code> and &lt;code>reduce&lt;/code> functions must be &lt;em>pure&lt;/em> functions, they cannot perform additional database queries and they must not have any side effects. These restrictions allow the database to run the functions anywhere, in any order, and rerun them on failure.&lt;/p>
&lt;p>A usability problem with MapReduce is that you have to write two carefully coordinated functions. A declarative language offers more opportunities for a query optimiser to improve the performance of a query. For there reasons, MongoDB 2.2 added support for a declarative query language called &lt;em>aggregation pipeline&lt;/em>&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-js" data-lang="js">&lt;span class="line">&lt;span class="cl">&lt;span class="nx">db&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">observations&lt;/span>&lt;span class="p">.&lt;/span>&lt;span class="nx">aggregate&lt;/span>&lt;span class="p">([&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span> &lt;span class="nx">$match&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="nx">family&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s2">&amp;#34;Sharks&amp;#34;&lt;/span> &lt;span class="p">}&lt;/span> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">$group&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">_id&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">year&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="nx">$year&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s2">&amp;#34;$observationTimestamp&amp;#34;&lt;/span> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">month&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="nx">$month&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s2">&amp;#34;$observationTimestamp&amp;#34;&lt;/span> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nx">totalAnimals&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="p">{&lt;/span> &lt;span class="nx">$sum&lt;/span>&lt;span class="o">:&lt;/span> &lt;span class="s2">&amp;#34;$numAnimals&amp;#34;&lt;/span> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="p">},&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="p">]);&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="graph-like-data-models">Graph-like data models&lt;/h3>
&lt;p>If many-to-many relationships are very common in your application, it becomes more natural to start modelling your data as a graph.&lt;/p>
&lt;p>A graph consists of &lt;em>vertices&lt;/em> (&lt;em>nodes&lt;/em> or &lt;em>entities&lt;/em>) and &lt;em>edges&lt;/em> (&lt;em>relationships&lt;/em> or &lt;em>arcs&lt;/em>).&lt;/p>
&lt;p>Well-known algorithms can operate on these graphs, like the shortest path between two points, or popularity of a web page.&lt;/p>
&lt;p>There are several ways of structuring and querying the data. The &lt;em>property graph&lt;/em> model (implemented by Neo4j, Titan, and Infinite Graph) and the &lt;em>triple-store&lt;/em> model (implemented by Datomic, AllegroGraph, and others). There are also three declarative query languages for graphs: Cypher, SPARQL, and Datalog.&lt;/p>
&lt;h4 id="property-graphs">Property graphs&lt;/h4>
&lt;p>Each vertex consists of:&lt;/p>
&lt;ul>
&lt;li>Unique identifier&lt;/li>
&lt;li>Outgoing edges&lt;/li>
&lt;li>Incoming edges&lt;/li>
&lt;li>Collection of properties (key-value pairs)&lt;/li>
&lt;/ul>
&lt;p>Each edge consists of:&lt;/p>
&lt;ul>
&lt;li>Unique identifier&lt;/li>
&lt;li>Vertex at which the edge starts (&lt;em>tail vertex&lt;/em>)&lt;/li>
&lt;li>Vertex at which the edge ends (&lt;em>head vertex&lt;/em>)&lt;/li>
&lt;li>Label to describe the kind of relationship between the two vertices&lt;/li>
&lt;li>A collection of properties (key-value pairs)&lt;/li>
&lt;/ul>
&lt;p>Graphs provide a great deal of flexibility for data modelling. Graphs are good for evolvability.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>&lt;em>Cypher&lt;/em> is a declarative language for property graphs created by Neo4j&lt;/li>
&lt;li>Graph queries in SQL. In a relational database, you usually know in advance which joins you need in your query. In a graph query, the number if joins is not fixed in advance. In Cypher &lt;code>:WITHIN*0...&lt;/code> expresses &amp;ldquo;follow a &lt;code>WITHIN&lt;/code> edge, zero or more times&amp;rdquo; (like the &lt;code>*&lt;/code> operator in a regular expression). This idea of variable-length traversal paths in a query can be expressed using something called &lt;em>recursive common table expressions&lt;/em> (the &lt;code>WITH RECURSIVE&lt;/code> syntax).&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h4 id="triple-stores-and-sparql">Triple-stores and SPARQL&lt;/h4>
&lt;p>In a triple-store, all information is stored in the form of very simple three-part statements: &lt;em>subject&lt;/em>, &lt;em>predicate&lt;/em>, &lt;em>object&lt;/em> (peg: &lt;em>Jim&lt;/em>, &lt;em>likes&lt;/em>, &lt;em>bananas&lt;/em>). A triple is equivalent to a vertex in graph.&lt;/p>
&lt;h4 id="the-sparql-query-language">The SPARQL query language&lt;/h4>
&lt;p>&lt;em>SPARQL&lt;/em> is a query language for triple-stores using the RDF data model.&lt;/p>
&lt;h4 id="the-foundation-datalog">The foundation: Datalog&lt;/h4>
&lt;p>&lt;em>Datalog&lt;/em> provides the foundation that later query languages build upon. Its model is similar to the triple-store model, generalised a bit. Instead of writing a triple (&lt;em>subject&lt;/em>, &lt;em>predicate&lt;/em>, &lt;em>object&lt;/em>), we write as &lt;em>predicate(subject, object)&lt;/em>.&lt;/p>
&lt;p>We define &lt;em>rules&lt;/em> that tell the database about new predicates and rules can refer to other rules, just like functions can call other functions or recursively call themselves.&lt;/p>
&lt;p>Rules can be combined and reused in different queries. It&amp;rsquo;s less convenient for simple one-off queries, but it can cope better if your data is complex.&lt;/p>
&lt;h2 id="storage-and-retrieval">Storage and retrieval&lt;/h2>
&lt;p>Databases need to do two things: store the data and give the data back to you.&lt;/p>
&lt;h3 id="data-structures-that-power-up-your-database">Data structures that power up your database&lt;/h3>
&lt;p>Many databases use a &lt;em>log&lt;/em>, which is append-only data file. Real databases have more issues to deal with tho (concurrency control, reclaiming disk space so the log doesn&amp;rsquo;t grow forever and handling errors and partially written records).&lt;/p>
&lt;blockquote>
&lt;p>A &lt;em>log&lt;/em> is an append-only sequence of records&lt;/p>
&lt;/blockquote>
&lt;p>In order to efficiently find the value for a particular key, we need a different data structure: an &lt;em>index&lt;/em>. An index is an &lt;em>additional&lt;/em> structure that is derived from the primary data.&lt;/p>
&lt;p>Well-chosen indexes speed up read queries but every index slows down writes. That&amp;rsquo;s why databases don&amp;rsquo;t index everything by default, but require you to choose indexes manually using your knowledge on typical query patterns.&lt;/p>
&lt;h4 id="hash-indexes">Hash indexes&lt;/h4>
&lt;p>Key-value stores are quite similar to the &lt;em>dictionary&lt;/em> type (hash map or hash table).&lt;/p>
&lt;p>Let&amp;rsquo;s say our storage consists only of appending to a file. The simplest indexing strategy is to keep an in-memory hash map where every key is mapped to a byte offset in the data file. Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote.&lt;/p>
&lt;p>Bitcask (the default storage engine in Riak) does it like that. The only requirement it has is that all the keys fit in the available RAM. Values can use more space than there is available in memory, since they can be loaded from disk.&lt;/p>
&lt;p>A storage engine like Bitcask is well suited to situations where the value for each key is updated frequently. There are a lot of writes, but there are too many distinct keys, you have a large number of writes per key, but it&amp;rsquo;s feasible to keep all keys in memory.&lt;/p>
&lt;p>As we only ever append to a file, so how do we avoid eventually running out of disk space? &lt;strong>A good solution is to break the log into segments of certain size by closing the segment file when it reaches a certain size, and making subsequent writes to a new segment file. We can then perform &lt;em>compaction&lt;/em> on these segments.&lt;/strong> Compaction means throwing away duplicate keys in the log, and keeping only the most recent update for each key.&lt;/p>
&lt;p>We can also merge several segments together at the sae time as performing the compaction. Segments are never modified after they have been written, so the merged segment is written to a new file. Merging and compaction of frozen segments can be done in a background thread. After the merging process is complete, we switch read requests to use the new merged segment instead of the old segments, and the old segment files can simply be deleted.&lt;/p>
&lt;p>Each segment now has its own in-memory hash table, mapping keys to file offsets. In order to find a value for a key, we first check the most recent segment hash map; if the key is not present we check the second-most recent segment and so on. The merging process keeps the number of segments small, so lookups don&amp;rsquo;t need to check many hash maps.&lt;/p>
&lt;p>Some issues that are important in a real implementation:&lt;/p>
&lt;ul>
&lt;li>File format. It is simpler to use binary format.&lt;/li>
&lt;li>Deleting records. Append special deletion record to the data file (&lt;em>tombstone&lt;/em>) that tells the merging process to discard previous values.&lt;/li>
&lt;li>Crash recovery. If restarted, the in-memory hash maps are lost. You can recover from reading each segment but that would take long time. Bitcask speeds up recovery by storing a snapshot of each segment hash map on disk.&lt;/li>
&lt;li>Partially written records. The database may crash at any time. Bitcask includes checksums allowing corrupted parts of the log to be detected and ignored.&lt;/li>
&lt;li>Concurrency control. As writes are appended to the log in a strictly sequential order, a common implementation is to have a single writer thread. Segments are immutable, so they can be read concurrently by multiple threads.&lt;/li>
&lt;/ul>
&lt;p>Append-only design turns out to be good for several reasons:&lt;/p>
&lt;ul>
&lt;li>Appending and segment merging are sequential write operations, much faster than random writes, especially on magnetic spinning-disks.&lt;/li>
&lt;li>Concurrency and crash recovery are much simpler.&lt;/li>
&lt;li>Merging old segments avoids files getting fragmented over time.&lt;/li>
&lt;/ul>
&lt;p>Hash table has its limitations too:&lt;/p>
&lt;ul>
&lt;li>The hash table must fit in memory. It is difficult to make an on-disk hash map perform well.&lt;/li>
&lt;li>Range queries are not efficient.&lt;/li>
&lt;/ul>
&lt;h4 id="sstables-and-lsm-trees">SSTables and LSM-Trees&lt;/h4>
&lt;p>We introduce a new requirement to segment files: we require that the sequence of key-value pairs is &lt;em>sorted by key&lt;/em>.&lt;/p>
&lt;p>We call this &lt;em>Sorted String Table&lt;/em>, or &lt;em>SSTable&lt;/em>. We require that each key only appears once within each merged segment file (compaction already ensures that). SSTables have few big advantages over log segments with hash indexes&lt;/p>
&lt;ol>
&lt;li>&lt;strong>Merging segments is simple and efficient&lt;/strong> (we can use algorithms like &lt;em>mergesort&lt;/em>). When multiple segments contain the same key, we can keep the value from the most recent segment and discard the values in older segments.&lt;/li>
&lt;li>&lt;strong>You no longer need to keep an index of all the keys in memory.&lt;/strong> For a key like &lt;code>handiwork&lt;/code>, when you know the offsets for the keys &lt;code>handback&lt;/code> and &lt;code>handsome&lt;/code>, you know &lt;code>handiwork&lt;/code> must appear between those two. You can jump to the offset for &lt;code>handback&lt;/code> and scan from there until you find &lt;code>handiwork&lt;/code>, if not, the key is not present. You still need an in-memory index to tell you the offsets for some of the keys. One key for every few kilobytes of segment file is sufficient.&lt;/li>
&lt;li>Since read requests need to scan over several key-value pairs in the requested range anyway, &lt;strong>it is possible to group those records into a block and compress it&lt;/strong> before writing it to disk.&lt;/li>
&lt;/ol>
&lt;p>How do we get the data sorted in the first place? With red-black trees or AVL trees, you can insert keys in any order and read them back in sorted order.&lt;/p>
&lt;ul>
&lt;li>When a write comes in, add it to an in-memory balanced tree structure (&lt;em>memtable&lt;/em>).&lt;/li>
&lt;li>When the memtable gets bigger than some threshold (megabytes), write it out to disk as an SSTable file. Writes can continue to a new memtable instance.&lt;/li>
&lt;li>On a read request, try to find the key in the memtable, then in the most recent on-disk segment, then in the next-older segment, etc.&lt;/li>
&lt;li>From time to time, run merging and compaction in the background to discard overwritten and deleted values.&lt;/li>
&lt;/ul>
&lt;p>If the database crashes, the most recent writes are lost. We can keep a separate log on disk to which every write is immediately appended. That log is not in sorted order, but that doesn&amp;rsquo;t matter, because its only purpose is to restore the memtable after crash. Every time the memtable is written out to an SSTable, the log can be discarded.&lt;/p>
&lt;p>&lt;strong>Storage engines that are based on this principle of merging and compacting sorted files are often called LSM structure engines (Log Structure Merge-Tree).&lt;/strong>&lt;/p>
&lt;p>Lucene, an indexing engine for full-text search used by Elasticsearch and Solr, uses a similar method for storing its &lt;em>term dictionary&lt;/em>.&lt;/p>
&lt;p>LSM-tree algorithm can be slow when looking up keys that don&amp;rsquo;t exist in the database. To optimise this, storage engines often use additional &lt;em>Bloom filters&lt;/em> (a memory-efficient data structure for approximating the contents of a set).&lt;/p>
&lt;p>There are also different strategies to determine the order and timing of how SSTables are compacted and merged. Mainly two &lt;em>size-tiered&lt;/em> and &lt;em>leveled&lt;/em> compaction. LevelDB and RocksDB use leveled compaction, HBase use size-tiered, and Cassandra supports both. In size-tiered compaction, newer and smaller SSTables are successively merged into older and larger SSTables. In leveled compaction, the key range is split up into smaller SSTables and older data is moved into separate &amp;ldquo;levels&amp;rdquo;, which allows the compaction to use less disk space.&lt;/p>
&lt;h4 id="b-trees">B-trees&lt;/h4>
&lt;p>This is the most widely used indexing structure. B-tress keep key-value pairs sorted by key, which allows efficient key-value lookups and range queries.&lt;/p>
&lt;p>The log-structured indexes break the database down into variable-size &lt;em>segments&lt;/em> typically several megabytes or more. B-trees break the database down into fixed-size &lt;em>blocks&lt;/em> or &lt;em>pages&lt;/em>, traditionally 4KB.&lt;/p>
&lt;p>One page is designated as the &lt;em>root&lt;/em> and you start from there. The page contains several keys and references to child pages.&lt;/p>
&lt;p>If you want to update the value for an existing key in a B-tree, you search for the leaf page containing that key, change the value in that page, and write the page back to disk. If you want to add new key, find the page and add it to the page. If there isn&amp;rsquo;t enough free space in the page to accommodate the new key, it is split in two half-full pages, and the parent page is updated to account for the new subdivision of key ranges.&lt;/p>
&lt;p>Trees remain &lt;em>balanced&lt;/em>. A B-tree with &lt;em>n&lt;/em> keys always has a depth of &lt;em>O&lt;/em>(log &lt;em>n&lt;/em>).&lt;/p>
&lt;p>The basic underlying write operation of a B-tree is to overwrite a page on disk with new data. It is assumed that the overwrite does not change the location of the page, all references to that page remain intact. This is a big contrast to log-structured indexes such as LSM-trees, which only append to files.&lt;/p>
&lt;p>Some operations require several different pages to be overwritten. When you split a page, you need to write the two pages that were split, and also overwrite their parent. If the database crashes after only some of the pages have been written, you end up with a corrupted index.&lt;/p>
&lt;p>It is common to include an additional data structure on disk: a &lt;em>write-ahead log&lt;/em> (WAL, also know as the &lt;em>redo log&lt;/em>).&lt;/p>
&lt;p>Careful concurrency control is required if multiple threads are going to access, typically done protecting the tree internal data structures with &lt;em>latches&lt;/em> (lightweight locks).&lt;/p>
&lt;h4 id="b-trees-and-lsm-trees">B-trees and LSM-trees&lt;/h4>
&lt;p>LSM-trees are typically faster for writes, whereas B-trees are thought to be faster for reads. Reads are typically slower on LSM-tress as they have to check several different data structures and SSTables at different stages of compaction.&lt;/p>
&lt;p>Advantages of LSM-trees:&lt;/p>
&lt;ul>
&lt;li>LSM-trees are typically able to sustain higher write throughput than B-trees, party because they sometimes have lower write amplification: a write to the database results in multiple writes to disk. The more a storage engine writes to disk, the fewer writes per second it can handle.&lt;/li>
&lt;li>LSM-trees can be compressed better, and thus often produce smaller files on disk than B-trees. B-trees tend to leave disk space unused due to fragmentation.&lt;/li>
&lt;/ul>
&lt;p>Downsides of LSM-trees:&lt;/p>
&lt;ul>
&lt;li>Compaction process can sometimes interfere with the performance of ongoing reads and writes. B-trees can be more predictable. The bigger the database, the the more disk bandwidth is required for compaction. Compaction cannot keep up with the rate of incoming writes, if not configured properly you can run out of disk space.&lt;/li>
&lt;li>On B-trees, each key exists in exactly one place in the index. This offers strong transactional semantics. Transaction isolation is implemented using locks on ranges of keys, and in a B-tree index, those locks can be directly attached to the tree.&lt;/li>
&lt;/ul>
&lt;h4 id="other-indexing-structures">Other indexing structures&lt;/h4>
&lt;p>We&amp;rsquo;ve only discussed key-value indexes, which are like &lt;em>primary key&lt;/em> index. There are also &lt;em>secondary indexes&lt;/em>.&lt;/p>
&lt;p>A secondary index can be easily constructed from a key-value index. The main difference is that in a secondary index, the indexed values are not necessarily unique. There are two ways of doing this: making each value in the index a list of matching row identifiers or by making a each entry unique by appending a row identifier to it.&lt;/p>
&lt;h4 id="full-text-search-and-fuzzy-indexes">Full-text search and fuzzy indexes&lt;/h4>
&lt;p>Indexes don&amp;rsquo;t allow you to search for &lt;em>similar&lt;/em> keys, such as misspelled words. Such &lt;em>fuzzy&lt;/em> querying requires different techniques.&lt;/p>
&lt;p>Full-text search engines allow synonyms, grammatical variations, occurrences of words near each other.&lt;/p>
&lt;p>Lucene uses SSTable-like structure for its term dictionary. Lucene, the in-memory index is a finite state automaton, similar to a &lt;em>trie&lt;/em>.&lt;/p>
&lt;h4 id="keeping-everything-in-memory">Keeping everything in memory&lt;/h4>
&lt;p>Disks have two significant advantages: they are durable, and they have lower cost per gigabyte than RAM.&lt;/p>
&lt;p>It&amp;rsquo;s quite feasible to keep them entirely in memory, this has lead to &lt;em>in-memory&lt;/em> databases.&lt;/p>
&lt;p>Key-value stores, such as Memcached are intended for cache only, it&amp;rsquo;s acceptable for data to be lost if the machine is restarted. Other in-memory databases aim for durability, with special hardware, writing a log of changes to disk, writing periodic snapshots to disk or by replicating in-memory sate to other machines.&lt;/p>
&lt;p>When an in-memory database is restarted, it needs to reload its state, either from disk or over the network from a replica. The disk is merely used as an append-only log for durability, and reads are served entirely from memory.&lt;/p>
&lt;p>Products such as VoltDB, MemSQL, and Oracle TimesTime are in-memory databases. Redis and Couchbase provide weak durability.&lt;/p>
&lt;p>In-memory databases can be faster because they can avoid the overheads of encoding in-memory data structures in a form that can be written to disk.&lt;/p>
&lt;p>Another interesting area is that in-memory databases may provide data models that are difficult to implement with disk-based indexes.&lt;/p>
&lt;h3 id="transaction-processing-or-analytics">Transaction processing or analytics?&lt;/h3>
&lt;p>A &lt;em>transaction&lt;/em> is a group of reads and writes that form a logical unit, this pattern became known as &lt;em>online transaction processing&lt;/em> (OLTP).&lt;/p>
&lt;p>&lt;em>Data analytics&lt;/em> has very different access patterns. A query would need to scan over a huge number of records, only reading a few columns per record, and calculates aggregate statistics.&lt;/p>
&lt;p>These queries are often written by business analysts, and fed into reports. This pattern became known for &lt;em>online analytics processing&lt;/em> (OLAP).&lt;/p>
&lt;h4 id="data-warehousing">Data warehousing&lt;/h4>
&lt;p>A &lt;em>data warehouse&lt;/em> is a separate database that analysts can query to their heart&amp;rsquo;s content without affecting OLTP operations. It contains read-only copy of the dat in all various OLTP systems in the company. Data is extracted out of OLTP databases (through periodic data dump or a continuous stream of update), transformed into an analysis-friendly schema, cleaned up, and then loaded into the data warehouse (process &lt;em>Extract-Transform-Load&lt;/em> or ETL).&lt;/p>
&lt;p>A data warehouse is most commonly relational, but the internals of the systems can look quite different.&lt;/p>
&lt;p>Amazon RedShift is hosted version of ParAccel. Apache Hive, Spark SQL, Cloudera Impala, Facebook Presto, Apache Tajo, and Apache Drill. Some of them are based on ideas from Google&amp;rsquo;s Dremel.&lt;/p>
&lt;p>Data warehouses are used in fairly formulaic style known as a &lt;em>star schema&lt;/em>.&lt;/p>
&lt;p>Facts are captured as individual events, because this allows maximum flexibility of analysis later. The fact table can become extremely large.&lt;/p>
&lt;p>Dimensions represent the &lt;em>who&lt;/em>, &lt;em>what&lt;/em>, &lt;em>where&lt;/em>, &lt;em>when&lt;/em>, &lt;em>how&lt;/em> and &lt;em>why&lt;/em> of the event.&lt;/p>
&lt;p>The name &amp;ldquo;star schema&amp;rdquo; comes from the fact than when the table relationships are visualised, the fact table is in the middle, surrounded by its dimension tables, like the rays of a star.&lt;/p>
&lt;p>Fact tables often have over 100 columns, sometimes several hundred. Dimension tables can also be very wide.&lt;/p>
&lt;h3 id="column-oriented-storage">Column-oriented storage&lt;/h3>
&lt;p>In a row-oriented storage engine, when you do a query that filters on a specific field, the engine will load all those rows with all their fields into memory, parse them and filter out the ones that don&amp;rsquo;t meet the requirement. This can take a long time.&lt;/p>
&lt;p>&lt;em>Column-oriented storage&lt;/em> is simple: don&amp;rsquo;t store all the values from one row together, but store all values from each &lt;em>column&lt;/em> together instead. If each column is stored in a separate file, a query only needs to read and parse those columns that are used in a query, which can save a lot of work.&lt;/p>
&lt;p>Column-oriented storage often lends itself very well to compression as the sequences of values for each column look quite repetitive, which is a good sign for compression. A technique that is particularly effective in data warehouses is &lt;em>bitmap encoding&lt;/em>.&lt;/p>
&lt;p>Bitmap indexes are well suited for all kinds of queries that are common in a data warehouse.&lt;/p>
&lt;blockquote>
&lt;p>Cassandra and HBase have a concept of &lt;em>column families&lt;/em>, which they inherited from Bigtable.&lt;/p>
&lt;/blockquote>
&lt;p>Besides reducing the volume of data that needs to be loaded from disk, column-oriented storage layouts are also good for making efficient use of CPU cycles (&lt;em>vectorised processing&lt;/em>).&lt;/p>
&lt;p>&lt;strong>Column-oriented storage, compression, and sorting helps to make read queries faster and make sense in data warehouses, where most of the load consist on large read-only queries run by analysts. The downside is that writes are more difficult.&lt;/strong>&lt;/p>
&lt;p>An update-in-place approach, like B-tree use, is not possible with compressed columns. If you insert a row in the middle of a sorted table, you would most likely have to rewrite all column files.&lt;/p>
&lt;p>It&amp;rsquo;s worth mentioning &lt;em>materialised aggregates&lt;/em> as some cache of the counts ant the sums that queries use most often. A way of creating such a cache is with a &lt;em>materialised view&lt;/em>, on a relational model this is usually called a &lt;em>virtual view&lt;/em>: a table-like object whose contents are the results of some query. A materialised view is an actual copy of the query results, written in disk, whereas a virtual view is just a shortcut for writing queries.&lt;/p>
&lt;p>When the underlying data changes, a materialised view needs to be updated, because it is denormalised copy of the data. Database can do it automatically, but writes would become more expensive.&lt;/p>
&lt;p>A common special case of a materialised view is know as a &lt;em>data cube&lt;/em> or &lt;em>OLAP cube&lt;/em>, a grid of aggregates grouped by different dimensions.&lt;/p>
&lt;h2 id="encoding-and-evolution">Encoding and evolution&lt;/h2>
&lt;p>Change to an application&amp;rsquo;s features also requires a change to data it stores.&lt;/p>
&lt;p>Relational databases conforms to one schema although that schema can be changed, there is one schema in force at any point in time. &lt;strong>Schema-on-read (or schemaless) contain a mixture of older and newer data formats.&lt;/strong>&lt;/p>
&lt;p>In large applications changes don&amp;rsquo;t happen instantaneously. You want to perform a &lt;em>rolling upgrade&lt;/em> and deploy a new version to a few nodes at a time, gradually working your way through all the nodes without service downtime.&lt;/p>
&lt;p>Old and new versions of the code, and old and new data formats, may potentially all coexist. We need to maintain compatibility in both directions&lt;/p>
&lt;ul>
&lt;li>Backward compatibility, newer code can read data that was written by older code.&lt;/li>
&lt;li>Forward compatibility, older code can read data that was written by newer code.&lt;/li>
&lt;/ul>
&lt;h3 id="formats-for-encoding-data">Formats for encoding data&lt;/h3>
&lt;p>Two different representations:&lt;/p>
&lt;ul>
&lt;li>In memory&lt;/li>
&lt;li>When you want to write data to a file or send it over the network, you have to encode it&lt;/li>
&lt;/ul>
&lt;p>Thus, you need a translation between the two representations. In-memory representation to byte sequence is called &lt;em>encoding&lt;/em> (&lt;em>serialisation&lt;/em> or &lt;em>marshalling&lt;/em>), and the reverse is called &lt;em>decoding&lt;/em> (&lt;em>parsing&lt;/em>, &lt;em>deserialisation&lt;/em> or &lt;em>unmarshalling&lt;/em>).&lt;/p>
&lt;p>Programming languages come with built-in support for encoding in-memory objects into byte sequences, but is usually a bad idea to use them. Precisely because of a few problems.&lt;/p>
&lt;ul>
&lt;li>Often tied to a particular programming language.&lt;/li>
&lt;li>The decoding process needs to be able to instantiate arbitrary classes and this is frequently a security hole.&lt;/li>
&lt;li>Versioning&lt;/li>
&lt;li>Efficiency&lt;/li>
&lt;/ul>
&lt;p>Standardised encodings can be written and read by many programming languages.&lt;/p>
&lt;p>JSON, XML, and CSV are human-readable and popular specially as data interchange formats, but they have some subtle problems:&lt;/p>
&lt;ul>
&lt;li>Ambiguity around the encoding of numbers and dealing with large numbers&lt;/li>
&lt;li>Support of Unicode character strings, but no support for binary strings. People get around this by encoding binary data as Base64, which increases the data size by 33%.&lt;/li>
&lt;li>There is optional schema support for both XML and JSON&lt;/li>
&lt;li>CSV does not have any schema&lt;/li>
&lt;/ul>
&lt;h4 id="binary-encoding">Binary encoding&lt;/h4>
&lt;p>JSON is less verbose than XML, but both still use a lot of space compared to binary formats. There are binary encodings for JSON (MesagePack, BSON, BJSON, UBJSON, BISON and Smile), similar thing for XML (WBXML and Fast Infoset).&lt;/p>
&lt;p>&lt;strong>Apache Thrift and Protocol Buffers (protobuf) are binary encoding libraries.&lt;/strong>&lt;/p>
&lt;p>Thrift offers two different protocols:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>BinaryProtocol&lt;/strong>, there are no field names like &lt;code>userName&lt;/code>, &lt;code>favouriteNumber&lt;/code>. Instead the data contains &lt;em>field tags&lt;/em>, which are numbers (&lt;code>1&lt;/code>, &lt;code>2&lt;/code>)&lt;/li>
&lt;li>&lt;strong>CompactProtocol&lt;/strong>, which is equivalent to BinaryProtocol but it packs the same information in less space. It packs the field type and the tag number into the same byte.&lt;/li>
&lt;/ul>
&lt;p>Protocol Buffers are very similar to Thrift&amp;rsquo;s CompactProtocol, bit packing is a bit different and that might allow smaller compression.&lt;/p>
&lt;p>Schemas inevitable need to change over time (&lt;em>schema evolution&lt;/em>), how do Thrift and Protocol Buffers handle schema changes while keeping backward and forward compatibility changes?&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Forward compatible support&lt;/strong>. As with new fields you add new tag numbers, old code trying to read new code, it can simply ignore not recognised tags.&lt;/li>
&lt;li>&lt;strong>Backwards compatible support&lt;/strong>. As long as each field has a unique tag number, new code can always read old data. Every field you add after initial deployment of schema must be optional or have a default value.&lt;/li>
&lt;/ul>
&lt;p>Removing fields is just like adding a field with backward and forward concerns reversed. You can only remove a field that is optional, and you can never use the same tag again.&lt;/p>
&lt;p>What about changing the data type of a field? There is a risk that values will lose precision or get truncated.&lt;/p>
&lt;h5 id="avro">Avro&lt;/h5>
&lt;p>Apache Avro is another binary format that has two schema languages, one intended for human editing (Avro IDL), and one (based on JSON) that is more easily machine-readable.&lt;/p>
&lt;p>You go go through the fields in the order they appear in the schema and use the schema to tell you the datatype of each field. Any mismatch in the schema between the reader and the writer would mean incorrectly decoded data.&lt;/p>
&lt;p>What about schema evolution? When an application wants to encode some data, it encodes the data using whatever version of the schema it knows (&lt;em>writer&amp;rsquo;s schema&lt;/em>).&lt;/p>
&lt;p>When an application wants to decode some data, it is expecting the data to be in some schema (&lt;em>reader&amp;rsquo;s schema&lt;/em>).&lt;/p>
&lt;p>In Avro the writer&amp;rsquo;s schema and the reader&amp;rsquo;s schema &lt;em>don&amp;rsquo;t have to be the same&lt;/em>. The Avro library resolves the differences by looking at the writer&amp;rsquo;s schema and the reader&amp;rsquo;s schema.&lt;/p>
&lt;p>Forward compatibility means you can have a new version of the schema as writer and an old version of the schema as reader. Conversely, backward compatibility means that you can have a new version of the schema as reader and an old version as writer.&lt;/p>
&lt;p>To maintain compatibility, you may only add or remove a field that has a default value.&lt;/p>
&lt;p>If you were to add a field that has no default value, new readers wouldn&amp;rsquo;t be able to read data written by old writers.&lt;/p>
&lt;p>Changing the datatype of a field is possible, provided that Avro can convert the type. Changing the name of a filed is tricky (backward compatible but not forward compatible).&lt;/p>
&lt;p>The schema is identified encoded in the data. In a large file with lots of records, the writer of the file can just include the schema at the beginning of the file. On a database with individually written records, you cannot assume all the records will have the same schema, so you have to include a version number at the beginning of every encoded record. While sending records over the network, you can negotiate the schema version on connection setup.&lt;/p>
&lt;p>Avro is friendlier to &lt;em>dynamically generated schemas&lt;/em> (dumping into a file the database). You can fairly easily generate an Avro schema in JSON.&lt;/p>
&lt;p>If the database schema changes, you can just generate a new Avro schema for the updated database schema and export data in the new Avro schema.&lt;/p>
&lt;p>By contrast with Thrift and Protocol Buffers, every time the database schema changes, you would have to manually update the mappings from database column names to field tags.&lt;/p>
&lt;hr>
&lt;p>Although textual formats such as JSON, XML and CSV are widespread, binary encodings based on schemas are also a viable option. As they have nice properties:&lt;/p>
&lt;ul>
&lt;li>Can be much more compact, since they can omit field names from the encoded data.&lt;/li>
&lt;li>Schema is a valuable form of documentation, required for decoding, you can be sure it is up to date.&lt;/li>
&lt;li>Database of schemas allows you to check forward and backward compatibility changes.&lt;/li>
&lt;li>Generate code from the schema is useful, since it enables type checking at compile time.&lt;/li>
&lt;/ul>
&lt;h3 id="modes-of-dataflow">Modes of dataflow&lt;/h3>
&lt;p>Different process on how data flows between processes&lt;/p>
&lt;h4 id="via-databases">Via databases&lt;/h4>
&lt;p>The process that writes to the database encodes the data, and the process that reads from the database decodes it.&lt;/p>
&lt;p>A value in the database may be written by a &lt;em>newer&lt;/em> version of the code, and subsequently read by an &lt;em>older&lt;/em> version of the code that is still running.&lt;/p>
&lt;p>When a new version of your application is deployed, you may entirely replace the old version with the new version within a few minutes. The same is not true in databases, the five-year-old data will still be there, in the original encoding, unless you have explicitly rewritten it. &lt;em>Data outlives code&lt;/em>.&lt;/p>
&lt;p>Rewriting (&lt;em>migrating&lt;/em>) is expensive, most relational databases allow simple schema changes, such as adding a new column with a &lt;code>null&lt;/code> default value without rewriting existing data. When an old row is read, the database fills in &lt;code>null&lt;/code>s for any columns that are missing.&lt;/p>
&lt;h4 id="via-service-calls">Via service calls&lt;/h4>
&lt;p>You have processes that need to communicate over a network of &lt;em>clients&lt;/em> and &lt;em>servers&lt;/em>.&lt;/p>
&lt;p>Services are similar to databases, each service should be owned by one team. and that team should be able to release versions of the service frequently, without having to coordinate with other teams. We should expect old and new versions of servers and clients to be running at the same time.&lt;/p>
&lt;p>&lt;em>Remote procedure calls&lt;/em> (RPC) tries to make a request to a remote network service look the same as calling a function or method in your programming language, it seems convenient at first but the approach is flawed:&lt;/p>
&lt;ul>
&lt;li>A network request is unpredictable&lt;/li>
&lt;li>A network request it may return without a result, due a &lt;em>timeout&lt;/em>&lt;/li>
&lt;li>Retrying will cause the action to be performed multiple times, unless you build a mechanism for deduplication (&lt;em>idempotence&lt;/em>).&lt;/li>
&lt;li>A network request is much slower than a function call, and its latency is wildly variable.&lt;/li>
&lt;li>Parameters need to be encoded into a sequence of bytes that can be sent over the network and becomes problematic with larger objects.&lt;/li>
&lt;li>The RPC framework must translate datatypes from one language to another, not all languages have the same types.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>There is no point trying to make a remote service look too much like a local object in your programming language, because it&amp;rsquo;s a fundamentally different thing.&lt;/strong>&lt;/p>
&lt;p>New generation of RPC frameworks are more explicit about the fact that a remote request is different from a local function call. Fiangle and Rest.li use &lt;em>features&lt;/em> (&lt;em>promises&lt;/em>) to encapsulate asyncrhonous actions.&lt;/p>
&lt;p>RESTful API has some significant advantages like being good for experimentation and debugging.&lt;/p>
&lt;p>REST seems to be the predominant style for public APIs. The main focus of RPC frameworks is on requests between services owned by the same organisation, typically within the same datacenter.&lt;/p>
&lt;h4 id="via-asynchronous-message-passing">Via asynchronous message passing&lt;/h4>
&lt;p>In an &lt;em>asynchronous message-passing&lt;/em> systems, a client&amp;rsquo;s request (usually called a &lt;em>message&lt;/em>) is delivered to another process with low latency. The message goes via an intermediary called a &lt;em>message broker&lt;/em> (&lt;em>message queue&lt;/em> or &lt;em>message-oriented middleware&lt;/em>) which stores the message temporarily. This has several advantages compared to direct RPC:&lt;/p>
&lt;ul>
&lt;li>It can act as a buffer if the recipient is unavailable or overloaded&lt;/li>
&lt;li>It can automatically redeliver messages to a process that has crashed and prevent messages from being lost&lt;/li>
&lt;li>It avoids the sender needing to know the IP address and port number of the recipient (useful in a cloud environment)&lt;/li>
&lt;li>It allows one message to be sent to several recipients&lt;/li>
&lt;li>&lt;strong>Decouples the sender from the recipient&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>The communication happens only in one direction. The sender doesn&amp;rsquo;t wait for the message to be delivered, but simply sends it and then forgets about it (&lt;em>asynchronous&lt;/em>).&lt;/p>
&lt;p>Open source implementations for message brokers are RabbitMQ, ActiveMQ, HornetQ, NATS, and Apache Kafka.&lt;/p>
&lt;p>One process sends a message to a named &lt;em>queue&lt;/em> or &lt;em>topic&lt;/em> and the broker ensures that the message is delivered to one or more &lt;em>consumers&lt;/em> or &lt;em>subscribers&lt;/em> to that queue or topic.&lt;/p>
&lt;p>Message brokers typically don&amp;rsquo;t enforce a particular data model, you can use any encoding format.&lt;/p>
&lt;p>An &lt;em>actor model&lt;/em> is a programming model for concurrency in a single process. Rather than dealing with threads (and their complications), logic is encapsulated in &lt;em>actors&lt;/em>. Each actor typically represent one client or entity, it may have some local state, and it communicates with other actors by sending and receiving asynchronous messages. Message deliver is not guaranteed. Since each actor processes only one message at a time, it doesn&amp;rsquo;t need to worry about threads.&lt;/p>
&lt;p>In &lt;em>distributed actor frameworks&lt;/em>, this programming model is used to scale an application across multiple nodes. It basically integrates a message broker and the actor model into a single framework.&lt;/p>
&lt;ul>
&lt;li>&lt;em>Akka&lt;/em> uses Java&amp;rsquo;s built-in serialisation by default, which does not provide forward or backward compatibility. You can replace it with something like Protocol Buffers and the ability to do rolling upgrades.&lt;/li>
&lt;li>&lt;em>Orleans&lt;/em> by default uses custom data encoding format that does not support rolling upgrade deployments.&lt;/li>
&lt;li>In &lt;em>Erlang OTP&lt;/em> it is surprisingly hard to make changes to record schemas.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>What happens if multiple machines are involved in storage and retrieval of data?&lt;/p>
&lt;p>Reasons for distribute a database across multiple machines:&lt;/p>
&lt;ul>
&lt;li>Scalability&lt;/li>
&lt;li>Fault tolerance/high availability&lt;/li>
&lt;li>Latency, having servers at various locations worldwide&lt;/li>
&lt;/ul>
&lt;h2 id="replication">Replication&lt;/h2>
&lt;p>Reasons why you might want to replicate data:&lt;/p>
&lt;ul>
&lt;li>To keep data geographically close to your users&lt;/li>
&lt;li>Increase availability&lt;/li>
&lt;li>Increase read throughput&lt;/li>
&lt;/ul>
&lt;p>The difficulty in replication lies in handling &lt;em>changes&lt;/em> to replicated data. Popular algorithms for replicating changes between nodes: &lt;em>single-leader&lt;/em>, &lt;em>multi-leader&lt;/em>, and &lt;em>leaderless&lt;/em> replication.&lt;/p>
&lt;h3 id="leaders-and-followers">Leaders and followers&lt;/h3>
&lt;p>Each node that stores a copy of the database is called a &lt;em>replica&lt;/em>.&lt;/p>
&lt;p>Every write to the database needs to be processed by every replica. The most common solution for this is called &lt;em>leader-based replication&lt;/em> (&lt;em>active/passive&lt;/em> or &lt;em>master-slave replication&lt;/em>).&lt;/p>
&lt;ol>
&lt;li>One of the replicas is designated the &lt;em>leader&lt;/em> (&lt;em>master&lt;/em> or &lt;em>primary&lt;/em>). Writes to the database must send requests to the leader.&lt;/li>
&lt;li>Other replicas are known as &lt;em>followers&lt;/em> (&lt;em>read replicas&lt;/em>, &lt;em>slaves&lt;/em>, &lt;em>secondaries&lt;/em> or &lt;em>hot stanbys&lt;/em>). The leader sends the data change to all of its followers as part of a &lt;em>replication log&lt;/em> or &lt;em>change stream&lt;/em>.&lt;/li>
&lt;li>Reads can be query the leader or any of the followers, while writes are only accepted on the leader.&lt;/li>
&lt;/ol>
&lt;p>MySQL, Oracle Data Guard, SQL Server&amp;rsquo;s AlwaysOn Availability Groups, MongoDB, RethinkDB, Espresso, Kafka and RabbitMQ are examples of these kind of databases.&lt;/p>
&lt;h4 id="synchronous-vs-asynchronous">Synchronous vs asynchronous&lt;/h4>
&lt;p>&lt;strong>The advantage of synchronous replication is that the follower is guaranteed to have an up-to-date copy of the data that is consistent with the leader. The disadvantage is that it the synchronous follower doesn&amp;rsquo;t respond, the write cannot be processed.&lt;/strong>&lt;/p>
&lt;p>It&amp;rsquo;s impractical for all followers to be synchronous. If you enable synchronous replication on a database, it usually means that &lt;em>one&lt;/em> of the followers is synchronous, and the others are asynchronous. This guarantees up-to-date copy of the data on at least two nodes (this is sometimes called &lt;em>semi-synchronous&lt;/em>).&lt;/p>
&lt;p>Often, leader-based replication is asynchronous. Writes are not guaranteed to be durable, the main advantage of this approach is that the leader can continue processing writes.&lt;/p>
&lt;h4 id="setting-up-new-followers">Setting up new followers&lt;/h4>
&lt;p>Copying data files from one node to another is typically not sufficient.&lt;/p>
&lt;p>Setting up a follower can usually be done without downtime. The process looks like:&lt;/p>
&lt;ol>
&lt;li>Take a snapshot of the leader&amp;rsquo;s database&lt;/li>
&lt;li>Copy the snapshot to the follower node&lt;/li>
&lt;li>Follower requests data changes that have happened since the snapshot was taken&lt;/li>
&lt;li>Once follower processed the backlog of data changes since snapshot, it has &lt;em>caught up&lt;/em>.&lt;/li>
&lt;/ol>
&lt;h4 id="handling-node-outages">Handling node outages&lt;/h4>
&lt;p>How does high availability works with leader-based replication?&lt;/p>
&lt;h4 id="follower-failure-catchup-recovery">Follower failure: catchup recovery&lt;/h4>
&lt;p>Follower can connect to the leader and request all the data changes that occurred during the time when the follower was disconnected.&lt;/p>
&lt;h4 id="leader-failure-failover">Leader failure: failover&lt;/h4>
&lt;p>One of the followers needs to be promoted to be the new leader, clients need to be reconfigured to send their writes to the new leader and followers need to start consuming data changes from the new leader.&lt;/p>
&lt;p>Automatic failover consists:&lt;/p>
&lt;ol>
&lt;li>Determining that the leader has failed. If a node does not respond in a period of time it&amp;rsquo;s considered dead.&lt;/li>
&lt;li>Choosing a new leader. The best candidate for leadership is usually the replica with the most up-to-date changes from the old leader.&lt;/li>
&lt;li>Reconfiguring the system to use the new leader. The system needs to ensure that the old leader becomes a follower and recognises the new leader.&lt;/li>
&lt;/ol>
&lt;p>Things that could go wrong:&lt;/p>
&lt;ul>
&lt;li>If asynchronous replication is used, the new leader may have received conflicting writes in the meantime.&lt;/li>
&lt;li>Discarding writes is especially dangerous if other storage systems outside of the database need to be coordinated with the database contents.&lt;/li>
&lt;li>It could happen that two nodes both believe that they are the leader (&lt;em>split brain&lt;/em>). Data is likely to be lost or corrupted.&lt;/li>
&lt;li>What is the right time before the leader is declared dead?&lt;/li>
&lt;/ul>
&lt;p>For these reasons, some operation teams prefer to perform failovers manually, even if the software supports automatic failover.&lt;/p>
&lt;h4 id="implementation-of-replication-logs">Implementation of replication logs&lt;/h4>
&lt;h5 id="statement-based-replication">Statement-based replication&lt;/h5>
&lt;p>The leader logs every &lt;em>statement&lt;/em> and sends it to its followers (every &lt;code>INSERT&lt;/code>, &lt;code>UPDATE&lt;/code> or &lt;code>DELETE&lt;/code>).&lt;/p>
&lt;p>This type of replication has some problems:&lt;/p>
&lt;ul>
&lt;li>Non-deterministic functions such as &lt;code>NOW()&lt;/code> or &lt;code>RAND()&lt;/code> will generate different values on replicas.&lt;/li>
&lt;li>Statements that depend on existing data, like auto-increments, must be executed in the same order in each replica.&lt;/li>
&lt;li>Statements with side effects may result on different results on each replica.&lt;/li>
&lt;/ul>
&lt;p>A solution to this is to replace any nondeterministic function with a fixed return value in the leader.&lt;/p>
&lt;h5 id="write-ahead-log-wal-shipping">Write-ahead log (WAL) shipping&lt;/h5>
&lt;p>The log is an append-only sequence of bytes containing all writes to the database. The leader can send it to its followers. This way of replication is used in PostgresSQL and Oracle.&lt;/p>
&lt;p>The main disadvantage is that the log describes the data at a very low level (like which bytes were changed in which disk blocks), coupling it to the storage engine.&lt;/p>
&lt;p>Usually is not possible to run different versions of the database in leaders and followers. This can have a big operational impact, like making it impossible to have a zero-downtime upgrade of the database.&lt;/p>
&lt;h5 id="logical-row-based-log-replication">Logical (row-based) log replication&lt;/h5>
&lt;p>Basically a sequence of records describing writes to database tables at the granularity of a row:&lt;/p>
&lt;ul>
&lt;li>For an inserted row, the new values of all columns.&lt;/li>
&lt;li>For a deleted row, the information that uniquely identifies that column.&lt;/li>
&lt;li>For an updated row, the information to uniquely identify that row and all the new values of the columns.&lt;/li>
&lt;/ul>
&lt;p>A transaction that modifies several rows, generates several of such logs, followed by a record indicating that the transaction was committed. MySQL binlog uses this approach.&lt;/p>
&lt;p>Since logical log is decoupled from the storage engine internals, it&amp;rsquo;s easier to make it backwards compatible.&lt;/p>
&lt;p>Logical logs are also easier for external applications to parse, useful for data warehouses, custom indexes and caches (&lt;em>change data capture&lt;/em>).&lt;/p>
&lt;h5 id="trigger-based-replication">Trigger-based replication&lt;/h5>
&lt;p>There are some situations were you may need to move replication up to the application layer.&lt;/p>
&lt;p>A trigger lets you register custom application code that is automatically executed when a data change occurs. This is a good opportunity to log this change into a separate table, from which it can be read by an external process.&lt;/p>
&lt;p>Main disadvantages is that this approach has greater overheads, is more prone to bugs but it may be useful due to its flexibility.&lt;/p>
&lt;h3 id="problems-with-replication-lag">Problems with replication lag&lt;/h3>
&lt;p>Node failures is just one reason for wanting replication. Other reasons are scalability and latency.&lt;/p>
&lt;p>In a &lt;em>read-scaling&lt;/em> architecture, you can increase the capacity for serving read-only requests simply by adding more followers. However, this only realistically works on asynchronous replication. The more nodes you have, the likelier is that one will be down, so a fully synchronous configuration would be unreliable.&lt;/p>
&lt;p>With an asynchronous approach, a follower may fall behind, leading to inconsistencies in the database (&lt;em>eventual consistency&lt;/em>).&lt;/p>
&lt;p>The &lt;em>replication lag&lt;/em> could be a fraction of a second or several seconds or even minutes.&lt;/p>
&lt;p>The problems that may arise and how to solve them.&lt;/p>
&lt;h4 id="reading-your-own-writes">Reading your own writes&lt;/h4>
&lt;p>&lt;em>Read-after-write consistency&lt;/em>, also known as &lt;em>read-your-writes consistency&lt;/em> is a guarantee that if the user reloads the page, they will always see any updates they submitted themselves.&lt;/p>
&lt;p>How to implement it:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>When reading something that the user may have modified, read it from the leader.&lt;/strong> For example, user profile information on a social network is normally only editable by the owner. A simple rule is always read the user&amp;rsquo;s own profile from the leader.&lt;/li>
&lt;li>You could track the time of the latest update and, for one minute after the last update, make all reads from the leader.&lt;/li>
&lt;li>The client can remember the timestamp of the most recent write, then the system can ensure that the replica serving any reads for that user reflects updates at least until that timestamp.&lt;/li>
&lt;li>If your replicas are distributed across multiple datacenters, then any request needs to be routed to the datacenter that contains the leader.&lt;/li>
&lt;/ul>
&lt;p>Another complication is that the same user is accessing your service from multiple devices, you may want to provide &lt;em>cross-device&lt;/em> read-after-write consistency.&lt;/p>
&lt;p>Some additional issues to consider:&lt;/p>
&lt;ul>
&lt;li>Remembering the timestamp of the user&amp;rsquo;s last update becomes more difficult. The metadata will need to be centralised.&lt;/li>
&lt;li>If replicas are distributed across datacenters, there is no guarantee that connections from different devices will be routed to the same datacenter. You may need to route requests from all of a user&amp;rsquo;s devices to the same datacenter.&lt;/li>
&lt;/ul>
&lt;h4 id="monotonic-reads">Monotonic reads&lt;/h4>
&lt;p>Because of followers falling behind, it&amp;rsquo;s possible for a user to see things &lt;em>moving backward in time&lt;/em>.&lt;/p>
&lt;p>When you read data, you may see an old value; monotonic reads only means that if one user makes several reads in sequence, they will not see time go backward.&lt;/p>
&lt;p>Make sure that each user always makes their reads from the same replica. The replica can be chosen based on a hash of the user ID. If the replica fails, the user&amp;rsquo;s queries will need to be rerouted to another replica.&lt;/p>
&lt;h4 id="consistent-prefix-reads">Consistent prefix reads&lt;/h4>
&lt;p>If a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.&lt;/p>
&lt;p>This is a particular problem in partitioned (sharded) databases as there is no global ordering of writes.&lt;/p>
&lt;p>A solution is to make sure any writes casually related to each other are written to the same partition.&lt;/p>
&lt;h4 id="solutions-for-replication-lag">Solutions for replication lag&lt;/h4>
&lt;p>&lt;em>Transactions&lt;/em> exist so there is a way for a database to provide stronger guarantees so that the application can be simpler.&lt;/p>
&lt;h3 id="multi-leader-replication">Multi-leader replication&lt;/h3>
&lt;p>Leader-based replication has one major downside: there is only one leader, and all writes must go through it.&lt;/p>
&lt;p>A natural extension is to allow more than one node to accept writes (&lt;em>multi-leader&lt;/em>, &lt;em>master-master&lt;/em> or &lt;em>active/active&lt;/em> replication) where each leader simultaneously acts as a follower to the other leaders.&lt;/p>
&lt;h4 id="use-cases-for-multi-leader-replication">Use cases for multi-leader replication&lt;/h4>
&lt;p>It rarely makes sense to use multi-leader setup within a single datacenter.&lt;/p>
&lt;h5 id="multi-datacenter-operation">Multi-datacenter operation&lt;/h5>
&lt;p>You can have a leader in &lt;em>each&lt;/em> datacenter. Within each datacenter, regular leader-follower replication is used. Between datacenters, each datacenter leader replicates its changes to the leaders in other datacenters.&lt;/p>
&lt;p>Compared to a single-leader replication model deployed in multi-datacenters&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Performance.&lt;/strong> With single-leader, every write must go across the internet to wherever the leader is, adding significant latency. In multi-leader every write is processed in the local datacenter and replicated asynchronously to other datacenters. The network delay is hidden from users and perceived performance may be better.&lt;/li>
&lt;li>&lt;strong>Tolerance of datacenter outages.&lt;/strong> In single-leader if the datacenter with the leader fails, failover can promote a follower in another datacenter. In multi-leader, each datacenter can continue operating independently from others.&lt;/li>
&lt;li>&lt;strong>Tolerance of network problems.&lt;/strong> Single-leader is very sensitive to problems in this inter-datacenter link as writes are made synchronously over this link. Multi-leader with asynchronous replication can tolerate network problems better.&lt;/li>
&lt;/ul>
&lt;p>Multi-leader replication is implemented with Tungsten Replicator for MySQL, BDR for PostgreSQL or GoldenGate for Oracle.&lt;/p>
&lt;p>It&amp;rsquo;s common to fall on subtle configuration pitfalls. Autoincrementing keys, triggers and integrity constraints can be problematic. Multi-leader replication is often considered dangerous territory and avoided if possible.&lt;/p>
&lt;h5 id="clients-with-offline-operation">Clients with offline operation&lt;/h5>
&lt;p>If you have an application that needs to continue to work while it is disconnected from the internet, every device that has a local database can act as a leader, and there will be some asynchronous multi-leader replication process (imagine, a Calendar application).&lt;/p>
&lt;p>CouchDB is designed for this mode of operation.&lt;/p>
&lt;h4 id="collaborative-editing">Collaborative editing&lt;/h4>
&lt;p>&lt;em>Real-time collaborative editing&lt;/em> applications allow several people to edit a document simultaneously. Like Etherpad or Google Docs.&lt;/p>
&lt;p>The user edits a document, the changes are instantly applied to their local replica and asynchronously replicated to the server and any other user.&lt;/p>
&lt;p>If you want to avoid editing conflicts, you must the lock the document before a user can edit it.&lt;/p>
&lt;p>For faster collaboration, you may want to make the unit of change very small (like a keystroke) and avoid locking.&lt;/p>
&lt;h4 id="handling-write-conflicts">Handling write conflicts&lt;/h4>
&lt;p>The biggest problem with multi-leader replication is when conflict resolution is required. This problem does not happen in a single-leader database.&lt;/p>
&lt;h5 id="synchronous-vs-asynchronous-conflict-detection">Synchronous vs asynchronous conflict detection&lt;/h5>
&lt;p>In single-leader the second writer can be blocked and wait the first one to complete, forcing the user to retry the write. On multi-leader if both writes are successful, the conflict is only detected asynchronously later in time.&lt;/p>
&lt;p>If you want synchronous conflict detection, you might as well use single-leader replication.&lt;/p>
&lt;h5 id="conflict-avoidance">Conflict avoidance&lt;/h5>
&lt;p>The simplest strategy for dealing with conflicts is to avoid them. If all writes for a particular record go through the sae leader, then conflicts cannot occur.&lt;/p>
&lt;p>On an application where a user can edit their own data, you can ensure that requests from a particular user are always routed to the same datacenter and use the leader in that datacenter for reading and writing.&lt;/p>
&lt;h5 id="converging-toward-a-consistent-state">Converging toward a consistent state&lt;/h5>
&lt;p>On single-leader, the last write determines the final value of the field.&lt;/p>
&lt;p>In multi-leader, it&amp;rsquo;s not clear what the final value should be.&lt;/p>
&lt;p>The database must resolve the conflict in a &lt;em>convergent&lt;/em> way, all replicas must arrive a the same final value when all changes have been replicated.&lt;/p>
&lt;p>Different ways of achieving convergent conflict resolution.&lt;/p>
&lt;ul>
&lt;li>Five each write a unique ID (timestamp, long random number, UUID, or a has of the key and value), pick the write with the highest ID as the &lt;em>winner&lt;/em> and throw away the other writes. This is known as &lt;em>last write wins&lt;/em> (LWW) and it is dangerously prone to data loss.&lt;/li>
&lt;li>Give each replica a unique ID, writes that originated at a higher-numbered replica always take precedence. This approach also implies data loss.&lt;/li>
&lt;li>Somehow merge the values together.&lt;/li>
&lt;li>Record the conflict and write application code that resolves it a to some later time (perhaps prompting the user).&lt;/li>
&lt;/ul>
&lt;h5 id="custom-conflict-resolution">Custom conflict resolution&lt;/h5>
&lt;p>Multi-leader replication tools let you write conflict resolution logic using application code.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>On write.&lt;/strong> As soon as the database system detects a conflict in the log of replicated changes, it calls the conflict handler.&lt;/li>
&lt;li>&lt;strong>On read.&lt;/strong> All the conflicting writes are stored. On read, multiple versions of the data are returned to the application. The application may prompt the user or automatically resolve the conflict. CouchDB works this way.&lt;/li>
&lt;/ul>
&lt;h4 id="multi-leader-replication-topologies">Multi-leader replication topologies&lt;/h4>
&lt;p>A &lt;em>replication topology&lt;/em> describes the communication paths along which writes are propagated from one node to another.&lt;/p>
&lt;p>The most general topology is &lt;em>all-to-all&lt;/em> in which every leader sends its writes to every other leader. MySQL uses &lt;em>circular topology&lt;/em>, where each nodes receives writes from one node and forwards those writes to another node. Another popular topology has the shape of a &lt;em>star&lt;/em>, one designated node forwards writes to all of the other nodes.&lt;/p>
&lt;p>In circular and star topologies a write might need to pass through multiple nodes before they reach all replicas. To prevent infinite replication loops each node is given a unique identifier and the replication log tags each write with the identifiers of the nodes it has passed through. When a node fails it can interrupt the flow of replication messages.&lt;/p>
&lt;p>In all-to-all topology fault tolerance is better as messages can travel along different paths avoiding a single point of failure. It has some issues too, some network links may be faster than others and some replication messages may &amp;ldquo;overtake&amp;rdquo; others. To order events correctly. there is a technique called &lt;em>version vectors&lt;/em>. PostgresSQL BDR does not provide casual ordering of writes, and Tungsten Replicator for MySQL doesn&amp;rsquo;t even try to detect conflicts.&lt;/p>
&lt;h3 id="leaderless-replication">Leaderless replication&lt;/h3>
&lt;p>Simply put, any replica can directly accept writes from clients. Databases like look like Amazon&amp;rsquo;s in-house &lt;em>Dynamo&lt;/em> datastore. &lt;em>Riak&lt;/em>, &lt;em>Cassandra&lt;/em> and &lt;em>Voldemort&lt;/em> follow the &lt;em>Dynamo style&lt;/em>.&lt;/p>
&lt;p>In a leaderless configuration, failover does not exist. Clients send the write to all replicas in parallel.&lt;/p>
&lt;p>&lt;em>Read requests are also sent to several nodes in parallel&lt;/em>. The client may get different responses. Version numbers are used to determine which value is newer.&lt;/p>
&lt;p>Eventually, all the data is copied to every replica. After a unavailable node come back online, it has two different mechanisms to catch up:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Read repair.&lt;/strong> When a client detect any stale responses, write the newer value back to that replica.&lt;/li>
&lt;li>&lt;strong>Anti-entropy process.&lt;/strong> There is a background process that constantly looks for differences in data between replicas and copies any missing data from one replica to he other. It does not copy writes in any particular order.&lt;/li>
&lt;/ul>
&lt;h4 id="quorums-for-reading-and-writing">Quorums for reading and writing&lt;/h4>
&lt;p>If there are &lt;em>n&lt;/em> replicas, every write must be confirmed by &lt;em>w&lt;/em> nodes to be considered successful, and we must query at least &lt;em>r&lt;/em> nodes for each read. As long as &lt;em>w&lt;/em> + &lt;em>r&lt;/em> &amp;gt; &lt;em>n&lt;/em>, we expect to get an up-to-date value when reading. &lt;em>r&lt;/em> and &lt;em>w&lt;/em> values are called &lt;em>quorum&lt;/em> reads and writes. Are the minimum number of votes required for the read or write to be valid.&lt;/p>
&lt;p>A common choice is to make &lt;em>n&lt;/em> and odd number (typically 3 or 5) and to set &lt;em>w&lt;/em> = &lt;em>r&lt;/em> = (&lt;em>n&lt;/em> + 1)/2 (rounded up).&lt;/p>
&lt;p>Limitations:&lt;/p>
&lt;ul>
&lt;li>Sloppy quorum, the &lt;em>w&lt;/em> writes may end up on different nodes than the &lt;em>r&lt;/em> reads, so there is no longer a guaranteed overlap.&lt;/li>
&lt;li>If two writes occur concurrently, and is not clear which one happened first, the only safe solution is to merge them. Writes can be lost due to clock skew.&lt;/li>
&lt;li>If a write happens concurrently with a read, the write may be reflected on only some of the replicas.&lt;/li>
&lt;li>If a write succeeded on some replicas but failed on others, it is not rolled back on the replicas where it succeeded. Reads may or may not return the value from that write.&lt;/li>
&lt;li>If a node carrying a new value fails, and its data is restored from a replica carrying an old value, the number of replicas storing the new value may break the quorum condition.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Dynamo-style databases are generally optimised for use cases that can tolerate eventual consistency.&lt;/strong>&lt;/p>
&lt;h4 id="sloppy-quorums-and-hinted-handoff">Sloppy quorums and hinted handoff&lt;/h4>
&lt;p>Leaderless replication may be appealing for use cases that require high availability and low latency, and that can tolerate occasional stale reads.&lt;/p>
&lt;p>It&amp;rsquo;s likely that the client won&amp;rsquo;t be able to connect to &lt;em>some&lt;/em> database nodes during a network interruption.&lt;/p>
&lt;ul>
&lt;li>Is it better to return errors to all requests for which we cannot reach quorum of &lt;em>w&lt;/em> or &lt;em>r&lt;/em> nodes?&lt;/li>
&lt;li>Or should we accept writes anyway, and write them to some nodes that are reachable but aren&amp;rsquo;t among the &lt;em>n&lt;/em> nodes on which the value usually lives?&lt;/li>
&lt;/ul>
&lt;p>The latter is known as &lt;em>sloppy quorum&lt;/em>: writes and reads still require &lt;em>w&lt;/em> and &lt;em>r&lt;/em> successful responses, but those may include nodes that are not among the designated &lt;em>n&lt;/em> &amp;ldquo;home&amp;rdquo; nodes for a value.&lt;/p>
&lt;p>Once the network interruption is fixed, any writes are sent to the appropriate &amp;ldquo;home&amp;rdquo; nodes (&lt;em>hinted handoff&lt;/em>).&lt;/p>
&lt;p>Sloppy quorums are useful for increasing write availability: as long as any &lt;em>w&lt;/em> nodes are available, the database can accept writes. This also means that you cannot be sure to read the latest value for a key, because it may have been temporarily written to some nodes outside of &lt;em>n&lt;/em>.&lt;/p>
&lt;h5 id="multi-datacenter-operation-1">Multi-datacenter operation&lt;/h5>
&lt;p>Each write from a client is sent to all replicas, regardless of datacenter, but the client usually only waits for acknowledgement from a quorum of nodes within its local datacenter so that it is unaffected by delays and interruptions on cross-datacenter link.&lt;/p>
&lt;h4 id="detecting-concurrent-writes">Detecting concurrent writes&lt;/h4>
&lt;p>In order to become eventually consistent, the replicas should converge toward the same value. If you want to avoid losing data, you application developer, need to know a lot about the internals of your database&amp;rsquo;s conflict handling.&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Last write wins (discarding concurrent writes).&lt;/strong> Even though the writes don&amp;rsquo; have a natural ordering, we can force an arbitrary order on them. We can attach a timestamp to each write and pick the most recent. There are some situations such caching on which lost writes are acceptable. If losing data is not acceptable, LWW is a poor choice for conflict resolution.&lt;/li>
&lt;li>&lt;strong>The &amp;ldquo;happens-before&amp;rdquo; relationship and concurrency.&lt;/strong> Whether one operation happens before another operation is the key to defining what concurrency means. &lt;strong>We can simply say that to operations are &lt;em>concurrent&lt;/em> if neither happens before the other.&lt;/strong> Either A happened before B, or B happened before A, or A and B are concurrent.&lt;/li>
&lt;/ul>
&lt;h5 id="capturing-the-happens-before-relationship">Capturing the happens-before relationship&lt;/h5>
&lt;p>The server can determine whether two operations are concurrent by looking at the version numbers.&lt;/p>
&lt;ul>
&lt;li>The server maintains a version number for every key, increments the version number every time that key is written, and stores the new version number along the value written.&lt;/li>
&lt;li>Client reads a key, the server returns all values that have not been overwrite, as well as the latest version number. A client must read a key before writing.&lt;/li>
&lt;li>Client writes a key, it must include the version number from the prior read, and it must merge together all values that it received in the prior read.&lt;/li>
&lt;li>Server receives a write with a particular version number, it can overwrite all values with that version number or below, but it must keep all values with a higher version number.&lt;/li>
&lt;/ul>
&lt;h5 id="merging-concurrently-written-values">Merging concurrently written values&lt;/h5>
&lt;p>No data is silently dropped. It requires clients do some extra work, they have to clean up afterward by merging the concurrently written values. Riak calls these concurrent values &lt;em>siblings&lt;/em>.&lt;/p>
&lt;p>Merging sibling values is the same problem as conflict resolution in multi-leader replication. A simple approach is to just pick one of the values on a version number or timestamp (last write wins). You may need to do something more intelligent in application code to avoid losing data.&lt;/p>
&lt;p>If you want to allow people to &lt;em>remove&lt;/em> things, union of siblings may not yield the right result. An item cannot simply be deleted from the database when it is removed, the system must leave a marker with an appropriate version number to indicate that the item has been removed when merging siblings (&lt;em>tombstone&lt;/em>).&lt;/p>
&lt;p>Merging siblings in application code is complex and error-prone, there are efforts to design data structures that can perform this merging automatically (CRDTs).&lt;/p>
&lt;h4 id="version-vectors">Version vectors&lt;/h4>
&lt;p>We need a version number &lt;em>per replica&lt;/em> as well as per key. Each replica increments its own version number when processing a write, and also keeps track of the version numbers it has seen from each of the other replicas.&lt;/p>
&lt;p>The collection of version numbers from all the replicas is called a &lt;em>version vector&lt;/em>.&lt;/p>
&lt;p>Version vector are sent from the database replicas to clients when values are read, and need to be sent back to the database when a value is subsequently written. Riak calls this &lt;em>casual context&lt;/em>. Version vectors allow the database to distinguish between overwrites and concurrent writes.&lt;/p>
&lt;h2 id="partitioning">Partitioning&lt;/h2>
&lt;p>Replication, for very large datasets or very high query throughput is not sufficient, we need to break the data up into &lt;em>partitions&lt;/em> (&lt;em>sharding&lt;/em>).&lt;/p>
&lt;p>Basically, each partition is a small database of its own.&lt;/p>
&lt;p>The main reason for wanting to partition data is &lt;em>scalability&lt;/em>, query load can be load cabe distributed across many processors. Throughput can be scaled by adding more nodes.&lt;/p>
&lt;h3 id="partitioning-and-replication">Partitioning and replication&lt;/h3>
&lt;p>Each record belongs to exactly one partition, it may still be stored on several nodes for fault tolerance.&lt;/p>
&lt;p>A node may store more than one partition.&lt;/p>
&lt;h3 id="partition-of-key-value-data">Partition of key-value data&lt;/h3>
&lt;p>Our goal with partitioning is to spread the data and the query load evenly across nodes.&lt;/p>
&lt;p>If partition is unfair, we call it &lt;em>skewed&lt;/em>. It makes partitioning much less effective. A partition with disproportionately high load is called a &lt;em>hot spot&lt;/em>.&lt;/p>
&lt;p>The simplest approach is to assign records to nodes randomly. The main disadvantage is that if you are trying to read a particular item, you have no way of knowing which node it is on, so you have to query all nodes in parallel.&lt;/p>
&lt;h4 id="partition-by-key-range">Partition by key range&lt;/h4>
&lt;p>Assign a continuous range of keys, like the volumes of a paper encyclopaedia. Boundaries might be chose manually by an administrator, or the database can choose them automatically. On each partition, keys are in sorted order so scans are easy.&lt;/p>
&lt;p>The downside is that certain access patterns can lead to hot spots.&lt;/p>
&lt;h4 id="partitioning-by-hash-of-key">Partitioning by hash of key&lt;/h4>
&lt;p>A good hash function takes skewed data and makes it uniformly distributed. There is no need to be cryptographically strong (MongoDB uses MD5 and Cassandra uses Murmur3). You can assign each partition a range of hashes. The boundaries can be evenly spaced or they can be chosen pseudorandomly (&lt;em>consistent hashing&lt;/em>).&lt;/p>
&lt;p>Unfortunately we lose the ability to do efficient range queries. Keys that were once adjacent are now scattered across all the partitions. Any range query has to be sent to all partitions.&lt;/p>
&lt;h4 id="skewed-workloads-and-relieving-hot-spots">Skewed workloads and relieving hot spots&lt;/h4>
&lt;p>You can&amp;rsquo;t avoid hot spots entirely. For example, you may end up with large volume of writes to the same key.&lt;/p>
&lt;p>It&amp;rsquo;s the responsibility of the application to reduce the skew. A simple technique is to add a random number to the beginning or end of the key.&lt;/p>
&lt;p>Splitting writes across different keys, makes reads now to do some extra work and combine them.&lt;/p>
&lt;h3 id="partitioning-and-secondary-indexes">Partitioning and secondary indexes&lt;/h3>
&lt;p>The situation gets more complicated if secondary indexes are involved. A secondary index usually doesn&amp;rsquo;t identify the record uniquely. They don&amp;rsquo;t map neatly to partitions.&lt;/p>
&lt;h4 id="partitioning-secondary-indexes-by-document">Partitioning secondary indexes by document&lt;/h4>
&lt;p>Each partition maintains its secondary indexes, covering only the documents in that partition (&lt;em>local index&lt;/em>).&lt;/p>
&lt;p>You need to send the query to &lt;em>all&lt;/em> partitions, and combine all the results you get back (&lt;em>scatter/gather&lt;/em>). This is prone to tail latency amplification and is widely used in MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud and VoltDB.&lt;/p>
&lt;h4 id="partitioning-secondary-indexes-by-term">Partitioning secondary indexes by term&lt;/h4>
&lt;p>We construct a &lt;em>global index&lt;/em> that covers data in all partitions. The global index must also be partitioned so it doesn&amp;rsquo;t become the bottleneck.&lt;/p>
&lt;p>It is called the &lt;em>term-partitioned&lt;/em> because the term we&amp;rsquo;re looking for determines the partition of the index.&lt;/p>
&lt;p>Partitioning by term can be useful for range scans, whereas partitioning on a hash of the term gives a more even distribution load.&lt;/p>
&lt;p>The advantage is that it can make reads more efficient: rather than doing scatter/gather over all partitions, a client only needs to make a request to the partition containing the term that it wants. The downside of a global index is that writes are slower and complicated.&lt;/p>
&lt;h3 id="rebalancing-partitions">Rebalancing partitions&lt;/h3>
&lt;p>The process of moving load from one node in the cluster to another.&lt;/p>
&lt;p>Strategies for rebalancing:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>How not to do it: Hash mod n.&lt;/strong> The problem with &lt;em>mod N&lt;/em> is that if the number of nodes &lt;em>N&lt;/em> changes, most of the keys will need to be moved from one node to another.&lt;/li>
&lt;li>&lt;strong>Fixed number of partitions.&lt;/strong> Create many more partitions than there are nodes and assign several partitions to each node. If a node is added to the cluster, we can &lt;em>steal&lt;/em> a few partitions from every existing node until partitions are fairly distributed once again. The number of partitions does not change, nor does the assignment of keys to partitions. The only thing that change is the assignment of partitions to nodes. This is used in Riak, Elasticsearch, Couchbase, and Voldemport. &lt;strong>You need to choose a high enough number of partitions to accomodate future growth.&lt;/strong> Neither too big or too small.&lt;/li>
&lt;li>&lt;strong>Dynamic partitioning.&lt;/strong> The number of partitions adapts to the total data volume. An empty database starts with an empty partition. While the dataset is small, all writes have to processed by a single node while the others nodes sit idle. HBase and MongoDB allow an initial set of partitions to be configured (&lt;em>pre-splitting&lt;/em>).&lt;/li>
&lt;li>&lt;strong>Partitioning proportionally to nodes.&lt;/strong> Cassandra and Ketama make the number of partitions proportional to the number of nodes. Have a fixed number of partitions &lt;em>per node&lt;/em>. This approach also keeps the size of each partition fairly stable.&lt;/li>
&lt;/ul>
&lt;h4 id="automatic-versus-manual-rebalancing">Automatic versus manual rebalancing&lt;/h4>
&lt;p>Fully automated rebalancing may seem convenient but the process can overload the network or the nodes and harm the performance of other requests while the rebalancing is in progress.&lt;/p>
&lt;p>It can be good to have a human in the loop for rebalancing. You may avoid operational surprises.&lt;/p>
&lt;h3 id="request-routing">Request routing&lt;/h3>
&lt;p>This problem is also called &lt;em>service discovery&lt;/em>. There are different approaches:&lt;/p>
&lt;ol>
&lt;li>Allow clients to contact any node and make them handle the request directly, or forward the request to the appropriate node.&lt;/li>
&lt;li>Send all requests from clients to a routing tier first that acts as a partition-aware load balancer.&lt;/li>
&lt;li>Make clients aware of the partitioning and the assignment of partitions to nodes.&lt;/li>
&lt;/ol>
&lt;p>In many cases the problem is: how does the component making the routing decision learn about changes in the assignment of partitions to nodes?&lt;/p>
&lt;p>Many distributed data systems rely on a separate coordination service such as ZooKeeper to keep track of this cluster metadata. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of partitions to nodes. The routing tier or the partitioning-aware client, can subscribe to this information in ZooKeeper. HBase, SolrCloud and Kafka use ZooKeeper to track partition assignment. MongoDB relies on its own &lt;em>config server&lt;/em>. Cassandra and Riak take a different approach: they use a &lt;em>gossip protocol&lt;/em>.&lt;/p>
&lt;h4 id="parallel-query-execution">Parallel query execution&lt;/h4>
&lt;p>&lt;em>Massively parallel processing&lt;/em> (MPP) relational database products are much more sophisticated in the types of queries they support.&lt;/p>
&lt;h2 id="transactions">Transactions&lt;/h2>
&lt;p>Implementing fault-tolerant mechanisms is a lot of work.&lt;/p>
&lt;h3 id="the-slippery-concept-of-a-transaction">The slippery concept of a transaction&lt;/h3>
&lt;p>&lt;em>Transactions&lt;/em> have been the mechanism of choice for simplifying these issues. Conceptually, all the reads and writes in a transaction are executed as one operation: either the entire transaction succeeds (&lt;em>commit&lt;/em>) or it fails (&lt;em>abort&lt;/em>, &lt;em>rollback&lt;/em>).&lt;/p>
&lt;p>The application is free to ignore certain potential error scenarios and concurrency issues (&lt;em>safety guarantees&lt;/em>).&lt;/p>
&lt;h4 id="acid">ACID&lt;/h4>
&lt;ul>
&lt;li>&lt;strong>Atomicity.&lt;/strong> Is &lt;em>not&lt;/em> about concurrency. It is what happens if a client wants to make several writes, but a fault occurs after some of the writes have been processed. &lt;em>Abortability&lt;/em> would have been a better term than &lt;em>atomicity&lt;/em>.&lt;/li>
&lt;li>&lt;strong>Consistency.&lt;/strong> &lt;em>Invariants&lt;/em> on your data must always be true. The idea of consistency depends on the application&amp;rsquo;s notion of invariants. Atomicity, isolation, and durability are properties of the database, whereas consistency (in an ACID sense) is a property of the application.&lt;/li>
&lt;li>&lt;strong>Isolation.&lt;/strong> Concurrently executing transactions are isolated from each other. It&amp;rsquo;s also called &lt;em>serializability&lt;/em>, each transaction can pretend that it is the only transaction running on the entire database, and the result is the same as if they had run &lt;em>serially&lt;/em> (one after the other).&lt;/li>
&lt;li>&lt;strong>Durability.&lt;/strong> Once a transaction has committed successfully, any data it has written will not be forgotten, even if there is a hardware fault or the database crashes. In a single-node database this means the data has been written to nonvolatile storage. In a replicated database it means the data has been successfully copied to some number of nodes.&lt;/li>
&lt;/ul>
&lt;p>Atomicity can be implemented using a log for crash recovery, and isolation can be implemented using a lock on each object, allowing only one thread to access an object at any one time.&lt;/p>
&lt;p>&lt;strong>A transaction is a mechanism for grouping multiple operations on multiple objects into one unit of execution.&lt;/strong>&lt;/p>
&lt;h4 id="handling-errors-and-aborts">Handling errors and aborts&lt;/h4>
&lt;p>A key feature of a transaction is that it can be aborted and safely retried if an error occurred.&lt;/p>
&lt;p>In datastores with leaderless replication is the application&amp;rsquo;s responsibility to recover from errors.&lt;/p>
&lt;p>The whole point of aborts is to enable safe retries.&lt;/p>
&lt;h3 id="weak-isolation-levels">Weak isolation levels&lt;/h3>
&lt;p>Concurrency issues (race conditions) come into play when one transaction reads data that is concurrently modified by another transaction, or when two transactions try to simultaneously modify the same data.&lt;/p>
&lt;p>Databases have long tried to hide concurrency issues by providing &lt;em>transaction isolation&lt;/em>.&lt;/p>
&lt;p>In practice, is not that simple. Serializable isolation has a performance cost. It&amp;rsquo;s common for systems to use weaker levels of isolation, which protect against &lt;em>some&lt;/em> concurrency issues, but not all.&lt;/p>
&lt;p>Weak isolation levels used in practice:&lt;/p>
&lt;h4 id="read-committed">Read committed&lt;/h4>
&lt;p>It makes two guarantees:&lt;/p>
&lt;ol>
&lt;li>When reading from the database, you will only see data that has been committed (no &lt;em>dirty reads&lt;/em>). Writes by a transaction only become visible to others when that transaction commits.&lt;/li>
&lt;li>When writing to the database, you will only overwrite data that has been committed (no &lt;em>dirty writes&lt;/em>). Dirty writes are prevented usually by delaying the second write until the first write&amp;rsquo;s transaction has committed or aborted.&lt;/li>
&lt;/ol>
&lt;p>Most databases prevent dirty writes by using row-level locks that hold the lock until the transaction is committed or aborted. Only one transaction can hold the lock for any given object.&lt;/p>
&lt;p>On dirty reads, requiring read locks does not work well in practice as one long-running write transaction can force many read-only transactions to wait. For every object that is written, the database remembers both the old committed value and the new value set by the transaction that currently holds the write lock. While the transaction is ongoing, any other transactions that read the object are simply given the old value.&lt;/p>
&lt;h4 id="snapshot-isolation-and-repeatable-read">Snapshot isolation and repeatable read&lt;/h4>
&lt;p>There are still plenty of ways in which you can have concurrency bugs when using this isolation level.&lt;/p>
&lt;p>&lt;em>Nonrepeatable read&lt;/em> or &lt;em>read skew&lt;/em>, when you read at the same time you committed a change you may see temporal and inconsistent results.&lt;/p>
&lt;p>There are some situations that cannot tolerate such temporal inconsistencies:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Backups.&lt;/strong> During the time that the backup process is running, writes will continue to be made to the database. If you need to restore from such a backup, inconsistencies can become permanent.&lt;/li>
&lt;li>&lt;strong>Analytic queries and integrity checks.&lt;/strong> You may get nonsensical results if they observe parts of the database at different points in time.&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Snapshot isolation&lt;/em> is the most common solution. Each transaction reads from a &lt;em>consistent snapshot&lt;/em> of the database.&lt;/p>
&lt;p>The implementation of snapshots typically use write locks to prevent dirty writes.&lt;/p>
&lt;p>The database must potentially keep several different committed versions of an object (&lt;em>multi-version concurrency control&lt;/em> or MVCC).&lt;/p>
&lt;p>Read committed uses a separate snapshot for each query, while snapshot isolation uses the same snapshot for an entire transaction.&lt;/p>
&lt;p>How do indexes work in a multi-version database? One option is to have the index simply point to all versions of an object and require an index query to filter out any object versions that are not visible to the current transaction.&lt;/p>
&lt;p>Snapshot isolation is called &lt;em>serializable&lt;/em> in Oracle, and &lt;em>repeatable read&lt;/em> in PostgreSQL and MySQL.&lt;/p>
&lt;h4 id="preventing-lost-updates">Preventing lost updates&lt;/h4>
&lt;p>This might happen if an application reads some value from the database, modifies it, and writes it back. If two transactions do this concurrently, one of the modifications can be lost (later write &lt;em>clobbers&lt;/em> the earlier write).&lt;/p>
&lt;h5 id="atomic-write-operations">Atomic write operations&lt;/h5>
&lt;p>A solution for this it to avoid the need to implement read-modify-write cycles and provide atomic operations such us&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">UPDATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">counters&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">WHERE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">key&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;foo&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>MongoDB provides atomic operations for making local modifications, and Redis provides atomic operations for modifying data structures.&lt;/p>
&lt;h5 id="explicit-locking">Explicit locking&lt;/h5>
&lt;p>The application explicitly lock objects that are going to be updated.&lt;/p>
&lt;h5 id="automatically-detecting-lost-updates">Automatically detecting lost updates&lt;/h5>
&lt;p>Allow them to execute in parallel, if the transaction manager detects a lost update, abort the transaction and force it to retry its read-modify-write cycle.&lt;/p>
&lt;p>MySQL/InnoDB&amp;rsquo;s repeatable read does not detect lost updates.&lt;/p>
&lt;h5 id="compare-and-set">Compare-and-set&lt;/h5>
&lt;p>If the current value does not match with what you previously read, the update has no effect.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-SQL" data-lang="SQL">&lt;span class="line">&lt;span class="cl">&lt;span class="k">UPDATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">wiki_pages&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;new content&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w"> &lt;/span>&lt;span class="k">WHERE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1234&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">AND&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">content&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;old content&amp;#39;&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h5 id="conflict-resolution-and-replication">Conflict resolution and replication&lt;/h5>
&lt;p>With multi-leader or leaderless replication, compare-and-set do not apply.&lt;/p>
&lt;p>A common approach in replicated databases is to allow concurrent writes to create several conflicting versions of a value (also know as &lt;em>siblings&lt;/em>), and to use application code or special data structures to resolve and merge these versions after the fact.&lt;/p>
&lt;h4 id="write-skew-and-phantoms">Write skew and phantoms&lt;/h4>
&lt;p>Imagine Alice and Bob are two on-call doctors for a particular shift. Imagine both the request to leave because they are feeling unwell. Unfortunately they happen to click the button to go off call at approximately the same time.&lt;/p>
&lt;pre>&lt;code>ALICE BOB
┌─ BEGIN TRANSACTION ┌─ BEGIN TRANSACTION
│ │
├─ currently_on_call = ( ├─ currently_on_call = (
│ select count(*) from doctors │ select count(*) from doctors
│ where on_call = true │ where on_call = true
│ and shift_id = 1234 │ and shift_id = 1234
│ ) │ )
│ // now currently_on_call = 2 │ // now currently_on_call = 2
│ │
├─ if (currently_on_call 2) { │
│ update doctors │
│ set on_call = false │
│ where name = 'Alice' │
│ and shift_id = 1234 ├─ if (currently_on_call &amp;gt;= 2) {
│ } │ update doctors
│ │ set on_call = false
└─ COMMIT TRANSACTION │ where name = 'Bob'
│ and shift_id = 1234
│ }
│
└─ COMMIT TRANSACTION
&lt;/code>&lt;/pre>
&lt;p>Since database is using snapshot isolation, both checks return 2. Both transactions commit, and now no doctor is on call. The requirement of having at least one doctor has been violated.&lt;/p>
&lt;p>Write skew can occur if two transactions read the same objects, and then update some of those objects. You get a dirty write or lost update anomaly.&lt;/p>
&lt;p>Ways to prevent write skew are a bit more restricted:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Atomic operations don&amp;rsquo;t help as things involve more objects.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Automatically prevent write skew requires true serializable isolation.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>The second-best option in this case is probably to explicitly lock the rows that the transaction depends on.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sql" data-lang="sql">&lt;span class="line">&lt;span class="cl">&lt;span class="k">BEGIN&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">TRANSACTION&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">SELECT&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FROM&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">doctors&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">WHERE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">on_call&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">true&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AND&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">shift_id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1234&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">FOR&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">UPDATE&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">UPDATE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">doctors&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">SET&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">on_call&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="k">false&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">WHERE&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">name&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="s1">&amp;#39;Alice&amp;#39;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">AND&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="n">shift_id&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="w"> &lt;/span>&lt;span class="mi">1234&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="w">&lt;/span>&lt;span class="k">COMMIT&lt;/span>&lt;span class="p">;&lt;/span>&lt;span class="w">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;/ul>
&lt;h3 id="serializability">Serializability&lt;/h3>
&lt;p>This is the strongest isolation level. It guarantees that even though transactions may execute in parallel, the end result is the same as if they had executed one at a time, &lt;em>serially&lt;/em>, without concurrency. Basically, the database prevents &lt;em>all&lt;/em> possible race conditions.&lt;/p>
&lt;p>There are three techniques for achieving this:&lt;/p>
&lt;ul>
&lt;li>Executing transactions in serial order&lt;/li>
&lt;li>Two-phase locking&lt;/li>
&lt;li>Serializable snapshot isolation.&lt;/li>
&lt;/ul>
&lt;h4 id="actual-serial-execution">Actual serial execution&lt;/h4>
&lt;p>The simplest way of removing concurrency problems is to remove concurrency entirely and execute only one transaction at a time, in serial order, on a single thread. This approach is implemented by VoltDB/H-Store, Redis and Datomic.&lt;/p>
&lt;h5 id="encapsulating-transactions-in-stored-procedures">Encapsulating transactions in stored procedures&lt;/h5>
&lt;p>With interactive style of transaction, a lot of time is spent in network communication between the application and the database.&lt;/p>
&lt;p>For this reason, systems with single-threaded serial transaction processing don&amp;rsquo;t allow interactive multi-statement transactions. The application must submit the entire transaction code to the database ahead of time, as a &lt;em>stored procedure&lt;/em>, so all the data required by the transaction is in memory and the procedure can execute very fast.&lt;/p>
&lt;p>There are a few pros and cons for stored procedures:&lt;/p>
&lt;ul>
&lt;li>Each database vendor has its own language for stored procedures. They usually look quite ugly and archaic from today&amp;rsquo;s point of view, and they lack the ecosystem of libraries.&lt;/li>
&lt;li>It&amp;rsquo;s harder to debug, more awkward to keep in version control and deploy, trickier to test, and difficult to integrate with monitoring.&lt;/li>
&lt;/ul>
&lt;p>Modern implementations of stored procedures include general-purpose programming languages instead: VoltDB uses Java or Groovy, Datomic uses Java or Clojure, and Redis uses Lua.&lt;/p>
&lt;h5 id="partitioning-1">Partitioning&lt;/h5>
&lt;p>Executing all transactions serially limits the transaction throughput to the speed of a single CPU.&lt;/p>
&lt;p>In order to scale to multiple CPU cores you can potentially partition your data and each partition can have its own transaction processing thread. You can give each CPU core its own partition.&lt;/p>
&lt;p>For any transaction that needs to access multiple partitions, the database must coordinate the transaction across all the partitions. They will be vastly slower than single-partition transactions.&lt;/p>
&lt;h4 id="two-phase-locking-2pl">Two-phase locking (2PL)&lt;/h4>
&lt;blockquote>
&lt;p>Two-phase locking (2PL) sounds similar to two-phase &lt;em>commit&lt;/em> (2PC) but be aware that they are completely different things.&lt;/p>
&lt;/blockquote>
&lt;p>Several transactions are allowed to concurrently read the same object as long as nobody is writing it. When somebody wants to write (modify or delete) an object, exclusive access is required.&lt;/p>
&lt;p>Writers don&amp;rsquo;t just block other writers; they also block readers and vice versa. It protects against all the race conditions discussed earlier.&lt;/p>
&lt;p>Blocking readers and writers is implemented by a having lock on each object in the database. The lock is used as follows:&lt;/p>
&lt;ul>
&lt;li>if a transaction want sot read an object, it must first acquire a lock in shared mode.&lt;/li>
&lt;li>If a transaction wants to write to an object, it must first acquire the lock in exclusive mode.&lt;/li>
&lt;li>If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock.&lt;/li>
&lt;li>After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). &lt;strong>First phase is when the locks are acquired, second phase is when all the locks are released.&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>It can happen that transaction A is stuck waiting for transaction B to release its lock, and vice versa (&lt;em>deadlock&lt;/em>).&lt;/p>
&lt;p>&lt;strong>The performance for transaction throughput and response time of queries are significantly worse under two-phase locking than under weak isolation.&lt;/strong>&lt;/p>
&lt;p>A transaction may have to wait for several others to complete before it can do anything.&lt;/p>
&lt;p>Databases running 2PL can have unstable latencies, and they can be very slow at high percentiles. One slow transaction, or one transaction that accesses a lot of data and acquires many locks can cause the rest of the system to halt.&lt;/p>
&lt;h5 id="predicate-locks">Predicate locks&lt;/h5>
&lt;p>With &lt;em>phantoms&lt;/em>, one transaction may change the results of another transaction&amp;rsquo;s search query.&lt;/p>
&lt;p>In order to prevent phantoms, we need a &lt;em>predicate lock&lt;/em>. Rather than a lock belonging to a particular object, it belongs to all objects that match some search condition.&lt;/p>
&lt;p>Predicate locks applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms).&lt;/p>
&lt;h5 id="index-range-locks">Index-range locks&lt;/h5>
&lt;p>Predicate locks do not perform well. Checking for matching locks becomes time-consuming and for that reason most databases implement &lt;em>index-range locking&lt;/em>.&lt;/p>
&lt;p>It&amp;rsquo;s safe to simplify a predicate by making it match a greater set of objects.&lt;/p>
&lt;p>These locks are not as precise as predicate locks would be, but since they have much lower overheads, they are a good compromise.&lt;/p>
&lt;h4 id="serializable-snapshot-isolation-ssi">Serializable snapshot isolation (SSI)&lt;/h4>
&lt;p>It provides full serializability and has a small performance penalty compared to snapshot isolation. SSI is fairly new and might become the new default in the future.&lt;/p>
&lt;h5 id="pesimistic-versus-optimistic-concurrency-control">Pesimistic versus optimistic concurrency control&lt;/h5>
&lt;p>Two-phase locking is called &lt;em>pessimistic&lt;/em> concurrency control because if anything might possibly go wrong, it&amp;rsquo;s better to wait.&lt;/p>
&lt;p>Serial execution is also &lt;em>pessimistic&lt;/em> as is equivalent to each transaction having an exclusive lock on the entire database.&lt;/p>
&lt;p>Serializable snapshot isolation is &lt;em>optimistic&lt;/em> concurrency control technique. Instead of blocking if something potentially dangerous happens, transactions continue anyway, in the hope that everything will turn out all right. The database is responsible for checking whether anything bad happened. If so, the transaction is aborted and has to be retried.&lt;/p>
&lt;p>If there is enough spare capacity, and if contention between transactions is not too high, optimistic concurrency control techniques tend to perform better than pessimistic ones.&lt;/p>
&lt;p>SSI is based on snapshot isolation, reads within a transaction are made from a consistent snapshot of the database. On top of snapshot isolation, SSI adds an algorithm for detecting serialization conflicts among writes and determining which transactions to abort.&lt;/p>
&lt;p>The database knows which transactions may have acted on an outdated premise and need to be aborted by:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Detecting reads of a stale MVCC object version.&lt;/strong> The database needs to track when a transaction ignores another transaction&amp;rsquo;s writes due to MVCC visibility rules. When a transaction wants to commit, the database checks whether any of the ignored writes have now been committed. If so, the transaction must be aborted.&lt;/li>
&lt;li>&lt;strong>Detecting writes that affect prior reads.&lt;/strong> As with two-phase locking, SSI uses index-range locks except that it does not block other transactions. When a transaction writes to the database, it must look in the indexes for any other transactions that have recently read the affected data. It simply notifies the transactions that the data they read may no longer be up to date.&lt;/li>
&lt;/ul>
&lt;h5 id="performance-of-serializable-snapshot-isolation">Performance of serializable snapshot isolation&lt;/h5>
&lt;p>Compared to two-phase locking, the big advantage of SSI is that one transaction doesn&amp;rsquo;t need to block waiting for locks held by another transaction. Writers don&amp;rsquo;t block readers, and vice versa.&lt;/p>
&lt;p>Compared to serial execution, SSI is not limited to the throughput of a single CPU core. Transactions can read and write data in multiple partitions while ensuring serializable isolation.&lt;/p>
&lt;p>The rate of aborts significantly affects the overall performance of SSI. SSI requires that read-write transactions be fairly short (long-running read-only transactions may be okay).&lt;/p>
&lt;h2 id="the-trouble-with-distributed-systems">The trouble with distributed systems&lt;/h2>
&lt;h3 id="faults-and-partial-failures">Faults and partial failures&lt;/h3>
&lt;p>A program on a single computer either works or it doesn&amp;rsquo;t. There is no reason why software should be flaky (non deterministic).&lt;/p>
&lt;p>In a distributed systems we have no choice but to confront the messy reality of the physical world. There will be parts that are broken in an unpredictable way, while others work. Partial failures are &lt;em>nondeterministic&lt;/em>. Things will unpredicably fail.&lt;/p>
&lt;p>We need to accept the possibility of partial failure and build fault-tolerant mechanism into the software. &lt;strong>We need to build a reliable system from unreliable components.&lt;/strong>&lt;/p>
&lt;h3 id="unreliable-networks">Unreliable networks&lt;/h3>
&lt;p>Focusing on &lt;em>shared-nothing systems&lt;/em> the network is the only way machines communicate.&lt;/p>
&lt;p>The internet and most internal networks are &lt;em>asynchronous packet networks&lt;/em>. A message is sent and the network gives no guarantees as to when it will arrive, or whether it will arrive at all. Things that could go wrong:&lt;/p>
&lt;ol>
&lt;li>Request lost&lt;/li>
&lt;li>Request waiting in a queue to be delivered later&lt;/li>
&lt;li>Remote node may have failed&lt;/li>
&lt;li>Remote node may have temporarily stoped responding&lt;/li>
&lt;li>Response has been lost on the network&lt;/li>
&lt;li>The response has been delayed and will be delivered later&lt;/li>
&lt;/ol>
&lt;p>If you send a request to another node and don&amp;rsquo;t receive a response, it is &lt;em>impossible&lt;/em> to tell why.&lt;/p>
&lt;p>&lt;strong>The usual way of handling this issue is a &lt;em>timeout&lt;/em>&lt;/strong>: after some time you give up waiting and assume that the response is not going to arrive.&lt;/p>
&lt;p>Nobody is immune to network problems. You do need to know how your software reacts to network problems to ensure that the system can recover from them. It may make sense to deliberately trigger network problems and test the system&amp;rsquo;s response.&lt;/p>
&lt;p>If you want to be sure that a request was successful, you need a positive response from the application itself.&lt;/p>
&lt;p>If something has gone wrong, you have to assume that you will get no response at all.&lt;/p>
&lt;h4 id="timeouts-and-unbounded-delays">Timeouts and unbounded delays&lt;/h4>
&lt;p>A long timeout means a long wait until a node is declared dead. A short timeout detects faults faster, but carries a higher risk of incorrectly declaring a node dead (when it could be a slowdown).&lt;/p>
&lt;p>Premature declaring a node is problematic, if the node is actually alive the action may end up being performed twice.&lt;/p>
&lt;p>When a node is declared dead, its responsibilities need to be transferred to other nodes, which places additional load on other nodes and the network.&lt;/p>
&lt;h4 id="network-congestion-and-queueing">Network congestion and queueing&lt;/h4>
&lt;ul>
&lt;li>Different nodes try to send packets simultaneously to the same destination, the network switch must queue them and feed them to the destination one by one. The switch will discard packets when filled up.&lt;/li>
&lt;li>If CPU cores are busy, the request is queued by the operative system, until applications are ready to handle it.&lt;/li>
&lt;li>In virtual environments, the operative system is often paused while another virtual machine uses a CPU core. The VM queues the incoming data.&lt;/li>
&lt;li>TCP performs &lt;em>flow control&lt;/em>, in which a node limits its own rate of sending in order to avoid overloading a network link or the receiving node. This means additional queuing at the sender.&lt;/li>
&lt;/ul>
&lt;p>You can choose timeouts experimentally by measuring the distribution of network round-trip times over an extended period.&lt;/p>
&lt;p>Systems can continually measure response times and their variability (&lt;em>jitter&lt;/em>), and automatically adjust timeouts according to the observed response time distribution.&lt;/p>
&lt;h4 id="synchronous-vs-ashynchronous-networks">Synchronous vs ashynchronous networks&lt;/h4>
&lt;p>A telephone network estabilishes a &lt;em>circuit&lt;/em>, we say is &lt;em>synchronous&lt;/em> even as the data passes through several routers as it does not suffer from queing. The maximum end-to-end latency of the network is fixed (&lt;em>bounded delay&lt;/em>).&lt;/p>
&lt;p>A circuit is a fixed amount of reserved bandwidth which nobody else can use while the circuit is established, whereas packets of a TCP connection opportunistically use whatever network bandwidth is available.&lt;/p>
&lt;p>&lt;strong>Using circuits for bursty data transfers wastes network capacity and makes transfer unnecessary slow. By contrast, TCP dinamycally adapts the rate of data transfer to the available network capacity.&lt;/strong>&lt;/p>
&lt;p>We have to assume that network congestion, queueing, and unbounded delays will happen. Consequently, there&amp;rsquo;s no &amp;ldquo;correct&amp;rdquo; value for timeouts, they need to be determined experimentally.&lt;/p>
&lt;h3 id="unreliable-clocks">Unreliable clocks&lt;/h3>
&lt;p>The time when a message is received is always later than the time when it is sent, we don&amp;rsquo;t know how much later due to network delays. This makes difficult to determine the order of which things happened when multiple machines are involved.&lt;/p>
&lt;p>Each machine on the network has its own clock, slightly faster or slower than the other machines. It is possible to synchronise clocks with Network Time Protocol (NTP).&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Time-of-day clocks&lt;/strong>. Return the current date and time according to some calendar (&lt;em>wall-clock time&lt;/em>). If the local clock is toof ar ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in time. &lt;strong>This makes it is unsuitable for measuring elapsed time.&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Monotonic clocks&lt;/strong>. Peg: &lt;code>System.nanoTime()&lt;/code>. They are guaranteed to always move forward. The difference between clock reads can tell you how much time elapsed beween two checks. &lt;strong>The &lt;em>absolute&lt;/em> value of the clock is meaningless.&lt;/strong> NTP allows the clock rate to be speeded up or slowed down by up to 0.05%, but &lt;strong>NTP cannot cause the monotonic clock to jump forward or backward&lt;/strong>. &lt;strong>In a distributed system, using a monotonic clock for measuring elapsed time (peg: timeouts), is usually fine&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>If some piece of sofware is relying on an accurately synchronised clock, the result is more likely to be silent and subtle data loss than a dramatic crash.&lt;/p>
&lt;p>You need to carefully monitor the clock offsets between all the machines.&lt;/p>
&lt;h4 id="timestamps-for-ordering-events">Timestamps for ordering events&lt;/h4>
&lt;p>&lt;strong>It is tempting, but dangerous to rely on clocks for ordering of events across multiple nodes.&lt;/strong> This usually imply that &lt;em>last write wins&lt;/em> (LWW), often used in both multi-leader replication and leaderless databases like Cassandra and Riak, and data-loss may happen.&lt;/p>
&lt;p>The definition of &amp;ldquo;recent&amp;rdquo; also depends on local time-of-day clock, which may well be incorrect.&lt;/p>
&lt;p>&lt;em>Logical clocks&lt;/em>, based on counters instead of oscillating quartz crystal, are safer alternative for ordering events. Logical clocks do not measure time of the day or elapsed time, only relative ordering of events. This contrasts with time-of-the-day and monotic clocks (also known as &lt;em>physical clocks&lt;/em>).&lt;/p>
&lt;h4 id="clock-readings-have-a-confidence-interval">Clock readings have a confidence interval&lt;/h4>
&lt;p>It doesn&amp;rsquo;t make sense to think of a clock reading as a point in time, it is more like a range of times, within a confidence internval: for example, 95% confident that the time now is between 10.3 and 10.5.&lt;/p>
&lt;p>The most common implementation of snapshot isolation requires a monotonically increasing transaction ID.&lt;/p>
&lt;p>Spanner implements snapshot isolation across datacenters by using clock&amp;rsquo;s confidence interval. If you have two confidence internvals where&lt;/p>
&lt;pre tabindex="0">&lt;code>A = [A earliest, A latest]
B = [B earliest, B latest]
&lt;/code>&lt;/pre>&lt;p>And those two intervals do not overlap (&lt;code>A earliest&lt;/code> &amp;lt; &lt;code>A latest&lt;/code> &amp;lt; &lt;code>B earliest&lt;/code> &amp;lt; &lt;code>B latest&lt;/code>), then B definetively happened after A.&lt;/p>
&lt;p>Spanner deliberately waits for the length of the confidence interval before commiting a read-write transaction, so their confidence intervals do not overlap.&lt;/p>
&lt;p>Spanner needs to keep the clock uncertainty as small as possible, that&amp;rsquo;s why Google deploys a GPS receiver or atomic clock in each datacenter.&lt;/p>
&lt;h4 id="process-pauses">Process pauses&lt;/h4>
&lt;p>How does a node know that it is still leader?&lt;/p>
&lt;p>One option is for the leader to obtain a &lt;em>lease&lt;/em> from other nodes (similar ot a lock with a timeout). It will be the leader until the lease expires; to remain leader, the node must periodically renew the lease. If the node fails, another node can takeover when it expires.&lt;/p>
&lt;p>We have to be very careful making assumptions about the time that has passed for processing requests (and holding the lease), as there are many reasons a process would be paused:&lt;/p>
&lt;ul>
&lt;li>Garbage collector (stop the world)&lt;/li>
&lt;li>Virtual machine can be suspended&lt;/li>
&lt;li>In laptops execution may be suspended&lt;/li>
&lt;li>Operating system context-switches&lt;/li>
&lt;li>Synchronous disk access&lt;/li>
&lt;li>Swapping to disk (paging)&lt;/li>
&lt;li>Unix process can be stopped (&lt;code>SIGSTOP&lt;/code>)&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>You cannot assume anything about timing&lt;/strong>&lt;/p>
&lt;h5 id="response-time-guarantees">Response time guarantees&lt;/h5>
&lt;p>There are systems that require software to respond before a specific &lt;em>deadline&lt;/em> (&lt;em>real-time operating system, or RTOS&lt;/em>).&lt;/p>
&lt;p>Library functions must document their worst-case execution times; dynamic memory allocation may be restricted or disallowed and enormous amount of testing and measurement must be done.&lt;/p>
&lt;p>Garbage collection could be treated like brief planned outages. If the runtime can warn the application that a node soon requires a GC pause, the application can stop sending new requests to that node and perform GC while no requests are in progress.&lt;/p>
&lt;p>A variant of this idea is to use the garbage collector only for short-lived objects and to restart the process periodically.&lt;/p>
&lt;h3 id="knowledge-truth-and-lies">Knowledge, truth and lies&lt;/h3>
&lt;p>A node cannot necessarily trust its own judgement of a situation. Many distributed systems rely on a &lt;em>quorum&lt;/em> (voting among the nodes).&lt;/p>
&lt;p>Commonly, the quorum is an absolute majority of more than half of the nodes.&lt;/p>
&lt;h4 id="fencing-tokens">Fencing tokens&lt;/h4>
&lt;p>Assume every time the lock server grant sa lock or a lease, it also returns a &lt;em>fencing token&lt;/em>, which is a number that increases every time a lock is granted (incremented by the lock service). Then we can require every time a client sends a write request to the storage service, it must include its current fencing token.&lt;/p>
&lt;p>The storage server remembers that it has already processed a write with a higher token number, so it rejects the request with the last token.&lt;/p>
&lt;p>If ZooKeeper is used as lock service, the transaciton ID &lt;code>zcid&lt;/code> or the node version &lt;code>cversion&lt;/code> can be used as a fencing token.&lt;/p>
&lt;h4 id="byzantine-faults">Byzantine faults&lt;/h4>
&lt;p>Fencing tokens can detect and block a node that is &lt;em>inadvertently&lt;/em> acting in error.&lt;/p>
&lt;p>Distributed systems become much harder if there is a risk that nodes may &amp;ldquo;lie&amp;rdquo; (&lt;em>byzantine fault&lt;/em>).&lt;/p>
&lt;p>A system is &lt;em>Byzantine fault-tolerant&lt;/em> if it continues to operate correctly even if some of the nodes are malfunctioning.&lt;/p>
&lt;ul>
&lt;li>Aerospace environments&lt;/li>
&lt;li>Multiple participating organisations, some participants may attempt ot cheat or defraud others&lt;/li>
&lt;/ul>
&lt;h2 id="consistency-and-consensus">Consistency and consensus&lt;/h2>
&lt;p>The simplest way of handling faults is to simply let the entire service fail. We need to find ways of &lt;em>tolerating&lt;/em> faults.&lt;/p>
&lt;h3 id="consistency-guarantees">Consistency guarantees&lt;/h3>
&lt;p>Write requests arrive on different nodes at different times.&lt;/p>
&lt;p>Most replicated databases provide at least &lt;em>eventual consistency&lt;/em>. The inconsistency is temporary, and eventually resolves itself (&lt;em>convergence&lt;/em>).&lt;/p>
&lt;p>With weak guarantees, you need to be constantly aware of its limitations. Systems with stronger guarantees may have worse performance or be less fault-tolerant than systems with weaker guarantees.&lt;/p>
&lt;h3 id="linearizability">Linearizability&lt;/h3>
&lt;p>Make a system appear as if there were only one copy of the data, and all operaitons on it are atomic.&lt;/p>
&lt;ul>
&lt;li>&lt;code>read(x) =&amp;gt; v&lt;/code> Read from register &lt;em>x&lt;/em>, database returns value &lt;em>v&lt;/em>.&lt;/li>
&lt;li>&lt;code>write(x,v) =&amp;gt; r&lt;/code> &lt;em>r&lt;/em> could be &lt;em>ok&lt;/em> or &lt;em>error&lt;/em>.&lt;/li>
&lt;/ul>
&lt;p>If one client read returns the new value, all subsequent reads must also return the new value.&lt;/p>
&lt;ul>
&lt;li>&lt;code>cas(x_old, v_old, v_new) =&amp;gt; r&lt;/code> an atomic &lt;em>compare-and-set&lt;/em> operation. If the value of the register &lt;em>x&lt;/em> equals &lt;em>v_old&lt;/em>, it is atomically set to &lt;em>v_new&lt;/em>. If &lt;code>x != v_old&lt;/code> the registers is unchanged and it returns an error.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Serializability&lt;/strong>: Transactions behave the same as if they had executed &lt;em>some&lt;/em> serial order.&lt;/p>
&lt;p>&lt;strong>Linearizability&lt;/strong>: Recency guarantee on reads and writes of a register (individual object).&lt;/p>
&lt;h4 id="locking-and-leader-election">Locking and leader election&lt;/h4>
&lt;p>To ensure that there is indeed only one leader, a lock is used. It must be linearizable: all nodes must agree which nodes owns the lock; otherwise is useless.&lt;/p>
&lt;p>Apache ZooKeepr and etcd are often used for distributed locks and leader election.&lt;/p>
&lt;h4 id="constraints-and-uniqueness-guarantees">Constraints and uniqueness guarantees&lt;/h4>
&lt;p>Unique constraints, like a username or an email address require a situation similiar to a lock.&lt;/p>
&lt;p>A hard uniqueness constraint in relational databases requires linearizability.&lt;/p>
&lt;h4 id="implementing-linearizable-systems">Implementing linearizable systems&lt;/h4>
&lt;p>The simplest approach would be to have a single copy of the data, but this would not be able to tolerate faults.&lt;/p>
&lt;ul>
&lt;li>Single-leader repolication is potentially linearizable.&lt;/li>
&lt;li>Consensus algorithms is linearizable.&lt;/li>
&lt;li>Multi-leader replication is not linearizable.&lt;/li>
&lt;li>Leaderless replication is probably not linearizable.&lt;/li>
&lt;/ul>
&lt;p>Multi-leader replication is often a good choice for multi-datacenter replication. On a network interruption betwen data-centers will force a choice between linearizability and availability.&lt;/p>
&lt;p>With multi-leader configuraiton, each data center can operate normally with interruptions.&lt;/p>
&lt;p>With single-leader replication, the leader must be in one of the datacenters. If the application requires linearizable reads and writes, the network interruption causes the application to become unavailable.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>If your applicaiton &lt;em>requires&lt;/em> linearizability, and some replicas are disconnected from the other replicas due to a network problem, the some replicas cannot process request while they are disconnected (unavailable).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>If your application &lt;em>does not require&lt;/em>, then it can be written in a way tha each replica can process requests independently, even if it is disconnected from other replicas (peg: multi-leader), becoming &lt;em>available&lt;/em>.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>If an application does not require linearizability it can be more tolerant of network problems.&lt;/strong>&lt;/p>
&lt;h4 id="the-unhelpful-cap-theorem">The unhelpful CAP theorem&lt;/h4>
&lt;p>CAP is sometimes presented as &lt;em>Consistency, Availability, Partition tolerance: pick 2 out of 3&lt;/em>. Or being said in another way &lt;em>either Consistency or Available when Partitioned&lt;/em>.&lt;/p>
&lt;p>CAP only considers one consistency model (linearizability) and one kind of fault (&lt;em>network partitions&lt;/em>, or nodes that are alive but disconnected from each other). It doesn&amp;rsquo;t say anything about network delays, dead nodes, or other trade-offs. CAP has been historically influential, but nowadays has little practical value for designing systems.&lt;/p>
&lt;hr>
&lt;p>The main reason for dropping linearizability is &lt;em>performance&lt;/em>, not fault tolerance. Linearizabilit is slow and this is true all the time, not on only during a network fault.&lt;/p>
&lt;h3 id="ordering-guarantees">Ordering guarantees&lt;/h3>
&lt;p>Cause comes before the effect. Causal order in the system is what happened before what (&lt;em>causally consistent&lt;/em>).&lt;/p>
&lt;p>&lt;em>Total order&lt;/em> allows any two elements to be compared. Peg, natural numbers are totally ordered.&lt;/p>
&lt;p>Some cases one set is greater than another one.&lt;/p>
&lt;p>Different consistency models:&lt;/p>
&lt;ul>
&lt;li>Linearizablity. &lt;em>total order&lt;/em> of operations: if the system behaves as if there is only a single copy of the data.&lt;/li>
&lt;li>Causality. Two events are ordered if they are causally related. Causality defines &lt;em>a partial order&lt;/em>, not a total one (incomparable if they are concurrent).&lt;/li>
&lt;/ul>
&lt;p>Linearizability is not the only way of preserving causality. &lt;strong>Causal consistency is the strongest possible consistency model that does not slow down due to network delays, and remains available in the face of network failures.&lt;/strong>&lt;/p>
&lt;p>You need to know which operation &lt;em>happened before&lt;/em>.&lt;/p>
&lt;p>In order to determine the causal ordering, the database needs to know which version of the data was read by the application. &lt;strong>The version number from the prior operation is passed back to the database on a write.&lt;/strong>&lt;/p>
&lt;p>We can create sequence numbers in a total order that is &lt;em>consistent with causality&lt;/em>.&lt;/p>
&lt;p>With a single-leader replication, the leader can simply increment a counter for each operation, and thus assign a monotonically increasing sequence number to each operation in the replication log.&lt;/p>
&lt;p>If there is not a single leader (multi-leader or leaderless database):&lt;/p>
&lt;ul>
&lt;li>Each node can generate its own independent set of sequence numbers. One node can generate only odd numbers and the other only even numbers.&lt;/li>
&lt;li>Attach a timestamp from a time-of-day clock.&lt;/li>
&lt;li>Preallocate blocks of sequence numbers.&lt;/li>
&lt;/ul>
&lt;p>The only problem is that the sequence numbers they generate are &lt;em>not consistent with causality&lt;/em>. They do not correctly capture ordering of operations across different nodes.&lt;/p>
&lt;p>There is simple method for generating sequence numbers that &lt;em>is&lt;/em> consistent with causality: &lt;em>Lamport timestamps&lt;/em>.&lt;/p>
&lt;p>Each node has a unique identifier, and each node keeps a counter of the number of operations it has processed. The lamport timestamp is then simply a pair of (&lt;em>counter&lt;/em>, &lt;em>node ID&lt;/em>). It provides total order, as if you have two timestamps one with a greater counter value is the greater timestamp. If the counter values are the same, the one with greater node ID is the greater timestamp.&lt;/p>
&lt;p>Every node and every client keeps track of the &lt;em>maximum&lt;/em> counter value it has seen so far, and includes that maximum on every request. When a node receives a request of response with a maximum counter value greater than its own counter value, it inmediately increases its own counter to that maximum.&lt;/p>
&lt;p>As long as the maximum counter value is carried along with every operation, this scheme ensure that the ordering from the lamport timestamp is consistent with causality.&lt;/p>
&lt;p>Total order of oepration only emerges after you have collected all of the operations.&lt;/p>
&lt;p>Total order broadcast:&lt;/p>
&lt;ul>
&lt;li>Reliable delivery: If a message is delivered to one node, it is delivered to all nodes.&lt;/li>
&lt;li>Totally ordered delivery: Mesages are delivered to every node in the same order.&lt;/li>
&lt;/ul>
&lt;p>ZooKeeper and etcd implement total order broadcast.&lt;/p>
&lt;p>If every message represents a write to the database, and every replica processes the same writes in the same order, then the replcias will remain consistent with each other (&lt;em>state machine replication&lt;/em>).&lt;/p>
&lt;p>A node is not allowed to retroactgively insert a message into an earlier position in the order if subsequent messages have already been dlivered.&lt;/p>
&lt;p>Another way of looking at total order broadcast is that it is a way of creating a &lt;em>log&lt;/em>. Delivering a message is like appending to the log.&lt;/p>
&lt;p>If you have total order broadcast, you can build linearizable storage on top of it.&lt;/p>
&lt;p>Because log entries are delivered to all nodes in the same order, if therer are several concurrent writes, all nodes will agree on which one came first. Choosing the first of the conflicting writes as the winner and aborting later ones ensures that all nodes agree on whether a write was commited or aborted.&lt;/p>
&lt;p>This procedure ensures linearizable writes, it doesn&amp;rsquo;t guarantee linearizable reads.&lt;/p>
&lt;p>To make reads linearizable:&lt;/p>
&lt;ul>
&lt;li>You can sequence reads through the log by appending a message, reading the log, and performing the actual read when the message is delivered back to you (etcd works something like this).&lt;/li>
&lt;li>Fetch the position of the latest log message in a linearizable way, you can query that position to be delivered to you, and then perform the read (idea behind ZooKeeper&amp;rsquo;s &lt;code>sync()&lt;/code>).&lt;/li>
&lt;li>You can make your read from a replica that is synchronously updated on writes.&lt;/li>
&lt;/ul>
&lt;p>For every message you want to send through total order broadcast, you increment-and-get the linearizable integer and then attach the value you got from the register as a sequence number to the message. YOu can send the message to all nodes, and the recipients will deliver the message consecutively by sequence number.&lt;/p>
&lt;h3 id="distributed-transactions-and-consensus">Distributed transactions and consensus&lt;/h3>
&lt;p>Basically &lt;em>getting several nodes to agree on something&lt;/em>.&lt;/p>
&lt;p>There are situations in which it is important for nodes to agree:&lt;/p>
&lt;ul>
&lt;li>Leader election: All nodes need to agree on which node is the leader.&lt;/li>
&lt;li>Atomic commit: Get all nodes to agree on the outcome of the transacction, either they all abort or roll back.&lt;/li>
&lt;/ul>
&lt;h4 id="atomic-commit-and-two-phase-commit-2pc">Atomic commit and two-phase commit (2PC)&lt;/h4>
&lt;p>A transaction either succesfully &lt;em>commit&lt;/em>, or &lt;em>abort&lt;/em>. Atomicity prevents half-finished results.&lt;/p>
&lt;p>On a single node, transaction commitment depends on the &lt;em>order&lt;/em> in which data is writen to disk: first the data, then the commit record.&lt;/p>
&lt;p>2PC uses a coordinartor (&lt;em>transaction manager&lt;/em>). When the application is ready to commit, the coordinator begins phase 1: it sends a &lt;em>prepare&lt;/em> request to each of the nodes, asking them whether are able to commit.&lt;/p>
&lt;ul>
&lt;li>If all participants reply &amp;ldquo;yes&amp;rdquo;, the coordinator sends out a &lt;em>commit&lt;/em> request in phase 2, and the commit takes place.&lt;/li>
&lt;li>If any of the participants replies &amp;ldquo;no&amp;rdquo;, the coordinator sends an &lt;em>abort&lt;/em> request to all nodes in phase 2.&lt;/li>
&lt;/ul>
&lt;p>When a participant votes &amp;ldquo;yes&amp;rdquo;, it promises that it will definitely be able to commit later; and once the coordiantor decides, that decision is irrevocable. Those promises ensure the atomicity of 2PC.&lt;/p>
&lt;p>If one of the participants or the network fails during 2PC (prepare requests fail or time out), the coordinator aborts the transaction. If any of the commit or abort request fail, the coordinator retries them indefinitely.&lt;/p>
&lt;p>If the coordinator fails before sending the prepare requests, a participant can safely abort the transaction.&lt;/p>
&lt;p>The only way 2PC can complete is by waiting for the coordinator to revover in case of failure. This is why the coordinator must write its commit or abort decision to a transaction log on disk before sending commit or abort requests to participants.&lt;/p>
&lt;h4 id="three-phase-commit">Three-phase commit&lt;/h4>
&lt;p>2PC is also called a &lt;em>blocking&lt;/em> atomic commit protocol, as 2Pc can become stuck waiting for the coordinator to recover.&lt;/p>
&lt;p>There is an alternative called &lt;em>three-phase commit&lt;/em> (3PC) that requires a &lt;em>perfect failure detector&lt;/em>.&lt;/p>
&lt;hr>
&lt;p>Distributed transactions carry a heavy performance penalty due the disk forcing in 2PC required for crash recovery and additional network round-trips.&lt;/p>
&lt;p>XA (X/Open XA for eXtended Architecture) is a standard for implementing two-phase commit across heterogeneous technologies. Supported by many traditional relational databases (PostgreSQL, MySQL, DB2, SQL Server, and Oracle) and message brokers (ActiveMQ, HornetQ, MSQMQ, and IBM MQ).&lt;/p>
&lt;p>The problem with &lt;em>locking&lt;/em> is that database transactions usually take a row-level exclusive lock on any rows they modify, to prevent dirty writes.&lt;/p>
&lt;p>While those locks are held, no other transaction can modify those rows.&lt;/p>
&lt;p>When a coordinator fails, &lt;em>orphaned&lt;/em> in-doubt transactions do ocurr, and the only way out is for an administrator to manually decide whether to commit or roll back the transaction.&lt;/p>
&lt;h4 id="fault-tolerant-consensus">Fault-tolerant consensus&lt;/h4>
&lt;p>One or more nodes may &lt;em>propose&lt;/em> values, and the consensus algorithm &lt;em>decides&lt;/em> on those values.&lt;/p>
&lt;p>Consensus algorithm must satisfy the following properties:&lt;/p>
&lt;ul>
&lt;li>Uniform agreement: No two nodes decide differently.&lt;/li>
&lt;li>Integrity: No node decides twice.&lt;/li>
&lt;li>Validity: If a node decides the value &lt;em>v&lt;/em>, then &lt;em>v&lt;/em> was proposed by some node.&lt;/li>
&lt;li>Termination: Every node that does not crash eventually decides some value.&lt;/li>
&lt;/ul>
&lt;p>If you don&amp;rsquo;t care about fault tolerance, then satisfying the first three properties is easy: you can just hardcode one node to be the &amp;ldquo;dictator&amp;rdquo; and let that node make all of the decisions.&lt;/p>
&lt;p>The termination property formalises the idea of fault tolerance. Even if some nodes fail, the other nodes must still reach a decision. Termination is a liveness property, whereas the other three are safety properties.&lt;/p>
&lt;p>&lt;strong>The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR), Paxos, Raft and Zab.&lt;/strong>&lt;/p>
&lt;p>Total order broadcast requires messages to be delivered exactly once, in the same order, to all nodes.&lt;/p>
&lt;p>So total order broadcast is equivalent to repeated rounds of consensus:&lt;/p>
&lt;ul>
&lt;li>Due to agreement property, all nodes decide to deliver the same messages in the same order.&lt;/li>
&lt;li>Due to integrity, messages are not duplicated.&lt;/li>
&lt;li>Due to validity, messages are not corrupted.&lt;/li>
&lt;li>Due to termination, messages are not lost.&lt;/li>
&lt;/ul>
&lt;h5 id="single-leader-replication-and-consensus">Single-leader replication and consensus&lt;/h5>
&lt;p>All of the consensus protocols dicussed so far internally use a leader, but they don&amp;rsquo;t guarantee that the lader is unique. Protocols define an &lt;em>epoch number&lt;/em> (&lt;em>ballot number&lt;/em> in Paxos, &lt;em>view number&lt;/em> in Viewstamped Replication, and &lt;em>term number&lt;/em> in Raft). Within each epoch, the leader is unique.&lt;/p>
&lt;p>Every time the current leader is thought to be dead, a vote is started among the nodes to elect a new leader. This election is given an incremented epoch number, and thus epoch numbers are totallly ordered and monotonically increasing. If there is a conflic, the leader with the higher epoch number prevails.&lt;/p>
&lt;p>A node cannot trust its own judgement. It must collect votes from a &lt;em>quorum&lt;/em> of nodes. For every decision that a leader wants to make, it must send the proposed value to the other nodes and wait for a quorum of nodes to respond in favor of the proposal.&lt;/p>
&lt;p>There are two rounds of voting, once to choose a leader, and second time to vote on a leader&amp;rsquo;s proposal. The quorums for those two votes must overlap.&lt;/p>
&lt;p>The biggest difference with 2PC, is that 2PC requires a &amp;ldquo;yes&amp;rdquo; vote for &lt;em>every&lt;/em> participant.&lt;/p>
&lt;p>The benefits of consensus come at a cost. The process by which nodes vote on proposals before they are decided is kind of synchronous replication.&lt;/p>
&lt;p>Consensus always require a strict majority to operate.&lt;/p>
&lt;p>Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that you can&amp;rsquo;t just add or remove nodes in the cluster. &lt;em>Dynamic membership&lt;/em> extensions are much less well understood than static membership algorithms.&lt;/p>
&lt;p>Consensus systems rely on timeouts to detect failed nodes. In geographically distributed systems, it often happens that a node falsely believes the leader to have failed due to a network issue. This implies frequest leader elecctions resulting in terrible performance, spending more time choosing a leader than doing any useful work.&lt;/p>
&lt;h4 id="membership-and-coordination-services">Membership and coordination services&lt;/h4>
&lt;p>ZooKeeper or etcd are often described as &amp;ldquo;distributed key-value stores&amp;rdquo; or &amp;ldquo;coordination and configuration services&amp;rdquo;.&lt;/p>
&lt;p>They are designed to hold small amounts of data that can fit entirely in memory, you wouldn&amp;rsquo;t want to store all of your application&amp;rsquo;s data here. Data is replicated across all the nodes using a fault-tolerant total order broadcast algorithm.&lt;/p>
&lt;p>ZooKeeper is modeled after Google&amp;rsquo;s Chubby lock service and it provides some useful features:&lt;/p>
&lt;ul>
&lt;li>Linearizable atomic operations: Usuing an atomic compare-and-set operation, you can implement a lock.&lt;/li>
&lt;li>Total ordering of operations: When some resource is protected by a lock or lease, you need a &lt;em>fencing token&lt;/em> to prevent clients from conflicting with each other in the case of a process pause. The fencing token is some number that monotonically increases every time the lock is acquired.&lt;/li>
&lt;li>Failure detection: Clients maintain a long-lived session on ZooKeeper servers. When a ZooKeeper node fails, the session remains active. When ZooKeeper declares the session to be dead all locks held are automatically released.&lt;/li>
&lt;li>Change notifications: Not only can one client read locks and values, it can also watch them for changes.&lt;/li>
&lt;/ul>
&lt;p>ZooKeeper is super useful for distributed coordination.&lt;/p>
&lt;p>ZooKeeper/Chubby model works well when you have several instances of a process or service, and one of them needs to be chosen as a leader or primary. If the leader fails, one of the other nodes should take over. This is useful for single-leader databases and for job schedulers and similar stateful systems.&lt;/p>
&lt;p>ZooKeeper runs on a fixed number of nodes, and performs its majority votes among those nodes while supporting a potentially large number of clients.&lt;/p>
&lt;p>The kind of data managed by ZooKeeper is quite slow-changing like &amp;ldquo;the node running on 10.1.1.23 is the leader for partition 7&amp;rdquo;. It is not intended for storing the runtime state of the application. If application state needs to be replicated there are other tools (like Apache BookKeeper).&lt;/p>
&lt;p>ZooKeeper, etcd, and Consul are also often used for &lt;em>service discovery&lt;/em>, find out which IP address you need to connect to in order to reach a particular service. In cloud environments, it is common for virtual machines to continually come an go, you often don&amp;rsquo;t know the IP addresses of your services ahead of time. Your services when they start up they register their network endpoints ina service registry, where they can then be found by other services.&lt;/p>
&lt;p>ZooKeeper and friends can be seen as part of a long history of research into &lt;em>membership services&lt;/em>, determining which nodes are currently active and live members of a cluster.&lt;/p>
&lt;h2 id="batch-processing">Batch processing&lt;/h2>
&lt;ul>
&lt;li>Service (online): waits for a request, sends a response back&lt;/li>
&lt;li>Batch processing system (offline): takes a large amount of input data, runs a &lt;em>job&lt;/em> to process it, and produces some output.&lt;/li>
&lt;li>Stream processing systems (near-real-time): a stream processor consumes input and produces outputs. A stream job operates on events shortly after they happen.&lt;/li>
&lt;/ul>
&lt;h3 id="batch-processing-with-unix-tools">Batch processing with Unix tools&lt;/h3>
&lt;p>We can build a simple log analysis job to get the five most popular pages on your site&lt;/p>
&lt;pre tabindex="0">&lt;code>cat /var/log/nginx/access.log |
awk &amp;#39;{print $7}&amp;#39; |
sort |
uniq -c |
sort -r -n |
head -n 5 |
&lt;/code>&lt;/pre>&lt;p>You could write the same thing with a simpel program.&lt;/p>
&lt;p>The difference is that with Unix commands automatically handle larger-than-memory datasets and automatically paralelizes sorting across multiple CPU cores.&lt;/p>
&lt;p>Programs must have the same data format to pass information to one another. In Unix, that interface is a file (file descriptor), an ordered sequence of bytes.&lt;/p>
&lt;p>By convention Unix programs treat this sequence of bytes as ASCII text.&lt;/p>
&lt;p>The unix approach works best if a program simply uses &lt;code>stdin&lt;/code> and &lt;code>stdout&lt;/code>. This allows a shell user to wire up the input and output in whatever way they want; the program doesn&amp;rsquo;t know or care where the input is coming from and where the output is going to.&lt;/p>
&lt;p>Part of what makes Unix tools so successful is that they make it quite easy to see what is going on.&lt;/p>
&lt;h3 id="map-reduce-and-distributed-filesystems">Map reduce and distributed filesystems&lt;/h3>
&lt;p>A single MapReduce job is comparable to a single Unix process.&lt;/p>
&lt;p>Running a MapReduce job normally does not modify the input and does not have any side effects other than producing the output.&lt;/p>
&lt;p>While Unix tools use &lt;code>stdin&lt;/code> and &lt;code>stdout&lt;/code> as input and output, MapReduce jobs read and write files on a distributed filesystem. In Hadoop, that filesystem is called HDFS (Haddoop Distributed File System).&lt;/p>
&lt;p>HDFS is based on the &lt;em>shared-nothing&lt;/em> principe. Implemented by centralised storage appliance, often using custom hardware and special network infrastructure.&lt;/p>
&lt;p>HDFS consists of a daemon process running on each machine, exposing a network service that allows other nodes to access files stored on that machine. A central server called the &lt;em>NameNode&lt;/em> keeps track of which file blocks are stored on which machine.&lt;/p>
&lt;p>File blocks are replciated on multiple machines. Reaplication may mean simply several copies of the same data on multiple machines, or an &lt;em>erasure coding&lt;/em> scheme such as Reed-Solomon codes, which allow lost data to be recovered.&lt;/p>
&lt;p>MapReduce is a programming framework with which you can write code to process large datasets in a distributed filesystem like HDFS.&lt;/p>
&lt;ol>
&lt;li>Read a set of input files, and break it up into &lt;em>records&lt;/em>.&lt;/li>
&lt;li>Call the mapper function to extract a key and value from each input record.&lt;/li>
&lt;li>Sort all of the key-value pairs by key.&lt;/li>
&lt;li>Call the reducer function to iterate over the sorted key-value pairs.&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Mapper: Called once for every input record, and its job is to extract the key and value from the input record.&lt;/li>
&lt;li>Reducer: Takes the key-value pairs produced by the mappers, collects all the values belonging to the same key, and calls the reducer with an interator over that collection of vaues.&lt;/li>
&lt;/ul>
&lt;p>MapReduce can parallelise a computation across many machines, without you having ot write code to explicitly handle the parallelism. THe mapper and reducer only operate on one record at a time; they don&amp;rsquo;t need to know where their input is coming from or their output is going to.&lt;/p>
&lt;p>In Hadoop MapReduce, the mapper and reducer are each a Java class that implements a particular interface.&lt;/p>
&lt;p>The MapReduce scheduler tries to run each mapper on one of the machines that stores a replica of the input file, &lt;em>putting the computation near the data&lt;/em>.&lt;/p>
&lt;p>The reduce side of the computation is also partitioned. While the number of map tasks is determined by the number of input file blocks, the number of reduce tasks is configured by the job author. To ensure that all key-value pairs with the same key end up in the same reducer, the framework uses a hash of the key.&lt;/p>
&lt;p>The dataset is likely too large to be sorted with a conventional sorting algorithm on a single machine. Sorting is performed in stages.&lt;/p>
&lt;p>Whenever a mapper finishes reading its input file and writing its sorted output files, the MapReduce scheduler notifies the reducers that they can start fetching the output files from that mapper. The reducers connect to each of the mappers and download the files of sorted key-value pairs for their partition. Partitioning by reducer, sorting and copying data partitions from mappers to reducers is called &lt;em>shuffle&lt;/em>.&lt;/p>
&lt;p>The reduce task takes the files from the mappers and merges them together, preserving the sort order.&lt;/p>
&lt;p>MapReduce jobs can be chained together into &lt;em>workflows&lt;/em>, the output of one job becomes the input to the next job. In Hadoop this chaining is done implicitly by directory name: the first job writes its output to a designated directory in HDFS, the second job reads that same directory name as its input.&lt;/p>
&lt;p>Compared with the Unix example, it could be seen as in each sequence of commands each command output is written to a temporary file, and the next command reads from the temporary file.&lt;/p>
&lt;p>It is common in datasets for one record to have an association with another record: a &lt;em>foreign key&lt;/em> in a relational model, a &lt;em>document reference&lt;/em> in a document model, or an &lt;em>edge&lt;/em> in graph model.&lt;/p>
&lt;p>If the query involves joins, it may require multiple index lookpus. MapReduce has no concept of indexes.&lt;/p>
&lt;p>When a MapReduce job is given a set of files as input, it reads the entire content of all of those files, like a &lt;em>full table scan&lt;/em>.&lt;/p>
&lt;p>In analytics it is common to want to calculate aggregates over a large number of records. Scanning the entire input might be quite reasonable.&lt;/p>
&lt;p>In order to achieve good throughput in a batch process, the computation must be local to one machine. Requests over the network are too slow and nondeterministic. Queries to other database for example would be prohibitive.&lt;/p>
&lt;p>A better approach is to take a copy of the data (peg: the database) and put it in the same distributed filesystem.&lt;/p>
&lt;p>MapReduce programming model has separated the physical network communication aspects of the computation (getting the data to the right machine) from the application logic (processing the data once you have it).&lt;/p>
&lt;p>In an example of a social network, small number of celebrities may have many millions of followers. Such disproportionately active database records are known as &lt;em>linchpin objects&lt;/em> or &lt;em>hot keys&lt;/em>.&lt;/p>
&lt;p>A single reducer can lead to significant &lt;em>skew&lt;/em> that is, one reducer that must process significantly more records than the others.&lt;/p>
&lt;p>The &lt;em>skewed join&lt;/em> method in Pig first runs a sampling job to determine which keys are hot and then records related to the hot key need to be replicated to &lt;em>all&lt;/em> reducers handling that key.&lt;/p>
&lt;p>Handling the hot key over several reducers is called &lt;em>shared join&lt;/em> method. In Crunch is similar but requires the hot keys to be specified explicitly.&lt;/p>
&lt;p>Hive&amp;rsquo;s skewed join optimisation requries hot keys to be specified explicitly and it uses map-side join. If you &lt;em>can&lt;/em> make certain assumptions about your input data, it is possible to make joins faster. A MapReducer job with no reducers and no sorting, each mapper simply reads one input file and writes one output file.&lt;/p>
&lt;p>The output of a batch process is often not a report, but some other kind of structure.&lt;/p>
&lt;p>Google&amp;rsquo;s original use of MapReduce was to build indexes for its search engine. Hadoop MapReduce remains a good way of building indexes for Lucene/Solr.&lt;/p>
&lt;p>If you need to perform a full-text search, a batch process is very effective way of building indexes: the mappers partition the set of documents as needed, each reducer builds the index for its partition, and the index files are written to the distributed filesystem. It pararellises very well.&lt;/p>
&lt;p>Machine learning systems such as clasifiers and recommendation systems are a common use for batch processing.&lt;/p>
&lt;h4 id="key-value-stores-as-batch-process-output">Key-value stores as batch process output&lt;/h4>
&lt;p>The output of those batch jobs is often some kind of database.&lt;/p>
&lt;p>So, how does the output from the batch process get back into a database?&lt;/p>
&lt;p>Writing from the batch job directly to the database server is a bad idea:&lt;/p>
&lt;ul>
&lt;li>Making a network request for every single record is magnitude slower than the normal throughput of a batch task.&lt;/li>
&lt;li>Mappers or reducers concurrently write to the same output database an it can be easily overwhelmed.&lt;/li>
&lt;li>You have to worry about the results from partially completed jobs being visible to other systems.&lt;/li>
&lt;/ul>
&lt;p>A much better solution is to build a brand-new database &lt;em>inside&lt;/em> the batch job an write it as files to the job&amp;rsquo;s output directory, so it can be loaded in bulk into servers that handle read-only queries. Various key-value stores support building database files in MapReduce including Voldemort, Terrapin, ElephanDB and HBase bulk loading.&lt;/p>
&lt;hr>
&lt;p>By treating inputs as immutable and avoiding side effects (such as writing to external databases), batch jobs not only achieve good performance but also become much easier to maintain.&lt;/p>
&lt;p>Design principles that worked well for Unix also seem to be working well for Hadoop.&lt;/p>
&lt;p>The MapReduce paper was not at all new. The sections we&amp;rsquo;ve seen had been already implemented in so-called &lt;em>massively parallel processing&lt;/em> (MPP) databases.&lt;/p>
&lt;p>The biggest difference is that MPP databases focus on parallel execution of analytic SQL queries on a cluster of machines, while the combination of MapReduce and a distributed filesystem provides something much more like a general-purpose operating system that can run arbitraty programs.&lt;/p>
&lt;p>Hadoop opened up the possibility of indiscriminately dumpint data into HDFS. MPP databases typically require careful upfront modeling of the data and query patterns before importing data into the database&amp;rsquo;s proprietary storage format.&lt;/p>
&lt;p>In MapReduce instead of forcing the producer of a dataset to bring it into a standarised format, the interpretation of the data becomes the consumer&amp;rsquo;s problem.&lt;/p>
&lt;p>If you have HDFS and MapReduce, you &lt;em>can&lt;/em> build a SQL query execution engine on top of it, and indeed this is what the Hive project did.&lt;/p>
&lt;p>If a node crashes while a query is executing, most MPP databases abort the entire query. MPP databases also prefer to keep as much data as possible in memory.&lt;/p>
&lt;p>MapReduce can tolerate the failure of a map or reduce task without it affecting the job. It is also very eager to write data to disk, partly for fault tolerance, and partly because the dataset might not fit in memory anyway.&lt;/p>
&lt;p>MapReduce is more appropriate for larger jobs.&lt;/p>
&lt;p>At Google, a MapReduce task that runs for an hour has an approximately 5% risk of being terminated to make space for higher-priority process.&lt;/p>
&lt;p>Ths is why MapReduce is designed to tolerate frequent unexpected task termination.&lt;/p>
&lt;h3 id="beyond-mapreduce">Beyond MapReduce&lt;/h3>
&lt;p>In response to the difficulty of using MapReduce directly, various higher-level programming models emerged on top of it: Pig, Hive, Cascading, Crunch.&lt;/p>
&lt;p>MapReduce has poor performance for some kinds of processing. It&amp;rsquo;s very robust, you can use it to process almost arbitrarily large quantities of data on an unreliable multi-tenant system with frequent task terminations, and it will still get the job done.&lt;/p>
&lt;p>The files on the distributed filesystem are simply &lt;em>intermediate state&lt;/em>: a means of passing data from one job to the next.&lt;/p>
&lt;p>The process of writing out the intermediate state to files is called &lt;em>materialisation&lt;/em>.&lt;/p>
&lt;p>MapReduce&amp;rsquo;s approach of fully materialising state has some downsides compared to Unix pipes:&lt;/p>
&lt;ul>
&lt;li>A MapReduce job can only start when all tasks in the preceding jobs have completed, whereas rocesses connected by a Unix pipe are started at the same time.&lt;/li>
&lt;li>Mappers are often redundant: they just read back the same file that was just written by a reducer.&lt;/li>
&lt;li>Files are replicated across several nodes, which is often overkill for such temporary data.&lt;/li>
&lt;/ul>
&lt;p>To fix these problems with MapReduce, new execution engines for distributed batch computations were developed, Spark, Tez and Flink. These new ones can handle an entire workflow as one job, rather than breaking it up into independent subjobs (&lt;em>dataflow engines&lt;/em>).&lt;/p>
&lt;p>These functions need not to take the strict roles of alternating map and reduce, they are assembled in flexible ways, in functions called &lt;em>operators&lt;/em>.&lt;/p>
&lt;p>Spark, Flink, and Tex avoid writing intermediate state to HDFS, so they take a different approach to tolerating faults: if a machine fails and the intermediate state on that machine is lost, it is recomputed from other data that is still available.&lt;/p>
&lt;p>The framework must keep track of how a given piece of data was computed. Spark uses the resilient distributed dataset (RDD) to track ancestry data, while Flink checkpoints operator state, allowing it to resume running an operator that ran into a fault during its execution.&lt;/p>
&lt;h4 id="graphs-and-iterative-processing">Graphs and iterative processing&lt;/h4>
&lt;p>It&amp;rsquo;s interesting to look at graphs in batch processing context, where the goal is to perform some kind of offline processing or analysis on an entire graph. This need often arises in machine learning applications such as recommednation engines, or in ranking systems.&lt;/p>
&lt;p>&amp;ldquo;repeating until done&amp;rdquo; cannot be expressed in plain MapReduce as it runs in a single pass over the data and some extra trickery is necessary.&lt;/p>
&lt;p>An optimisation for batch processing graphs, the &lt;em>bulk synchronous parallel&lt;/em> (BSP) has become popular. It is implemented by Apache Giraph, Spark&amp;rsquo;s GraphX API, and Flink&amp;rsquo;s Gelly API (_Pregel model, as Google Pregel paper popularised it).&lt;/p>
&lt;p>One vertex can &amp;ldquo;send a message&amp;rdquo; to another vertex, and typically those messages are sent along the edges in a graph.&lt;/p>
&lt;p>The difference from MapReduce is that a vertex remembers its state in memory from one iteration to the next.&lt;/p>
&lt;p>The fact that vertices can only communicate by message passing helps improve the performance of Pregel jobs, since messages can be batched.&lt;/p>
&lt;p>Fault tolerance is achieved by periodically checkpointing the state of all vertices at the end of an interation.&lt;/p>
&lt;p>The framework may partition the graph in arbitrary ways.&lt;/p>
&lt;p>Graph algorithms often have a lot of cross-machine communication overhead, and the intermediate state is often bigger than the original graph.&lt;/p>
&lt;p>If your graph can fit into memory on a single computer, it&amp;rsquo;s quite likely that a single-machine algorithm will outperform a distributed batch process. If the graph is too big to fit on a single machine, a distributed approach such as Pregel is unavoidable.&lt;/p>
&lt;h2 id="stream-processing">Stream processing&lt;/h2>
&lt;p>We can run the processing continuously, abandoning the fixed time slices entirely and simply processing every event as it happens, that&amp;rsquo;s the idea behind &lt;em>stream processing&lt;/em>. Data that is incrementally made available over time.&lt;/p>
&lt;h3 id="transmitting-event-streams">Transmitting event streams&lt;/h3>
&lt;p>A record is more commonly known as an &lt;em>event&lt;/em>. Something that happened at some point in time, it usually contains a timestamp indicating when it happened acording to a time-of-day clock.&lt;/p>
&lt;p>An event is generated once by a &lt;em>producer&lt;/em> (&lt;em>publisher&lt;/em> or &lt;em>sender&lt;/em>), and then potentially processed by multiple &lt;em>consumers&lt;/em> (&lt;em>subcribers&lt;/em> or &lt;em>recipients&lt;/em>). Related events are usually grouped together into a &lt;em>topic&lt;/em> or a &lt;em>stream&lt;/em>.&lt;/p>
&lt;p>A file or a database is sufficient to connect producers and consumers: a producer writes every event that it generates to the datastore, and each consumer periodically polls the datastore to check for events that have appeared since it last ran.&lt;/p>
&lt;p>However, when moving toward continual processing, polling becomes expensive. It is better for consumers to be notified when new events appear.&lt;/p>
&lt;p>Databases offer &lt;em>triggers&lt;/em> but they are limited, so specialised tools have been developed for the purpose of delivering event notifications.&lt;/p>
&lt;h4 id="messaging-systems">Messaging systems&lt;/h4>
&lt;h5 id="direct-messaging-from-producers-to-consumers">Direct messaging from producers to consumers&lt;/h5>
&lt;p>Within the &lt;em>publish&lt;/em>/&lt;em>subscribe&lt;/em> model, we can differentiate the systems by asking two questions:&lt;/p>
&lt;ol>
&lt;li>&lt;em>What happens if the producers send messages faster than the consumers can process them?&lt;/em> The system can drop messages, buffer the messages in a queue, or apply &lt;em>backpressure&lt;/em> (&lt;em>flow control&lt;/em>, blocking the producer from sending more messages).&lt;/li>
&lt;li>&lt;em>What happens if nodes crash or temporarily go offline, are any messages lost?&lt;/em> Durability may require some combination of writing to disk and/or replication.&lt;/li>
&lt;/ol>
&lt;p>A number of messaging systems use direct communication between producers and consumers without intermediary nodes:&lt;/p>
&lt;ul>
&lt;li>UDP multicast, where low latency is important, application-level protocols can recover lost packets.&lt;/li>
&lt;li>Brokerless messaging libraries such as ZeroMQ&lt;/li>
&lt;li>StatsD and Brubeck use unreliable UDP messaging for collecting metrics&lt;/li>
&lt;li>If the consumer expose a service on the network, producers can make a direct HTTP or RPC request to push messages to the consumer. This is the idea behind webhooks, a callback URL of one service is registered with another service, and makes a request to that URL whenever an event occurs&lt;/li>
&lt;/ul>
&lt;p>These direct messaging systems require the application code to be aware of the possibility of message loss. The faults they can tolerate are quite limited as they assume that producers and consumers are constantly online.&lt;/p>
&lt;p>If a consumer if offline, it may miss messages. Some protocols allow the producer to retry failed message deliveries, but it may break down if the producer crashes losing the buffer or messages.&lt;/p>
&lt;h5 id="message-brokers">Message brokers&lt;/h5>
&lt;p>An alternative is to send messages via a &lt;em>message broker&lt;/em> (or &lt;em>message queue&lt;/em>), which is a kind of database that is optimised for handling message streams. It runs as a server, with producers and consumers connecting to it as clients. Producers write messages to the broker, and consumers receive them by reading them from the broker.&lt;/p>
&lt;p>By centralising the data, these systems can easily tolerate clients that come and go, and the question of durability is moved to the broker instead. Some brokers only keep messages in memory, while others write them down to disk so that they are not lost inc ase of a broker crash.&lt;/p>
&lt;p>A consequence of queueing is that consuemrs are generally &lt;em>asynchronous&lt;/em>: the producer only waits for the broker to confirm that it has buffered the message and does not wait for the message to be processed by consumers.&lt;/p>
&lt;p>Some brokers can even participate in two-phase commit protocols using XA and JTA. This makes them similar to databases, aside some practical differences:&lt;/p>
&lt;ul>
&lt;li>Most message brokers automatically delete a message when it has been successfully delivered to its consumers. This makes them not suitable for long-term storage.&lt;/li>
&lt;li>Most message brokers assume that their working set is fairly small. If the broker needs to buffer a lot of messages, each individual message takes longer to process, and the overall throughput may degrade.&lt;/li>
&lt;li>Message brokers often support some way of subscribing to a subset of topics matching some pattern.&lt;/li>
&lt;li>Message brokers do not support arbitrary queries, but they do notify clients when data changes.&lt;/li>
&lt;/ul>
&lt;p>This is the traditional view of message brokers, encapsulated in standards like JMS and AMQP, and implemented in RabbitMQ, ActiveMQ, HornetQ, Qpid, TIBCO Enterprise Message Service, IBM MQ, Azure Service Bus, and Google Cloud Pub/Sub.&lt;/p>
&lt;p>When multiple consumers read messages in the same topic, to main patterns are used:&lt;/p>
&lt;ul>
&lt;li>Load balancing: Each message is delivered to &lt;em>one&lt;/em> of the consumers. The broker may assign messages to consumers arbitrarily.&lt;/li>
&lt;li>Fan-out: Each message is delivered to &lt;em>all&lt;/em> of the consumers.&lt;/li>
&lt;/ul>
&lt;p>In order to ensure that the message is not lost, message brokers use &lt;em>acknowledgements&lt;/em>: a client must explicitly tell the broker when it has finished processing a message so that the broker can remove it from the queue.&lt;/p>
&lt;p>The combination of laod balancing with redelivery inevitably leads to messages being reordered. To avoid this issue, youc an use a separate queue per consumer (not use the load balancing feature).&lt;/p>
&lt;h5 id="partitioned-logs">Partitioned logs&lt;/h5>
&lt;p>A key feature of barch process is that you can run them repeatedly without the risk of damaging the input. This is not the case with AMQP/JMS-style messaging: receiving a message is destructive if the acknowledgement causes it to be deleted from the broker.&lt;/p>
&lt;p>If you add a new consumer to a messaging system, any prior messages are already gone and cannot be recovered.&lt;/p>
&lt;p>We can have a hybrid, combining the durable storage approach of databases with the low-latency notifications facilities of messaging, this is the idea behind &lt;em>log-based message brokers&lt;/em>.&lt;/p>
&lt;p>A log is simply an append-only sequence of records on disk. The same structure can be used to implement a message broker: a producer sends a message by appending it to the end of the log, and consumer receives messages by reading the log sequentially. If a consumer reaches the end of the log, it waits for a notification that a new message has been appended.&lt;/p>
&lt;p>To scale to higher throughput than a single disk can offer, the log can be &lt;em>partitioned&lt;/em>. Different partitions can then be hosted on different machines. A topic can then be defined as a group of partitions that all carry messages of the same type.&lt;/p>
&lt;p>Within each partition, the broker assigns monotonically increasing sequence number, or &lt;em>offset&lt;/em>, to every message.&lt;/p>
&lt;p>Apache Kafka, Amazon Kinesis Streams, and Twitter&amp;rsquo;s DistributedLog, are log-based message brokers that work like this.&lt;/p>
&lt;p>The log-based approach trivially supports fan-out messaging, as several consumers can independently read the log reading without affecint each other. Reading a message does not delete it from the log. To eachieve load balancing the broker can assign entire partitions to nodes in the consumer group. Each client then consumes &lt;em>all&lt;/em> the messages in the partition it has been assigned. This approach has some downsides.&lt;/p>
&lt;ul>
&lt;li>The number of nodes sharing the work of consuming a topic can be at most the number of log partitions in that topic.&lt;/li>
&lt;li>If a single message is slow to process, it holds up the processing of subsequent messages in that partition.&lt;/li>
&lt;/ul>
&lt;p>In situations where messages may be expensive to process and you want to pararellise processing on a message-by-message basis, and where message ordering is not so important, the JMS/AMQP style of message broker is preferable. In situations with high message throughput, where each message is fast to process and where message ordering is important, the log-based approach works very well.&lt;/p>
&lt;p>It is easy to tell which messages have been processed: al messages with an offset less than a consumer current offset have already been processed, and all messages with a greater offset have not yet been seen.&lt;/p>
&lt;p>The offset is very similar to the &lt;em>log sequence number&lt;/em> that is commonly found in single-leader database replication. The message broker behaves like a leader database, and the consumer like a follower.&lt;/p>
&lt;p>If a consumer node fails, another node in the consumer group starts consuming messages at the last recorded offset. If the consumer had processed subsequent messages but not yet recorded their offset, those messages will be processed a second time upon restart.&lt;/p>
&lt;p>If you only ever append the log, you will eventually run out of disk space. From time to time old segments are deleted or moved to archive.&lt;/p>
&lt;p>If a slow consumer cannot keep with the rate of messages, and it falls so far behind that its consumer offset poitns to a deleted segment, it will miss some of the messages.&lt;/p>
&lt;p>The throughput of a log remains more or less constant, since every message is written to disk anyway. This is in contrast to messaging systems that keep messages in memory by default and only write them to disk if the queue grows too large: systems are fast when queues are short and become much slower when they start writing to disk, throughput depends on the amount of history retained.&lt;/p>
&lt;p>If a consumer cannot keep up with producers, the consumer can drop messages, buffer them or applying backpressure.&lt;/p>
&lt;p>You can monitor how far a consumer is behind the head of the log, and raise an alert if it falls behind significantly.&lt;/p>
&lt;p>If a consumer does fall too far behind and start missing messages, only that consumer is affected.&lt;/p>
&lt;p>With AMQP and JMS-style message brokers, processing and acknowledging messages is a destructive operation, since it causes the messages to be deleted on the broker. In a log-based message broker, consuming messages is more like reading from a file.&lt;/p>
&lt;p>The offset is under the consumer&amp;rsquo;s control, so you can easily be manipulated if necessary, like for replaying old messages.&lt;/p>
&lt;h3 id="databases-and-streams">Databases and streams&lt;/h3>
&lt;p>A replciation log is a stream of a database write events, produced by the leader as it processes transactions. Followers apply that stream of writes to their own copy of the database and thus end up with an accurate copy of the same data.&lt;/p>
&lt;p>If periodic full database dumps are too slow, an alternative that is sometimes used is &lt;em>dual writes&lt;/em>. For example, writing to the database, then updating the search index, then invalidating the cache.&lt;/p>
&lt;p>Dual writes have some serious problems, one of which is race conditions. If you have concurrent writes, one value will simply silently overwrite another value.&lt;/p>
&lt;p>One of the writes may fail while the other succeeds and two systems will become inconsistent.&lt;/p>
&lt;p>The problem with most databases replication logs is that they are considered an internal implementation detail, not a public API.&lt;/p>
&lt;p>Recently there has been a growing interest in &lt;em>change data capture&lt;/em> (CDC), which is the process of observing all data changes written to a database and extracting them in a form in which they can be replicated to other systems.&lt;/p>
&lt;p>For example, you can capture the changes in a database and continually apply the same changes to a search index.&lt;/p>
&lt;p>We can call log consumers &lt;em>derived data systems&lt;/em>: the data stored in the search index and the data warehouse is just another view. Change data capture is a mechanism for ensuring that all changes made to the system of record are also reflected in the derived data systems.&lt;/p>
&lt;p>Change data capture makes one database the leader, and turns the others into followers.&lt;/p>
&lt;p>Database triggers can be used to implement change data capture, but they tend to be fragile and have significant performance overheads. Parsing the replication log can be a more robust approach.&lt;/p>
&lt;p>LinkedIn&amp;rsquo;s Databus, Facebook&amp;rsquo;s Wormhole, and Yahoo!&amp;rsquo;s Sherpa use this idea at large scale. Bottled Watter implements CDC for PostgreSQL decoding the write-ahead log, Maxwell and Debezium for something similar for MySQL by parsing the binlog, Mongoriver reads the MongoDB oplog, and GoldenGate provide similar facilities for Oracle.&lt;/p>
&lt;p>Keeping all changes forever would require too much disk space, and replaying it would take too long, so the log needs to be truncated.&lt;/p>
&lt;p>You can start with a consistent snapshot of the database, and it must correspond to a known position or offset in the change log.&lt;/p>
&lt;p>The storage engine periodically looks for log records with the same key, throws away any duplicates, and keeps only the most recent update for each key.&lt;/p>
&lt;p>An update with a special null value (a &lt;em>tombstone&lt;/em>) indicates that a key was deleted.&lt;/p>
&lt;p>The same idea works in the context of log-based mesage brokers and change data capture.&lt;/p>
&lt;p>RethinkDB allows queries to subscribe to notifications, Firebase and CouchDB provide data synchronisation based on change feed.&lt;/p>
&lt;p>Kafka Connect integrates change data capture tools for a wide range of database systems with Kafka.&lt;/p>
&lt;h4 id="event-sourcing">Event sourcing&lt;/h4>
&lt;p>There are some parallels between the ideas we&amp;rsquo;ve discussed here and &lt;em>event sourcing&lt;/em>.&lt;/p>
&lt;p>Similarly to change data capture, event sourcing involves storing all changes to the application state as a log of change events. Event sourcing applyies the idea at a different level of abstraction.&lt;/p>
&lt;p>Event sourcing makes it easier to evolve applications over time, helps with debugging by making it easier to understand after the fact why something happened, and guards against application bugs.&lt;/p>
&lt;p>Specialised databases such as Event Store have been developed to support applications using event sourcing.&lt;/p>
&lt;p>Applications that use event sourcing need to take the log of evetns and transform it into application state that is suitable for showing to a user.&lt;/p>
&lt;p>Replying the event log allows you to reconstruct the current state of the system.&lt;/p>
&lt;p>Applications that use event sourcing typically have some mechanism for storing snapshots.&lt;/p>
&lt;p>Event sourcing philosophy is careful to distinguis between &lt;em>events&lt;/em> and &lt;em>commands&lt;/em>. When a request from a user first arrives, it is initially a command: it may still fail (like some integrity condition is violated). If the validation is successful, it becomes an event, which is durable and immutable.&lt;/p>
&lt;p>A consumer of the event stream is not allowed to reject an event: Any validation of a command needs to happen synchronously, before it becomes an event. For example, by using a serializable transaction that atomically validates the command and publishes the event.&lt;/p>
&lt;p>Alternatively, the user request to serve a seat could be split into two events: first a tentative reservation, and then a separate confirmation event once the reservation has been validated. This split allows the validation to take place in an asynchronous process.&lt;/p>
&lt;p>Whenever you have state changes, that state is the result of the events that mutated it over time.&lt;/p>
&lt;p>Mutable state and an append-only log of immutable events do not contradict each other.&lt;/p>
&lt;p>As an example, financial bookkeeping is recorded as an append-only &lt;em>ledger&lt;/em>. It is a log of events describing money, good, or services that have changed hands. Profit and loss or the balance sheet are derived from the ledger by adding them up.&lt;/p>
&lt;p>If a mistake is made, accountants don&amp;rsquo;t erase or change the incorrect transaction, instead, they add another transaction that compensates for the mistake.&lt;/p>
&lt;p>If buggy code writes bad data to a database, recovery is much harder if the code is able to destructively overwrite data.&lt;/p>
&lt;p>Immutable events also capture more information than just the current state. If you persisted a cart into a regular database, deleting an item would effectively loose that event.&lt;/p>
&lt;p>You can derive views from the same event log, Druid ingests directly from Kafka, Pistachio is a distributed key-value sotre that uses Kafka as a commit log, Kafka Connect sinks can export data from Kafka to various different databases and indexes.&lt;/p>
&lt;p>Storing data is normally quite straightforward if you don&amp;rsquo;t have to worry about how it is going to be queried and accessed. You gain a lot of flexibility by separating the form in which data is written from the form it is read, this idea is known as &lt;em>command query responsibility segregation&lt;/em> (CQRS).&lt;/p>
&lt;p>There is this fallacy that data must be written in the same form as it will be queried.&lt;/p>
&lt;p>The biggest downside of event sourcing and change data capture is that consumers of the event log are usually asynchronous, a user may make a write to the log, then read from a log derived view and find that their write has not yet been reflected.&lt;/p>
&lt;p>The limitations on immutable event history depends on the amount of churn in the dataset. Some workloads mostly add data and rarely update or delete; they are wasy to make immutable. Other workloads have a high rate of updates and deletes on a comparaively small dataset; in these cases immutable history becomes an issue because of fragmentation, performance compaction and garbage collection.&lt;/p>
&lt;p>There may also be circumstances in which you need data to be deleted for administrative reasons.&lt;/p>
&lt;p>Sometimes you may want to rewrite history, Datomic calls this feature &lt;em>excision&lt;/em>.&lt;/p>
&lt;h3 id="processing-streams">Processing Streams&lt;/h3>
&lt;p>What you can do with the stream once you have it:&lt;/p>
&lt;ol>
&lt;li>You can take the data in the events and write it to the database, cache, search index, or similar storage system, from where it can thenbe queried by other clients.&lt;/li>
&lt;li>You can push the events to users in some way, for example by sending email alerts or push notifications, or to a real-time dashboard.&lt;/li>
&lt;li>You can process one or more input streams to produce one or more output streams.&lt;/li>
&lt;/ol>
&lt;p>Processing streams to produce other, derived streams is what an &lt;em>operator job&lt;/em> does. The one crucial difference to batch jobs is that a stream never ends.&lt;/p>
&lt;p>&lt;em>Complex event processing&lt;/em> (CEP) is an approach for analising event streams where you can specify rules to search for certain patterns of events in them.&lt;/p>
&lt;p>When a match is found, the engine emits a &lt;em>complex event&lt;/em>.&lt;/p>
&lt;p>Queries are stored long-term, and events from the input streams continuously flow past them in search of a query that matches an event pattern.&lt;/p>
&lt;p>Implementations of CEP include Esper, IBM InfoSphere Streams, Apama, TIBCO StreamBase, and SQLstream.&lt;/p>
&lt;p>The boundary between CEP and stream analytics is blurry, analytics tends to be less interested in finding specific event sequences and is more oriented toward aggregations and statistical metrics.&lt;/p>
&lt;p>Frameworks with analytics in mind are: Apache Storm, Spark Streaming, Flink, Concord, Samza, and Kafka Streams. Hosted services include Google Cloud Dataflow and Azure Stream Analytics.&lt;/p>
&lt;p>Sometimes there is a need to search for individual events continually, such as full-text search queries over streams.&lt;/p>
&lt;p>Message-passing ystems are also based on messages and events, we normally don&amp;rsquo;t think of them as stream processors.&lt;/p>
&lt;p>There is some crossover area between RPC-like systems and stream processing. Apache Storm has a feature called &lt;em>distributed RPC&lt;/em>.&lt;/p>
&lt;p>In a batch process, the time at which the process is run has nothing to do with the time at which the events actually occurred.&lt;/p>
&lt;p>Many stream processing frameworks use the local system clock on the processing machine (&lt;em>processing time&lt;/em>) to determine windowing. It is a simple approach that breaks down if there is any significant processing lag.&lt;/p>
&lt;p>Confusing event time and processing time leads to bad data. Processing time may be unreliable as the stream processor may queue events, restart, etc. It&amp;rsquo;s better to take into account the original event time to count rates.&lt;/p>
&lt;p>You can never be sure when you have received all the events.&lt;/p>
&lt;p>You can time out and declare a window ready after you have not seen any new events for a while, but it could still happen that some events are delayed due a network interruption. You need to be able to handle such &lt;em>stranggler&lt;/em> events that arrive after the window has already been declared complete.&lt;/p>
&lt;ol>
&lt;li>You can ignore the stranggler events, tracking the number of dropped events as a metric.&lt;/li>
&lt;li>Publish a &lt;em>correction&lt;/em>, an updated value for the window with stranglers included. You may also need to retrat the previous output.&lt;/li>
&lt;/ol>
&lt;p>To adjust for incofrrect device clocks, one approach is to log three timestamps:&lt;/p>
&lt;ul>
&lt;li>The time at which the event occurred, according to the device clock&lt;/li>
&lt;li>The time at which the event was sent to the server, according to the device clock&lt;/li>
&lt;li>The time at which the event was received by the server, according to the server clock.&lt;/li>
&lt;/ul>
&lt;p>You can estimate the offset between the device clock and the server clock, then apply that offset to the event timestamp, and thus estimate the true time at which the event actually ocurred.&lt;/p>
&lt;p>Several types of windows are in common use:&lt;/p>
&lt;ul>
&lt;li>Tumbling window: Fixed length. If you have a 1-minute tumbling window, all events between 10:03:00 and 10:03:59 will be grouped in one window, next window would be 10:04:00-10:04:59&lt;/li>
&lt;li>Hopping window: Fixed length, but allows windows to overlap in order to provide some smoothing. If you have a 5-minute window with a hop size of 1 minute, it would contain the events between 10:03:00 and 10:07:59, next window would cover 10:04:00-10:08:59&lt;/li>
&lt;li>Sliding window: Events that occur within some interval of each other. For example, a 5-minute sliding window would cover 10:03:39 and 10:08:12 because they are less than 4 minutes apart.&lt;/li>
&lt;li>Session window: No fixed duration. All events for the same user, the window ends when the user has been inactive for some time (30 minutes). Common in website analytics&lt;/li>
&lt;/ul>
&lt;p>The fact that new events can appear anytime on a stream makes joins on stream challenging.&lt;/p>
&lt;h4 id="stream-stream-joins">Stream-stream joins&lt;/h4>
&lt;p>You want to detect recent trends in searched-for URLs. You log an event containing the query. Someone clicks one of the search results, you log another event recording the click. You need to bring together the events for the search action and the click action.&lt;/p>
&lt;p>For this type of join, a stream processor needs to maintain &lt;em>state&lt;/em>: All events that occurred in the last hour, indexed by session ID. Whenever a search event or click event occurs, it is added to the appropriate index, and the stream processor also checks the other index to see if another event for the same session ID has already arrived. If there is a matching event, you emit an event saying search result was clicked.&lt;/p>
&lt;h4 id="stream-table-joins">Stream-table joins&lt;/h4>
&lt;p>Sometimes know as &lt;em>enriching&lt;/em> the activity events with information from the database.&lt;/p>
&lt;p>Imagine two datasets: a set of usr activity events, and a database of user profiles. Activity events include the user ID, and the the resulting stream should have the augmented profile information based upon the user ID.&lt;/p>
&lt;p>The stream process needs to look at one activity event at a time, look up the event&amp;rsquo;s user ID in the database, and add the profile information to the activity event. THe database lookup could be implemented by querying a remote database., however this would be slow and risk overloading the database.&lt;/p>
&lt;p>Another approach is to load a copy of the database into the stream processor so that it can be queried locally without a network round-trip. The stream processor&amp;rsquo;s local copy of the database needs to be kept up to date; this can be solved with change data capture.&lt;/p>
&lt;h4 id="table-table-join">Table-table join&lt;/h4>
&lt;p>The stream process needs to maintain a database containing the set of followers for each user so it knows which timelines need to be updated when a new tweet arrives.&lt;/p>
&lt;h4 id="time-dependence-join">Time-dependence join&lt;/h4>
&lt;p>The previous three types of join require the stream processor to maintain some state.&lt;/p>
&lt;p>If state changes over time, and you join with some state, what point in time do you use for the join?&lt;/p>
&lt;p>If the ordering of events across streams is undetermined, the join becomes nondeterministic.&lt;/p>
&lt;p>This issue is known as &lt;em>slowly changing dimension&lt;/em> (SCD), often addressed by using a unique identifier for a particular version of the joined record. For example, we can turn the system deterministic if every time the tax rate changes, it is given a new identifier, and the invoice includes the identifier for the tax rate at the time of sale. But as a consequence makes log compation impossible.&lt;/p>
&lt;h4 id="fault-tolerance">Fault tolerance&lt;/h4>
&lt;p>Batch processing frameworks can tolerate faults fairly easy:if a task in a MapReduce job fails, it can simply be started again on another machine, input files are immutable and the output is written to a separate file.&lt;/p>
&lt;p>Even though restarting tasks means records can be processed multiple times, the visible effect in the output is as if they had only been processed once (&lt;em>exactly-once-semantics&lt;/em> or &lt;em>effectively-once&lt;/em>).&lt;/p>
&lt;p>With stream processing waiting until a tasks if finished before making its ouput visible is not an option, stream is infinite.&lt;/p>
&lt;p>One solution is to break the stream into small blocks, and treat each block like a minuature batch process (&lt;em>micro-batching&lt;/em>). This technique is used in Spark Streaming, and the batch size is typically around one second.&lt;/p>
&lt;p>An alternative approach, used in Apache Flint, is to periodically generate rolling checkpoints of state and write them to durable storage. If a stream operator crashes, it can restart from its most recent checkpoint.&lt;/p>
&lt;p>Microbatching and chekpointing approaches provide the same exactly-once semantics as batch processing. However, as soon as output leaves the stream processor, the framework is no longer able to discard the output of a failed batch.&lt;/p>
&lt;p>In order to give appearance of exactly-once processing, things either need to happen atomically or none of must happen. Things should not go out of sync of each other. Distributed transactions and two-phase commit can be used.&lt;/p>
&lt;p>This approach is used in Google Cloud Dataflow and VoltDB, and there are plans to add similar features to Apache Kafka.&lt;/p>
&lt;p>Our goal is to discard the partial output of failed tasks so that they can be safely retired without taking effect twice. Distributed transactions are one way of achieving that goal, but another way is to rely on &lt;em>idempotence&lt;/em>.&lt;/p>
&lt;p>An idempotent operation is one that you can perform multiple times, and it has the same effect as if you performed it only once.&lt;/p>
&lt;p>Even if an operation is not naturally idempotent, it can often be made idempotent with a bit of extra metadata. You can tell wether an update has already been applied.&lt;/p>
&lt;p>Idempotent operations can be an effective way of achieving exactly-once semantics with only a small overhead.&lt;/p>
&lt;p>Any stream process that requires state must ensure tha this state can be recovered after a failure.&lt;/p>
&lt;p>One option is to keep the state in a remote datastore and replicate it, but it is slow.&lt;/p>
&lt;p>An alternative is to keep state local to the stream processor and replicate it periodically.&lt;/p>
&lt;p>Flink periodically captures snapshots and writes them to durable storage such as HDFS; Samza and Kafka Streams replicate state changes by sending them to a dedicated Kafka topic with log compaction. VoltDB replicates state by redundantly processing each input message on several nodes.&lt;/p>
&lt;h2 id="the-future-of-data-systems">The future of data systems&lt;/h2>
&lt;h3 id="data-integration">Data integration&lt;/h3>
&lt;p>Updating a derived data system based on an event log can often be made determinisitic and idempotent.&lt;/p>
&lt;p>Distributed transactions decide on an ordering of writes by using locks for mutual exclusion, while CDC and event sourcing use a log for ordering. Distributed transactions use atomic commit to ensure exactly once semantics, while log-based systems are based on deterministic retry and idempotence.&lt;/p>
&lt;p>Transaction systems provide linearizability, useful guarantees as reading your own writes. On the other hand, derived systems are often updated asynchronously, so they do not by default offer the same timing guarantees.&lt;/p>
&lt;p>In the absence of widespread support for a good distributed transaction protocol, log-based derived data is the most promising approach for integrating different data systems.&lt;/p>
&lt;p>However, as systems are scaled towards bigger and more coplex worloads, limitiations emerge:&lt;/p>
&lt;ul>
&lt;li>Constructing a totally ordered log requires all events to pass through a &lt;em>single leader node&lt;/em> that decides on the ordering.&lt;/li>
&lt;li>An undefined ordering of events that originate on multiple datacenters.&lt;/li>
&lt;li>When two events originate in different services, there is no defined order for those events.&lt;/li>
&lt;li>Some applications maintain client-side state. Clients and servers are very likely to see events in different orders.&lt;/li>
&lt;/ul>
&lt;p>Deciding on a total order of events is known as &lt;em>total order broadcast&lt;/em>, which is equivalent to consensus. It is still an open research problem to design consensus algorithms that can scale beyond the throughput of a single node.&lt;/p>
&lt;h4 id="batch-and-stream-processing">Batch and stream processing&lt;/h4>
&lt;p>The fundamental difference between batch processors and batch processes is that the stream processors operate on unbounded datasets whereas batch processes inputs are of a known finite size.&lt;/p>
&lt;p>Spark performs stream processing on top of batch processing. Apache Flink performs batch processing in top of stream processing.&lt;/p>
&lt;p>Batch processing has a quite strong functional flavour. The output depends only on the input, there are no side-effects. Stream processing is similar but it allows managed, fault-tolerant state.&lt;/p>
&lt;p>Derived data systems could be maintained synchronously. However, asynchrony is what makes systems based on event logs robust: it allows a fault in one part of the system to be contained locally.&lt;/p>
&lt;p>Stream processing allows changes in the input to be reflected in derived views with low delay, whereas batch processing allows large amounts of accumulated historical data to be reprocessed in order to derive new views onto an existing dataset.&lt;/p>
&lt;p>Derived views allow &lt;em>gradual&lt;/em> evolution. If you want to restructure a dataset, you do not need to perform the migration as a sudden switch. Instead, you can maintain the old schema and the new schema side by side as two independent derived views onto the same underlying data, eventually you can drop the old view.&lt;/p>
&lt;h4 id="lambda-architecture">Lambda architecture&lt;/h4>
&lt;p>The whole idea behind lambda architecture is that incoming data should be recorded by appending immutable events to an always-growing dataset, similarly to event sourcing. From these events, read-optimised vuews are derived. Lambda architecture proposes running two different systems in parallel: a batch processing system such as Hadoop MapReduce, and a stream-processing system as Storm.&lt;/p>
&lt;p>The stream processor produces an approximate update to the view: the batch processor produces a corrected version of the derived view.&lt;/p>
&lt;p>The stream process can use fast approximation algorithms while the batch process uses slower exact algorithms.&lt;/p>
&lt;h3 id="unbundling-databases">Unbundling databases&lt;/h3>
&lt;h4 id="creating-an-index">Creating an index&lt;/h4>
&lt;p>Batch and stream processors are like elaborate implementations of triggers, stored procedures, and materialised view maintenance routines. The derived data systems they maintain are like different index types.&lt;/p>
&lt;p>There are two avenues by which different storate and processing tools can nevertheless be composed into a cohesive system:&lt;/p>
&lt;ul>
&lt;li>Federated databases: unifying reads. It is possible to provide a unified query interface to a wide variety of underlying storate engines and processing methods, this is known as &lt;em>federated database&lt;/em> or &lt;em>polystore&lt;/em>. An example is PostgreSQL&amp;rsquo;s &lt;em>foreign data wrapper&lt;/em>.&lt;/li>
&lt;li>Unbundled databases: unifying writes. When we compose several storage systems, we need to ensure that all data changes end up in all the right places, even in the face of faults, it is like &lt;em>unbundling&lt;/em> a database&amp;rsquo;s index-maintenance features in a way that can synchronise writes across disparate technologies.&lt;/li>
&lt;/ul>
&lt;p>Keeping the writes to several storage systems in sync is the harder engineering problem.&lt;/p>
&lt;p>Synchronising writes requires distributed transactions across heterogeneous storage systems which may be the wrong solution. An asynchronous event log with idempotent writes is a much more robust and practical approach.&lt;/p>
&lt;p>The big advantage is &lt;em>loose coupling&lt;/em> between various components:&lt;/p>
&lt;ol>
&lt;li>Asynchronous event streams make the system as a whole more robust to outages or performance degradation of individual components.&lt;/li>
&lt;li>Unbundling data systems allows different software components and services to be developed, improved and maintained independently from each other by different teams.&lt;/li>
&lt;/ol>
&lt;p>If there is a single technology that does everything you need, you&amp;rsquo;re most likely best off simply using that product rather than trying to reimplement it yourself from lower-level components. The advantages of unbundling and composition only come into the picture when there is no single piece of software that satisfies all your requirements.&lt;/p>
&lt;h4 id="separation-of-application-code-and-state">Separation of application code and state&lt;/h4>
&lt;p>It makes sense to have some parts of a system that specialise in durable data storage, and other parts that specialise in running application code. The two can interact while still remaining independent.&lt;/p>
&lt;p>The trend has been to keep stateless application logic separate from state management (databases): not putting application logic in the database and not putting persistent state in the application.&lt;/p>
&lt;h4 id="dataflow-interplay-between-state-changes-and-application-code">Dataflow, interplay between state changes and application code&lt;/h4>
&lt;p>Instead of treating the database as a passive variable that is manipulated by the application, application code responds to state changes in one place by triggering state changes in another place.&lt;/p>
&lt;h4 id="stream-processors-and-services">Stream processors and services&lt;/h4>
&lt;p>A customer is purchasing an item that is priced in one currency but paid in another currency. In order to perform the currency conversion, you need to know the current exchange rate.&lt;/p>
&lt;p>This could be implemented in two ways:&lt;/p>
&lt;ul>
&lt;li>Microservices approach, the code that processes the purchase would probably wuery an exchange-rate service or a database in order to obtain the current rate for a particular currency.&lt;/li>
&lt;li>Dataflow approach, the code that processes purchases would subscribe to a stream of exchange rate updates ahead of time, and record the current rate in a local database whenever it changes. When it comes to processing the purchase, it only needs to query the local database.&lt;/li>
&lt;/ul>
&lt;p>The dataflow is not only faster, but it is also more robust to the failure of another service.&lt;/p>
&lt;h4 id="observing-derived-state">Observing derived state&lt;/h4>
&lt;h5 id="materialised-views-and-caching">Materialised views and caching&lt;/h5>
&lt;p>A full-text search index is a good example: the write path updates the index, and the read path searches the index for keywords.&lt;/p>
&lt;p>If you don&amp;rsquo;t have an index, a search query would have to scan over all documents, which is very expensive. No index means less work on the write path (no index to update), but a lot more work on the read path.&lt;/p>
&lt;p>Another option would be to precompute the search results for only a fixed set of the most common queries. The uncommon queries can still be served from the inxed. This is what we call a &lt;em>cache&lt;/em> although it could also be called a materialised view.&lt;/p>
&lt;h5 id="read-are-events-too">Read are events too&lt;/h5>
&lt;p>It is also possible to represent read requests as streams of events, and send both the read events and write events through a stream processor; the processor responds to read events by emiting the result of the read to an output stream.&lt;/p>
&lt;p>It would allow you to reconstruct what the user saw before they made a particular decision.&lt;/p>
&lt;p>Enables better tracking of casual dependencies.&lt;/p>
&lt;h3 id="aiming-for-correctness">Aiming for correctness&lt;/h3>
&lt;p>If your application can tolerate occasionally corrupting or losing data in unpredictable ways, life is a lot simpler. If you need stronger assurances of correctness, the serializability and atomic commit are established approaches.&lt;/p>
&lt;p>While traditional transaction approach is not going away, there are some ways of thinking about correctness in the context of dataflow architectures.&lt;/p>
&lt;h4 id="the-end-to-end-argument-for-databases">The end-to-end argument for databases&lt;/h4>
&lt;p>Bugs occur, and people make mistakes. Favour of immutable and append-only data, because it is easier to recover from such mistakes.&lt;/p>
&lt;p>We&amp;rsquo;ve seen the idea of &lt;em>exactly-once&lt;/em> (or &lt;em>effectively-once&lt;/em>) semantics. If something goes wrong while processing a message, you can either give up or try again. If you try again, there is the risk that it actually succeeded the first time, the message ends up being processed twice.&lt;/p>
&lt;p>&lt;em>Exactly-once&lt;/em> means arranging the computation such that the final effect is the same as if no faults had occurred.&lt;/p>
&lt;p>One of the most effective approaches is to make the operation &lt;em>idempotent&lt;/em>, to ensure that it has the same effect, no matter whether it is executed once or multiple times. Idempotence requires some effort and care: you may need to maintain some additional metadata (operation IDs), and ensure fencing when failing over from one node to another.&lt;/p>
&lt;p>Two-phase commit unfortunately is not sufficient to ensure that the transaction will only be executed once.&lt;/p>
&lt;p>You need to consider &lt;em>end-to-end&lt;/em> flow of the request.&lt;/p>
&lt;p>You can generate a unique identifier for an operation (such as a UUID) and include it as a hidden form field in the client application, or calculate a hash of all the relevant form fields to derive the operation ID. If the web browser submits the POST request twice, the two requests will have the same operation ID. You can then pass that operation ID all the way through to the database and check that you only ever execute one operation with a given ID. You can then save those requests to be processed, uniquely identified by the operation ID.&lt;/p>
&lt;p>Is not enough to prevent a user from submitting a duplicate request if the first one times out. Solving the problem requires an end-to-end solution: a transaction indentifier that is passed all the way from the end-user client to the database.&lt;/p>
&lt;p>Low-level reliability mechanisms such as those in TCP, work quite well, and so the remaining higher-level faults occur fairly rarely.&lt;/p>
&lt;p>Transactions have long been seen as a good abstraction, they are useful but not enough.&lt;/p>
&lt;p>It is worth exploring F=fault-tolerance abstractions that make it easy to provide application-specific end-to-end correctness properties, but also maintain good performance and good operational characteristics.&lt;/p>
&lt;h4 id="enforcing-constraints">Enforcing constraints&lt;/h4>
&lt;h5 id="uniqueness-constraints-require-consensus">Uniqueness constraints require consensus&lt;/h5>
&lt;p>The most common way of achieving consensus is to make a single node the leadder, and put it in charge of making all decisions. If you need to tolerate the leader failing, you&amp;rsquo;re back at the consensus problem again.&lt;/p>
&lt;p>Uniqueness checking can be scaled out by partitioning based on the value that needs to be unique. For example, if you need usernames to be unique, you can partition by hash or username.&lt;/p>
&lt;p>Asynchronous multi-master replication is ruled out as different masters concurrently may accept conflicting writes, so values are no longer unique. If you want to be able to immediately reject any writes that would violate the constraint, synchronous coordination is unavoidable.&lt;/p>
&lt;h5 id="uniqueness-in-log-based-messaging">Uniqueness in log-based messaging&lt;/h5>
&lt;p>A stream processor consumes all the messages in a log partition sequentially on a single thread. A stream processor can unambiguously and deterministically decide which one of several conflicting operations came first.&lt;/p>
&lt;ol>
&lt;li>Every request for a username is encoded as a message.&lt;/li>
&lt;li>A stream processor sequentially reads the requests in the log. For every request for a username tht is available, it records the name as taken and emits a success message to an output stream. For every request for a username that is already taken, it emits a rejection message to an output stream.&lt;/li>
&lt;li>The client waits for a success or rejection message corresponding to its request.&lt;/li>
&lt;/ol>
&lt;p>The approach works not only for uniqueness constraints, but also for many other kinds of constraints.&lt;/p>
&lt;h5 id="multi-partition-request-processing">Multi-partition request processing&lt;/h5>
&lt;p>There are potentially three partitions: the one containing the request ID, the one containing the payee account, and one containing the payer account.&lt;/p>
&lt;p>The traditional approach to databases, executing this transaction would require an atomic commit across all three partitions.&lt;/p>
&lt;p>Equivalent correctness can be achieved with partitioned logs, and without an atomic commit.&lt;/p>
&lt;ol>
&lt;li>The request to transfer money from account A to account B is given a unique request ID by the client, and appended to a log partition based on the request ID.&lt;/li>
&lt;li>A stream processor reads the log of requests. For each request message it emits two messages to output streams: a debit instruction to the payer account A (partitioned by A), and a credit instruction to the payee account B (partitioned by B). The original request ID is included in those emitted messages.&lt;/li>
&lt;li>Further processors consume the streams of credit and debit instructions, deduplicate by request ID, and apply the chagnes to the account balances.&lt;/li>
&lt;/ol>
&lt;h4 id="timeliness-and-integrity">Timeliness and integrity&lt;/h4>
&lt;p>Consumers of a log are asynchronous by design, so a sender does not wait until its message has been proccessed by consumers. However, it is possible for a client to wait for a message to appear on an output stream.&lt;/p>
&lt;p>&lt;em>Consistency&lt;/em> conflates two different requirements:&lt;/p>
&lt;ul>
&lt;li>Timeliness: users observe the system in an up-to-date state.&lt;/li>
&lt;li>Integrity: Means absence of corruption. No data loss, no contradictory or false data. The derivation must be correct.&lt;/li>
&lt;/ul>
&lt;p>Violations of timeless are &amp;ldquo;eventual consistency&amp;rdquo; whereas violations of integrity are &amp;ldquo;perpetual inconsistency&amp;rdquo;.&lt;/p>
&lt;h4 id="correctness-and-dataflow-systems">Correctness and dataflow systems&lt;/h4>
&lt;p>When processing event streams asynchronously, there is no guarantee of timeliness, unless you explicitly build consumers that wait for a message to arrive before returning. But integrity is in fact central to streaming systems.&lt;/p>
&lt;p>&lt;em>Exactly-once&lt;/em> or &lt;em>effectively-once&lt;/em> semantics is a mechanism for preserving integrity. Fault-tolerant message delivery and duplicate supression are important for maintaining the integrity of a data system in the face of faults.&lt;/p>
&lt;p>Stream processing systems can preserve integrity without requireing distributed transactions and an atomic commit protocol, which means they can potentially achieve comparable correctness with much better performance and operational robustness. Integrity can be achieved through a combination of mechanisms:&lt;/p>
&lt;ul>
&lt;li>Representing the content of the write operation as a single message, this fits well with event-sourcing&lt;/li>
&lt;li>Deriving all other state updates from that single message using deterministic derivation functions&lt;/li>
&lt;li>Passing a client-generated request ID, enabling end-to-end duplicate supression and idempotence&lt;/li>
&lt;li>Making messages immutable and allowing derived data to be reprocessed from time to time&lt;/li>
&lt;/ul>
&lt;p>In many businesses contexts, it is actually acceptable to temporarily violate a constraint and fix it up later apologising. The cost of the apology (money or reputation), it is often quite low.&lt;/p>
&lt;h4 id="coordination-avoiding-data-systems">Coordination-avoiding data-systems&lt;/h4>
&lt;ol>
&lt;li>Dataflow systems can maintain integrity guarantees on derived data without atomic commit, linearizability, or synchronous cross-partition coordination.&lt;/li>
&lt;li>Although strict uniqueness constraints require timeliness and coordination, many applications are actually fine with loose constraints than may be temporarily violated and fixed up later.&lt;/li>
&lt;/ol>
&lt;p>Dataflow systems can provide the data management services for many applications without requiring coordination, while still giving strong integrity guarantees. &lt;em>Coordination-avoiding&lt;/em> data systems can achieve better performance and fault tolerance than systems that need to perform synchronous coordination.&lt;/p>
&lt;h4 id="trust-but-verify">Trust, but verify&lt;/h4>
&lt;p>Checking the integrity of data is know as &lt;em>auditing&lt;/em>.&lt;/p>
&lt;p>If you want to be sure that your data is still there, you have to actually read it and check. It is important to try restoring from your backups from time to time. Don&amp;rsquo;t just blindly trust that it is working.&lt;/p>
&lt;p>&lt;em>Self-validating&lt;/em> or &lt;em>self-auditing&lt;/em> systems continually check their own integrity.&lt;/p>
&lt;p>ACID databases has led us toward developing applications on the basis of blindly trusting technology, neglecting any sort of auditability in the process.&lt;/p>
&lt;p>By contrast, event-based systems can provide better auditability (like with event sourcing).&lt;/p>
&lt;p>Cryptographic auditing and integrity checking often relies on &lt;em>Merkle trees&lt;/em>. Outside of the hype for cryptocurrencies, &lt;em>certificate transparency&lt;/em> is a security technology that relies on Merkle trees to check the validity of TLS/SSL certificates.&lt;/p>
&lt;h3 id="doing-the-right-thing">Doing the right thing&lt;/h3>
&lt;p>Many datasets are about people: their behaviour, their interests, their identity. We must treat such data with humanity and respect. Users are humans too, and human dignitity is paramount.&lt;/p>
&lt;p>There are guidelines to navigate these issues such as ACM&amp;rsquo;s Software Engineering Code of Ethics and Professional Practice&lt;/p>
&lt;p>It is not sufficient for software engineers to focus exclusively on the technology and ignore its consequences: the ethical responsibility is ours to bear also.&lt;/p>
&lt;p>In countries that respect human rights, the criminal justice system presumes innocence until proven guilty; on the other hand, automated systems can systematically and artbitrarily exclude a person from participating in society without any proof of guilt, and with little chance of appeal.&lt;/p>
&lt;p>If there is a systematic bias in the input to an algorithm, the system will most likely learn and amplify bias in its output.&lt;/p>
&lt;p>It seems ridiculous to believe that an algorithm could somehow take biased data as input and produce fair and impartial output from it. Yet this believe often seems to be implied by proponents of data-driven decision making.&lt;/p>
&lt;p>If we want the future to be better than the past, moral imagination is required, and that&amp;rsquo;s something only humans can provide. Data and models should be our tools, not our masters.&lt;/p>
&lt;p>If a human makes a mistake, they can be held accountable. Algorithms make mistakes too, but who is accountable if they go wrong?&lt;/p>
&lt;p>A credit score summarises &amp;ldquo;How did you behave in the past?&amp;rdquo; whereas predictive analytics usually work on the basis of &amp;ldquo;Who is similar to you, and how did people like you behave in the past?&amp;rdquo; Drawing parallels to others&amp;rsquo; behaviour implies stereotyping people.&lt;/p>
&lt;p>We will also need to figure outhow to prevent data being used to harm people, and realise its positive potential instead, this power could be used to focus aid an support to help people who most need it.&lt;/p>
&lt;p>When services become good at predicting what content users want to se, they may end up showing people only opinions they already agree with, leading to echo chambers in which stereotypes, misinformation and polaristaion can breed.&lt;/p>
&lt;p>Many consequences can be predicted by thinking about the entire system (not just the computerised parts), an approach known as &lt;em>systems thinking&lt;/em>.&lt;/p>
&lt;h4 id="privacy-and-tracking">Privacy and tracking&lt;/h4>
&lt;p>When a system only stores data that a user has explicitly entered, because they want the system to store and process it in a certain way, the system is performing a service for the user: the user is the customer.&lt;/p>
&lt;p>But when a user&amp;rsquo;s activity is tracked and logged as a side effect of other things they are doing, the relationship is less clear. The service no longer just does what the users tells it to do, but it takes on interests of its own, which may conflict with the user&amp;rsquo;s interest.&lt;/p>
&lt;p>If the service is funded through advertising, the advertirsers are the actual customers, and the users&amp;rsquo; interests take second place.&lt;/p>
&lt;p>The user is given a free service and is coaxed into engaging with it as much as possible. The tracking of the user serves the needs of the advertirses who are funding the service. This is basically &lt;em>surveillance&lt;/em>.&lt;/p>
&lt;p>As a thougt experiment, try replacing the word &lt;em>data&lt;/em> with &lt;em>surveillance&lt;/em>.&lt;/p>
&lt;p>Even themost totalitarian and repressive regimes could only dream of putting a microphone in every room and forcing every person to constantly carry a device capable of tracking their location and movements. Yet we apparently voluntarily, even enthusiastically, throw ourselves into this world of total surveillance. The difference is just that the data is being collected by corporations rather than government agencies.&lt;/p>
&lt;p>Perhaps you feel you have nothing to hide, you are totally in line with existing power structures, you are not a marginalised minority, and you needn&amp;rsquo;t fear persecution. Not everyone is so fortunate.&lt;/p>
&lt;p>Without understanding what happens to their data, users cannot give any meaningful consent. Often, data from one user also says things about other people who are not users of the service and who have not agreed to any terms.&lt;/p>
&lt;p>For a user who does not consent to surveillance, the only real alternative is simply to not user the service. But this choice is not free either: if a service is so popular that it is &amp;ldquo;regarded by most people as essential for basic social participation&amp;rdquo;, then it is not reasonable to expect people to opt out of this service. Especially when a service has network effects, there is a social cost to people choosing &lt;em>not&lt;/em> to use it.&lt;/p>
&lt;p>Declining to use a service due to its tracking of users is only an option for the small number of people who are priviledged enough to have the time and knowledge to understand its privacy policy, and who can affort to potentially miss out on social participation or professional opportunities that may have arisen if they ahd participated in the service. For people in a less priviledged position, there is no meaningful freedom of choice: surveillance becomes inescapable.&lt;/p>
&lt;p>Having privacy does not mean keeping everything secret; it means having the freedom to choose which things to reveal to whom, what to make public, and what to keep secret.&lt;/p>
&lt;p>Companies that acquire data essentially say &amp;ldquo;trust us to do the right thing with your data&amp;rdquo; which means that the right to decide what to reveal and what to keep secret is transferred from the individual to the company.&lt;/p>
&lt;p>Even if the service promises not to sell the data to third parties, it usually grants itself unrestricted rights to process and analyse the data internally, often going much further than what is overtly visible to users.&lt;/p>
&lt;p>If targeted advertising is what pays for a service, then behavioral data about people is the service&amp;rsquo;s core asset.&lt;/p>
&lt;p>When collecting data, we need to consider not just today&amp;rsquo;s political environment, but also future governments too. There is no guarantee that every government elected in the future will respect human rights and civil liberties, so &amp;ldquo;it is poor civic hygiene to install technologies that could someday facilitate a police state&amp;rdquo;.&lt;/p>
&lt;p>To scrutinise others while avoiding scrutiny oneself is one of the most important forms of power.&lt;/p>
&lt;p>In the industrial revolution tt took a long time before safeguards were established, such as environmental protection regulations, safety protocols for workplaces, outlawing child labor, and health inspections for food. Undoubtedly the cost of doing business increased when factories could no longer dump their waste into rivers, sell tainted foods, or exploit workers. But society as a whole benefited hugely, and few of us would want to return to a time before those regulations.&lt;/p>
&lt;p>We should stop regarding users as metrics to be optimised, and remember that they are humans who deserve respect, dignity, and agency. We should self-regulate our data collection and processing practices in order to establish an maintain the trust of the people who depend on our software. And we should take it upon ourselves to educate end users about how their data is used, rather than keeping them in the dark.&lt;/p>
&lt;p>We should allow each individual to maintain their privacy, their control over their own data, and not steal that control from them through surveillance.&lt;/p>
&lt;p>We should not retain data forever, but purge it as soon as it is no longer needed.&lt;/p></description></item><item><title>2017-Kleppmann-《Designing Data Intensive Applications》-中文概述</title><link>https://ng-tech.icu/books/awesome-cs-books/distributed/dataengineering/ddia/2017-kleppmann-designing-data-intensive-applications-%E4%B8%AD%E6%96%87%E6%A6%82%E8%BF%B0/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/awesome-cs-books/distributed/dataengineering/ddia/2017-kleppmann-designing-data-intensive-applications-%E4%B8%AD%E6%96%87%E6%A6%82%E8%BF%B0/</guid><description>&lt;blockquote>
&lt;p>参考地址：https://ngte.cowtransfer.com/s/63c41532fba247&lt;/p>
&lt;/blockquote>
&lt;h1 id="designing-data-intensive-applications">Designing Data-Intensive Applications&lt;/h1>
&lt;p>《Designing Data-Intensive Applications》着实是一本非常优秀的讲解数据库与分布式系统底层实现原理的书籍，特别是 Martin Kleppmann 从理论到实践深入浅出地介绍了这些系统的实现路径。&lt;/p>
&lt;p>本书主要分为三个部分：1~4 章介绍 Foundations of Data Systems，5~9 章介绍了 Distributed Data，10~12 章介绍了 Derived Data。&lt;/p>
&lt;h1 id="1-foundations-of-data-systems">1. Foundations of Data Systems&lt;/h1>
&lt;p>介绍性章节定义了可靠性，可伸缩性和可维护性。我特别喜欢 Twitter 向关注者传递推文的演变方式的例子。关于响应时间也有一个好处：当最终用户需要多个后端呼叫时，即使只有一小部分单独的请求很慢（尾部延迟放大），许多用户也可能会遇到延迟。&lt;/p>
&lt;h1 id="2-data-models-and-query-languages">2. Data Models and Query Languages&lt;/h1>
&lt;p>本章讨论了众所周知的关系模型和文档模型（NoSQL）。有一个很好的示例说明了如何将 LinkedIn 配置文件表示为 JSON 文档–配置文件的树状结构非常适合用 JSON 表示。但是，随着数据在文档之间变得更加相互关联（在 LinkedIn 示例中：对公司和大学的引用，用户之间的建议），可能会出现问题。在文档模型中，联接从数据库转移到应用程序。&lt;/p>
&lt;p>文档数据库有时称为无模式（schema-less）。这是不正确的，因为存在隐式架构。更好的方式是写模式（schema-on-write，数据库在存储值时强制执行模式）和读模式（schema-on-read，模式是隐式的，仅在读取数据时才解释）。这类似于编程语言中的静态类型和动态类型。&lt;/p>
&lt;p>接下来，将讨论查询语言，并很好地说明了命令式查询（编程语言中的循环）与声明式查询（SQL 中的 SELECT 语句）之间的对比。最后，讨论了类似图形的数据模型（例如，由 Neo4J 使用）和该模型的各种查询语言。&lt;/p>
&lt;h1 id="3-storage-and-retrieval">3. Storage and Retrieval&lt;/h1>
&lt;p>本章首先说明 LSM 树和 B 树的工作方式。&lt;/p>
&lt;h2 id="lsm-trees">LSM-trees&lt;/h2>
&lt;p>Kleppmann 首先用两行 bash 代码创建一个简单的数据库。通过将行（键后跟值）添加到文件中来存储键和值。要检索给定密钥的值，请遍历文件，并在找到密钥后获取该值。写入现有密钥仅意味着将新行附加到文件中。文件中保留了带有键和值的旧行。但是 get 函数仅返回为给定键找到的最后一个值，因此文件中保留旧行并不重要。&lt;/p>
&lt;p>这里的关键思想是仅将其追加到文件中是快速的（与更改文件中的值相反）。但是，扫描整个文件以获取键的值很慢。一种提高速度的方法是保留一个单独的散列，该散列具有一个指针，该指针指向文件中每个键的起始位置。然后，读取操作首先查找要使用的偏移量，然后从文件中的该位置开始读取。&lt;/p>
&lt;p>接下来，我们假设文件中的所有行均按键排序。这些文件称为字符串排序表，缩写为 SSTables。如果我们有许多在不同时间创建的文件，则给定密钥可能会出现在许多文件中。但是，密钥的最新值是唯一的有效值-所有其他值都已过时（由较新的值代替）。我们可以将这些文件合并为一个文件，同时摆脱所有过时的行。这称为压缩，并且由于所有文件均按键排序，因此可以有效地完成合并排序的方式。&lt;/p>
&lt;p>首先如何创建排序的文件？您维护一个称为内存表的内存中树结构（例如红黑树或 AVL 树）。这样可以使数据保持排序，并且一旦树达到一定大小（几兆字节），它就会作为新的 SSTable 被写出。添加或更改键的值只是意味着将其添加到内存表中（如果已经存在，则可能会覆盖它）。要查找键的值，请先在内存表中搜索，然后在最新的 SSTable 中搜索，然后在下一个较旧的 SSTable 中搜索，依此类推。&lt;/p>
&lt;p>这种存储结构称为日志结构合并树（LSM-tree），例如在 Cassandra 中使用。&lt;/p>
&lt;h2 id="b-trees">B-trees&lt;/h2>
&lt;p>B 树是最广泛使用的索引结构（在传统 SQL 数据库中使用），并且与 LSM 树有很大不同。B 树还按键对键/值对进行排序，以便快速查找。但是，数据以固定大小的块（也称为页面）存储在磁盘上，传统上大小为 4 KB。要更改块中的值，需要写入整个块。通过使用指向子块的指针来创建树，在该子块中，键范围变得越来越具体，直到找到包含所需值的叶块为止。分支因子描述了父节点可以具有多少个子节点。&lt;/p>
&lt;p>大多数数据库都可以容纳在三层或四层深的 B 树中–具有 4 KB 块且分支因子为 500 的四层树最多可以存储 256 TB。B 树结构是 1970 年由波音公司的 Rudolf Bayer 和 Edward McCreight 引入的。目前尚不清楚 B 的含义-根据维基百科，已经提出了波音，平衡，宽阔，浓密和拜耳的建议。&lt;/p>
&lt;p>LSM 树通常用于写入，而 B 树则用于读取。但是，有几个因素会影响性能，这些因素将在本章中进行讨论。描述了不同类型的优化，以及用于处理崩溃的策略，例如使用预写日志（WAL，也称为重做日志）。&lt;/p>
&lt;h2 id="transactions-and-analytics">Transactions and Analytics&lt;/h2>
&lt;p>当数据库首次出现时，它们通常用于商业交易，例如进行销售或支付薪水。即使将数据库用于其他任务，术语 transaction 仍然存在。现在数据库用法分为两大类：在线事务处理（OLTP）和在线分析处理（OLAP）。OLTP 通常在面对最终用户时使用，并且交易通常很小。OLAP 在数据仓库环境中使用得更多，在该环境中查询可以聚合数百万条记录中的值。OLAP 数据库通常以星型模式进行组织。有时面向列的存储用于 OLAP 用例。当处理一列中的多个值时（例如求和或求平均值），这可能会更有效率。本章提供了一个很好的示例，说明如何使用位图和游程长度编码来压缩存储的值。&lt;/p>
&lt;p>本章很长，但是非常好。我在工作中同时使用 MySQL 和 Cassandra，这有助于理解内部存储模型之间的差异。&lt;/p>
&lt;h1 id="4-encoding-and-evolution">4. Encoding and Evolution&lt;/h1>
&lt;p>当程序中的数据需要保存到文件或通过网络发送时，需要以某种格式进行编码。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>向后兼容性–新代码可以读取由旧代码编写的数据。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>前向兼容性–旧代码可以读取新代码编写的数据。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>目前常见的有三种类型的数据编码：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>第一种类型是特定于语言的格式，例如 Java 的 java.io.Serializable 或 Python 的 pickle。使用此类编码方式存在许多问题：安全性，版本控制，性能以及它们与特定编程语言绑定的事实。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>接下来是标准化的文本编码，例如 XML，JSON 和 CSV。他们也有问题。例如，它们不支持二进制字符串（没有字符编码的字节序列）。JSON 不能区分整数和浮点数，也没有指定精度。CSV 是一种模糊的格式。例如，如何处理逗号和换行符？&lt;/p>
&lt;/li>
&lt;li>
&lt;p>最后，还有几种著名的二进制编码格式：Thrift（最初来自 Facebook），Protocol Buffers（最初来自 Google）和 Avro（最初来自 Hadoop 项目）。它们都有模式，并使用一些巧妙的技巧使编码紧凑。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="5-replication">5. Replication&lt;/h1>
&lt;p>这是“分布式数据”部分的第一章。复制意味着相同的数据存储在多台计算机上。复制的一些原因是：即使系统的某些部分发生故障也要继续工作（可用性提高），使数据在地理位置上更接近用户（减少延迟），以及通过从许多计算机提供相同的数据来增加读取吞吐量。&lt;/p>
&lt;p>涵盖了三种不同的模型：单 Leader（Single Leader），多 Leader（Multi Leader）和无 Leader（Leaderless）。复制可以是同步的也可以是异步的。如果是同步的，则 Leader 和所有关注者在确认写回用户之前会先对其进行确认。如果任何跟随者速度慢或失败，则这可能会阻止所有写入。因此，异步复制更为常见。&lt;/p>
&lt;p>复制有很多棘手的方面。首先是在设置新的关注者时。由于 Leader 中的数据不断变化，因此通常您需要对 Leader 的数据库进行一致的快照并将其复制到新的关注者中。然后，您需要处理复制过程中发生的更改积压。&lt;/p>
&lt;p>如果关注者失败，则恢复后需要追赶。这意味着它需要跟踪失败之前 Leader 已经处理过的交易。如果 Leader 失败，则需要选择一个新的 Leader。这称为故障转移。这里很多事情都会出错。如果使用异步复制，则新的 Leader 可能尚未收到所有写入。如果在选择新的 Leader 后，前 Leader 重新加入集群，那些未复制的写入应该怎么办？&lt;/p>
&lt;p>执行复制也很棘手。如果使用 NOW()或 RAND()，则仅重复 SQL 语句就会引起问题。也有许多其他边缘情况，因此通常不使用此方法。而是使用数据库内部预写日志。这是一个仅附加字节序列，其中包含对数据库的所有写操作。&lt;/p>
&lt;h2 id="replication-lag">Replication Lag&lt;/h2>
&lt;p>使用异步复制时，您最终会保持一致。即使复制时滞通常很小，但各种因素（例如网络问题或高负载）也可能导致时滞变为几秒钟甚至几分钟。当您有复制滞后时，可能会发生一些问题：如果提交的数据是从与接收到的写入不同的服务器上完成的，则您提交数据然后重新加载页面时，可能看不到刚写入的数据。有几种可能的解决方案，可以保证您自己阅读内容。跨设备读取（从笔记本电脑更新，然后从手机读取）甚至更加棘手。&lt;/p>
&lt;p>另一个异常是时间似乎在向后移动。第一次读取返回用户 X 最近所做的注释。刷新页面时，读取来自另一个（滞后）服务器，并且尚未由用户 X 进行注释。单调读取是避免这种情况的一种方法，例如，通过将所有读取路由到同一服务器。如果最终的一致性太弱，则可以使用分布式事务。&lt;/p>
&lt;h2 id="multi-leader-replication">Multi-Leader Replication&lt;/h2>
&lt;p>在多数据中心操作中，使用多头复制很有意义。有几个优点：性能（写入不必全部都经过一个 Leader），数据中心中断的容忍度，网络问题的容忍度（临时性问题不会阻止写入过程）。但是，如果 Leader 多于一位，则存在写冲突的风险。有几种方法可以处理这些问题：上次写入获胜（基于时间戳或唯一 ID），自动将值合并在一起或保留冲突值并让用户选择要保留的值。&lt;/p>
&lt;h2 id="leaderless-replication">Leaderless Replication&lt;/h2>
&lt;p>Dynamo，Cassandra，Riak 和 Voldemort 都使用无 Leader 复制。为了确保没有写入丢失，并且没有读取返回过时的值，将使用仲裁读取和写入。如果有 n 个副本，则每次写入必须由 w 个节点确认是否成功，并且我们必须为每个读取至少查询 r 个节点。只要 w + r&amp;gt; n，我们期望在读取时获得最新的值。在最简单的情况下，n = 3，w = 2，r = 2。但是，仍然存在边缘情况–例如，如果写入与读取并发，则写入可能仅存在于某些副本中，并且不清楚是否应为读取返回新值或旧值。&lt;/p>
&lt;p>如果节点已关闭且具有过时的值，则从该节点读取值时可以检测到该节点。这可以启动修复。还可能在后台运行反熵过程，以寻找不一致之处并启动修复。&lt;/p>
&lt;p>本章以一个很好的例子作为结束，说明如何使用版本向量来跟踪并发事件之间的因果关系。&lt;/p>
&lt;h1 id="6-partitioning">6. Partitioning&lt;/h1>
&lt;p>将数据分为多个分区（也称为分片）的原因是可伸缩性。请注意，您可以同时进行分区和复制，每个分区可以复制到多个节点。每条数据恰好属于一个分区。分区可以在键范围内进行，也可以通过键的散列来完成。如果某些分区的命中率高于其他分区（工作负载偏斜），则分区的优势可能会丢失。二级索引对于分区数据库可能会比较棘手。解决此问题的一种方法是分散/聚集。&lt;/p>
&lt;p>随着数据的增长和更改，可能需要重新平衡，将负载从一个节点转移到另一个节点。您不应移动过多的数据。例如，仅使用 hash mod N 将导致太多更改。更好的方法是使用比节点更多的分区，并且仅在节点之间移动整个分区。如果自动进行重新平衡，则存在级联故障的风险。因此，最好仅手动执行此操作。请求到正确分区的路由通常由单独的协调服务（例如 Zookeeper）处理。&lt;/p>
&lt;h1 id="7-transactions">7. Transactions&lt;/h1>
&lt;p>本章介绍在一台计算机上的事务。一个事务组将一个读写单元写入一个逻辑单元。整个事务要么成功（提交），要么失败（中止，回滚）。这样一来，应用程序开发人员就不必处理许多潜在的问题：与数据库的连接断开，在写入所有数据之前数据库或应用程序崩溃，多个客户端可能会覆盖彼此的数据等。&lt;/p>
&lt;p>ACID 代表原子性，一致性，隔离性和耐久性。原子性与并发无关（隔离）。相反，这意味着所有写入都成功，或者没有成功。没有部分写入的数据。一致性意味着在事务之前和之后，不变性必须始终为真。例如，所有帐户的贷方和借方始终保持平衡。数据库可以检查某些不变量，例如外键约束和唯一性约束。但是通常，只有应用程序才能定义有效或无效的数据-数据库仅存储数据。&lt;/p>
&lt;p>隔离意味着并发执行的事务不会相互干扰。其余各章中的大多数都涉及到这一点。最后，持久性意味着成功写入的数据不会丢失。但是，即使将数据写入磁盘，也有许多方式可能会丢失数据：磁盘可能会损坏，固件错误可能会阻止您的读取，计算机可能死机并且即使磁盘上的数据很好也无法获取数据。&lt;/p>
&lt;h2 id="isolation-levels">Isolation Levels&lt;/h2>
&lt;p>当多个客户端同时读取和更新数据时，会有很多陷阱。为避免这些情况，有几种隔离级别可以防止出现问题。最基本的级别是已提交读。这意味着在读取时，您只会看到已提交的数据，而不是正在写入但尚未提交的数据。写入数据库时，您只会覆盖已提交的数据。这也称为无脏读和无脏写。&lt;/p>
&lt;p>快照隔离（Snapshot Isolation）处理不同部分之间的一致性。如果您有两个帐户，每个帐户中有 500，并且您从第一个帐户中读取了余额，然后从第二个帐户中读取了余额（两个都在同一笔交易中读取），那么它们的总和应为 1000。但是，在第一次读取和第二次读取之间，可能会有一笔并发交易将 100 从第二个帐户转移到第一个帐户。因此，第一次读取可能会得到 500，而第二次读取会返回 400（因为此处已减去 100）。如果我们再次重复读取同一帐户，则第一个帐户将获得 600，第二个帐户将获得 400，因此现在它们的总和达到了预期的 1000。如果避免了总和更改的问题，则称为快照隔离，也称为可重复读取。&lt;/p>
&lt;p>问题是我们希望在给定的时间点在数据库中看到的内容保持一致。在上面的示例中，我们不希望看到总额仅为\900 美元的情况。例如，这在进行备份时很重要。进行备份可能需要几个小时，并且数据会不断变化，但是我们希望存储在备份中的内容保持一致。长时间运行的查询也是如此–我们希望它们在一致的快照上执行。常见的解决方案是使用多版本并发控制（MVCC）。数据库可以保留一个对象的多个不同的提交版本，因为各种进行中的事务可能需要在不同的时间点查看数据库的状态。&lt;/p>
&lt;p>当两个事务都写入数据时，就有丢失更新的风险。例如，如果两个用户同时读取，递增和写入结果，则最终的计数器值可能仅被更新了一个，而不是两个。如果事务读取某些信息，基于该信息做出决定并写入结果，则可能存在写入偏斜。这意味着在写入结果时，其所基于的前提不再成立。例如，一个会议室预订系统试图避免重复预订。&lt;/p>
&lt;h2 id="serializable-transactions">Serializable transactions&lt;/h2>
&lt;p>避免出现问题的一种可靠方法是，如果所有事务都依次执行。但是，性能常常遭受太多损失。在某些情况下，您实际上可以串行执行所有事务。您需要将整个数据集存储在内存中，并使用存储过程来避免事务期间的网络往返，并且吞吐量必须足够低才能由单个 CPU 处理。&lt;/p>
&lt;p>大约 30 年以来，唯一可广泛使用的可序列化算法是两阶段锁定（2PL）。广泛的锁定意味着吞吐量下降。您也很容易陷入僵局。一种称为可序列化快照隔离（SSI）的新算法也可以提供可序列化性，但是由于乐观并发控制，与 2PL 使用的悲观并发控制相反，它具有更好的性能。&lt;/p>
&lt;h1 id="8-the-trouble-with-distributed-systems">8. The Trouble with Distributed Systems&lt;/h1>
&lt;p>这是本书中我最喜欢的另一章（以及第 3 章“存储和检索”）。即使我在分布式系统和并发问题上工作了很长时间，但本章对于我可能会出错的所有方式还是让我大开眼界。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>网络不可靠。节点之间的连接可能会以各种方式失败。除正常故障模式外，还列出了一些异常模式：交换机的软件升级导致所有网络数据包延迟一分钟以上，鲨鱼咬伤并损坏海底电缆，所有入站数据包均被丢弃，而出站数据包则被丢弃。发送成功。即使在一个数据中心等受控环境中，网络问题也很常见。一项研究表明每月有 12 个网络故障。检测网络问题的唯一解决方案是超时。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>不可靠的时钟。时钟根据某些日历返回当前日期和时间。它们通常与 NTP 同步。由于计算机上的本地时钟可能会漂移，因此它可能早于 NTP 时间，并且重置会使它看起来像是在时间上回跳。单调时钟更适合于测量经过的时间–保证它们始终向前移动。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>即使使用 NTP 同步不同的计算机，也存在许多可能的问题。由于 NTP 服务器存在网络延迟，因此时钟的精确度受到限制。seconds 秒也造成了许多大故障。这些和其他问题意味着一天可能没有精确的 86,400 秒，时钟可能向后移动，并且一个节点上的时间可能与另一节点上的时间完全不同。此外，不正确的时钟很容易被忽视。&lt;/p>
&lt;p>依靠时间戳来订购事件（例如 Cassandra 中的 Last Write Wins）可能会导致意外结果。Google Spanner 使用的一种解决方案是确定时间戳的置信区间，并确保在订购事件时不存在重叠。&lt;/p>
&lt;ul>
&lt;li>处理暂停。与时间有关的另一个问题是过程暂停。在执行操作之前，检查当前时间然后执行某些操作的代码可能已暂停了几秒钟。发生这种情况的方式有很多：垃圾回收，从代码中看不出来的同步磁盘访问（Java 类加载器延迟加载类文件），操作系统交换到磁盘（分页）等。&lt;/li>
&lt;/ul>
&lt;p>为了处理分布式系统中的这些各种问题，多数人定义了事实（在下一章中对此进行了更多讨论）。这里描述的可能的问题可能会导致一些非常棘手的情况。更糟糕的是，如果参与的节点故意尝试引起问题。那就是所谓的拜占庭式断层，但这在本书中没有涉及。通过控制所有涉及的服务器，可以避免此类问题。&lt;/p>
&lt;p>本章以定义安全性（Safety）和活力（Liveness）为结尾。安全意味着没有不好的事情发生（例如，错误的数据未写入数据库），活跃意味着最终发生了好事（例如，在 Leader 节点发生故障之后，最终选举了新的 Leader）。&lt;/p>
&lt;h1 id="9-consistency-and-consensus">9. Consistency and Consensus&lt;/h1>
&lt;p>分布式一致性主要是在面对延迟和故障时协调副本的状态。&lt;/p>
&lt;h2 id="linearizability">Linearizability&lt;/h2>
&lt;p>线性化意味着复制的数据看起来好像只有一个数据副本，并且所有操作看起来都像是对数据进行原子操作（您不会看到值在新值和旧值之间转换）。它使数据库的行为类似于单线程程序中的变量。问题是它运行缓慢，尤其是在网络延迟较大的情况下。&lt;/p>
&lt;p>CAP 定理有时表示为一致性，可用性，分区容限：从 3 中选择 2。但是网络分区是一个错误，因此您别无选择，它会发生。克莱普曼（Kleppmann）认为该定理最好表示为：划分时一致或可用。他还指出，CAP 定理仅涉及分区，没有提及其他故障，例如网络延迟或死节点。因此，CAP 定理不是那么有用。&lt;/p>
&lt;h2 id="ordering-guarantees">Ordering Guarantees&lt;/h2>
&lt;p>总顺序允许比较任何两个元素，并且您始终可以说哪个更大。线性化系统的总阶数。如果同时发生两个事件，则无法确定哪个事件先发生。这导致因果关系的一致性模型较弱。它定义了部分顺序：某些操作是相对于彼此进行排序的，但是有些操作是不可比较的（并发）。&lt;/p>
&lt;p>Lamport 时间戳提供与因果关系一致的总排序。但是，这还不足以解决诸如确保所选用户名唯一的问题（如果两个用户同时尝试选择相同的用户名，那么我们将在此之后才知道两个人中有谁）。这导致总顺序广播。它要求没有消息丢失；如果消息传递到一个节点，则将消息传递到所有节点。它还要求消息以相同顺序传递到每个节点。具有全部顺序广播可以实现正确的数据库复制。&lt;/p>
&lt;h2 id="distributed-transactions-and-consensus">Distributed Transactions and Consensus&lt;/h2>
&lt;p>对于分布式事务，可以使用两阶段提交（2PC）。它需要一个协调员。首先，它向每个节点发送准备消息。节点检查它们将能够执行写入，如果是，则回答“是”。如果所有节点都回答是，则下一条消息是提交。然后，每个节点都必须执行写操作。&lt;/p>
&lt;p>使用单个领导者数据库，所有决策都由领导者做出，因此您可以具有线性化的操作，唯一性约束，完全有序的复制日志等等（这些属性都可以简化为共识）。如果领导者失败，或者网络问题使您无法到达它，则有以下三种选择：等待其恢复，通过人工选择新领导者进行手动故障转移，或使用算法自动选择新领导者。ZooKeeper 和 etcd 之类的工具可以帮助您解决这一问题。&lt;/p>
&lt;p>但是，并非所有系统都需要共识。无领导者和多领导者复制系统通常没有这样做。相反，他们必须处理发生的冲突。这对我来说是最难理解的一章（我什至不确定我是否完全理解），尤其是关于如何将共识化为其他属性的解释。&lt;/p>
&lt;h1 id="10-batch-processing">10. Batch Processing&lt;/h1>
&lt;p>这是本书中处理派生数据的第一章。记录系统（保存数据的权威版本）与派生数据系统之间存在区别。派生数据系统中的数据是以某种方式转换或处理的现有数据，例如缓存或搜索索引。本章从使用 Unix 工具进行批处理的示例开始。为了从访问日志中找到五个最受欢迎的 URL，将命令 sort，awk，uniq 和 head 管道在一起。排序实际上比我想象的要强大得多。如果数据集不适合内存，它将自动存储到磁盘，并且将自动并行化多个 CPU 内核之间的排序（如果有）。&lt;/p>
&lt;p>MapReduce 的工作方式与管道式 Unix 工具的工作方式之间有相似之处。它们不修改输入，除了产生输出外没有其他副作用，并且文件以顺序方式写入一次。对于 Unix 工具，stdin 和 stdout 是输入和输出。MapReduce 作业在分布式文件系统上读写文件。由于输入是不可变的，并且没有副作用，因此失败的 MapReduce 作业可以再次运行。根据我们希望从 MapReduce 作业中获得什么结果，可以执行几种不同类型的联接：排序合并联接，广播哈希联接和分区哈希联接。&lt;/p>
&lt;p>使用管道化 Unix 命令的类比，MapReduce 就像将每个命令的输出写入一个临时文件。有一些新的数据流引擎，例如 Flink，可以比传统的 MapReduce 改善性能。例如，与每个阶段相比，不经常存储到文件中（实现），并且仅在需要时才进行排序。如果某些步骤避免了排序，那么您也不需要整个数据集，并且可以流水线化各个阶段。&lt;/p>
&lt;h1 id="11-stream-processing">11. Stream Processing&lt;/h1>
&lt;p>事件是一个小的，独立的，不可变的对象，其中包含某个时间点发生的事情的详细信息。例如，事件可以由用户在网页上执行操作，传感器的温度测量，服务器指标（例如 CPU 利用率）或股票价格生成。流处理与批处理类似，但是对无限制的流而不是固定大小的输入连续进行。用这种类比，消息代理是文件系统的流等效项。消息代理有两大类，具体取决于它们是在处理完消息后丢弃还是保留消息。基于日志的消息代理（例如 Kafka）保留消息，因此可以返回并重读旧消息。这类似于数据库和日志结构化存储引擎中的复制日志。&lt;/p>
&lt;p>将写入数据库视为流也可能很有用。日志压缩可以减少所需的存储空间，同时仍然允许流保留数据库的完整副本。将数据库表示为流允许诸如搜索索引，缓存和分析系统之类的派生数据系统不断更新。这是通过使用更改日志并将其应用于派生系统来完成的。也可以从头开始创建新视图，并使用所有事件直到现在。这也与事件源非常相似。&lt;/p>
&lt;p>通常，每个事件都有一个时间戳。此时间戳与服务器处理事件的时间不同，这可能导致某些奇怪的情况。例如，用户发出一个由 Web 服务器 A 处理的 Web 请求。然后，用户发出另一个由 Web 服务器 B 处理的请求。两个 Web 服务器都发出事件，但是 B 的事件首先到达消息代理（可能是由于排队或网络故障）。因此，消息代理从 B 看到事件，然后从 A 看到事件，即使它们以相反的顺序发生。&lt;/p>
&lt;p>您还可以对流执行分析。例如，测量某物的速率，计算某个时间段内的滚动平均值或将当前统计信息与以前的时间间隔进行比较以检测趋势。可以使用各种类型的窗口：翻滚，跳动，滑动或会话。同样，就像批处理作业一样，您可以将流数据与数据库表连接起来以丰富事件数据。&lt;/p>
&lt;h1 id="12-the-future-of-data-systems">12. The Future of Data Systems&lt;/h1>
&lt;p>在本章中，Kleppmann 描述了他对如何设计数据系统的看法。它基于第 11 章的思想，使用记录系统中的事件流创建数据的各种派生视图。由于派生是异步的并且是松散耦合的，因此一个区域中的问题不会像紧密集成的系统中那样传播到其他不相关的领域。此外，这些类型的系统可以更好地处理错误。如果处理数据的代码存在错误，则可以修复该错误，然后可以重新处理数据。&lt;/p>
&lt;p>还讨论了诸如交易之类的内部措施不足以防止两次错误执行操作的问题。检查需要从应用程序端到端进行。例如，可以通过为操作分配唯一标识符来确保操作是幂等的，并检查该 ID 仅对操作执行一次。有时，在出现问题时能够进行补偿也要好得多，而不是花费大量精力来防止它发生。例如，如果帐户已透支，则为补偿性交易；如果航班已超额预订，则为道歉和补偿。如果这种情况不太经常发生，那么大多数企业都可以接受。通过异步检查约束，您可以避免大多数协调并保持完整性，同时还能保持良好的性能。&lt;/p>
&lt;p>与数据流相关的讨论是从请求/响应系统转移到发布/订阅数据流。如果您收到所有更改的通知，则可以使视图保持最新（与电子表格的工作方式比较（更改在单元格中波动））。但是，这样做很难，因为请求/响应的假设已在数据库，库和框架中根深蒂固。&lt;/p>
&lt;p>本章的最后部分讨论了在开发数据处理系统时的道德考虑。一个有趣的思想实验是用监视替换单词数据。在我们的监控驱动组织中，我们收集实时监控流并将其存储在我们的监控仓库中。我们的监视科学家使用高级分析和监视处理程序来获得新见解。&lt;/p>
&lt;h1 id="nuggets">Nuggets&lt;/h1>
&lt;p>在整本书中，我发现很多有趣的信息。这是我的一些最爱。&lt;/p>
&lt;ul>
&lt;li>
&lt;p>内存数据库的速度并不比基于磁盘的数据库快，因为它们可以从内存中读取，而传统的数据库可以从磁盘读取。操作系统仍然将最近使用的磁盘块缓存在内存中。相反，速度优势来自于不必将内存中的数据结构编码为适合写入磁盘的格式（第 89 页）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>某些语言中的内置哈希函数不适用于获取分区键，因为同一键在不同进程中可能具有不同的哈希值。例如，Java 中的 Object.hashCode() 和 Ruby 中的 Object＃hash（第 203 页）。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>在 Google 中，运行一个小时的 MapReduce 任务被终止的风险为 5％。该速率比由于硬件问题，计算机重新启动等导致的故障率高出一个数量级。MapReduce 旨在承受频繁的意外任务终止的原因不是因为硬件特别不可靠，而是因为任意选择的自由终止过程可以在计算群集中更好地利用资源（第 418 页）。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>每章均以引号开头。我特别喜欢其中的两个。第 5 章中的第一个是我在软件开发方面一直以来最喜欢的报价之一：总是可以找到一个有效的复杂系统，它是从一个简单的有效系统演变而来的。相反的命题也似乎是正确的：从头开始设计的复杂系统永远行不通，也无法运转。约翰·加尔（John Gall），Systemantics，1975 年&lt;/p>
&lt;p>这是第 11 章的第二个引用：可能出错的事物和不可能出错的事物之间的主要区别在于，当不可能出错的事物出错时，通常发现根本无法修理–道格拉斯·亚当斯（Douglas Adams），《几乎无害》（1992 年）&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>如今，感觉大多数系统都是以一种或另一种方式分布的系统。对于所有软件开发人员来说，设计数据密集型应用程序几乎都是必读的。因此，了解其中的许多概念确实很有用。&lt;/p>
&lt;p>本书中描述和解决的许多问题都归结为并发问题。通常，有很好的图片和图表说明了这些要点。在每一章的开头，都有一张幻想样式的地图，其中列出了下一章的关键概念。我很喜欢那些。&lt;/p>
&lt;p>设计数据密集型应用程序很厚-超过 550 页。这让我开始犹豫，几乎觉得太过气了。幸运的是，我们今年春天在工作中为读书俱乐部选择了它。这给了我足够的微动，可以开始并继续前进。我真的很高兴开始，因为其中包含了很多很好的信息。我特别喜欢它同时具有理论和实践意义。&lt;/p>
&lt;p>如果您喜欢此摘要，则一定要阅读整本书。还有更多的细节和示例，它们都很有趣。强烈推荐！&lt;/p>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://henrikwarne.com/2019/07/27/book-review-designing-data-intensive-applications/" target="_blank" rel="noopener">https://henrikwarne.com/2019/07/27/book-review-designing-data-intensive-applications/&lt;/a>&lt;/li>
&lt;/ul></description></item></channel></rss>