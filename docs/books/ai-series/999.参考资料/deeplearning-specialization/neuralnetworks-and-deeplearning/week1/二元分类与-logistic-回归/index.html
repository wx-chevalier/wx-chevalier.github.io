<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="二元分类与 Logistic 回归 本部分将会介绍神经网格构建与训练的基础知识；一般来说，网络的计算过程由正向传播(Forward Propagation)与反向传播(Back Propagation)两部分组成。这里我们将"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E4%B8%8E-logistic-%E5%9B%9E%E5%BD%92/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E4%B8%8E-logistic-%E5%9B%9E%E5%BD%92/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E4%B8%8E-logistic-%E5%9B%9E%E5%BD%92/"><meta property="og:title" content="二元分类与 Logistic 回归 | Next-gen Tech Edu"><meta property="og:description" content="二元分类与 Logistic 回归 本部分将会介绍神经网格构建与训练的基础知识；一般来说，网络的计算过程由正向传播(Forward Propagation)与反向传播(Back Propagation)两部分组成。这里我们将"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>二元分类与 Logistic 回归 | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=6292b06e5ea7b2353c8fcefe3e780815><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">Week1</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id3097b94f8235ac9933d199ba03bad2b7")' href=#id3097b94f8235ac9933d199ba03bad2b7 aria-expanded=false aria-controls=id3097b94f8235ac9933d199ba03bad2b7 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/>NeuralNetworks-And-DeepLearning</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id3097b94f8235ac9933d199ba03bad2b7 aria-expanded=false aria-controls=id3097b94f8235ac9933d199ba03bad2b7><i class="fa-solid fa-angle-down" id=caret-id3097b94f8235ac9933d199ba03bad2b7></i></a></div><ul class="nav docs-sidenav collapse show" id=id3097b94f8235ac9933d199ba03bad2b7><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-ida3dd19f4206aa12ee53c3377289eda08")' href=#ida3dd19f4206aa12ee53c3377289eda08 aria-expanded=false aria-controls=ida3dd19f4206aa12ee53c3377289eda08 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/>Week1</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#ida3dd19f4206aa12ee53c3377289eda08 aria-expanded=false aria-controls=ida3dd19f4206aa12ee53c3377289eda08><i class="fa-solid fa-angle-down" id=caret-ida3dd19f4206aa12ee53c3377289eda08></i></a></div><ul class="nav docs-sidenav collapse show" id=ida3dd19f4206aa12ee53c3377289eda08><li class="child level active"><a href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB%E4%B8%8E-logistic-%E5%9B%9E%E5%BD%92/>二元分类与 Logistic 回归</a></li><li class="child level"><a href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E5%9F%BA%E4%BA%8E-logistic-%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%AE%9E%E8%B7%B5/>基于 Logistic 回归的图像分类实践</a></li><li class="child level"><a href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E5%9F%BA%E4%BA%8E-numpy-%E7%9A%84-python-%E5%90%91%E9%87%8F%E6%93%8D%E4%BD%9C/>基于 Numpy 的 Python 向量操作</a></li><li class="child level"><a href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E6%B5%85%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>浅层神经网络</a></li><li class="child level"><a href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9C%89%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>神经网络、有监督学习与深度学习</a></li><li class="child level"><a href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%8E%E5%90%91%E9%87%8F%E5%8C%96%E6%93%8D%E4%BD%9C/>梯度下降与向量化操作</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id5d1b8bc4dc13c5008d7376c9e12a52cd")' href=#id5d1b8bc4dc13c5008d7376c9e12a52cd aria-expanded=false aria-controls=id5d1b8bc4dc13c5008d7376c9e12a52cd aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-idc999ec421480b126503533c7f0ca5468")' href=#idc999ec421480b126503533c7f0ca5468 aria-expanded=false aria-controls=idc999ec421480b126503533c7f0ca5468 aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id4f7a4c34312beb63936ecf1b8bbcaf32")' href=#id4f7a4c34312beb63936ecf1b8bbcaf32 aria-expanded=false aria-controls=id4f7a4c34312beb63936ecf1b8bbcaf32 aria-hidden=false data-toggle=collapse></div></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#神经网络的符号>神经网络的符号</a><ul><li><a href=#尺寸>尺寸</a></li><li><a href=#对象>对象</a></li><li><a href=#通用前向传播等式>通用前向传播等式</a></li><li><a href=#损失函数>损失函数</a></li></ul></li><li><a href=#深度学习的表示>深度学习的表示</a></li></ul><ul><li><a href=#基础模型>基础模型</a></li><li><a href=#损失函数与代价函数>损失函数与代价函数</a></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>二元分类与 Logistic 回归</h1><div class=article-style><h1 id=二元分类与-logistic-回归>二元分类与 Logistic 回归</h1><p>本部分将会介绍神经网格构建与训练的基础知识；一般来说，网络的计算过程由正向传播(Forward Propagation)与反向传播(Back Propagation)两部分组成。这里我们将会以简单的 Logistic 回归为例，讲解如何解决常见的二元分类(Binary Classification)问题。这里我们将会尝试训练出简单的神经网络以自动识别某个图片是否为猫，为猫则输出 1，否则输出 0。计算机中的图片往往表示为红、绿、蓝三个通道的像素值；如果我们的图像是 64 _ 64 像素值大小，我们的单张图片的特征维度即为 64 _ 64 * 3 = 12288，即可以使用 $n_x = 12288$ 来表示特征向量的维度。</p><h1 id=深度学习的标准术语约定>深度学习的标准术语约定</h1><h2 id=神经网络的符号>神经网络的符号</h2><p>上标 $^{(i)}$ 表示第 $i$ 个训练用例，而上标 $^{[l]}$ 则表示第 $l$ 层。</p><h3 id=尺寸>尺寸</h3><ul><li>$m$：数据集中用例的数目。</li><li>$n_x$：输入的单个用例的特征向量维度。</li><li>$n_y$：输出的维度(待分类的数目)。</li><li>$n_h^{[l]}$：第 $l$ 个隐层的单元数目，在循环中，我们可能定义 $n_x = n_h^{[0]}$ 以及 $n_y = n_h^{number , of , layers + 1}$。</li><li>$L$：神经网络的层数。</li></ul><h3 id=对象>对象</h3><ul><li>$X \in R^{n_x \times m}$：输入的矩阵，即包含了 $m$ 个用例，每个用例的特征向量维度为 $n_x$。</li><li>$x^{(i)} \in R^{n_x}$：第 $i$ 个用例的特征向量，表示为列向量。</li><li>$Y \in R^{n_y \times m}$：标签矩阵。</li><li>$y^{(i)} \in R^{n_y}$：第 $i$ 个用例的输出标签。</li><li>$W^{[l]} \in R^{number , of , units , in , next , layer \times number , of , unites , in , the , previous , layer}$：第 $l$ 层与第 $l+1$ 层之间的权重矩阵，在简单的二元分类且仅有输入层与输出层的情况下，其维度就是 $ 1 \times n_x$。</li><li>$b^{[l]} \in R^{number , of , units , in , next , layer}$：第 $l$ 层的偏差矩阵。</li><li>$\hat{y} \in R^{n_y}$：输出的预测向量，也可以表示为 $a^{[L]}$，其中 $L$ 表示网络中的总层数。</li></ul><h3 id=通用前向传播等式>通用前向传播等式</h3><ul><li>$ a = g^{[l]}(W_xx^{(i)} + b_1) = g^{[l]}(z_1) $，其中 $g^{[l]}$ 表示第 $l$ 层的激活函数。</li><li>$\hat{y}^{(i)} = softmax(W_hh + b_2)$。</li><li>通用激活公式：$a<em>j^{[l]} = g^{[l]}(\sum_kw</em>{jk}^{[l]}a_k^{[l-1]} + b_j^{[l]}) = g^{[l]}(z_j^{[l]})$。</li><li>$J(x, W, b, y)$ 或者 $J(\hat{y}, y)$ 表示损失函数。</li></ul><h3 id=损失函数>损失函数</h3><ul><li>$J*{CE(\hat{y},y)} = - \sum*{i=0}^m y^{(i)}log \hat{y}^{(i)}$</li><li>$J*{1(\hat{y},y)} = \sum*{i=0}^m | y^{(i)} - \hat{y}^{(i)} |$</li></ul><h2 id=深度学习的表示>深度学习的表示</h2><p>在深度学习中，使用结点代表输入、激活函数或者数据，边代表权重或者偏差，下图即是两个典型的神经网络：</p><h1 id=logistic-回归>Logistic 回归</h1><h2 id=基础模型>基础模型</h2><p>在猫咪识别问题中，我们给定了未知的图片，可以将其表示为 $X \in R^{n_x}$ 的特征向量；我们的任务就是寻找合适的算法，来根据特征向量推导出该图片是猫咪的概率。在上面的介绍中我们假设了 Logistic 函数的参数为 $w \in R^{n_x} $ 以及 $b \in R$，则输出的计算公式可以表示为：</p><p>$$
\hat{y} = \sigma(w^Tx + b)
$$</p><p>这里的 $\sigma$ 表示 Sigmoid 函数，该函数的表达式与线型如下：</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img src=https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Sigmoid-function-2.svg/2000px-Sigmoid-function-2.svg.png alt loading=lazy data-zoomable></div></div></figure></p><p>上图中可以发现，当 $t$ 非常大时，$e^{-t}$ 趋近于 0，整体的函数值趋近于 1；反之，如果 $t$ 非常小的时候，整体的函数值趋近于 0。</p><h2 id=损失函数与代价函数>损失函数与代价函数</h2><p>我们的训练目标是在给定训练数据 ${(x^{(1)}, y^{(1)}),&mldr;,(x^{(m)},y^{(m)})}$ 的情况下使得 $\hat{y}^{(i)}$ 尽可能接近 $y^{(i)}$，而所谓的损失函数即是用于衡量预测结果与真实值之间的误差。最简单的损失函数定义方式为平方差损失：</p><p>$$
L(\hat{y},y) = \frac{1}{2} (\hat{y} - y)^2
$$</p><p>不过 Logistic 回归中我们并不倾向于使用这样的损失函数，因为其对于梯度下降并不友好，很多情况下会陷入非凸状态而只能得到局部最优解。这里我们将会使用如下的损失函数：</p><p>$$
L(\hat{y},y) = -(ylog\hat{y} + (1-y)log(1-\hat{y}))
$$</p><p>我们的优化目标是希望损失函数值越小越好，这里我们考虑两个极端情况，当 $y = 1$ 时，损失函数值为 $-log\hat{y}$；此时如果 $\hat{y} = 1$，则损失函数为 0。反之如果 $\hat{y} = 0$，则损失函数值趋近于无穷大。当 $y = 0$ 时，损失函数值为 $-log(1-\hat{y})$；如果 $\hat{y} = 1$，则损失函数值也变得无穷大。这样我们可以将 Logistic 回归中总的代价函数定义为：</p><p>$$
J(w,b) =
\frac{1}{m}\sum*{i=1}^mL(\hat{y}^{(i)} - y^{(i)}) =
-\frac{1}{m} \sum*{i=1}^m [y^{(i)}log\hat{y}^{(i)} + (1-y^{(i)})log(1-\hat{y}^{(i)})]
$$</p><p>在深度学习的模型训练中我们常常会接触到损失函数(Loss Function)与代价函数(Cost Function)的概念，其中损失函数代指单个训练用例的错误程度，而代价函数往往是整个训练集中所有训练用例的损失函数值的平均。</p></div><div class=article-widget><div class="container-xl row post-nav"><div class="col-6 post-nav-item"><div class=meta-nav>下一页</div><a href=/books/ai-series/999.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/deeplearning-specialization/neuralnetworks-and-deeplearning/week1/%E5%9F%BA%E4%BA%8E-logistic-%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E5%AE%9E%E8%B7%B5/ rel=prev>基于 Logistic 回归的图像分类实践</a></div></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte-website.oss-accelerate.aliyuncs.com/disqus/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte-website.oss-accelerate.aliyuncs.com/disqus/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>