<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>00.导论 | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/</link>
      <atom:link href="https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/index.xml" rel="self" type="application/rss+xml" />
    <description>00.导论</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>00.导论</title>
      <link>https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/</link>
    </image>
    
    <item>
      <title>深度学习简史</title>
      <link>https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8F%B2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%80%E5%8F%B2/</guid>
      <description>&lt;h1 id=&#34;深度学习简史&#34;&gt;深度学习简史&lt;/h1&gt;
&lt;h1 id=&#34;起源&#34;&gt;起源&lt;/h1&gt;
&lt;p&gt;虽然深度学习似乎是最近几年刚兴起的名词，但它所基于的神经网络模型和用数据编程的核心思想已经被研究了数百年。自古以来，人类就一直渴望能从数据中分析出预知未来的窍门。实际上，数据分析正是大部分自然科学的本质，我们希望从日常的观测中提取规则，并找寻不确定性。&lt;/p&gt;
&lt;p&gt;早在 17 世纪，雅各比·伯努利（1655–1705）提出了描述只有两种结果的随机过程（如抛掷一枚硬币）的伯努利分布。大约一个世纪之后，卡尔·弗里德里希·高斯（1777–1855）发明了今日仍广泛用在从保险计算到医学诊断等领域的最小二乘法。概率论、统计学和模式识别等工具帮助自然科学的实验学家们从数据回归到自然定律，从而发现了如欧姆定律（描述电阻两端电压和流经电阻电流关系的定律）这类可以用线性模型完美表达的一系列自然法则。&lt;/p&gt;
&lt;p&gt;即使是在中世纪，数学家也热衷于利用统计学来做出估计。例如，在雅各比·科贝尔（1460–1533）的几何书中记载了使用 16 名男子的平均脚长来估计男子的平均脚长。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://s1.ax1x.com/2020/08/09/a7yW3F.md.png&#34; alt=&#34;在中世纪，16名男子的平均脚长被用来估计男子的平均脚长&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在这个研究中，16 位成年男子被要求在离开教堂时站成一排并把脚贴在一起，而后他们脚的总长度除以 16 得到了一个估计：这个数字大约相当于今日的一英尺。这个算法之后又被改进，以应对特异形状的脚：最长和最短的脚不计入，只对剩余的脚长取平均值，即裁剪平均值的雏形。&lt;/p&gt;
&lt;p&gt;现代统计学在 20 世纪的真正起飞要归功于数据的收集和发布。统计学巨匠之一罗纳德·费雪（1890–1962）对统计学理论和统计学在基因学中的应用功不可没。他发明的许多算法和公式，例如线性判别分析和费雪信息，仍经常被使用。即使是他在 1936 年发布的 Iris 数据集，仍然偶尔被用于演示机器学习算法。&lt;/p&gt;
&lt;p&gt;克劳德·香农（1916–2001）的信息论以及阿兰·图灵（1912–1954）的计算理论也对机器学习有深远影响。图灵在他著名的论文《计算机器与智能》中提出了“机器可以思考吗？”这样一个问题 [1]。在他描述的“图灵测试”中，如果一个人在使用文本交互时不能区分他的对话对象到底是人类还是机器的话，那么即可认为这台机器是有智能的。时至今日，智能机器的发展可谓日新月异。&lt;/p&gt;
&lt;p&gt;另一个对深度学习有重大影响的领域是神经科学与心理学。既然人类显然能够展现出智能，那么对于解释并逆向工程人类智能机理的探究也在情理之中。最早的算法之一是由唐纳德·赫布（1904–1985）正式提出的。在他开创性的著作《行为的组织》中，他提出神经是通过正向强化来学习的，即赫布理论。赫布理论是感知机学习算法的原型，并成为支撑今日深度学习的随机梯度下降算法的基石：强化合意的行为、惩罚不合意的行为，最终获得优良的神经网络参数。&lt;/p&gt;
&lt;p&gt;来源于生物学的灵感是神经网络名字的由来。这类研究者可以追溯到一个多世纪前的亚历山大·贝恩（1818–1903）和查尔斯·斯科特·谢灵顿（1857–1952）。研究者们尝试组建模仿神经元互动的计算电路。随着时间发展，神经网络的生物学解释被稀释，但仍保留了这个名字。时至今日，绝大多数神经网络都包含以下的核心原则。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;交替使用线性处理单元与非线性处理单元，它们经常被称为“层”。&lt;/li&gt;
&lt;li&gt;使用链式法则（即反向传播）来更新网络的参数。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在最初的快速发展之后，自约 1995 年起至 2005 年，大部分机器学习研究者的视线从神经网络上移开了。这是由于多种原因。首先，训练神经网络需要极强的计算力。尽管 20 世纪末内存已经足够，计算力却不够充足。其次，当时使用的数据集也相对小得多。费雪在 1936 年发布的的 Iris 数据集仅有 150 个样本，并被广泛用于测试算法的性能。具有 6 万个样本的 MNIST 数据集在当时已经被认为是非常庞大了，尽管它如今已被认为是典型的简单数据集。由于数据和计算力的稀缺，从经验上来说，如核方法、决策树和概率图模型等统计工具更优。它们不像神经网络一样需要长时间的训练，并且在强大的理论保证下提供可以预测的结果。&lt;/p&gt;
&lt;h1 id=&#34;深度学习与机器学习&#34;&gt;深度学习与机器学习&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;“Deep learning is a branch of machine learning based on a set of algorithms that attempt to model high-level abstractions in data by using multiple processing layers, with complex structures or otherwise, composed of multiple non-linear transformations.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;传统的机器学习主要分两部分，一部分是特征提取，需要人工从数据中提取出特征，并且想办法用计算机能理解的格式来表达这些特征。另外一部分是是模型，使用计算机能理解的特征数据来训练模型。人工提取的特征慢，而且很容易遗漏。深度学习能够从原始数据中学习出特征，然后用来训练模型。理论上来说 参数越多的模型复杂度越高 “ 容量 ” (capacity) 越大, 这意味着它能完成吏复杂的学习任务 但一股倩形下, 复杂模型的训练效率低 易陷入过拟合, 因此难以受到人们青睐。机器学习研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出，而本书要重点探讨的深度学习是具有多级表示的表征学习方法。在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。&lt;/p&gt;
&lt;p&gt;而随着云计算、大数据时代的到来 计算能力 的大幅提高可缓解训练低效性 训练数据的大幅增加则可降低过拟合 f 因此, 以 &amp;ldquo;深度学习” (deep1earning) 为代表的复杂模型开始受到人们的关注。典型的深度学习模型就是很深层的神经网络 . 显然, 对神经网络模型 提高容量的一个简单办法是增加隐层的数且 隐层多工 相应的神经元连接机 阈值等参数就会更多模型复杂度也可通过单纯增加隐层神经元的数目来实玑前面我们谈到过, 单隐层的多层呤前馈网络己具有很强大的学习能力 ; 但从增加模型复杂度的角觑肴 增加隐层的数目显然比增加隐层神经元的数目更眦因为增加隐层数不仅增加丁拥有激活函数的神经元数目, 还增加了激活函数嵌套的层数 然而, 多隐层神经网络难以直接用经典算法恻如标准 BP 算淘进行训练, 因为误差在多隐层内逆传播盹 往往会 “ 发散 ” (diverge) 而不能收敛到稳定状态&lt;/p&gt;
&lt;p&gt;深度学习可以逐级表示越来越抽象的概念或模式。以图像为例，它的输入是一堆原始像素值。深度学习模型中，图像可以逐级表示为特定位置和角度的边缘、由边缘组合得出的花纹、由多种花纹进一步汇合得到的特定部位的模式等。最终，模型能够较容易根据更高级的表示完成给定的任务，如识别图像中的物体。值得一提的是，作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。&lt;/p&gt;
&lt;p&gt;因此，深度学习的一个外在特点是端到端的训练。也就是说，并不是将单独调试的部分拼凑起来组成一个系统，而是将整个系统组建好之后一起训练。比如说，计算机视觉科学家之前曾一度将特征抽取与机器学习模型的构建分开处理，像是 Canny 边缘探测 [20] 和 SIFT 特征提取 [21] 曾占据统治性地位达 10 年以上，但这也就是人类能找到的最好方法了。当深度学习进入这个领域后，这些特征提取方法就被性能更强的自动优化的逐级过滤器替代了。&lt;/p&gt;
&lt;p&gt;相似地，在自然语言处理领域，词袋模型多年来都被认为是不二之选 [22]。词袋模型是将一个句子映射到一个词频向量的模型，但这样的做法完全忽视了单词的排列顺序或者句中的标点符号。不幸的是，我们也没有能力来手工抽取更好的特征。但是自动化的算法反而可以从所有可能的特征中搜寻最好的那个，这也带来了极大的进步。例如，语义相关的词嵌入能够在向量空间中完成如下推理：“柏林 - 德国 + 中国 = 北京”。可以看出，这些都是端到端训练整个系统带来的效果。&lt;/p&gt;
&lt;p&gt;除端到端的训练以外，我们也正在经历从含参数统计模型转向完全无参数的模型。当数据非常稀缺时，我们需要通过简化对现实的假设来得到实用的模型。当数据充足时，我们就可以用能更好地拟合现实的无参数模型来替代这些含参数模型。这也使我们可以得到更精确的模型，尽管需要牺牲一些可解释性。&lt;/p&gt;
&lt;p&gt;相对其它经典的机器学习方法而言，深度学习的不同在于：对非最优解的包容、对非凸非线性优化的使用，以及勇于尝试没有被证明过的方法。这种在处理统计问题上的新经验主义吸引了大量人才的涌入，使得大量实际问题有了更好的解决方案。尽管大部分情况下需要为深度学习修改甚至重新发明已经存在数十年的工具，但是这绝对是一件非常有意义并令人兴奋的事。&lt;/p&gt;
&lt;p&gt;最后，深度学习社区长期以来以在学术界和企业之间分享工具而自豪，并开源了许多优秀的软件库、统计模型和预训练网络。正是本着开放开源的精神，本书的内容和基于它的教学视频可以自由下载和随意分享。我们致力于为所有人降低学习深度学习的门槛，并希望大家从中获益。&lt;/p&gt;
&lt;h1 id=&#34;deep-architectures&#34;&gt;Deep Architectures&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>深度学习前沿</title>
      <link>https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%89%8D%E6%B2%BF/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%89%8D%E6%B2%BF/</guid>
      <description>&lt;h1 id=&#34;深度学习前沿&#34;&gt;深度学习前沿&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>深度学习算法</title>
      <link>https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/deeplearning-notes/00.%E5%AF%BC%E8%AE%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</guid>
      <description>&lt;h1 id=&#34;深度学习算法&#34;&gt;深度学习算法&lt;/h1&gt;
&lt;h1 id=&#34;links&#34;&gt;Links&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://theaisummer.com/Deep-Learning-Algorithms/#neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://theaisummer.com/Deep-Learning-Algorithms/#neural-networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
