<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>激活函数 | Next-gen Tech Edu</title>
    <link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link>
      <atom:link href="https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/index.xml" rel="self" type="application/rss+xml" />
    <description>激活函数</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://ng-tech.icu/media/sharing.png</url>
      <title>激活函数</title>
      <link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link>
    </image>
    
    <item>
      <title>ReLU</title>
      <link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu/</guid>
      <description>&lt;h1 id=&#34;relu-函数&#34;&gt;ReLU 函数&lt;/h1&gt;
&lt;p&gt;ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素 $x$，该函数定义为：&lt;/p&gt;
&lt;p&gt;$$
\operatorname{ReLU}(x)=\max (x, 0)
$$&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://assets.ng-tech.icu/item/20230416205129.png&#34; alt=&#34;ReLU 对比&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;显然，当输入为负数时，ReLU 函数的导数为 0；当输入为正数时，ReLU 函数的导数为 1。尽管输入为 0 时 ReLU 函数不可导，但是我们可以取此处的导数为 0。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>sigmoid</title>
      <link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid/</guid>
      <description>&lt;h1 id=&#34;sigmoid-函数&#34;&gt;sigmoid 函数&lt;/h1&gt;
&lt;p&gt;sigmoid 函数可以将元素的值变换到 0 和 1 之间：&lt;/p&gt;
&lt;p&gt;$$
\operatorname{sigmoid}(x)=\frac{1}{1+\exp (-x)}
$$&lt;/p&gt;
&lt;p&gt;sigmoid 函数在早期的神经网络中较为普遍，但它目前逐渐被更简单的 ReLU 函数取代。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://assets.ng-tech.icu/item/20230416204701.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;依据链式法则，sigmoid 函数的导数为：&lt;/p&gt;
&lt;p&gt;$$
\text { sigmoid }^{\prime}(x)=\operatorname{sigmoid}(x)(1-\operatorname{sigmoid}(x))
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>tanh</title>
      <link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh/</guid>
      <description>&lt;h1 id=&#34;tanh-函数&#34;&gt;tanh 函数&lt;/h1&gt;
&lt;p&gt;tanh（双曲正切）函数可以将元素的值变换到-1 和 1 之间：&lt;/p&gt;
&lt;p&gt;$$
\tanh (x)=\frac{1-\exp (-2 x)}{1+\exp (-2 x)}
$$&lt;/p&gt;
&lt;p&gt;我们接着绘制 tanh 函数。当输入接近 0 时，tanh 函数接近线性变换。虽然该函数的形状和 sigmoid 函数的形状很像，但 tanh 函数在坐标系的原点上对称。&lt;/p&gt;
&lt;p&gt;
















  &lt;figure  &gt;
    &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
      &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://assets.ng-tech.icu/item/20230416204757.png&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
    &lt;/div&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;依据链式法则，tanh 函数的导数：&lt;/p&gt;
&lt;p&gt;$$
\tanh ^{\prime}(x)=1-\tanh ^{2}(x)
$$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
