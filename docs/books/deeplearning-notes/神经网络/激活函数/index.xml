<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>激活函数 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link><atom:link href="https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/index.xml" rel="self" type="application/rss+xml"/><description>激活函数</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>激活函数</title><link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</link></image><item><title>ReLU</title><link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/relu/</guid><description>&lt;h1 id="relu-函数">ReLU 函数&lt;/h1>
&lt;p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素 $x$，该函数定义为：&lt;/p>
&lt;p>$$
\operatorname{ReLU}(x)=\max (x, 0)
$$&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://assets.ng-tech.icu/item/20230416205129.png" alt="ReLU 对比" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>显然，当输入为负数时，ReLU 函数的导数为 0；当输入为正数时，ReLU 函数的导数为 1。尽管输入为 0 时 ReLU 函数不可导，但是我们可以取此处的导数为 0。&lt;/p></description></item><item><title>sigmoid</title><link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/sigmoid/</guid><description>&lt;h1 id="sigmoid-函数">sigmoid 函数&lt;/h1>
&lt;p>sigmoid 函数可以将元素的值变换到 0 和 1 之间：&lt;/p>
&lt;p>$$
\operatorname{sigmoid}(x)=\frac{1}{1+\exp (-x)}
$$&lt;/p>
&lt;p>sigmoid 函数在早期的神经网络中较为普遍，但它目前逐渐被更简单的 ReLU 函数取代。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://assets.ng-tech.icu/item/20230416204701.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>依据链式法则，sigmoid 函数的导数为：&lt;/p>
&lt;p>$$
\text { sigmoid }^{\prime}(x)=\operatorname{sigmoid}(x)(1-\operatorname{sigmoid}(x))
$$&lt;/p></description></item><item><title>tanh</title><link>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/deeplearning-notes/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/tanh/</guid><description>&lt;h1 id="tanh-函数">tanh 函数&lt;/h1>
&lt;p>tanh（双曲正切）函数可以将元素的值变换到-1 和 1 之间：&lt;/p>
&lt;p>$$
\tanh (x)=\frac{1-\exp (-2 x)}{1+\exp (-2 x)}
$$&lt;/p>
&lt;p>我们接着绘制 tanh 函数。当输入接近 0 时，tanh 函数接近线性变换。虽然该函数的形状和 sigmoid 函数的形状很像，但 tanh 函数在坐标系的原点上对称。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://assets.ng-tech.icu/item/20230416204757.png" alt="" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>依据链式法则，tanh 函数的导数：&lt;/p>
&lt;p>$$
\tanh ^{\prime}(x)=1-\tanh ^{2}(x)
$$&lt;/p></description></item></channel></rss>