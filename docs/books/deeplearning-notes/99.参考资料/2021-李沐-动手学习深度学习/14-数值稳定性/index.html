<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="14-数值稳定性+模型初始化和激活函数 本节目录 1. 数值稳定性 1.1 神经网络的梯度 1.2 数值稳定性的常见两个问题 1.3 例子：MLP 1.3 梯度爆炸 1.3.1 使用 ReLU 作为激活函数 1.3.2 梯度爆炸问题 1.4 梯度消失 1.4.1 使用 Sigmoid 作为激活函数 1.4.2 梯度消失的"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/"><meta property="og:title" content="14-数值稳定性 | Next-gen Tech Edu"><meta property="og:description" content="14-数值稳定性+模型初始化和激活函数 本节目录 1. 数值稳定性 1.1 神经网络的梯度 1.2 数值稳定性的常见两个问题 1.3 例子：MLP 1.3 梯度爆炸 1.3.1 使用 ReLU 作为激活函数 1.3.2 梯度爆炸问题 1.4 梯度消失 1.4.1 使用 Sigmoid 作为激活函数 1.4.2 梯度消失的"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>14-数值稳定性 | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=e895c6f332fb6fdf31f2a5f15a1bf49f><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">2021-李沐-《动手学习深度学习》</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id1cb603993076354ae684c031037738c1")' href=#id1cb603993076354ae684c031037738c1 aria-expanded=false aria-controls=id1cb603993076354ae684c031037738c1 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/>99.参考资料</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id1cb603993076354ae684c031037738c1 aria-expanded=false aria-controls=id1cb603993076354ae684c031037738c1><i class="fa-solid fa-angle-down" id=caret-id1cb603993076354ae684c031037738c1></i></a></div><ul class="nav docs-sidenav collapse show" id=id1cb603993076354ae684c031037738c1><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6d7da24804841595467ee06940ffadad")' href=#id6d7da24804841595467ee06940ffadad aria-expanded=false aria-controls=id6d7da24804841595467ee06940ffadad aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id424867f26fc15069d5e95f339f8da715")' href=#id424867f26fc15069d5e95f339f8da715 aria-expanded=false aria-controls=id424867f26fc15069d5e95f339f8da715 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/>2019-Andrew Ng-深度学习课程</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id424867f26fc15069d5e95f339f8da715 aria-expanded=false aria-controls=id424867f26fc15069d5e95f339f8da715><i class="fa-solid fa-angle-right" id=caret-id424867f26fc15069d5e95f339f8da715></i></a></div><ul class="nav docs-sidenav collapse" id=id424867f26fc15069d5e95f339f8da715><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/interview/>interview</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week1/>lesson1-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week2/>lesson1-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week3/>lesson1-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week4/>lesson1-week4</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week1/>lesson2-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week2/>lesson2-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week3/>lesson2-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson3-week1/>lesson3-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson3-week2/>lesson3-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week1/>lesson4-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week2/>lesson4-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week3/>lesson4-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week4/>lesson4-week4</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week1/>lesson5-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week2/>lesson5-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week3/>lesson5-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/math/>math</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/notation/>notation</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/summary/>SUMMARY</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6d61b824616f345f413069d041a91bcd")' href=#id6d61b824616f345f413069d041a91bcd aria-expanded=false aria-controls=id6d61b824616f345f413069d041a91bcd aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>2021-李沐-《动手学习深度学习》</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id6d61b824616f345f413069d041a91bcd aria-expanded=false aria-controls=id6d61b824616f345f413069d041a91bcd><i class="fa-solid fa-angle-down" id=caret-id6d61b824616f345f413069d041a91bcd></i></a></div><ul class="nav docs-sidenav collapse show" id=id6d61b824616f345f413069d041a91bcd><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00-%E9%A2%84%E5%91%8A/>00-预告</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01-%E8%AF%BE%E7%A8%8B%E5%AE%89%E6%8E%92/>01-课程安排</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>02-深度学习介绍</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03-%E5%AE%89%E8%A3%85/>03-安装</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C/>04-数据读取和操作</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/>05-线性代数</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06-%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/>06-矩阵计算</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/>07-自动求导</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/08-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92+%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/>08-线性回归+基础优化算法</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09-softmax%E5%9B%9E%E5%BD%92/>09-softmax回归</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/10-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/>10-多层感知机</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/11-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9+%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88/>11-模型选择+过拟合和欠拟合</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/12-%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80/>12-权重衰退</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/13-%E4%B8%A2%E5%BC%83%E6%B3%95/>13-丢弃法</a></li><li class="child level active"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/>14-数值稳定性</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/15-%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/>15-实战Kaggle比赛：预测房价</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/16-pytorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/>16-Pytorch神经网络基础</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/17-%E4%BD%BF%E7%94%A8%E5%92%8C%E8%B4%AD%E4%B9%B0gpu/>17-使用和购买GPU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/18-%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7%E7%AB%9E%E8%B5%9B%E6%80%BB%E7%BB%93/>18-预测房价竞赛总结</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/19-%E5%8D%B7%E7%A7%AF%E5%B1%82/>19-卷积层</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/20-%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85/>20-填充和步幅</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/21-%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93/>21-多输入输出通道</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/22-%E6%B1%A0%E5%8C%96%E5%B1%82/>22-池化层</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/23-%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Clenet/>23-经典卷积神经网络LeNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/24-alexnet/>24-AlexNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/25-%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9Cvgg/>25-使用块的网络VGG</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/26-nin/>26-NiN</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/27-googlenet/>27-GoogLeNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/28-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/>28-批量归一化</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/29-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet/>29-残差网络ResNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/30-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E5%AE%8C%E7%BB%93%E7%AB%9E%E8%B5%9B%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/>30-第二部分完结竞赛：图片分类</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/31-cpu%E5%92%8Cgpu/>31-CPU和GPU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/32-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%A1%AC%E4%BB%B6/>32-深度学习硬件</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/33-%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C/>33-单机多卡并行</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/34-%E5%A4%9Agpu%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0only-qa/>34-多GPU训练实现(only QA)</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/35-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>35-分布式训练</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/36-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF/>36-数据增广</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/37-%E5%BE%AE%E8%B0%83/>37-微调</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/38-%E7%AC%AC%E4%BA%8C%E6%AC%A1%E7%AB%9E%E8%B5%9B%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C/>38-第二次竞赛树叶分类结果</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/39-%E5%AE%9E%E6%88%98kaggle%E7%AB%9E%E8%B5%9Bcifar-10/>39-实战Kaggle竞赛：CIFAR-10</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/41-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86/>41-物体检测和数据集</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/43-%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E7%AB%9E%E8%B5%9B%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/>43-树叶分类竞赛技术总结</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/44-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95r-cnnssdyolo/>44-物体检测算法：R-CNN,SSD,YOLO</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/46-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/>46-语义分割</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/47-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF/>47-转置卷积</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/48-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cfcn/>48-全连接卷积神经网络（FCN）</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/49-%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB/>49-样式迁移</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/50-%E8%AF%BE%E7%A8%8B%E7%AB%9E%E8%B5%9B%E7%89%9B%E4%BB%94%E8%A1%8C%E5%A4%B4%E6%A3%80%E6%B5%8B/>50-课程竞赛：牛仔行头检测</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/51-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/>51-序列模型</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/53-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>53-语言模型</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/54-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/>54-循环神经网络RNN</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/56-gru/>56-GRU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/57-lstm/>57-LSTM</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/58-%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>58-深层循环神经网络</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/61-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84/>61-编码器-解码器架构</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/62-%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0/>62-序列到序列学习</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/63-%E6%9D%9F%E6%90%9C%E7%B4%A2/>63-束搜索</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/65-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0/>65-注意力分数</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/68-transformer/>68-Transformer</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/69-bert%E9%A2%84%E8%AE%AD%E7%BB%83/>69-bert预训练</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/70-bert%E5%BE%AE%E8%B0%83/>70-BERT微调</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/72-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/>72-优化算法</a></li></ul></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#14-数值稳定性模型初始化和激活函数>14-数值稳定性+模型初始化和激活函数</a><ul><li><a href=#本节目录>本节目录</a></li><li><a href=#1-数值稳定性>1. 数值稳定性</a></li><li><a href=#2-模型初始化和激活函数>2. 模型初始化和激活函数</a></li><li><a href=#3-总结>3. 总结</a></li><li><a href=#4qa>4.Q&A</a></li></ul></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>14-数值稳定性</h1><div class=article-style><h2 id=14-数值稳定性模型初始化和激活函数>14-数值稳定性+模型初始化和激活函数</h2><h3 id=本节目录>本节目录</h3><ul><li><a href=#1-%e6%95%b0%e5%80%bc%e7%a8%b3%e5%ae%9a%e6%80%a7>1. 数值稳定性</a><ul><li><a href=#11-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%a2%af%e5%ba%a6>1.1 神经网络的梯度</a></li><li><a href=#12-%e6%95%b0%e5%80%bc%e7%a8%b3%e5%ae%9a%e6%80%a7%e7%9a%84%e5%b8%b8%e8%a7%81%e4%b8%a4%e4%b8%aa%e9%97%ae%e9%a2%98>1.2 数值稳定性的常见两个问题</a></li><li><a href=#13-%e4%be%8b%e5%ad%90mlp>1.3 例子：MLP</a></li><li><a href=#13-%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8>1.3 梯度爆炸</a><ul><li><a href=#131-%e4%bd%bf%e7%94%a8relu%e4%bd%9c%e4%b8%ba%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>1.3.1 使用 ReLU 作为激活函数</a></li><li><a href=#132-%e6%a2%af%e5%ba%a6%e7%88%86%e7%82%b8%e9%97%ae%e9%a2%98>1.3.2 梯度爆炸问题</a></li></ul></li><li><a href=#14-%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1>1.4 梯度消失</a><ul><li><a href=#141-%e4%bd%bf%e7%94%a8sigmoid%e4%bd%9c%e4%b8%ba%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>1.4.1 使用 Sigmoid 作为激活函数</a></li><li><a href=#142-%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e7%9a%84%e9%97%ae%e9%a2%98>1.4.2 梯度消失的问题</a></li></ul></li></ul></li><li><a href=#2-%e6%a8%a1%e5%9e%8b%e5%88%9d%e5%a7%8b%e5%8c%96%e5%92%8c%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>2. 模型初始化和激活函数</a><ul><li><a href=#21-%e8%ae%a9%e8%ae%ad%e7%bb%83%e6%9b%b4%e5%8a%a0%e7%a8%b3%e5%ae%9a>2.1 让训练更加稳定</a></li><li><a href=#22-%e5%9f%ba%e6%9c%ac%e5%81%87%e8%ae%be%e8%ae%a9%e6%af%8f%e5%b1%82%e7%9a%84%e5%9d%87%e5%80%bc%e6%96%b9%e5%b7%ae%e6%98%af%e4%b8%80%e4%b8%aa%e5%b8%b8%e6%95%b0>2.2 基本假设：让每层的均值/方差是一个常数</a></li><li><a href=#23-%e6%9d%83%e9%87%8d%e5%88%9d%e5%a7%8b%e5%8c%96>2.3 权重初始化</a></li><li><a href=#24-%e4%be%8b%e5%ad%90mlp>2.4 例子：MLP</a><ul><li><a href=#241-%e6%a8%a1%e5%9e%8b%e5%81%87%e8%ae%be>2.4.1 模型假设</a></li><li><a href=#242-%e6%ad%a3%e5%90%91%e6%96%b9%e5%b7%ae>2.4.2 正向方差</a></li><li><a href=#243-%e5%8f%8d%e5%90%91%e5%9d%87%e5%80%bc%e5%92%8c%e6%96%b9%e5%b7%ae>2.4.3 反向均值和方差</a></li><li><a href=#244-xavier%e5%88%9d%e5%a7%8b>2.4.4 Xavier 初始</a></li><li><a href=#245-%e5%81%87%e8%ae%be%e7%ba%bf%e6%80%a7%e7%9a%84%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>2.4.5 假设线性的激活函数</a></li><li><a href=#246-%e6%a3%80%e6%9f%a5%e5%b8%b8%e7%94%a8%e6%bf%80%e6%b4%bb%e5%87%bd%e6%95%b0>2.4.6 检查常用激活函数</a></li></ul></li></ul></li><li><a href=#3-%e6%80%bb%e7%bb%93>3. 总结</a></li><li><a href=#4qa>4.Q&A</a></li></ul><h3 id=1-数值稳定性>1. 数值稳定性</h3><p>数值稳定性是深度学习中比较重要的点，特别是当神经网络变得很深的时候，数值通常很容易变得不稳定。</p><h4 id=11-神经网络的梯度>1.1 神经网络的梯度</h4><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-01.png alt=image align=center width=500></div><p><strong>考虑 d 层神经网络</strong></p><ul><li><p>t 表示层数，<img src=https://latex.codecogs.com/svg.image?h^{t-1} title=h^{t-1}>表示第<em>t-1</em>层的输出，经过一个<img src=https://latex.codecogs.com/svg.image?f_{t} title=f_{t}>函数后，得到第<em>t</em>层的输出。</p></li><li><p>最终输出 y 的表示：输入 x 经过若干层(<em>d</em>层)的函数作用，最后被损失函数作用得到输出 y。</p></li></ul><p><strong>计算损失函数<em>L</em>关于第<em>t</em>层参数<img src=https://latex.codecogs.com/svg.image?W_{t}&space; title="W_{t} ">的梯度</strong></p><ul><li><p>由链导法则得到上图中乘积公式</p></li><li><p>需要进行 d-t 次<strong>矩阵乘法</strong>（为什么是矩阵乘法？答：由于所有的<em>h</em>都是一些<strong>向量</strong>，导数中分子分母均为向量，所以求导得到的是矩阵，维数为[分子维度]x[分母维度]，可以参考第 6 节<a href=https://www.bilibili.com/video/BV1eZ4y1w7PY target=_blank rel=noopener>视频</a>和<a href=./06-%e7%9f%a9%e9%98%b5%e8%ae%a1%e7%ae%97.md>笔记</a>）。这也是导致数值稳定性问题的<strong>主要因素</strong>，由于做了太多次的矩阵乘法。</p></li></ul><h4 id=12-数值稳定性的常见两个问题>1.2 数值稳定性的常见两个问题</h4><p><strong>梯度爆炸</strong></p><p>假设梯度都是一些比 1 大的数比如 1.5，做 100 次乘积之后得到<img src=https://latex.codecogs.com/svg.image?4\times&space;10^{17} title="4\times 10^{17}">，这个数字很容易带来一些浮点数上限的问题（需了解更多请参考计算机系统-计算机中浮点数的存储方式）。</p><p><strong>梯度消失</strong></p><p>假设梯度都是一些比 1 小的数比如 0.8，做 100 次乘积之后得到<img src=https://latex.codecogs.com/svg.image?2\times10^{-10} title=2\times10^{-10}>，也可能会带来浮点数下溢的问题。</p><h4 id=13-例子mlp>1.3 例子：MLP</h4><p>此处我们着重探讨<a href=#11-%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%9a%84%e6%a2%af%e5%ba%a6>1.1 节</a>中所述的求梯度时所做的 d-t 次矩阵乘法，并以一个实例 MLP 来探讨其结果的具体形式。</p><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-02.png alt=image align=center width=500></div><ul><li><p>第一行公式，定义<img src=https://latex.codecogs.com/svg.image?h^{t} title=h^{t}>和<img src=https://latex.codecogs.com/svg.image?h^{t-1} title=h^{t-1}>(均为向量)的函数关系<img src=https://latex.codecogs.com/svg.image?f_{t} title=f_{t}>，第 t 层的权重矩阵作用于 t-1 层的输出<img src=https://latex.codecogs.com/svg.image?h^{t-1} title=h^{t-1}>后经过激活函数<img src=https://latex.codecogs.com/svg.image?\sigma&space; title="\sigma ">得到<img src=https://latex.codecogs.com/svg.image?h^{t} title=h^{t}>，注意激活函数<img src=https://latex.codecogs.com/svg.image?\sigma&space; title="\sigma ">逐元素计算。</p></li><li><p>第二行公式：这里用到链导法则，激活函数<img src=https://latex.codecogs.com/svg.image?\sigma&space; title="\sigma ">先对内部向量逐元素求导，然后把求导后这个向量变成对角矩阵（可以理解为链导法则中内部向量<img src=https://latex.codecogs.com/svg.image?W_{t}h_{t-1} title=W_{t}h_{t-1}>对自身进行求导，变成一个 nxn 的对角矩阵，更多请参考<a href=https://nndl.github.io/nndl-book.pdf target=_blank rel=noopener>邱锡鹏 《神经网络与深度学习》</a>[^ 图片 1]）</p><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-03.png alt=image align=center width=500></div></li></ul><ul><li>视频中<strong>勘误说明</strong>：链导法则中 <img src="https://latex.codecogs.com/svg.image?\frac{\partial&space;W^{t}h^{t-1}}{\partial&space;h^{t-1}}=&space;W^{t}" title="\frac{\partial W^{t}h^{t-1}}{\partial h^{t-1}}= W^{t}"> 而不是<img src=https://latex.codecogs.com/svg.image?\left&space;(W^{t}&space;&space;\right&space;)^{T} title="\left (W^{t} \right )^{T}">（这点由分子分母维度也容易推出），故最终求导结果包含<img src=https://latex.codecogs.com/svg.image?W^{t} title=W^{t}>，而不是其转置。</li></ul><h4 id=13-梯度爆炸>1.3 梯度爆炸</h4><h5 id=131-使用-relu-作为激活函数>1.3.1 使用 ReLU 作为激活函数</h5><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-04.png alt=image align=center width=500></div><p>由于激活函数 Relu 求导后或者是 1 或者是 0，变为对角矩阵的斜对角线元素后，与<img src=https://latex.codecogs.com/svg.image?W^{i} title=W^{i}>做乘积，斜对角线为 1 的部分会使得 W 中元素保留，最终该连乘式中有一些元素来自<img src=https://latex.codecogs.com/svg.image?\prod\left&space;(&space;W^{i}&space;\right&space;)&space; title="\prod\left ( W^{i} \right ) ">，如果大部分<img src=https://latex.codecogs.com/svg.image?W^{i} title=W^{i}>中 值都大于 1，且层数比较大，那么连乘之后可能导致梯度爆炸的问题。</p><h5 id=132-梯度爆炸问题>1.3.2 梯度爆炸问题</h5><ul><li><p>值超出值域（infinity）</p><ul><li>对于 16 位浮点数尤为严重（数值区间 [6e-5 , 6e4]），GPU 用 16 位浮点数更快</li></ul></li><li><p>对学习率敏感</p><ul><li><p>如果学习率太大 → 大参数值 → 更大的梯度，如此循环几次，容易导致梯度爆炸</p></li><li><p>如果学习率太小 → 训练无进展</p></li><li><p>我们可能需要在训练过程中不断调整学习率</p></li></ul></li></ul><h4 id=14-梯度消失>1.4 梯度消失</h4><h5 id=141-使用-sigmoid-作为激活函数>1.4.1 使用 Sigmoid 作为激活函数</h5><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-05.png alt=image align=center width=500></div><ul><li>蓝色曲线为函数值</li><li>黄色曲线为梯度，注意到当输入 x 值取 ±6 时，此时梯度已经变得很小，由图也可以看出，当输入值稍大或稍小都很容易引起小梯度。</li></ul><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-06.png alt=image align=center width=500></div><p>所以最终连乘式中<img src="https://latex.codecogs.com/svg.image?\prod&space;diag\left&space;(&space;\sigma&space;^{'}\left&space;(&space;W^{i}h^{i-1}&space;\right&space;)&space;\right&space;)&space;" title="\prod diag\left ( \sigma ^{'}\left ( W^{i}h^{i-1} \right ) \right ) ">项乘出来会很小，导致整个梯度很小，产生梯度消失问题。</p><h5 id=142-梯度消失的问题>1.4.2 梯度消失的问题</h5><ul><li><p>梯度值变为 0</p><ul><li>对 16 位浮点数尤为严重</li></ul></li><li><p>训练没有进展</p><ul><li>不管如何选择学习率，由于梯度已经为 0 了，学习率 x 梯度=0</li></ul></li><li><p>对于底部层尤为严重</p><ul><li>仅仅顶部层训练得较好。第<em>t</em>层导数包含 d-t 个矩阵乘积，越往底层走，t 越小，乘得越多，梯度消失越严重，所以底部层效果更差。</li><li>无法让神经网络更深。只能把顶部层训练得比较好，底部层跑不动，这和给一个浅的神经网络没有什么区别。</li></ul></li></ul><h3 id=2-模型初始化和激活函数>2. 模型初始化和激活函数</h3><h4 id=21-让训练更加稳定>2.1 让训练更加稳定</h4><p>我们的一个核心目标是如何让训练更稳定，梯度值不要太大也不要太小</p><ul><li><p>目标：让梯度值在合理的范围内</p><ul><li>例如 [1e-6, 1e3]</li></ul></li><li><p>常用方法：</p><ul><li><p>将乘法变加法：</p><ul><li>ResNet（跳跃连接，如果很多层，加入加法进去）</li><li>LSTM（引入记忆细胞，更新门，遗忘门，通过门权重求和，控制下一步是否更新）</li></ul></li><li><p>归一化：</p><ul><li><p>梯度归一化（归一化均值，方差）</p></li><li><p>梯度裁剪(clipping)：比如大于/小于一个固定的阈值，就让梯度等于这个阈值，将梯度限制在一个范围中。（可以缓解梯度爆炸）</p></li></ul></li><li><p>合理的权重初始和激活函数：本节课讲述重点</p></li></ul></li></ul><p><strong>下面我们重点探讨最后一种方法：合理的权重初始和激活函数</strong></p><h4 id=22-基本假设让每层的均值方差是一个常数>2.2 基本假设：让每层的均值/方差是一个常数</h4><ul><li><p><strong>将每层的输出和梯度都看做随机变量</strong></p><p>比如第 i 层有 100 维，就将输出和梯度分别看成 100 个随机变量</p></li><li><p><strong>让它们的均值和方差都保持一致</strong></p><p>我们的目标，这样不管神经网络多深，最后一层总与第一层差不多，从而不会梯度爆炸和消失</p></li></ul><p>根据我们的假设，可以列出如下方程式：</p><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-07.png alt=image align=center width=500></div><h4 id=23-权重初始化>2.3 权重初始化</h4><ul><li>在合理值区间里随机初始参数</li><li>训练<strong>开始</strong>的时候更容易有数值不稳定<ul><li>远离最优解的地方损失函数表面可能很复杂</li><li>最优解附近表面会比较平</li></ul></li><li>使用 N(0, 0.01)分布来初始可能对小网络没问题，但不能保证深度神经网络</li></ul><h4 id=24-例子mlp>2.4 例子：MLP</h4><p>下面我们以 MLP 为例，考虑需要什么条件，才能满足<a href=#22-%e5%9f%ba%e6%9c%ac%e5%81%87%e8%ae%be%ef%bc%9a%e8%ae%a9%e6%af%8f%e5%b1%82%e7%9a%84%e5%9d%87%e5%80%bc/%e6%96%b9%e5%b7%ae%e6%98%af%e4%b8%80%e4%b8%aa%e5%b8%b8%e6%95%b0>2.2 节</a>的假设。</p><h5 id=241-模型假设>2.4.1 模型假设</h5><ul><li>每一层<strong>权重</strong>中的变量均为<strong>独立同分布</strong>，并设出均值、方差。</li><li>每一层<strong>输入</strong>的变量<strong>独立于</strong>该层<strong>权重</strong>变量。同时<strong>输入变量</strong>之间<strong>独立同分布</strong>。</li><li>假设没有激活函数(先简化分析，之后会考虑有激活函数的情况)，可以求得该层输出的期望为 0。</li></ul><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-08.png alt=image align=center width=500></div><p>此处用到了一个重要性质：</p><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-09.png alt=image align=center width=500></div><p>更多均值、方差运算可以参考<a href=https://blog.csdn.net/MissXy_/article/details/80705828 target=_blank rel=noopener>期望、方差、协方差及相关系数的基本运算</a></p><h5 id=242-正向方差>2.4.2 正向方差</h5><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-10.png alt=image align=center width=500></div><ul><li><p>第二行的计算中仍然用到了<a href=241%e6%a8%a1%e5%9e%8b%e5%81%87%e8%ae%be>2.4.1 节</a>的期望的重要性质：如果两个变量独立，它们乘积的均值=均值的乘积，再结合 w 的期望为 0(注意 w 和 h 独立，w 之间独立同分布)，即有第二行末项期望为 0。</p></li><li><p>最后一行由于 wi,j 独立同分布，方差相同，加上做了 hj 独立同分布的假设，所以可以写成 <strong>[t-1 层输出维度] x [t 层权重方差] x [t-1 层输出方差]</strong> 的形式</p></li><li><p>此时，我们回过头来看我们的终极目标<a href=#22-%e5%9f%ba%e6%9c%ac%e5%81%87%e8%ae%be%ef%bc%9a%e8%ae%a9%e6%af%8f%e5%b1%82%e7%9a%84%e5%9d%87%e5%80%bc/%e6%96%b9%e5%b7%ae%e6%98%af%e4%b8%80%e4%b8%aa%e5%b8%b8%e6%95%b0>2.2 节</a>的假设，每层输出期望为 0 我们已经可以满足(2.4.1 节已经推导出)，而方差相同这一目标，通过上图的推导，我们发现需要<img src="https://latex.codecogs.com/svg.image?&space;n_{t-1}\gamma&space;_{t}=1&space;" title=" n_{t-1}\gamma _{t}=1 ">。</p></li></ul><h5 id=243-反向均值和方差>2.4.3 反向均值和方差</h5><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-11.png alt=image align=center width=500></div><p>反向的情况和正向的类似，不过此时我们需要满足的式子变为<img src="https://latex.codecogs.com/svg.image?&space;n_{t}\gamma&space;_{t}=1&space;" title=" n_{t}\gamma _{t}=1 ">。</p><h5 id=244-xavier-初始>2.4.4 Xavier 初始</h5><ul><li><p>上述推导带来的问题：难以同时满足<img src="https://latex.codecogs.com/svg.image?&space;n_{t-1}\gamma&space;_{t}=1&space;" title=" n_{t-1}\gamma _{t}=1 ">和<img src="https://latex.codecogs.com/svg.image?&space;n_{t}\gamma&space;_{t}=1&space;" title=" n_{t}\gamma _{t}=1 ">。（需要每层输出的维度都相同）</p></li><li><p>采用 Xavier 折中解决，不能同时满足上面两式，转而满足 [<strong>上面两式做加法后除以 2</strong>] 得到的式子，用两种分布进行初始化（每层方差、均值满足推导式）。</p><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-12.png alt=image align=center width=500></div></li><li><p>如果能确定每层输入、输出维度大小，则能确定该层权重的方差大小。</p></li><li><p>权重初始化方式：正态分布、均匀分布，均值/方差满足 Xavier 的假设。</p></li></ul><h5 id=245-假设线性的激活函数>2.4.5 假设线性的激活函数</h5><p>真实情况下，我们并不会用线性的激活函数（这样相当于没有进行激活），这里为了简化问题，假设激活函数是线性的。</p><ul><li><strong>正向</strong></li></ul><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-13.png alt=image align=center width=500></div><p>上述推导表明，为了使得前向传播的均值为 0，方差固定的话，激活函数必须 f(x)=x，这种恒等映射。</p><ul><li><strong>反向</strong></li></ul><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-14.png alt=image align=center width=500></div><p>PPT 上的推导似乎有点问题（上图中第二行方程），笔者重新进行了下述推导，读者也可自行推导验证：</p><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-15.png alt=image align=center width=500></div><p><strong>通过正向和反向的推导，我们可以得出的【结论】是：当激活函数为 f(x)=x，这种恒等映射更有利于维持神经网络的稳定性。</strong></p><h5 id=246-检查常用激活函数>2.4.6 检查常用激活函数</h5><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/14/14-16.png alt=image align=center width=500></div><p>对于常用激活函数：tanh，relu 满足在零点附近有 f(x)=x，而 sigmoid 函数在零点附近不满足要求，可以对 sigmoid 函数进行调整（根据 Taylor 展开式，调整其过原点）</p><h3 id=3-总结>3. 总结</h3><ul><li><p>当数值过大或者过小时，会导致数值问题。</p></li><li><p>常发生在深度模型中，因为其会对 n 个数累乘。</p></li><li><p>合理的权重初始值(如 Xavier)和激活函数的选取(如 relu, tanh, 调整后的 sigmoid)可以提升数值稳定性。</p></li></ul><h3 id=4qa>4.Q&A</h3><p><strong>问题：nan, inf 是怎么产生的以及怎么解决的？</strong></p><blockquote><p>NaN 和 Inf 怎么产生的：参考<a href=https://blog.csdn.net/qq_16334327/article/details/86526854 target=_blank rel=noopener>出现 nan、inf 原因</a></p></blockquote><blockquote><p>如何解决：参考<a href=https://blog.csdn.net/u011119817/article/details/103908065 target=_blank rel=noopener>深度学习中 nan 和 inf 的解决</a>以及[训练网络 loss 出现 Nan 解决办法 ](<a href="https://zhuanlan.zhihu.com/p/89588946#:~:text=" target=_blank rel=noopener>https://zhuanlan.zhihu.com/p/89588946#:~:text=</a>一般来说，出现NaN有以下几种情况： 1.,如果在迭代的 100 轮以内，出现 NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。 可以不断降低学习率直至不出现 NaN 为止，一般来说低于现有学习率 1-10 倍即可。)</p></blockquote><p><strong>问题：训练过程中，如果网络层的输出的中间层特征元素的值突然变成 nan 了，是发生梯度爆炸了吗？</strong></p><blockquote><p>参考[训练网络 loss 出现 Nan 解决办法 ](<a href="https://zhuanlan.zhihu.com/p/89588946#:~:text=" target=_blank rel=noopener>https://zhuanlan.zhihu.com/p/89588946#:~:text=</a>一般来说，出现NaN有以下几种情况： 1.,如果在迭代的 100 轮以内，出现 NaN，一般情况下的原因是因为你的学习率过高，需要降低学习率。 可以不断降低学习率直至不出现 NaN 为止，一般来说低于现有学习率 1-10 倍即可。)</p></blockquote><p><strong>问题：老师，让每层方差是一个常数的方法，您指的是 batch normalization 吗？想问一下 bn 层为什么要有伽马和贝塔？去掉可以吗</strong></p><blockquote><p>让每层方差是一个常数，和 batch norm 没有太多关系，(本节课介绍的方法是合理地初始化权重和设置激活函数)。batch norm 可以让你的输出变成一个均值为 0，方差差不多是一个固定值的东西，但它不一定能保证你的梯度。</p></blockquote><p>(此处节选几个重要的 Q&A，建议观看完整 Q&A，获得更深的理解)</p></div><div class=article-widget><div class="container-xl row post-nav"><div class="col-6 post-nav-item"><div class=meta-nav>上一页</div><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/13-%E4%B8%A2%E5%BC%83%E6%B3%95/ rel=next>13-丢弃法</a></div><div class="col-6 post-nav-item"><div class=meta-nav>下一页</div><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/15-%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/ rel=prev>15-实战Kaggle比赛：预测房价</a></div></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div><script type=text/javascript id=clstr_globe async src="//clustrmaps.com/globe.js?d=kgpJG5sWZQpKujBmD-uW1B54-WBPol-DuDtrB2KFjKs"></script></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>