<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="04-数据读取和操作 本节目录 1. 数据操作 1.1 基本操作 1.2 简单运算 1.3 广播机制 1.4 索引和切片 1.5 节约内存 1.6 转换为其他Python对象 2. 数据预处理 2.1 读取数据集 2.2 处理缺失值 2.3 转换为张量格式 3. Q&A 1. 数据操作 为了能够完成各种"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C/"><meta property="og:title" content="04-数据读取和操作 | Next-gen Tech Edu"><meta property="og:description" content="04-数据读取和操作 本节目录 1. 数据操作 1.1 基本操作 1.2 简单运算 1.3 广播机制 1.4 索引和切片 1.5 节约内存 1.6 转换为其他Python对象 2. 数据预处理 2.1 读取数据集 2.2 处理缺失值 2.3 转换为张量格式 3. Q&A 1. 数据操作 为了能够完成各种"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>04-数据读取和操作 | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=ba37a4197b24499bc154c60359098eae><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">2021-李沐-《动手学习深度学习》</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id1cb603993076354ae684c031037738c1")' href=#id1cb603993076354ae684c031037738c1 aria-expanded=false aria-controls=id1cb603993076354ae684c031037738c1 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/>99.参考资料</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id1cb603993076354ae684c031037738c1 aria-expanded=false aria-controls=id1cb603993076354ae684c031037738c1><i class="fa-solid fa-angle-down" id=caret-id1cb603993076354ae684c031037738c1></i></a></div><ul class="nav docs-sidenav collapse show" id=id1cb603993076354ae684c031037738c1><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6d7da24804841595467ee06940ffadad")' href=#id6d7da24804841595467ee06940ffadad aria-expanded=false aria-controls=id6d7da24804841595467ee06940ffadad aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id424867f26fc15069d5e95f339f8da715")' href=#id424867f26fc15069d5e95f339f8da715 aria-expanded=false aria-controls=id424867f26fc15069d5e95f339f8da715 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/>2019-Andrew Ng-深度学习课程</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id424867f26fc15069d5e95f339f8da715 aria-expanded=false aria-controls=id424867f26fc15069d5e95f339f8da715><i class="fa-solid fa-angle-right" id=caret-id424867f26fc15069d5e95f339f8da715></i></a></div><ul class="nav docs-sidenav collapse" id=id424867f26fc15069d5e95f339f8da715><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/interview/>interview</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week1/>lesson1-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week2/>lesson1-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week3/>lesson1-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week4/>lesson1-week4</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week1/>lesson2-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week2/>lesson2-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week3/>lesson2-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson3-week1/>lesson3-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson3-week2/>lesson3-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week1/>lesson4-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week2/>lesson4-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week3/>lesson4-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week4/>lesson4-week4</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week1/>lesson5-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week2/>lesson5-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week3/>lesson5-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/math/>math</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/notation/>notation</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/summary/>SUMMARY</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6d61b824616f345f413069d041a91bcd")' href=#id6d61b824616f345f413069d041a91bcd aria-expanded=false aria-controls=id6d61b824616f345f413069d041a91bcd aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>2021-李沐-《动手学习深度学习》</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id6d61b824616f345f413069d041a91bcd aria-expanded=false aria-controls=id6d61b824616f345f413069d041a91bcd><i class="fa-solid fa-angle-down" id=caret-id6d61b824616f345f413069d041a91bcd></i></a></div><ul class="nav docs-sidenav collapse show" id=id6d61b824616f345f413069d041a91bcd><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00-%E9%A2%84%E5%91%8A/>00-预告</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01-%E8%AF%BE%E7%A8%8B%E5%AE%89%E6%8E%92/>01-课程安排</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>02-深度学习介绍</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03-%E5%AE%89%E8%A3%85/>03-安装</a></li><li class="child level active"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C/>04-数据读取和操作</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/>05-线性代数</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06-%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/>06-矩阵计算</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/>07-自动求导</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/08-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92+%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/>08-线性回归+基础优化算法</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09-softmax-%E5%9B%9E%E5%BD%92/>09-Softmax 回归</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/10-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/>10-多层感知机</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/11-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9+%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88/>11-模型选择+过拟合和欠拟合</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/12-%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80/>12-权重衰退</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/13-%E4%B8%A2%E5%BC%83%E6%B3%95/>13-丢弃法</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/>14-数值稳定性</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/15-%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/>15-实战Kaggle比赛：预测房价</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/16-pytorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/>16-Pytorch神经网络基础</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/17-%E4%BD%BF%E7%94%A8%E5%92%8C%E8%B4%AD%E4%B9%B0gpu/>17-使用和购买GPU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/18-%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7%E7%AB%9E%E8%B5%9B%E6%80%BB%E7%BB%93/>18-预测房价竞赛总结</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/19-%E5%8D%B7%E7%A7%AF%E5%B1%82/>19-卷积层</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/20-%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85/>20-填充和步幅</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/21-%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93/>21-多输入输出通道</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/22-%E6%B1%A0%E5%8C%96%E5%B1%82/>22-池化层</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/23-%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Clenet/>23-经典卷积神经网络LeNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/24-alexnet/>24-AlexNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/25-%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C-vgg/>25-使用块的网络 VGG</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/26-nin/>26-NiN</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/27-googlenet/>27-GoogLeNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/28-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/>28-批量归一化</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/29-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet/>29-残差网络ResNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/30-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E5%AE%8C%E7%BB%93%E7%AB%9E%E8%B5%9B%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/>30-第二部分完结竞赛：图片分类</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/31-cpu%E5%92%8Cgpu/>31-CPU和GPU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/32-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%A1%AC%E4%BB%B6/>32-深度学习硬件</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/33-%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C/>33-单机多卡并行</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/34-%E5%A4%9Agpu%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0only-qa/>34-多GPU训练实现(only QA)</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/35-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>35-分布式训练</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/36-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF/>36-数据增广</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/37-%E5%BE%AE%E8%B0%83/>37-微调</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/38-%E7%AC%AC%E4%BA%8C%E6%AC%A1%E7%AB%9E%E8%B5%9B%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C/>38-第二次竞赛树叶分类结果</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/39-%E5%AE%9E%E6%88%98kaggle%E7%AB%9E%E8%B5%9Bcifar-10/>39-实战Kaggle竞赛：CIFAR-10</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/41-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86/>41-物体检测和数据集</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/43-%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E7%AB%9E%E8%B5%9B%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/>43-树叶分类竞赛技术总结</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/44-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95r-cnnssdyolo/>44-物体检测算法：R-CNN,SSD,YOLO</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/46-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/>46-语义分割</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/47-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF/>47-转置卷积</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/48-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cfcn/>48-全连接卷积神经网络（FCN）</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/49-%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB/>49-样式迁移</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/50-%E8%AF%BE%E7%A8%8B%E7%AB%9E%E8%B5%9B%E7%89%9B%E4%BB%94%E8%A1%8C%E5%A4%B4%E6%A3%80%E6%B5%8B/>50-课程竞赛：牛仔行头检测</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/51-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/>51-序列模型</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/53-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>53-语言模型</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/54-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/>54-循环神经网络RNN</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/56-gru/>56-GRU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/57-lstm/>57-LSTM</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/58-%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>58-深层循环神经网络</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/61-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84/>61-编码器-解码器架构</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/62-%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0/>62-序列到序列学习</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/63-%E6%9D%9F%E6%90%9C%E7%B4%A2/>63-束搜索</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/65-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0/>65-注意力分数</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/68-transformer/>68-Transformer</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/69-bert-%E9%A2%84%E8%AE%AD%E7%BB%83/>69-BERT 预训练</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/70-bert-%E5%BE%AE%E8%B0%83/>70-BERT 微调</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/72-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/>72-优化算法</a></li></ul></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#04-数据读取和操作>04-数据读取和操作</a><ul><li><a href=#本节目录>本节目录</a></li><li><a href=#1-数据操作>1. 数据操作</a></li><li><a href=#2-数据预处理>2. 数据预处理</a></li><li><a href=#3-qa>3. Q&A</a></li></ul></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>04-数据读取和操作</h1><div class=article-style><h2 id=04-数据读取和操作>04-数据读取和操作</h2><h3 id=本节目录>本节目录</h3><ul><li><a href=#1-%e6%95%b0%e6%8d%ae%e6%93%8d%e4%bd%9c>1. 数据操作</a><ul><li><a href=#11-%e5%9f%ba%e6%9c%ac%e6%93%8d%e4%bd%9c>1.1 基本操作</a></li><li><a href=#12-%e7%ae%80%e5%8d%95%e8%bf%90%e7%ae%97>1.2 简单运算</a></li><li><a href=#13-%e5%b9%bf%e6%92%ad%e6%9c%ba%e5%88%b6>1.3 广播机制</a></li><li><a href=#14-%e7%b4%a2%e5%bc%95%e5%92%8c%e5%88%87%e7%89%87>1.4 索引和切片</a></li><li><a href=#15-%e8%8a%82%e7%ba%a6%e5%86%85%e5%ad%98>1.5 节约内存</a></li><li><a href=#16-%e8%bd%ac%e6%8d%a2%e4%b8%ba%e5%85%b6%e4%bb%96python%e5%af%b9%e8%b1%a1>1.6 转换为其他Python对象</a></li></ul></li><li><a href=#2-%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86>2. 数据预处理</a><ul><li><a href=#21-%e8%af%bb%e5%8f%96%e6%95%b0%e6%8d%ae%e9%9b%86>2.1 读取数据集</a></li><li><a href=#22-%e5%a4%84%e7%90%86%e7%bc%ba%e5%a4%b1%e5%80%bc>2.2 处理缺失值</a></li><li><a href=#23-%e8%bd%ac%e6%8d%a2%e4%b8%ba%e5%bc%a0%e9%87%8f%e6%a0%bc%e5%bc%8f>2.3 转换为张量格式</a></li></ul></li><li><a href=#3-qa>3. Q&A</a></li></ul><h3 id=1-数据操作>1. 数据操作</h3><p>为了能够完成各种数据操作，我们需要某种方法来存储和操作数据。通常，我们需要做两件重要的事：</p><ol><li>获取数据；</li><li>将数据读入计算机后对其进行处理。</li></ol><p>如果没有某种方法来存储数据，那么获取数据是没有意义的。</p><p>首先，我们介绍 n 维数组，也称为<strong>张量</strong>（tensor）。PyTorch的<strong>张量类</strong>与Numpy的<code>ndarray</code>类似。但在深度学习框架中应用PyTorch的<strong>张量类</strong>，又比Numpy的<code>ndarray</code>多一些重要功能：</p><ol><li>tensor可以在很好地支持GPU加速计算，而NumPy仅支持CPU计算；</li><li>tensor支持自动微分。</li></ol><p>这些功能使得张量类更适合深度学习。</p><h4 id=11-基本操作>1.1 基本操作</h4><p>[<strong>张量表示由一些数值组成的数组，这个数组可能有多个维度</strong>]。具有一个轴的张量对应数学上的<strong>向量</strong>（vector）；具有两个轴的张量对应数学上的<strong>矩阵</strong>（matrix）；具有两个轴以上的张量没有特殊的数学名称。</p><p>我们可以使用<code>arange</code>创建一个行向量<code>x</code>。这个行向量包含从0开始的前12个整数，它们被<strong>默认创建为浮点数</strong>。张量中的每个值都称为张量的<strong>元素</strong>（element）。例如，张量<code>x</code>中有12个元素。除非额外指定，新的张量默认将存储在内存中，并采用基于CPU的计算。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([</span> <span class=mi>0</span><span class=p>,</span>  <span class=mi>1</span><span class=p>,</span>  <span class=mi>2</span><span class=p>,</span>  <span class=mi>3</span><span class=p>,</span>  <span class=mi>4</span><span class=p>,</span>  <span class=mi>5</span><span class=p>,</span>  <span class=mi>6</span><span class=p>,</span>  <span class=mi>7</span><span class=p>,</span>  <span class=mi>8</span><span class=p>,</span>  <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>11</span><span class=p>])</span>
</span></span></code></pre></div><p>[<strong>可以通过张量的<code>shape</code>属性来访问张量（沿每个轴的长度）的<em>形状</em></strong>]。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>	<span class=n>torch</span><span class=o>.</span><span class=n>Size</span><span class=p>([</span><span class=mi>12</span><span class=p>])</span>
</span></span></code></pre></div><p>如果只想知道张量中元素的总数，即形状的所有元素乘积，可以检查它的大小（size）。</p><p>因为这里在处理的是一个向量，所以它的<code>shape</code>与它的<code>size</code>相同。在处理更高维度的的张量时，可以用这种方法获取张量中元素的个数。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>x</span><span class=o>.</span><span class=n>numel</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=mi>12</span>
</span></span></code></pre></div><p>[<strong>要想改变一个张量的形状而不改变元素数量和元素值，可以调用<code>reshape</code>函数。</strong>]</p><p>例如，可以把张量<code>x</code>从形状为（12,）的行向量转换为形状为（3,4）的矩阵。这个新的张量包含与转换前相同的值，但是它被看成一个3行4列的矩阵。值得注意的是，虽然张量的形状发生了改变，但其元素值并没有变。改变张量的形状时，张量中元素的个数不会改变。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[</span> <span class=mi>0</span><span class=p>,</span>  <span class=mi>1</span><span class=p>,</span>  <span class=mi>2</span><span class=p>,</span>  <span class=mi>3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span> <span class=mi>4</span><span class=p>,</span>  <span class=mi>5</span><span class=p>,</span>  <span class=mi>6</span><span class=p>,</span>  <span class=mi>7</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span> <span class=mi>8</span><span class=p>,</span>  <span class=mi>9</span><span class=p>,</span> <span class=mi>10</span><span class=p>,</span> <span class=mi>11</span><span class=p>]])</span>
</span></span></code></pre></div><p>我们不需要通过手动指定每个维度来改变形状。如果我们的目标形状是（高度 x 宽度），那么在知道宽度后，高度会被自动计算得出，不必我们自己做除法。我们可以通过<code>-1</code>来调用此自动计算出维度的功能即可以用<code>x.reshape(-1,4)</code>或<code>x.reshape(3,-1)</code>来取代<code>x.reshape(3,4)</code>。</p><p>有时，我们希望[<strong>使用全0、全1、其他常量，或者从特定分布中随机采样的数字</strong>]来初始化矩阵。我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0或者1。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[[</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        	<span class=p>[[</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>]]])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[[</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        	<span class=p>[[</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]]])</span>
</span></span></code></pre></div><p>有时我们想通过从某个特定的概率分布中随机采样来得到张量中每个元素的值。例如，当我们构造数组来作为神经网络中的参数时，我们通常会随机初始化参数的值。以下代码创建一个形状为（3,4）的张量。其中的每个元素都从均值为0、标准差为1的标准正态分布中随机采样。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[</span> <span class=mf>0.1364</span><span class=p>,</span>  <span class=mf>0.3546</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9091</span><span class=p>,</span> <span class=o>-</span><span class=mf>1.8926</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span> <span class=mf>0.5786</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.9019</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1305</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.1899</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span> <span class=mf>0.5696</span><span class=p>,</span>  <span class=mf>1.1626</span><span class=p>,</span> <span class=o>-</span><span class=mf>0.5987</span><span class=p>,</span>  <span class=mf>0.4085</span><span class=p>]])</span>
</span></span></code></pre></div><p>我们还可以[<strong>通过提供包含数值的Python列表（或嵌套列表），来为所需张量中的每个元素赋予确定值</strong>]。在这里，最外层的列表对应于轴0，内层的列表对应于轴1。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span></span></code></pre></div><h4 id=12-简单运算>1.2 简单运算</h4><p>我们想在这些数据上执行数学运算，其中最简单且最有用的操作是<strong>按元素</strong>（elementwise）运算。它们将标准标量运算符应用于数组的每个元素。对于将两个数组作为输入的函数，按元素运算将二元运算符应用于两个数组中的每对位置对应的元素。我们可以基于任何从标量到标量的函数来创建按元素函数。我们通过将标量函数升级为按元素向量运算来生成向量值$F: \mathbb{R}^d, \mathbb{R}^d \rightarrow \mathbb{R}^d$。</p><p>对于任意具有相同形状的张量，[<strong>常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算</strong>]。我们可以在同一形状的任意两个张量上调用按元素操作。我们使用逗号来表示一个具有5个元素的元组，其中每个元素都是按元素操作的结果。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>1.0</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>8</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>x</span> <span class=o>+</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>-</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>*</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>/</span> <span class=n>y</span><span class=p>,</span> <span class=n>x</span> <span class=o>**</span> <span class=n>y</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=n>tensor</span><span class=p>([</span> <span class=mf>3.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>,</span>  <span class=mf>6.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([</span><span class=o>-</span><span class=mf>1.</span><span class=p>,</span>  <span class=mf>0.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>6.</span><span class=p>]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>,</span>  <span class=mf>8.</span><span class=p>,</span> <span class=mf>16.</span><span class=p>]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([</span><span class=mf>0.5000</span><span class=p>,</span> <span class=mf>1.0000</span><span class=p>,</span> <span class=mf>2.0000</span><span class=p>,</span> <span class=mf>4.0000</span><span class=p>]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([</span> <span class=mf>1.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>,</span> <span class=mf>16.</span><span class=p>,</span> <span class=mf>64.</span><span class=p>]))</span>
</span></span></code></pre></div><p>(<strong>“按元素”方式可以应用更多的计算</strong>)，包括像求幂这样的一元运算符。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([</span><span class=mf>2.7183e+00</span><span class=p>,</span> <span class=mf>7.3891e+00</span><span class=p>,</span> <span class=mf>5.4598e+01</span><span class=p>,</span> <span class=mf>2.9810e+03</span><span class=p>])</span>
</span></span></code></pre></div><p>[<strong>我们也可以把多个张量<em>连结</em>（concatenate）在一起</strong>]，把它们端对端地叠起来形成一个更大的张量。我们只需要提供张量列表，并给出沿哪个轴连结。下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素）和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。我们可以看到，第一个输出张量的轴-0长度（$6$）是两个输入张量轴-0长度的总和（$3 + 3$）；第二个输出张量的轴-1长度（$8$）是两个输入张量轴-1长度的总和（$4 + 4$）。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span><span class=mi>4</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>Y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>2.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=n>tensor</span><span class=p>([[</span> <span class=mf>0.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>5.</span><span class=p>,</span>  <span class=mf>6.</span><span class=p>,</span>  <span class=mf>7.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span> <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>]]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([[</span> <span class=mf>0.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>5.</span><span class=p>,</span>  <span class=mf>6.</span><span class=p>,</span>  <span class=mf>7.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>]]))</span>
</span></span></code></pre></div><p>由上述例子可见，当需要按轴-x连结两个张量时，我们就在第x+1层括号内将两张量中的元素相组合。类似地，我们将两个三维张量相连结。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>12</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>Y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[[</span><span class=mf>2.0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>]],</span> <span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>]],</span> <span class=p>[[</span><span class=mi>4</span><span class=p>,</span> <span class=mi>3</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>0</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>Y</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=n>tensor</span><span class=p>([[[</span> <span class=mf>0.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>         	 <span class=p>[[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>5.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>6.</span><span class=p>,</span>  <span class=mf>7.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>         	 <span class=p>[[</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span><span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>         	 <span class=p>[[</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>         	 <span class=p>[[</span> <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>3.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>         	 <span class=p>[[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>]]]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([[[</span> <span class=mf>0.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>         	 <span class=p>[[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>5.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>6.</span><span class=p>,</span>  <span class=mf>7.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>3.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>         	 <span class=p>[[</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	  <span class=p>[</span><span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	  <span class=p>[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>]]]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([[[</span> <span class=mf>0.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>2.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>        	 <span class=p>[[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>5.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	  <span class=p>[</span> <span class=mf>6.</span><span class=p>,</span>  <span class=mf>7.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>]],</span>
</span></span><span class=line><span class=cl> 
</span></span><span class=line><span class=cl>         	 <span class=p>[[</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>,</span>  <span class=mf>4.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	  <span class=p>[</span><span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>]]]))</span>
</span></span></code></pre></div><p>有时，我们想[<strong>通过<em>逻辑运算符</em>构建二元张量</strong>]。 以<code>X == Y</code>为例： 对于每个位置，如果<code>X</code>和<code>Y</code>在该位置相等，则新张量中相应项的值为1。 这意味着逻辑语句<code>X == Y</code>在该位置处为真，否则该位置为0。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span> <span class=o>==</span> <span class=n>Y</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[</span><span class=kc>False</span><span class=p>,</span>  <span class=kc>True</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span>  <span class=kc>True</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span><span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=p>[</span><span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>,</span> <span class=kc>False</span><span class=p>]])</span>
</span></span></code></pre></div><p>[<strong>对张量中的所有元素进行求和，会产生一个单元素张量。</strong>]</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span><span class=o>.</span><span class=n>sum</span><span class=p>()</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>(</span><span class=mf>66.</span><span class=p>)</span>
</span></span></code></pre></div><h4 id=13-广播机制>1.3 广播机制</h4><p>在上面的部分中，我们看到了如何在相同形状的两个张量上执行按元素操作。在某些情况下，[<strong>即使形状不同，我们仍然可以通过调用<em>广播机制</em>（broadcasting mechanism）来执行按元素操作</strong>]。这种机制的工作条件是：两个张量从后开始数，每个维度相等或者其中一个为1。这种机制的工作方式如下：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。在大多数情况下，我们将沿着数组中长度为1的轴进行广播，如下例子：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>3</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>3</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>b</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>2</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>,</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>0</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mi>2</span><span class=p>]]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]]))</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	 <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	 <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>]])</span>
</span></span></code></pre></div><p>由于<code>a</code>和<code>b</code>分别是$3\times1$和$1\times2$矩阵，如果让它们相加，它们的形状不匹配。我们将两个矩阵<strong>广播</strong>为一个更大的$3\times2$矩阵，矩阵<code>a</code>复制列，矩阵<code>b</code>复制行，然后再按元素相加。需要注意的是，广播机制只能扩展维度，而不能凭空增加张量的维度，例如在计算沿某个轴的均值时，若张量维度不同，则会报错：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>C</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>24</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>C</span> <span class=o>/</span> <span class=n>C</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=ne>RuntimeError</span><span class=p>:</span> <span class=n>The</span> <span class=n>size</span> <span class=n>of</span> <span class=n>tensor</span> <span class=n>a</span> <span class=p>(</span><span class=mi>3</span><span class=p>)</span> <span class=n>must</span> <span class=n>match</span> <span class=n>the</span> <span class=n>size</span> <span class=n>of</span> <span class=n>tensor</span> <span class=n>b</span> <span class=p>(</span><span class=mi>2</span><span class=p>)</span> <span class=n>at</span> <span class=n>non</span><span class=o>-</span><span class=n>singleton</span> <span class=n>dimension</span> <span class=mi>1</span>
</span></span></code></pre></div><p>此时我们需要将<code>keepdims</code>设为True，才能正确利用广播机制扩展<code>C.sum(axis=1)</code>的维度：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>C</span> <span class=o>/</span> <span class=n>C</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>,</span> <span class=n>keepdims</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[[</span><span class=mf>0.0000</span><span class=p>,</span> <span class=mf>0.0667</span><span class=p>,</span> <span class=mf>0.1111</span><span class=p>,</span> <span class=mf>0.1429</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>0.3333</span><span class=p>,</span> <span class=mf>0.3333</span><span class=p>,</span> <span class=mf>0.3333</span><span class=p>,</span> <span class=mf>0.3333</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>0.6667</span><span class=p>,</span> <span class=mf>0.6000</span><span class=p>,</span> <span class=mf>0.5556</span><span class=p>,</span> <span class=mf>0.5238</span><span class=p>]],</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        	<span class=p>[[</span><span class=mf>0.2500</span><span class=p>,</span> <span class=mf>0.2549</span><span class=p>,</span> <span class=mf>0.2593</span><span class=p>,</span> <span class=mf>0.2632</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>0.3333</span><span class=p>,</span> <span class=mf>0.3333</span><span class=p>,</span> <span class=mf>0.3333</span><span class=p>,</span> <span class=mf>0.3333</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>0.4167</span><span class=p>,</span> <span class=mf>0.4118</span><span class=p>,</span> <span class=mf>0.4074</span><span class=p>,</span> <span class=mf>0.4035</span><span class=p>]]])</span>
</span></span></code></pre></div><h4 id=14-索引和切片>1.4 索引和切片</h4><p>就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1；可以指定范围以包含第一个元素和最后一个之前的元素。如下所示，我们[<strong>可以用<code>[-1]</code>选择最后一个元素，可以用<code>[1:3]</code>选择第二个和第三个元素</strong>]：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>X</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=n>tensor</span><span class=p>([</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>]),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>5.</span><span class=p>,</span>  <span class=mf>6.</span><span class=p>,</span>  <span class=mf>7.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>]]))</span>
</span></span></code></pre></div><p>我们[<strong>可以用<code>[::2]</code>每间隔一个元素选择一个元素，可以用<code>[::3]</code>每间隔两个元素选择一个元素</strong>]：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span><span class=p>[::</span><span class=mi>2</span><span class=p>,</span> <span class=p>::</span><span class=mi>3</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[</span> <span class=mf>0.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>          	<span class=p>[</span> <span class=mf>8.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>]])</span>
</span></span></code></pre></div><p>[<strong>除读取外，我们还可以通过指定索引来将元素写入矩阵。</strong>]</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span><span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>]</span> <span class=o>=</span> <span class=mi>9</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[</span> <span class=mf>0.</span><span class=p>,</span>  <span class=mf>1.</span><span class=p>,</span>  <span class=mf>2.</span><span class=p>,</span>  <span class=mf>3.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span> <span class=mf>4.</span><span class=p>,</span>  <span class=mf>5.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>,</span>  <span class=mf>7.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>       	 	<span class=p>[</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>]])</span>
</span></span></code></pre></div><p>如果我们想[<strong>为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。</strong>]例如，<code>[0:2, :]</code>访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。虽然我们讨论的是矩阵的索引，但这也适用于向量和超过2个维度的张量。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span><span class=p>[</span><span class=mi>0</span><span class=p>:</span><span class=mi>2</span><span class=p>,</span> <span class=p>:]</span> <span class=o>=</span> <span class=mi>12</span>
</span></span><span class=line><span class=cl>	<span class=n>tensor</span><span class=p>([[</span><span class=mf>12.</span><span class=p>,</span> <span class=mf>12.</span><span class=p>,</span> <span class=mf>12.</span><span class=p>,</span> <span class=mf>12.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>        	<span class=p>[</span><span class=mf>12.</span><span class=p>,</span> <span class=mf>12.</span><span class=p>,</span> <span class=mf>12.</span><span class=p>,</span> <span class=mf>12.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>       	 	<span class=p>[</span> <span class=mf>8.</span><span class=p>,</span>  <span class=mf>9.</span><span class=p>,</span> <span class=mf>10.</span><span class=p>,</span> <span class=mf>11.</span><span class=p>]])</span>
</span></span></code></pre></div><h4 id=15-节约内存>1.5 节约内存</h4><p>[<strong>如果在后续计算中没有重复使用<code>X</code>，我们也可以使用<code>X[:] = X + Y</code>或<code>X += Y</code>来减少操作的内存开销。</strong>]</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>before</span> <span class=o>=</span> <span class=nb>id</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span> <span class=o>+=</span> <span class=n>Y</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>id</span><span class=p>(</span><span class=n>X</span><span class=p>)</span> <span class=o>==</span> <span class=n>before</span>
</span></span><span class=line><span class=cl>	<span class=kc>True</span>
</span></span></code></pre></div><h4 id=16-转换为其他python对象>1.6 转换为其他Python对象</h4><p>将深度学习框架定义的张量[<strong>转换为NumPy张量（<code>ndarray</code>）</strong>]很容易，反之也同样容易。torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>A</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>B</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>A</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>type</span><span class=p>(</span><span class=n>A</span><span class=p>),</span> <span class=nb>type</span><span class=p>(</span><span class=n>B</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=n>numpy</span><span class=o>.</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>)</span>
</span></span></code></pre></div><p>要(<strong>将大小为1的张量转换为Python标量</strong>)，我们可以调用<code>item</code>函数或Python的内置函数。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>a</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([</span><span class=mf>3.5</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>a</span><span class=p>,</span> <span class=n>a</span><span class=o>.</span><span class=n>item</span><span class=p>(),</span> <span class=nb>float</span><span class=p>(</span><span class=n>a</span><span class=p>),</span> <span class=nb>int</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=n>tensor</span><span class=p>([</span><span class=mf>3.5000</span><span class=p>]),</span> <span class=mf>3.5</span><span class=p>,</span> <span class=mf>3.5</span><span class=p>,</span> <span class=mi>3</span><span class=p>)</span>
</span></span></code></pre></div><h3 id=2-数据预处理>2. 数据预处理</h3><p>为了能用深度学习来解决现实世界的问题，我们经常从预处理原始数据开始，而不是从那些准备好的张量格式数据开始。在Python中常用的数据分析工具中，我们通常使用<code>pandas</code>软件包。像庞大的Python生态系统中的许多其他扩展包一样，<code>pandas</code>可以与张量兼容。本节我们将简要介绍使用<code>pandas</code>预处理原始数据，并将原始数据转换为张量格式的步骤。</p><h4 id=21-读取数据集>2.1 读取数据集</h4><p>举一个例子，我们首先(<strong>创建一个人工数据集，并存储在CSV（逗号分隔值）文件</strong>)<code>../data/house_tiny.csv</code>中。以其他格式存储的数据也可以通过类似的方式进行处理。下面我们将数据集按行写入CSV文件中。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>os</span><span class=o>.</span><span class=n>makedirs</span><span class=p>(</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s1>&#39;..&#39;</span><span class=p>,</span> <span class=s1>&#39;data&#39;</span><span class=p>),</span> <span class=n>exist_ok</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>data_file</span> <span class=o>=</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=s1>&#39;..&#39;</span><span class=p>,</span> <span class=s1>&#39;data&#39;</span><span class=p>,</span> <span class=s1>&#39;house_tiny.csv&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=k>with</span> <span class=nb>open</span><span class=p>(</span><span class=n>data_file</span><span class=p>,</span> <span class=s1>&#39;w&#39;</span><span class=p>)</span> <span class=k>as</span> <span class=n>f</span><span class=p>:</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span>     <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NumRooms,Alley,Price</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>  <span class=c1># 列名</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span>     <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NA,Pave,127500</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>  <span class=c1># 每行表示一个数据样本</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span>     <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;2,NA,106000</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span>     <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;4,NA,178100</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span>     <span class=n>f</span><span class=o>.</span><span class=n>write</span><span class=p>(</span><span class=s1>&#39;NA,NA,140000</span><span class=se>\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span></code></pre></div><p>要[<strong>从创建的CSV文件中加载原始数据集</strong>]，我们导入<code>pandas</code>包并调用<code>read_csv</code>函数。该数据集有四行三列。其中每行描述了房间数量（“NumRooms”）、巷子类型（“Alley”）和房屋价格（“Price”）。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=kn>import</span> <span class=nn>pandas</span> <span class=k>as</span> <span class=nn>pd</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>data</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>read_csv</span><span class=p>(</span><span class=n>data_file</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=n>data</span><span class=p>)</span>
</span></span></code></pre></div><table><thead><tr><th style=text-align:center></th><th style=text-align:center>NumRooms</th><th style=text-align:center>Alley</th><th style=text-align:center>Price</th></tr></thead><tbody><tr><td style=text-align:center>0</td><td style=text-align:center>NaN</td><td style=text-align:center>Pave</td><td style=text-align:center>127500</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>2.0</td><td style=text-align:center>NaN</td><td style=text-align:center>106000</td></tr><tr><td style=text-align:center>2</td><td style=text-align:center>4.0</td><td style=text-align:center>NaN</td><td style=text-align:center>178100</td></tr><tr><td style=text-align:center>3</td><td style=text-align:center>NaN</td><td style=text-align:center>NaN</td><td style=text-align:center>140000</td></tr></tbody></table><h4 id=22-处理缺失值>2.2 处理缺失值</h4><p>“NaN”项代表缺失值。[<strong>为了处理缺失的数据，典型的方法包括<em>插值法</em>和<em>删除法</em>，</strong>]其中插值法用一个替代值弥补缺失值，而删除法则直接忽略缺失值。通过位置索引<code>iloc</code>，我们将<code>data</code>分成<code>inputs</code>和<code>outputs</code>，其中前者为<code>data</code>的前两列，而后者为<code>data</code>的最后一列。对于<code>inputs</code>中缺少的数值，我们用同一列的均值替换“NaN”项。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>inputs</span><span class=p>,</span> <span class=n>outputs</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>:</span><span class=mi>2</span><span class=p>],</span> <span class=n>data</span><span class=o>.</span><span class=n>iloc</span><span class=p>[:,</span> <span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>fillna</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>mean</span><span class=p>())</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></span></code></pre></div><table><thead><tr><th style=text-align:center></th><th style=text-align:center>NumRooms</th><th style=text-align:center>Alley</th></tr></thead><tbody><tr><td style=text-align:center>0</td><td style=text-align:center>3.0</td><td style=text-align:center>Pave</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>2.0</td><td style=text-align:center>NaN</td></tr><tr><td style=text-align:center>2</td><td style=text-align:center>4.0</td><td style=text-align:center>NaN</td></tr><tr><td style=text-align:center>3</td><td style=text-align:center>3.0</td><td style=text-align:center>NaN</td></tr></tbody></table><p>利用删除法，我们删除缺失元素最多的一个样本。首先，<code>data.isnull()</code>矩阵统计每个元素是否缺失，之后在轴-1的方向上将<code>data.isnull()</code>元素求和，得到每个样本缺失元素个数，取得缺失元素个数最大的样本的序号，并将其删除。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>nan_numer</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>isnull</span><span class=p>()</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>axis</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>nan_max_id</span> <span class=o>=</span> <span class=n>nan_numer</span><span class=o>.</span><span class=n>idxmax</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>data_delete</span> <span class=o>=</span> <span class=n>data</span><span class=o>.</span><span class=n>drop</span><span class=p>([</span><span class=n>nan_max_id</span><span class=p>],</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span></code></pre></div><table><thead><tr><th style=text-align:center></th><th style=text-align:center>NumRooms</th><th style=text-align:center>Alley</th><th style=text-align:center>Price</th></tr></thead><tbody><tr><td style=text-align:center>0</td><td style=text-align:center>NaN</td><td style=text-align:center>Pave</td><td style=text-align:center>127500</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>2.0</td><td style=text-align:center>NaN</td><td style=text-align:center>106000</td></tr><tr><td style=text-align:center>2</td><td style=text-align:center>4.0</td><td style=text-align:center>NaN</td><td style=text-align:center>178100</td></tr></tbody></table><p>一般情况下，利用<code>dropna</code>删除数据，其中•Axis哪个维度How如何删除，‘any’表示有nan即删除，‘all’表示全为nan删除，Thresh有多少个nan删除，Subset在哪些列中查找nan，Inplace是否原地修改。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>dropna</span><span class=p>(</span> <span class=n>axis</span><span class=o>=</span><span class=mi>0</span><span class=p>,</span> <span class=n>how</span><span class=o>=</span><span class=err>‘</span><span class=nb>any</span><span class=err>’</span><span class=p>,</span> <span class=n>thresh</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>subset</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span> <span class=n>inplace</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span></code></pre></div><p>[<strong>对于<code>inputs</code>中的类别值或离散值，我们将“NaN”视为一个类别。</strong>]由于“巷子类型”（“Alley”）列只接受两种类型的类别值“Pave”和“NaN”，<code>pandas</code>可以自动将此列转换为两列“Alley_Pave”和“Alley_nan”。巷子类型为“Pave”的行会将“Alley_Pave”的值设置为1，“Alley_nan”的值设置为0。缺少巷子类型的行会将“Alley_Pave”和“Alley_nan”分别设置为0和1。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>inputs</span> <span class=o>=</span> <span class=n>pd</span><span class=o>.</span><span class=n>get_dummies</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>dummy_na</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=nb>print</span><span class=p>(</span><span class=n>inputs</span><span class=p>)</span>
</span></span></code></pre></div><table><thead><tr><th style=text-align:center></th><th style=text-align:center>NumRooms</th><th style=text-align:center>Alley_Pave</th><th style=text-align:center>Alley_nan</th></tr></thead><tbody><tr><td style=text-align:center>0</td><td style=text-align:center>3.0</td><td style=text-align:center>1</td><td style=text-align:center>0</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>2.0</td><td style=text-align:center>0</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>2</td><td style=text-align:center>4.0</td><td style=text-align:center>0</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>3</td><td style=text-align:center>3.0</td><td style=text-align:center>0</td><td style=text-align:center>1</td></tr></tbody></table><h4 id=23-转换为张量格式>2.3 转换为张量格式</h4><p>[<strong>现在<code>inputs</code>和<code>outputs</code>中的所有条目都是数值类型，它们可以转换为张量格式。</strong>]</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=o>&gt;&gt;&gt;</span> <span class=n>X</span><span class=p>,</span> <span class=n>y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>values</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>outputs</span><span class=o>.</span><span class=n>values</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=p>(</span><span class=n>tensor</span><span class=p>([[</span><span class=mf>3.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>2.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>4.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>],</span>
</span></span><span class=line><span class=cl>         	 <span class=p>[</span><span class=mf>3.</span><span class=p>,</span> <span class=mf>0.</span><span class=p>,</span> <span class=mf>1.</span><span class=p>]],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float64</span><span class=p>),</span>
</span></span><span class=line><span class=cl> 	 <span class=n>tensor</span><span class=p>([</span><span class=mi>127500</span><span class=p>,</span> <span class=mi>106000</span><span class=p>,</span> <span class=mi>178100</span><span class=p>,</span> <span class=mi>140000</span><span class=p>]))</span>
</span></span></code></pre></div><h3 id=3-qa>3. Q&A</h3><p><strong><code>Q1：reshape和view的区别？</code></strong></p><blockquote><p>View为浅拷贝，只能作用于连续型张量；Contiguous函数将张量做深拷贝并转为连续型；Reshape在张量连续时和view相同，不连续时等价于先contiguous再view。</p></blockquote><p><strong><code>Q2：数组计算吃力怎么办？</code></strong></p><blockquote><p>学习numpy的知识。</p></blockquote><p><strong><code>Q3：如何快速区分维度？</code></strong></p><blockquote><p>利用<code>a.shape</code>或<code>a.dim()</code>。</p></blockquote><p><strong><code>Q4：Tensor和Array有什么区别？</code></strong></p><blockquote><p>Tensor是数学上定义的张量，Array是计算机概念数组，但在深度学习中有时将Tensor视为多维数组。</p></blockquote><p><strong><code>Q5：新分配了y的内存，那么之前y对应的内存会自动释放吗？</code></strong></p><blockquote><p>Python会在不需要时自动释放内存。</p></blockquote></div><div class=article-widget><div class="container-xl row post-nav"><div class="col-6 post-nav-item"><div class=meta-nav>上一页</div><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03-%E5%AE%89%E8%A3%85/ rel=next>03-安装</a></div><div class="col-6 post-nav-item"><div class=meta-nav>下一页</div><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/ rel=prev>05-线性代数</a></div></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div><script type=text/javascript id=clstr_globe async src="//clustrmaps.com/globe.js?d=kgpJG5sWZQpKujBmD-uW1B54-WBPol-DuDtrB2KFjKs"></script></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>