<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="BERT 预训练 1.目录： BERT 预训练 1.目录： 2.BERT: 2.1 NLP 里的迁移学习 2.2 BERT 的动机 2.3 BERT 架构 2.4 对输入的修改 2.5 预训练任务 2.5.1 带掩码的语言模型 2.5.2 下一个句子预测 2.6 总结 3.代码实现 3.1 获取输入： 3.2 BERT 实现 3.3 预训练任务 3.3.1 遮掩语言模型 3.3.2 下"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/69-bert%E9%A2%84%E8%AE%AD%E7%BB%83/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.63df6ae9fc2b4cc71b83f1774d780209.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/69-bert%E9%A2%84%E8%AE%AD%E7%BB%83/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/69-bert%E9%A2%84%E8%AE%AD%E7%BB%83/"><meta property="og:title" content="69-bert预训练 | Next-gen Tech Edu"><meta property="og:description" content="BERT 预训练 1.目录： BERT 预训练 1.目录： 2.BERT: 2.1 NLP 里的迁移学习 2.2 BERT 的动机 2.3 BERT 架构 2.4 对输入的修改 2.5 预训练任务 2.5.1 带掩码的语言模型 2.5.2 下一个句子预测 2.6 总结 3.代码实现 3.1 获取输入： 3.2 BERT 实现 3.3 预训练任务 3.3.1 遮掩语言模型 3.3.2 下"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>69-bert预训练 | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=de37f3501dbfe29ee353190d465fba3c><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">2021-李沐-《动手学习深度学习》</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id1cb603993076354ae684c031037738c1")' href=#id1cb603993076354ae684c031037738c1 aria-expanded=false aria-controls=id1cb603993076354ae684c031037738c1 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/>99.参考资料</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id1cb603993076354ae684c031037738c1 aria-expanded=false aria-controls=id1cb603993076354ae684c031037738c1><i class="fa-solid fa-angle-down" id=caret-id1cb603993076354ae684c031037738c1></i></a></div><ul class="nav docs-sidenav collapse show" id=id1cb603993076354ae684c031037738c1><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6d7da24804841595467ee06940ffadad")' href=#id6d7da24804841595467ee06940ffadad aria-expanded=false aria-controls=id6d7da24804841595467ee06940ffadad aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id424867f26fc15069d5e95f339f8da715")' href=#id424867f26fc15069d5e95f339f8da715 aria-expanded=false aria-controls=id424867f26fc15069d5e95f339f8da715 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/>2019-Andrew Ng-深度学习课程</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id424867f26fc15069d5e95f339f8da715 aria-expanded=false aria-controls=id424867f26fc15069d5e95f339f8da715><i class="fa-solid fa-angle-right" id=caret-id424867f26fc15069d5e95f339f8da715></i></a></div><ul class="nav docs-sidenav collapse" id=id424867f26fc15069d5e95f339f8da715><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/interview/>interview</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week1/>lesson1-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week2/>lesson1-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week3/>lesson1-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week4/>lesson1-week4</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week1/>lesson2-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week2/>lesson2-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week3/>lesson2-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson3-week1/>lesson3-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson3-week2/>lesson3-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week1/>lesson4-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week2/>lesson4-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week3/>lesson4-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week4/>lesson4-week4</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week1/>lesson5-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week2/>lesson5-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week3/>lesson5-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/math/>math</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/notation/>notation</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/summary/>SUMMARY</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6d61b824616f345f413069d041a91bcd")' href=#id6d61b824616f345f413069d041a91bcd aria-expanded=false aria-controls=id6d61b824616f345f413069d041a91bcd aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>2021-李沐-《动手学习深度学习》</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id6d61b824616f345f413069d041a91bcd aria-expanded=false aria-controls=id6d61b824616f345f413069d041a91bcd><i class="fa-solid fa-angle-down" id=caret-id6d61b824616f345f413069d041a91bcd></i></a></div><ul class="nav docs-sidenav collapse show" id=id6d61b824616f345f413069d041a91bcd><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00-%E9%A2%84%E5%91%8A/>00-预告</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01-%E8%AF%BE%E7%A8%8B%E5%AE%89%E6%8E%92/>01-课程安排</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>02-深度学习介绍</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03-%E5%AE%89%E8%A3%85/>03-安装</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C/>04-数据读取和操作</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/>05-线性代数</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06-%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/>06-矩阵计算</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/>07-自动求导</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/08-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92+%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/>08-线性回归+基础优化算法</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09-softmax%E5%9B%9E%E5%BD%92/>09-softmax回归</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/10-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/>10-多层感知机</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/11-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9+%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88/>11-模型选择+过拟合和欠拟合</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/12-%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80/>12-权重衰退</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/13-%E4%B8%A2%E5%BC%83%E6%B3%95/>13-丢弃法</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/>14-数值稳定性</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/15-%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/>15-实战Kaggle比赛：预测房价</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/16-pytorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/>16-Pytorch神经网络基础</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/17-%E4%BD%BF%E7%94%A8%E5%92%8C%E8%B4%AD%E4%B9%B0gpu/>17-使用和购买GPU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/18-%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7%E7%AB%9E%E8%B5%9B%E6%80%BB%E7%BB%93/>18-预测房价竞赛总结</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/19-%E5%8D%B7%E7%A7%AF%E5%B1%82/>19-卷积层</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/20-%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85/>20-填充和步幅</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/21-%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93/>21-多输入输出通道</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/22-%E6%B1%A0%E5%8C%96%E5%B1%82/>22-池化层</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/23-%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Clenet/>23-经典卷积神经网络LeNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/24-alexnet/>24-AlexNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/25-%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9Cvgg/>25-使用块的网络VGG</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/26-nin/>26-NiN</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/27-googlenet/>27-GoogLeNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/28-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/>28-批量归一化</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/29-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet/>29-残差网络ResNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/30-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E5%AE%8C%E7%BB%93%E7%AB%9E%E8%B5%9B%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/>30-第二部分完结竞赛：图片分类</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/31-cpu%E5%92%8Cgpu/>31-CPU和GPU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/32-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%A1%AC%E4%BB%B6/>32-深度学习硬件</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/33-%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C/>33-单机多卡并行</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/34-%E5%A4%9Agpu%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0only-qa/>34-多GPU训练实现(only QA)</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/35-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>35-分布式训练</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/36-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF/>36-数据增广</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/37-%E5%BE%AE%E8%B0%83/>37-微调</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/38-%E7%AC%AC%E4%BA%8C%E6%AC%A1%E7%AB%9E%E8%B5%9B%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C/>38-第二次竞赛树叶分类结果</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/39-%E5%AE%9E%E6%88%98kaggle%E7%AB%9E%E8%B5%9Bcifar-10/>39-实战Kaggle竞赛：CIFAR-10</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/41-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86/>41-物体检测和数据集</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/43-%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E7%AB%9E%E8%B5%9B%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/>43-树叶分类竞赛技术总结</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/44-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95r-cnnssdyolo/>44-物体检测算法：R-CNN,SSD,YOLO</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/46-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/>46-语义分割</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/47-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF/>47-转置卷积</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/48-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cfcn/>48-全连接卷积神经网络（FCN）</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/49-%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB/>49-样式迁移</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/50-%E8%AF%BE%E7%A8%8B%E7%AB%9E%E8%B5%9B%E7%89%9B%E4%BB%94%E8%A1%8C%E5%A4%B4%E6%A3%80%E6%B5%8B/>50-课程竞赛：牛仔行头检测</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/51-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/>51-序列模型</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/53-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>53-语言模型</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/54-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/>54-循环神经网络RNN</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/56-gru/>56-GRU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/57-lstm/>57-LSTM</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/58-%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>58-深层循环神经网络</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/61-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84/>61-编码器-解码器架构</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/62-%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0/>62-序列到序列学习</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/63-%E6%9D%9F%E6%90%9C%E7%B4%A2/>63-束搜索</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/65-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0/>65-注意力分数</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/68-transformer/>68-Transformer</a></li><li class="child level active"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/69-bert%E9%A2%84%E8%AE%AD%E7%BB%83/>69-bert预训练</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/70-bert%E5%BE%AE%E8%B0%83/>70-BERT微调</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/72-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/>72-优化算法</a></li></ul></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#bert-预训练>BERT 预训练</a><ul><li><a href=#1目录>1.目录：</a></li><li><a href=#2bert>2.BERT:</a></li><li><a href=#3代码实现>3.代码实现</a></li><li><a href=#qa>Q&A：</a></li></ul></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>69-bert预训练</h1><div class=article-style><h2 id=bert-预训练>BERT 预训练</h2><h3 id=1目录>1.目录：</h3><ul><li><a href=#bert%e9%a2%84%e8%ae%ad%e7%bb%83>BERT 预训练</a><ul><li><a href=#1%e7%9b%ae%e5%bd%95>1.目录：</a></li><li><a href=#2bert>2.BERT:</a><ul><li><a href=#21-nlp%e9%87%8c%e7%9a%84%e8%bf%81%e7%a7%bb%e5%ad%a6%e4%b9%a0>2.1 NLP 里的迁移学习</a></li><li><a href=#22-bert%e7%9a%84%e5%8a%a8%e6%9c%ba>2.2 BERT 的动机</a></li><li><a href=#23-bert%e6%9e%b6%e6%9e%84>2.3 BERT 架构</a></li><li><a href=#24-%e5%af%b9%e8%be%93%e5%85%a5%e7%9a%84%e4%bf%ae%e6%94%b9>2.4 对输入的修改</a></li><li><a href=#25-%e9%a2%84%e8%ae%ad%e7%bb%83%e4%bb%bb%e5%8a%a1>2.5 预训练任务</a><ul><li><a href=#251-%e5%b8%a6%e6%8e%a9%e7%a0%81%e7%9a%84%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b>2.5.1 带掩码的语言模型</a></li><li><a href=#252-%e4%b8%8b%e4%b8%80%e4%b8%aa%e5%8f%a5%e5%ad%90%e9%a2%84%e6%b5%8b>2.5.2 下一个句子预测</a></li></ul></li><li><a href=#26-%e6%80%bb%e7%bb%93>2.6 总结</a></li></ul></li><li><a href=#3%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0>3.代码实现</a><ul><li><a href=#31-%e8%8e%b7%e5%8f%96%e8%be%93%e5%85%a5>3.1 获取输入：</a></li><li><a href=#32-bert%e5%ae%9e%e7%8e%b0>3.2 BERT 实现</a></li><li><a href=#33-%e9%a2%84%e8%ae%ad%e7%bb%83%e4%bb%bb%e5%8a%a1>3.3 预训练任务</a><ul><li><a href=#331-%e9%81%ae%e6%8e%a9%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b>3.3.1 遮掩语言模型</a></li><li><a href=#332-%e4%b8%8b%e4%b8%80%e5%8f%a5%e9%a2%84%e6%b5%8b>3.3.2 下一句预测</a></li></ul></li><li><a href=#34-%e6%95%b4%e5%90%88%e4%bb%a3%e7%a0%81>3.4 整合代码</a></li><li><a href=#35-%e5%b0%8f%e7%bb%93>3.5 小结</a></li></ul></li><li><a href=#qa>Q&A：</a></li></ul></li></ul><h3 id=2bert>2.BERT:</h3><h4 id=21-nlp-里的迁移学习>2.1 NLP 里的迁移学习</h4><ul><li><p>使用预训练好的模型来抽取词，句子的特征</p><ul><li>例如 word2vec 或语言模型</li></ul></li><li><p>不更新预训练好的模型</p></li><li><p>需要构建新的网络来抓取任务需要的信息</p><ul><li>Word2vec 忽略了时序信息</li><li>语言模型只看了一个方向</li></ul></li></ul><h4 id=22-bert-的动机>2.2 BERT 的动机</h4><ul><li>基于微调的 NLP 模型</li><li>预训练的模型抽取了足够多的信息</li><li>新的任务只需要增加一个简单地输出层</li></ul><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/69/69-1.png alt=image align=center width=500></div><h4 id=23-bert-架构>2.3 BERT 架构</h4><ul><li><p>只有编码器的 Transformer</p></li><li><p>两个版本：</p><ul><li>Base:#blocks=12,hidden size=768,#heads=12,#parameters=110M</li><li>Large:#blocks=24,hidden size=1024,#heads=16,#paramerter=340M</li></ul></li><li><p>在大规模数据上训练>3B 词</p></li></ul><h4 id=24-对输入的修改>2.4 对输入的修改</h4><ul><li>每个样本是一个句子对</li><li>加入额外的片段嵌入</li><li>位置编码可学习</li></ul><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/69/69-2.png alt=image align=center width=500></div><h4 id=25-预训练任务>2.5 预训练任务</h4><h5 id=251-带掩码的语言模型>2.5.1 带掩码的语言模型</h5><ul><li>Transformer 的编码器是双向的，标准语言模型要求单向</li><li>带掩码的语言模型每次随机（15%概率）将一些词元换成<mask></li><li><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/57/57-5.png alt=image align=center width=500></div></li></ul><h5 id=252-下一个句子预测>2.5.2 下一个句子预测</h5><ul><li><p>预测一个句子对中两个句子是不是相邻</p></li><li><p>训练样本中：</p><ul><li>50%概率选择相邻句子对：<cls>this movie is great <sep>i like it <sep></li><li>50%概率选择随机句子对：<cls>this movie is great<sep> hello world<sep></li></ul></li><li><p>将<cls>对应的输出放到一个全连接层来预测</p></li></ul><h4 id=26-总结>2.6 总结</h4><ul><li>BERT 针对微调设计</li><li>基于 Transformer 的编码器做了如下修改<ul><li>模型更大，训练数据更多</li><li>输入句子对，片段嵌入，可学习的位置编码</li><li>训练时使用两个任务：<ul><li>带掩码的语言模型</li><li>下一个句子预测</li></ul></li></ul></li></ul><h3 id=3代码实现>3.代码实现</h3><h4 id=31-获取输入>3.1 获取输入：</h4><p>在自然语言处理中，有些任务（如情感分析）以单个文本作为输入，而有些任务（如自然语言推断）以一对文本序列作为输入。BERT 输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT 输入序列是特殊类别词元“<cls>”、文本序列的标记、以及特殊分隔词元“<sep>”的连结。当输入为文本对时，BERT 输入序列是“<cls>”、第一个文本序列的标记、“<sep>”、第二个文本序列标记、以及“<sep>”的连结。我们将始终如一地将术语“BERT 输入序列”与其他类型的“序列”区分开来。例如，一个<em>BERT 输入序列</em>可以包括一个<em>文本序列</em>或两个<em>文本序列</em>。</p><p>为了区分文本对，根据输入序列学到的片段嵌入 eA 和 eB 分别被添加到第一序列和第二序列的词元嵌入中。对于单文本输入，仅使用 eA。</p><p>下面的<code>get_tokens_and_segments</code>将一个句子或两个句子作为输入，然后返回 BERT 输入序列的标记及其相应的片段索引。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#@save</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_tokens_and_segments</span><span class=p>(</span><span class=n>tokens_a</span><span class=p>,</span> <span class=n>tokens_b</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;获取输入序列的词元及其片段索引&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=n>tokens</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;&lt;cls&gt;&#39;</span><span class=p>]</span> <span class=o>+</span> <span class=n>tokens_a</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;&lt;sep&gt;&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=c1># 0和1分别标记片段A和B</span>
</span></span><span class=line><span class=cl>    <span class=n>segments</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>*</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens_a</span><span class=p>)</span> <span class=o>+</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>tokens_b</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>tokens</span> <span class=o>+=</span> <span class=n>tokens_b</span> <span class=o>+</span> <span class=p>[</span><span class=s1>&#39;&lt;sep&gt;&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>segments</span> <span class=o>+=</span> <span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>*</span> <span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>tokens_b</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>tokens</span><span class=p>,</span> <span class=n>segments</span>
</span></span></code></pre></div><p>BERT 选择 Transformer 编码器作为其双向架构。在 Transformer 编码器中常见是，位置嵌入被加入到输入序列的每个位置。然而，与原始的 Transformer 编码器不同，BERT 使用<em>可学习的</em>位置嵌入。总之， 下图表明 BERT 输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。</p><div align=center><img src=https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/69/69-3.png alt=image align=center width=500></div><h4 id=32-bert-实现>3.2 BERT 实现</h4><p>下面的<code>BERTEncoder</code>类类似于 <a href=https://zh-v2.d2l.ai/chapter_attention-mechanisms/transformer.html#sec-transformer target=_blank rel=noopener>10.7 节</a>中实现的<code>TransformerEncoder</code>类。与<code>TransformerEncoder</code>不同，<code>BERTEncoder</code>使用片段嵌入和可学习的位置嵌入。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#@save</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BERTEncoder</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;BERT编码器&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>norm_shape</span><span class=p>,</span> <span class=n>ffn_num_input</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>ffn_num_hiddens</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>max_len</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>key_size</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=n>query_size</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=n>value_size</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>BERTEncoder</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>segment_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Embedding</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>blks</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_layers</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>blks</span><span class=o>.</span><span class=n>add_module</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;</span><span class=si>{</span><span class=n>i</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>,</span> <span class=n>d2l</span><span class=o>.</span><span class=n>EncoderBlock</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>key_size</span><span class=p>,</span> <span class=n>query_size</span><span class=p>,</span> <span class=n>value_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>norm_shape</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>ffn_num_input</span><span class=p>,</span> <span class=n>ffn_num_hiddens</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span> <span class=kc>True</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=c1># 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>pos_embedding</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Parameter</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>randn</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>max_len</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                                                      <span class=n>num_hiddens</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tokens</span><span class=p>,</span> <span class=n>segments</span><span class=p>,</span> <span class=n>valid_lens</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># 在以下代码段中，X的形状保持不变：（批量大小，最大序列长度，num_hiddens）</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>token_embedding</span><span class=p>(</span><span class=n>tokens</span><span class=p>)</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>segment_embedding</span><span class=p>(</span><span class=n>segments</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=n>X</span> <span class=o>+</span> <span class=bp>self</span><span class=o>.</span><span class=n>pos_embedding</span><span class=o>.</span><span class=n>data</span><span class=p>[:,</span> <span class=p>:</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>blk</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>blks</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>X</span> <span class=o>=</span> <span class=n>blk</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>valid_lens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>X</span>
</span></span></code></pre></div><p>假设词表大小为 10000，为了演示<code>BERTEncoder</code>的前向推断，让我们创建一个实例并初始化它的参数。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>ffn_num_hiddens</span><span class=p>,</span> <span class=n>num_heads</span> <span class=o>=</span> <span class=mi>10000</span><span class=p>,</span> <span class=mi>768</span><span class=p>,</span> <span class=mi>1024</span><span class=p>,</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl><span class=n>norm_shape</span><span class=p>,</span> <span class=n>ffn_num_input</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>dropout</span> <span class=o>=</span> <span class=p>[</span><span class=mi>768</span><span class=p>],</span> <span class=mi>768</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mf>0.2</span>
</span></span><span class=line><span class=cl><span class=n>encoder</span> <span class=o>=</span> <span class=n>BERTEncoder</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>norm_shape</span><span class=p>,</span> <span class=n>ffn_num_input</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                      <span class=n>ffn_num_hiddens</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>dropout</span><span class=p>)</span>
</span></span></code></pre></div><p>我们将<code>tokens</code>定义为长度为 8 的 2 个输入序列，其中每个词元是词表的索引。使用输入<code>tokens</code>的<code>BERTEncoder</code>的前向推断返回编码结果，其中每个词元由向量表示，其长度由超参数<code>num_hiddens</code>定义。此超参数通常称为 Transformer 编码器的<em>隐藏大小</em>（隐藏单元数）</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>tokens</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=mi>8</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>segments</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>encoded_X</span> <span class=o>=</span> <span class=n>encoder</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>segments</span><span class=p>,</span> <span class=kc>None</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>encoded_X</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></div><h4 id=33-预训练任务>3.3 预训练任务</h4><h5 id=331-遮掩语言模型>3.3.1 遮掩语言模型</h5><p>我们实现了下面的<code>MaskLM</code>类来预测 BERT 预训练的掩蔽语言模型任务中的掩蔽标记。预测使用单隐藏层的多层感知机（<code>self.mlp</code>）。在前向推断中，它需要两个输入：<code>BERTEncoder</code>的编码结果和用于预测的词元位置。输出是这些位置的预测结果。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#@save</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>MaskLM</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;BERT的掩蔽语言模型任务&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>num_inputs</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>MaskLM</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_inputs</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                                 <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                                 <span class=n>nn</span><span class=o>.</span><span class=n>LayerNorm</span><span class=p>(</span><span class=n>num_hiddens</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                                 <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_hiddens</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>,</span> <span class=n>pred_positions</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>num_pred_positions</span> <span class=o>=</span> <span class=n>pred_positions</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>pred_positions</span> <span class=o>=</span> <span class=n>pred_positions</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>batch_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># 假设batch_size=2，num_pred_positions=3</span>
</span></span><span class=line><span class=cl>        <span class=c1># 那么batch_idx是np.array（[0,0,0,1,1]）</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_idx</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>repeat_interleave</span><span class=p>(</span><span class=n>batch_idx</span><span class=p>,</span> <span class=n>num_pred_positions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>masked_X</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>batch_idx</span><span class=p>,</span> <span class=n>pred_positions</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>masked_X</span> <span class=o>=</span> <span class=n>masked_X</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>num_pred_positions</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>mlm_Y_hat</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlp</span><span class=p>(</span><span class=n>masked_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>mlm_Y_hat</span>
</span></span></code></pre></div><p>为了演示<code>MaskLM</code>的前向推断，我们创建了其实例<code>mlm</code>并对其进行了初始化。回想一下，来自<code>BERTEncoder</code>的正向推断<code>encoded_X</code>表示 2 个 BERT 输入序列。我们将<code>mlm_positions</code>定义为在<code>encoded_X</code>的任一输入序列中预测的 3 个指示。<code>mlm</code>的前向推断返回<code>encoded_X</code>的所有掩蔽位置<code>mlm_positions</code>处的预测结果<code>mlm_Y_hat</code>。对于每个预测，结果的大小等于词表的大小。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>mlm</span> <span class=o>=</span> <span class=n>MaskLM</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mlm_positions</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>6</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>5</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>mlm_Y_hat</span> <span class=o>=</span> <span class=n>mlm</span><span class=p>(</span><span class=n>encoded_X</span><span class=p>,</span> <span class=n>mlm_positions</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mlm_Y_hat</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></div><p>通过掩码下的预测词元<code>mlm_Y</code>的真实标签<code>mlm_Y_hat</code>，我们可以计算在 BERT 预训练中的遮蔽语言模型任务的交叉熵损失。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>mlm_Y</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>([[</span><span class=mi>7</span><span class=p>,</span> <span class=mi>8</span><span class=p>,</span> <span class=mi>9</span><span class=p>],</span> <span class=p>[</span><span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>,</span> <span class=mi>30</span><span class=p>]])</span>
</span></span><span class=line><span class=cl><span class=n>loss</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>CrossEntropyLoss</span><span class=p>(</span><span class=n>reduction</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>mlm_l</span> <span class=o>=</span> <span class=n>loss</span><span class=p>(</span><span class=n>mlm_Y_hat</span><span class=o>.</span><span class=n>reshape</span><span class=p>((</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>)),</span> <span class=n>mlm_Y</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>mlm_l</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></div><h5 id=332-下一句预测>3.3.2 下一句预测</h5><p>下面的<code>NextSentencePred</code>类使用单隐藏层的多层感知机来预测第二个句子是否是 BERT 输入序列中第一个句子的下一个句子。由于 Transformer 编码器中的自注意力，特殊词元“<cls>”的 BERT 表示已经对输入的两个句子进行了编码。因此，多层感知机分类器的输出层（<code>self.output</code>）以<code>X</code>作为输入，其中<code>X</code>是多层感知机隐藏层的输出，而 MLP 隐藏层的输入是编码后的“<cls>”词元。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#@save</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>NextSentencePred</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;BERT的下一句预测任务&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_inputs</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>NextSentencePred</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>num_inputs</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>X</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># X的形状：(batchsize,num_hiddens)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>output</span><span class=p>(</span><span class=n>X</span><span class=p>)</span>
</span></span></code></pre></div><p>我们可以看到，<code>NextSentencePred</code>实例的前向推断返回每个 BERT 输入序列的二分类预测。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>encoded_X</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>flatten</span><span class=p>(</span><span class=n>encoded_X</span><span class=p>,</span> <span class=n>start_dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># NSP的输入形状:(batchsize，num_hiddens)</span>
</span></span><span class=line><span class=cl><span class=n>nsp</span> <span class=o>=</span> <span class=n>NextSentencePred</span><span class=p>(</span><span class=n>encoded_X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span>
</span></span><span class=line><span class=cl><span class=n>nsp_Y_hat</span> <span class=o>=</span> <span class=n>nsp</span><span class=p>(</span><span class=n>encoded_X</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>nsp_Y_hat</span><span class=o>.</span><span class=n>shape</span>
</span></span></code></pre></div><h4 id=34-整合代码>3.4 整合代码</h4><p>在预训练 BERT 时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。现在我们可以通过实例化三个类<code>BERTEncoder</code>、<code>MaskLM</code>和<code>NextSentencePred</code>来定义<code>BERTModel</code>类。前向推断返回编码后的 BERT 表示<code>encoded_X</code>、掩蔽语言模型预测<code>mlm_Y_hat</code>和下一句预测<code>nsp_Y_hat</code>。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1>#@save</span>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BERTModel</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;BERT模型&#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>norm_shape</span><span class=p>,</span> <span class=n>ffn_num_input</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>ffn_num_hiddens</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span> <span class=n>dropout</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>max_len</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>key_size</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=n>query_size</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=n>value_size</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>hid_in_features</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span> <span class=n>mlm_in_features</span><span class=o>=</span><span class=mi>768</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                 <span class=n>nsp_in_features</span><span class=o>=</span><span class=mi>768</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>(</span><span class=n>BERTModel</span><span class=p>,</span> <span class=bp>self</span><span class=p>)</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span> <span class=o>=</span> <span class=n>BERTEncoder</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>norm_shape</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>ffn_num_input</span><span class=p>,</span> <span class=n>ffn_num_hiddens</span><span class=p>,</span> <span class=n>num_heads</span><span class=p>,</span> <span class=n>num_layers</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>dropout</span><span class=p>,</span> <span class=n>max_len</span><span class=o>=</span><span class=n>max_len</span><span class=p>,</span> <span class=n>key_size</span><span class=o>=</span><span class=n>key_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                    <span class=n>query_size</span><span class=o>=</span><span class=n>query_size</span><span class=p>,</span> <span class=n>value_size</span><span class=o>=</span><span class=n>value_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>hid_in_features</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                                    <span class=n>nn</span><span class=o>.</span><span class=n>Tanh</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>mlm</span> <span class=o>=</span> <span class=n>MaskLM</span><span class=p>(</span><span class=n>vocab_size</span><span class=p>,</span> <span class=n>num_hiddens</span><span class=p>,</span> <span class=n>mlm_in_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>nsp</span> <span class=o>=</span> <span class=n>NextSentencePred</span><span class=p>(</span><span class=n>nsp_in_features</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>tokens</span><span class=p>,</span> <span class=n>segments</span><span class=p>,</span> <span class=n>valid_lens</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>pred_positions</span><span class=o>=</span><span class=kc>None</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>encoded_X</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>encoder</span><span class=p>(</span><span class=n>tokens</span><span class=p>,</span> <span class=n>segments</span><span class=p>,</span> <span class=n>valid_lens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>pred_positions</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>mlm_Y_hat</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>mlm</span><span class=p>(</span><span class=n>encoded_X</span><span class=p>,</span> <span class=n>pred_positions</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>mlm_Y_hat</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>        <span class=c1># 用于下一句预测的多层感知机分类器的隐藏层，0是“&lt;cls&gt;”标记的索引</span>
</span></span><span class=line><span class=cl>        <span class=n>nsp_Y_hat</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>nsp</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden</span><span class=p>(</span><span class=n>encoded_X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>,</span> <span class=p>:]))</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>encoded_X</span><span class=p>,</span> <span class=n>mlm_Y_hat</span><span class=p>,</span> <span class=n>nsp_Y_hat</span>
</span></span></code></pre></div><h4 id=35-小结>3.5 小结</h4><ul><li>word2vec 和 GloVe 等词嵌入模型与上下文无关。它们将相同的预训练向量赋给同一个词，而不考虑词的上下文（如果有的话）。它们很难处理好自然语言中的一词多义或复杂语义。</li><li>对于上下文敏感的词表示，如 ELMo 和 GPT，词的表示依赖于它们的上下文。</li><li>ELMo 对上下文进行双向编码，但使用特定于任务的架构（然而，为每个自然语言处理任务设计一个特定的体系架构实际上并不容易）；而 GPT 是任务无关的，但是从左到右编码上下文。</li><li>BERT 结合了这两个方面的优点：它对上下文进行双向编码，并且需要对大量自然语言处理任务进行最小的架构更改。</li><li>BERT 输入序列的嵌入是词元嵌入、片段嵌入和位置嵌入的和。</li><li>预训练包括两个任务：掩蔽语言模型和下一句预测。前者能够编码双向上下文来表示单词，而后者则显式地建模文本对之间的逻辑关系。</li></ul><h3 id=qa>Q&A：</h3><h5 id=q1bert-是不是很少用在-cv-上>Q1:BERT 是不是很少用在 CV 上？</h5><blockquote><p>transformer 架构这几年在大量的用于 CV 上</p></blockquote><h5 id=q2展示一下-10w-batch-训练结果>Q2:展示一下 10W batch 训练结果？</h5><blockquote><p>微调时会用到</p></blockquote><h5 id=q3使用-bert-large-时显存不足有什么方法吗>Q3：使用 BERT large 时显存不足，有什么方法吗？</h5><blockquote><p>单机多卡，模型并行，或改用小模型</p></blockquote></div><div class=article-widget><div class="container-xl row post-nav"><div class="col-6 post-nav-item"><div class=meta-nav>上一页</div><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/68-transformer/ rel=next>68-Transformer</a></div><div class="col-6 post-nav-item"><div class=meta-nav>下一页</div><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/70-bert%E5%BE%AE%E8%B0%83/ rel=prev>70-BERT微调</a></div></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div><script type=text/javascript id=clstr_globe async src="//clustrmaps.com/globe.js?d=kgpJG5sWZQpKujBmD-uW1B54-WBPol-DuDtrB2KFjKs"></script></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>