<!doctype html><html lang=zh><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=google-site-verification content="google69a5cccb61297807"><meta name=baidu-site-verification content="cqmZHEleVh"><meta name=description content="机器学习的数学基础 [TOC] 高等数学 1.导数定义： 导数和微分的概念 $f&rsquo;({{x}{0}})=\underset{\Delta x\to 0}{\mathop{\lim }},\frac{f({{x}{0}}+\Delta x)-f({{x}_{0}})}{\Delta x}$ （1） 或者： $f&rsquo;({{x}{0}})=\underset{x\to {{x}{0}}}{\mathop{\lim }},\frac{f(x)-f({{x}{0}})}{x-{{x}{0}}}$ （2） 2.左右导数导数的几何意义和物理意义 函数$f(x)$在$x_0$处的左、右导数分别定义为： 左导数：${{{"><link rel=alternate hreflang=zh href=https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/math/><meta name=theme-color content="#0a55a7"><link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/instantsearch.css@7.4.5/themes/satellite-min.css integrity="sha256-TehzF/2QvNKhGQrrNpoOb2Ck4iGZ1J/DI4pkd2oUsBc=" crossorigin=anonymous><link rel=stylesheet href=/css/wowchemy.fab3cd1900ae35687457073b2d518207.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-40NYXJ8823"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-40NYXJ8823")</script><script>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?56df1177bce405601b0ecdd7208f75c6",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_hu0f7d075e895d6f5f1f5fdbc1e33dc138_10087_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/math/><meta property="twitter:card" content="summary_large_image"><meta property="twitter:site" content="@wx-chevalier"><meta property="twitter:creator" content="@wx-chevalier"><meta property="og:site_name" content="Next-gen Tech Edu"><meta property="og:url" content="https://ng-tech.icu/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/math/"><meta property="og:title" content="math | Next-gen Tech Edu"><meta property="og:description" content="机器学习的数学基础 [TOC] 高等数学 1.导数定义： 导数和微分的概念 $f&rsquo;({{x}{0}})=\underset{\Delta x\to 0}{\mathop{\lim }},\frac{f({{x}{0}}+\Delta x)-f({{x}_{0}})}{\Delta x}$ （1） 或者： $f&rsquo;({{x}{0}})=\underset{x\to {{x}{0}}}{\mathop{\lim }},\frac{f(x)-f({{x}{0}})}{x-{{x}{0}}}$ （2） 2.左右导数导数的几何意义和物理意义 函数$f(x)$在$x_0$处的左、右导数分别定义为： 左导数：${{{"><meta property="og:image" content="https://ng-tech.icu/media/sharing.png"><meta property="twitter:image" content="https://ng-tech.icu/media/sharing.png"><meta property="og:locale" content="zh"><title>math | Next-gen Tech Edu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=50a3aa55ba3b443e0b78fca4fa8f42a8><button onclick=topFunction() id=backTopBtn title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden=true></i></button>
<script src=/js/wowchemy-init.min.14a0ed61c6dbd594b9c75193b25be179.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class="col-6 search-title"><p>搜索</p></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=关闭><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box></div></section><section class=section-search-results><div id=search-hits></div><div id=search-common-queries></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label=切换导航>
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Next-gen Tech Edu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/books-gallery><span>笔记（万篇）</span></a></li><li class=nav-item><a class=nav-link href=/#knowledge-map><span>知识图谱</span></a></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>实验室</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=/galaxy-home/gh-craft><span>Craft 方块世界</span></a>
<a class=dropdown-item href=/galaxy-home/glossary-cards><span>3D 知识卡牌</span></a></div></li><style>.dropdown-item{display:inline-flex}</style><li class="nav-item dropdown"><a href=# class="nav-link dropdown-toggle" data-toggle=dropdown aria-haspopup=true><span>其他阅读渠道</span><span class=caret></span></a><div class=dropdown-menu><a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230218234451.png></img><span>知乎</span></a>
<a class=dropdown-item href=https://segmentfault.com/blog/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113556.png></img><span>SegmentFault</span></a>
<a class=dropdown-item href=https://zhuanlan.zhihu.com/wxyyxc1992><img style=width:16px;height:16px;display:inline-block;margin-right:8px src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230219113519.png></img><span>掘金</span></a></div></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=搜索><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class=nav-link href=https://github.com/wx-chevalier aria-label=GitHub><i class="fa-brands fa-github" aria-hidden=true></i></a></li><div></div><style>@media only screen and (max-width:600px){.jimmysong-template{display:none!important}}</style><li class=jimmysong-template style=color:#fff;font-size:12px><a href=https://jimmysong.io style=color:#fff>By Jimmy Song's Template</a></li></ul></div></nav></header></div><div class=page-body><link rel=stylesheet href=//unpkg.com/heti/umd/heti.min.css><div class="container-xl docs"><div class="row flex-xl-nowrap"><div class=docs-sidebar><form class="docs-search d-flex align-items-center"><button class="btn docs-toggle d-md-none p-0 mr-md-3 w-100" type=button data-toggle=collapse data-target=#docs-nav aria-controls=docs-nav aria-expanded=false aria-label="Toggle section navigation"><div class=d-flex><span class="d-md-none pl-1 flex-grow-1 text-left overflow-hidden">2019-Andrew Ng-深度学习课程</span>
<span><i class="fas fa-chevron-down"></i></span></div></button>
<button class="form-control sidebar-search js-search d-none d-md-flex">
<i class="fas fa-search pr-2"></i>
<span class=sidebar-search-text>搜索...</span>
<span class=sidebar-search-shortcut>/</span></button></form><nav class="collapse docs-links" id=docs-nav><ul class="nav docs-sidenav"><li style=display:inline-flex><a style=cursor:pointer onclick=window.history.back()><i class="fas fa-arrow-left pr-1"></i>
Back</a>
<span>|</span>
<a href=/books/><i class="fa-solid fa-house" style=margin-right:4px></i>
Books</a></li></ul><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id1cb603993076354ae684c031037738c1")' href=#id1cb603993076354ae684c031037738c1 aria-expanded=false aria-controls=id1cb603993076354ae684c031037738c1 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/>99.参考资料</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id1cb603993076354ae684c031037738c1 aria-expanded=false aria-controls=id1cb603993076354ae684c031037738c1><i class="fa-solid fa-angle-down" id=caret-id1cb603993076354ae684c031037738c1></i></a></div><ul class="nav docs-sidenav collapse show" id=id1cb603993076354ae684c031037738c1><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6d7da24804841595467ee06940ffadad")' href=#id6d7da24804841595467ee06940ffadad aria-expanded=false aria-controls=id6d7da24804841595467ee06940ffadad aria-hidden=false data-toggle=collapse></div></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id424867f26fc15069d5e95f339f8da715")' href=#id424867f26fc15069d5e95f339f8da715 aria-expanded=false aria-controls=id424867f26fc15069d5e95f339f8da715 aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/>2019-Andrew Ng-深度学习课程</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id424867f26fc15069d5e95f339f8da715 aria-expanded=false aria-controls=id424867f26fc15069d5e95f339f8da715><i class="fa-solid fa-angle-down" id=caret-id424867f26fc15069d5e95f339f8da715></i></a></div><ul class="nav docs-sidenav collapse show" id=id424867f26fc15069d5e95f339f8da715><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/interview/>interview</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week1/>lesson1-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week2/>lesson1-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week3/>lesson1-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson1-week4/>lesson1-week4</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week1/>lesson2-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week2/>lesson2-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson2-week3/>lesson2-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson3-week1/>lesson3-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson3-week2/>lesson3-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week1/>lesson4-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week2/>lesson4-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week3/>lesson4-week3</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson4-week4/>lesson4-week4</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week1/>lesson5-week1</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week2/>lesson5-week2</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week3/>lesson5-week3</a></li><li class="child level active"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/math/>math</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/notation/>notation</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/summary/>SUMMARY</a></li></ul></div><div class="docs-toc-item has-child"><div class="parent-node d-flex justify-content-between" onclick='Collapse("caret-id6d61b824616f345f413069d041a91bcd")' href=#id6d61b824616f345f413069d041a91bcd aria-expanded=false aria-controls=id6d61b824616f345f413069d041a91bcd aria-hidden=false data-toggle=collapse><a class="d-inline docs-toc-link" href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>2021-李沐-《动手学习深度学习》</a>
<a class="nav-toogle d-inline level" aria-hidden=false data-toggle=collapse href=#id6d61b824616f345f413069d041a91bcd aria-expanded=false aria-controls=id6d61b824616f345f413069d041a91bcd><i class="fa-solid fa-angle-right" id=caret-id6d61b824616f345f413069d041a91bcd></i></a></div><ul class="nav docs-sidenav collapse" id=id6d61b824616f345f413069d041a91bcd><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/00-%E9%A2%84%E5%91%8A/>00-预告</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/01-%E8%AF%BE%E7%A8%8B%E5%AE%89%E6%8E%92/>01-课程安排</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/02-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%BB%8B%E7%BB%8D/>02-深度学习介绍</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/03-%E5%AE%89%E8%A3%85/>03-安装</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/04-%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C/>04-数据读取和操作</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/05-%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/>05-线性代数</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/06-%E7%9F%A9%E9%98%B5%E8%AE%A1%E7%AE%97/>06-矩阵计算</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/07-%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC/>07-自动求导</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/08-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92+%E5%9F%BA%E7%A1%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/>08-线性回归+基础优化算法</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/09-softmax-%E5%9B%9E%E5%BD%92/>09-Softmax 回归</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/10-%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/>10-多层感知机</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/11-%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9+%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88/>11-模型选择+过拟合和欠拟合</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/12-%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80/>12-权重衰退</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/13-%E4%B8%A2%E5%BC%83%E6%B3%95/>13-丢弃法</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/14-%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7/>14-数值稳定性</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/15-%E5%AE%9E%E6%88%98kaggle%E6%AF%94%E8%B5%9B%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7/>15-实战Kaggle比赛：预测房价</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/16-pytorch%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/>16-Pytorch神经网络基础</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/17-%E4%BD%BF%E7%94%A8%E5%92%8C%E8%B4%AD%E4%B9%B0gpu/>17-使用和购买GPU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/18-%E9%A2%84%E6%B5%8B%E6%88%BF%E4%BB%B7%E7%AB%9E%E8%B5%9B%E6%80%BB%E7%BB%93/>18-预测房价竞赛总结</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/19-%E5%8D%B7%E7%A7%AF%E5%B1%82/>19-卷积层</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/20-%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85/>20-填充和步幅</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/21-%E5%A4%9A%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E9%80%9A%E9%81%93/>21-多输入输出通道</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/22-%E6%B1%A0%E5%8C%96%E5%B1%82/>22-池化层</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/23-%E7%BB%8F%E5%85%B8%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Clenet/>23-经典卷积神经网络LeNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/24-alexnet/>24-AlexNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/25-%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C-vgg/>25-使用块的网络 VGG</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/26-nin/>26-NiN</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/27-googlenet/>27-GoogLeNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/28-%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96/>28-批量归一化</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/29-%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9Cresnet/>29-残差网络ResNet</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/30-%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%E5%AE%8C%E7%BB%93%E7%AB%9E%E8%B5%9B%E5%9B%BE%E7%89%87%E5%88%86%E7%B1%BB/>30-第二部分完结竞赛：图片分类</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/31-cpu%E5%92%8Cgpu/>31-CPU和GPU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/32-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%A1%AC%E4%BB%B6/>32-深度学习硬件</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/33-%E5%8D%95%E6%9C%BA%E5%A4%9A%E5%8D%A1%E5%B9%B6%E8%A1%8C/>33-单机多卡并行</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/34-%E5%A4%9Agpu%E8%AE%AD%E7%BB%83%E5%AE%9E%E7%8E%B0only-qa/>34-多GPU训练实现(only QA)</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/35-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/>35-分布式训练</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/36-%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%B9%BF/>36-数据增广</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/37-%E5%BE%AE%E8%B0%83/>37-微调</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/38-%E7%AC%AC%E4%BA%8C%E6%AC%A1%E7%AB%9E%E8%B5%9B%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E7%BB%93%E6%9E%9C/>38-第二次竞赛树叶分类结果</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/39-%E5%AE%9E%E6%88%98kaggle%E7%AB%9E%E8%B5%9Bcifar-10/>39-实战Kaggle竞赛：CIFAR-10</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/41-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86/>41-物体检测和数据集</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/43-%E6%A0%91%E5%8F%B6%E5%88%86%E7%B1%BB%E7%AB%9E%E8%B5%9B%E6%8A%80%E6%9C%AF%E6%80%BB%E7%BB%93/>43-树叶分类竞赛技术总结</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/44-%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95r-cnnssdyolo/>44-物体检测算法：R-CNN,SSD,YOLO</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/46-%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2/>46-语义分割</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/47-%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF/>47-转置卷积</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/48-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cfcn/>48-全连接卷积神经网络（FCN）</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/49-%E6%A0%B7%E5%BC%8F%E8%BF%81%E7%A7%BB/>49-样式迁移</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/50-%E8%AF%BE%E7%A8%8B%E7%AB%9E%E8%B5%9B%E7%89%9B%E4%BB%94%E8%A1%8C%E5%A4%B4%E6%A3%80%E6%B5%8B/>50-课程竞赛：牛仔行头检测</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/51-%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B/>51-序列模型</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/53-%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/>53-语言模型</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/54-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Crnn/>54-循环神经网络RNN</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/56-gru/>56-GRU</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/57-lstm/>57-LSTM</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/58-%E6%B7%B1%E5%B1%82%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/>58-深层循环神经网络</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/61-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8%E6%9E%B6%E6%9E%84/>61-编码器-解码器架构</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/62-%E5%BA%8F%E5%88%97%E5%88%B0%E5%BA%8F%E5%88%97%E5%AD%A6%E4%B9%A0/>62-序列到序列学习</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/63-%E6%9D%9F%E6%90%9C%E7%B4%A2/>63-束搜索</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/65-%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0/>65-注意力分数</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/68-transformer/>68-Transformer</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/69-bert-%E9%A2%84%E8%AE%AD%E7%BB%83/>69-BERT 预训练</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/70-bert-%E5%BE%AE%E8%B0%83/>70-BERT 微调</a></li><li class="child level"><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2021-%E6%9D%8E%E6%B2%90-%E5%8A%A8%E6%89%8B%E5%AD%A6%E4%B9%A0%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/72-%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/>72-优化算法</a></li></ul></div></ul></div></nav></div><div class="d-none d-xl-block col-xl-2 docs-toc"><ul class="nav toc-top"><li><a href=# id=back_to_top class=docs-toc-title>目录</a></li></ul><nav id=TableOfContents><ul><li><a href=#机器学习的数学基础>机器学习的数学基础</a><ul><li><a href=#高等数学>高等数学</a></li><li><a href=#线性代数>线性代数</a></li><li><a href=#概率论和数理统计>概率论和数理统计</a></li></ul></li></ul></nav><div class="subscribe-module col-24 mt-1"><img src=https://ngte-superbed.oss-cn-beijing.aliyuncs.com/item/20230220172727.png alt=image title=王下邀月熊的微信公众号></div></div><main class="py-md-3 pl-md-3 docs-content col-xl-8" role=main><article class=article><h1>math</h1><div class=article-style><h2 id=机器学习的数学基础>机器学习的数学基础</h2><p>[TOC]</p><h3 id=高等数学>高等数学</h3><p><strong>1.导数定义：</strong></p><p>导数和微分的概念</p><p>$f&rsquo;({{x}<em>{0}})=\underset{\Delta x\to 0}{\mathop{\lim }},\frac{f({{x}</em>{0}}+\Delta x)-f({{x}_{0}})}{\Delta x}$ （1）</p><p>或者：</p><p>$f&rsquo;({{x}<em>{0}})=\underset{x\to {{x}</em>{0}}}{\mathop{\lim }},\frac{f(x)-f({{x}<em>{0}})}{x-{{x}</em>{0}}}$ （2）</p><p><strong>2.左右导数导数的几何意义和物理意义</strong></p><p>函数$f(x)$在$x_0$处的左、右导数分别定义为：</p><p>左导数：${{{f}&rsquo;}<em>{-}}({{x}</em>{0}})=\underset{\Delta x\to {{0}^{-}}}{\mathop{\lim }},\frac{f({{x}<em>{0}}+\Delta x)-f({{x}</em>{0}})}{\Delta x}=\underset{x\to x_{0}^{-}}{\mathop{\lim }},\frac{f(x)-f({{x}<em>{0}})}{x-{{x}</em>{0}}},(x={{x}_{0}}+\Delta x)$</p><p>右导数：${{{f}&rsquo;}<em>{+}}({{x}</em>{0}})=\underset{\Delta x\to {{0}^{+}}}{\mathop{\lim }},\frac{f({{x}<em>{0}}+\Delta x)-f({{x}</em>{0}})}{\Delta x}=\underset{x\to x_{0}^{+}}{\mathop{\lim }},\frac{f(x)-f({{x}<em>{0}})}{x-{{x}</em>{0}}}$</p><p><strong>3.函数的可导性与连续性之间的关系</strong></p><p><strong>Th1:</strong> 函数$f(x)$在$x_0$处可微$\Leftrightarrow f(x)$在$x_0$处可导</p><p><strong>Th2:</strong> 若函数在点$x_0$处可导，则$y=f(x)$在点$x_0$处连续，反之则不成立。即函数连续不一定可导。</p><p><strong>Th3:</strong> ${f}&rsquo;({{x}<em>{0}})$存在$\Leftrightarrow {{{f}&rsquo;}</em>{-}}({{x}<em>{0}})={{{f}&rsquo;}</em>{+}}({{x}_{0}})$</p><p><strong>4.平面曲线的切线和法线</strong></p><p>切线方程 : $y-{{y}<em>{0}}=f&rsquo;({{x}</em>{0}})(x-{{x}<em>{0}})$
法线方程：$y-{{y}</em>{0}}=-\frac{1}{f&rsquo;({{x}<em>{0}})}(x-{{x}</em>{0}}),f&rsquo;({{x}_{0}})\ne 0$</p><p><strong>5.四则运算法则</strong>
设函数$u=u(x)，v=v(x)$]在点$x$可导则
(1) $(u\pm v{)}&rsquo;={u}&rsquo;\pm {v}&rsquo;$ $d(u\pm v)=du\pm dv$
(2)$(uv{)}&rsquo;=u{v}&rsquo;+v{u}&rsquo;$ $d(uv)=udv+vdu$
(3) $(\frac{u}{v}{)}&rsquo;=\frac{v{u}&rsquo;-u{v}&rsquo;}{{{v}^{2}}}(v\ne 0)$ $d(\frac{u}{v})=\frac{vdu-udv}{{{v}^{2}}}$</p><p><strong>6.基本导数与微分表</strong>
(1) $y=c​$（常数） ${y}&rsquo;=0​$ $dy=0​$
(2) $y={{x}^{\alpha }}​$($\alpha ​$为实数) ${y}&rsquo;=\alpha {{x}^{\alpha -1}}​$ $dy=\alpha {{x}^{\alpha -1}}dx​$
(3) $y={{a}^{x}}​$ ${y}&rsquo;={{a}^{x}}\ln a​$ $dy={{a}^{x}}\ln adx​$
特例: $({{{e}}^{x}}{)}&rsquo;={{{e}}^{x}}​$ $d({{{e}}^{x}})={{{e}}^{x}}dx​$</p><p>(4) $y={{\log }_{a}}x$ ${y}&rsquo;=\frac{1}{x\ln a}$</p><p>$dy=\frac{1}{x\ln a}dx$
特例:$y=\ln x$ $(\ln x{)}&rsquo;=\frac{1}{x}$ $d(\ln x)=\frac{1}{x}dx$</p><p>(5) $y=\sin x$</p><p>${y}&rsquo;=\cos x$ $d(\sin x)=\cos xdx$</p><p>(6) $y=\cos x$</p><p>${y}&rsquo;=-\sin x$ $d(\cos x)=-\sin xdx$</p><p>(7) $y=\tan x$</p><p>${y}&rsquo;=\frac{1}{{{\cos }^{2}}x}={{\sec }^{2}}x$ $d(\tan x)={{\sec }^{2}}xdx$
(8) $y=\cot x$ ${y}&rsquo;=-\frac{1}{{{\sin }^{2}}x}=-{{\csc }^{2}}x$ $d(\cot x)=-{{\csc }^{2}}xdx$
(9) $y=\sec x$ ${y}&rsquo;=\sec x\tan x$</p><p>$d(\sec x)=\sec x\tan xdx$
(10) $y=\csc x$ ${y}&rsquo;=-\csc x\cot x$</p><p>$d(\csc x)=-\csc x\cot xdx$
(11) $y=\arcsin x$</p><p>${y}&rsquo;=\frac{1}{\sqrt{1-{{x}^{2}}}}$</p><p>$d(\arcsin x)=\frac{1}{\sqrt{1-{{x}^{2}}}}dx$
(12) $y=\arccos x$</p><p>${y}&rsquo;=-\frac{1}{\sqrt{1-{{x}^{2}}}}$ $d(\arccos x)=-\frac{1}{\sqrt{1-{{x}^{2}}}}dx$</p><p>(13) $y=\arctan x$</p><p>${y}&rsquo;=\frac{1}{1+{{x}^{2}}}$ $d(\arctan x)=\frac{1}{1+{{x}^{2}}}dx$</p><p>(14) $y=\operatorname{arc}\cot x$</p><p>${y}&rsquo;=-\frac{1}{1+{{x}^{2}}}$</p><p>$d(\operatorname{arc}\cot x)=-\frac{1}{1+{{x}^{2}}}dx$
(15) $y=shx$</p><p>${y}&rsquo;=chx$ $d(shx)=chxdx$</p><p>(16) $y=chx$</p><p>${y}&rsquo;=shx$ $d(chx)=shxdx$</p><p><strong>7.复合函数，反函数，隐函数以及参数方程所确定的函数的微分法</strong></p><p>(1) 反函数的运算法则: 设$y=f(x)$在点$x$的某邻域内单调连续，在点$x$处可导且${f}&rsquo;(x)\ne 0$，则其反函数在点$x$所对应的$y$处可导，并且有$\frac{dy}{dx}=\frac{1}{\frac{dx}{dy}}$
(2) 复合函数的运算法则:若$\mu =\varphi (x)$在点$x$可导,而$y=f(\mu )$在对应点$\mu $($\mu =\varphi (x)$)可导,则复合函数$y=f(\varphi (x))$在点$x$可导,且${y}&rsquo;={f}&rsquo;(\mu )\cdot {\varphi }&rsquo;(x)$
(3) 隐函数导数$\frac{dy}{dx}$的求法一般有三种方法：
1)方程两边对$x$求导，要记住$y$是$x$的函数，则$y$的函数是$x$的复合函数.例如$\frac{1}{y}$，${{y}^{2}}$，$ln y$，${{{e}}^{y}}$等均是$x$的复合函数.
对$x$求导应按复合函数连锁法则做.
2)公式法.由$F(x,y)=0$知 $\frac{dy}{dx}=-\frac{{{{{F}&rsquo;}}<em>{x}}(x,y)}{{{{{F}&rsquo;}}</em>{y}}(x,y)}$,其中，${{{F}&rsquo;}<em>{x}}(x,y)$，
${{{F}&rsquo;}</em>{y}}(x,y)$分别表示$F(x,y)$对$x$和$y$的偏导数
3)利用微分形式不变性</p><p><strong>8.常用高阶导数公式</strong></p><p>（1）$({{a}^{x}}){{,}^{(n)}}={{a}^{x}}{{\ln }^{n}}a\quad (a>{0})\quad \quad ({{{e}}^{x}}){{,}^{(n)}}={e}{{,}^{x}}$
（2）$(\sin kx{)}{{,}^{(n)}}={{k}^{n}}\sin (kx+n\cdot \frac{\pi }{{2}})$
（3）$(\cos kx{)}{{,}^{(n)}}={{k}^{n}}\cos (kx+n\cdot \frac{\pi }{{2}})$
（4）$({{x}^{m}}){{,}^{(n)}}=m(m-1)\cdots (m-n+1){{x}^{m-n}}$
（5）$(\ln x){{,}^{(n)}}={{(-{1})}^{(n-{1})}}\frac{(n-{1})!}{{{x}^{n}}}$
（6）莱布尼兹公式：若$u(x),,v(x)$均$n$阶可导，则
${{(uv)}^{(n)}}=\sum\limits_{i={0}}^{n}{c_{n}^{i}{{u}^{(i)}}{{v}^{(n-i)}}}$，其中${{u}^{({0})}}=u$，${{v}^{({0})}}=v$</p><p><strong>9.微分中值定理，泰勒公式</strong></p><p><strong>Th1:</strong>(费马定理)</p><p>若函数$f(x)$满足条件：
(1)函数$f(x)$在${{x}<em>{0}}$的某邻域内有定义，并且在此邻域内恒有
$f(x)\le f({{x}</em>{0}})$或$f(x)\ge f({{x}_{0}})$,</p><p>(2) $f(x)$在${{x}<em>{0}}$处可导,则有 ${f}&rsquo;({{x}</em>{0}})=0$</p><p><strong>Th2:</strong>(罗尔定理)</p><p>设函数$f(x)$满足条件：
(1)在闭区间$[a,b]$上连续；</p><p>(2)在$(a,b)$内可导；</p><p>(3)$f(a)=f(b)$；</p><p>则在$(a,b)$内一存在个$\xi $，使 ${f}&rsquo;(\xi )=0$
<strong>Th3:</strong> (拉格朗日中值定理)</p><p>设函数$f(x)$满足条件：
(1)在$[a,b]$上连续；</p><p>(2)在$(a,b)$内可导；</p><p>则在$(a,b)$内一存在个$\xi $，使 $\frac{f(b)-f(a)}{b-a}={f}&rsquo;(\xi )$</p><p><strong>Th4:</strong> (柯西中值定理)</p><p>设函数$f(x)$，$g(x)$满足条件：
(1) 在$[a,b]$上连续；</p><p>(2) 在$(a,b)$内可导且${f}&rsquo;(x)$，${g}&rsquo;(x)$均存在，且${g}&rsquo;(x)\ne 0$</p><p>则在$(a,b)$内存在一个$\xi $，使 $\frac{f(b)-f(a)}{g(b)-g(a)}=\frac{{f}&rsquo;(\xi )}{{g}&rsquo;(\xi )}$</p><p><strong>10.洛必达法则</strong>
法则Ⅰ ($\frac{0}{0}$型)
设函数$f\left( x \right),g\left( x \right)$满足条件：
$\underset{x\to {{x}<em>{0}}}{\mathop{\lim }},f\left( x \right)=0,\underset{x\to {{x}</em>{0}}}{\mathop{\lim }},g\left( x \right)=0$;</p><p>$f\left( x \right),g\left( x \right)$在${{x}<em>{0}}$的邻域内可导，(在${{x}</em>{0}}$处可除外)且${g}&rsquo;\left( x \right)\ne 0$;</p><p>$\underset{x\to {{x}_{0}}}{\mathop{\lim }},\frac{{f}&rsquo;\left( x \right)}{{g}&rsquo;\left( x \right)}$存在(或$\infty $)。</p><p>则:
$\underset{x\to {{x}<em>{0}}}{\mathop{\lim }},\frac{f\left( x \right)}{g\left( x \right)}=\underset{x\to {{x}</em>{0}}}{\mathop{\lim }},\frac{{f}&rsquo;\left( x \right)}{{g}&rsquo;\left( x \right)}$。
法则${{I}&rsquo;}$ ($\frac{0}{0}$型)设函数$f\left( x \right),g\left( x \right)$满足条件：
$\underset{x\to \infty }{\mathop{\lim }},f\left( x \right)=0,\underset{x\to \infty }{\mathop{\lim }},g\left( x \right)=0$;</p><p>存在一个$X>0$,当$\left| x \right|>X$时,$f\left( x \right),g\left( x \right)$可导,且${g}&rsquo;\left( x \right)\ne 0$;$\underset{x\to {{x}_{0}}}{\mathop{\lim }},\frac{{f}&rsquo;\left( x \right)}{{g}&rsquo;\left( x \right)}$存在(或$\infty $)。</p><p>则:
$\underset{x\to {{x}<em>{0}}}{\mathop{\lim }},\frac{f\left( x \right)}{g\left( x \right)}=\underset{x\to {{x}</em>{0}}}{\mathop{\lim }},\frac{{f}&rsquo;\left( x \right)}{{g}&rsquo;\left( x \right)}$
法则Ⅱ($\frac{\infty }{\infty }$型) 设函数$f\left( x \right),g\left( x \right)$满足条件：
$\underset{x\to {{x}<em>{0}}}{\mathop{\lim }},f\left( x \right)=\infty ,\underset{x\to {{x}</em>{0}}}{\mathop{\lim }},g\left( x \right)=\infty $; $f\left( x \right),g\left( x \right)$在${{x}<em>{0}}$ 的邻域内可导(在${{x}</em>{0}}$处可除外)且${g}&rsquo;\left( x \right)\ne 0$;$\underset{x\to {{x}<em>{0}}}{\mathop{\lim }},\frac{{f}&rsquo;\left( x \right)}{{g}&rsquo;\left( x \right)}$存在(或$\infty $)。则
$\underset{x\to {{x}</em>{0}}}{\mathop{\lim }},\frac{f\left( x \right)}{g\left( x \right)}=\underset{x\to {{x}_{0}}}{\mathop{\lim }},\frac{{f}&rsquo;\left( x \right)}{{g}&rsquo;\left( x \right)}.$同理法则${I{I}&rsquo;}$($\frac{\infty }{\infty }$型)仿法则${{I}&rsquo;}$可写出。</p><p><strong>11.泰勒公式</strong></p><p>设函数$f(x)$在点${{x}<em>{0}}$处的某邻域内具有$n+1$阶导数，则对该邻域内异于${{x}</em>{0}}$的任意点$x$，在${{x}<em>{0}}$与$x$之间至少存在
一个$\xi $，使得：
$f(x)=f({{x}</em>{0}})+{f}&rsquo;({{x}<em>{0}})(x-{{x}</em>{0}})+\frac{1}{2!}{f}&rsquo;&rsquo;({{x}<em>{0}}){{(x-{{x}</em>{0}})}^{2}}+\cdots $
$+\frac{{{f}^{(n)}}({{x}<em>{0}})}{n!}{{(x-{{x}</em>{0}})}^{n}}+{{R}<em>{n}}(x)$
其中 ${{R}</em>{n}}(x)=\frac{{{f}^{(n+1)}}(\xi )}{(n+1)!}{{(x-{{x}<em>{0}})}^{n+1}}$称为$f(x)$在点${{x}</em>{0}}$处的$n$阶泰勒余项。</p><p>令${{x}<em>{0}}=0$，则$n$阶泰勒公式
$f(x)=f(0)+{f}&rsquo;(0)x+\frac{1}{2!}{f}&rsquo;&rsquo;(0){{x}^{2}}+\cdots +\frac{{{f}^{(n)}}(0)}{n!}{{x}^{n}}+{{R}</em>{n}}(x)$……(1)
其中 ${{R}_{n}}(x)=\frac{{{f}^{(n+1)}}(\xi )}{(n+1)!}{{x}^{n+1}}$，$\xi $在0与$x$之间.(1)式称为麦克劳林公式</p><p><strong>常用五种函数在${{x}_{0}}=0$处的泰勒公式</strong></p><p>(1) ${{{e}}^{x}}=1+x+\frac{1}{2!}{{x}^{2}}+\cdots +\frac{1}{n!}{{x}^{n}}+\frac{{{x}^{n+1}}}{(n+1)!}{{e}^{\xi }}$</p><p>或 $=1+x+\frac{1}{2!}{{x}^{2}}+\cdots +\frac{1}{n!}{{x}^{n}}+o({{x}^{n}})$</p><p>(2) $\sin x=x-\frac{1}{3!}{{x}^{3}}+\cdots +\frac{{{x}^{n}}}{n!}\sin \frac{n\pi }{2}+\frac{{{x}^{n+1}}}{(n+1)!}\sin (\xi +\frac{n+1}{2}\pi )$</p><p>或 $=x-\frac{1}{3!}{{x}^{3}}+\cdots +\frac{{{x}^{n}}}{n!}\sin \frac{n\pi }{2}+o({{x}^{n}})$</p><p>(3) $\cos x=1-\frac{1}{2!}{{x}^{2}}+\cdots +\frac{{{x}^{n}}}{n!}\cos \frac{n\pi }{2}+\frac{{{x}^{n+1}}}{(n+1)!}\cos (\xi +\frac{n+1}{2}\pi )$</p><p>或 $=1-\frac{1}{2!}{{x}^{2}}+\cdots +\frac{{{x}^{n}}}{n!}\cos \frac{n\pi }{2}+o({{x}^{n}})$</p><p>(4) $\ln (1+x)=x-\frac{1}{2}{{x}^{2}}+\frac{1}{3}{{x}^{3}}-\cdots +{{(-1)}^{n-1}}\frac{{{x}^{n}}}{n}+\frac{{{(-1)}^{n}}{{x}^{n+1}}}{(n+1){{(1+\xi )}^{n+1}}}$</p><p>或 $=x-\frac{1}{2}{{x}^{2}}+\frac{1}{3}{{x}^{3}}-\cdots +{{(-1)}^{n-1}}\frac{{{x}^{n}}}{n}+o({{x}^{n}})$</p><p>(5) ${{(1+x)}^{m}}=1+mx+\frac{m(m-1)}{2!}{{x}^{2}}+\cdots +\frac{m(m-1)\cdots (m-n+1)}{n!}{{x}^{n}}$
$+\frac{m(m-1)\cdots (m-n+1)}{(n+1)!}{{x}^{n+1}}{{(1+\xi )}^{m-n-1}}$</p><p>或 ${{(1+x)}^{m}}=1+mx+\frac{m(m-1)}{2!}{{x}^{2}}+\cdots $ $+\frac{m(m-1)\cdots (m-n+1)}{n!}{{x}^{n}}+o({{x}^{n}})$</p><p><strong>12.函数单调性的判断</strong>
<strong>Th1:</strong> 设函数$f(x)$在$(a,b)$区间内可导，如果对$\forall x\in (a,b)$，都有$f,&rsquo;(x)>0$（或$f,&rsquo;(x)&lt;0$），则函数$f(x)$在$(a,b)$内是单调增加的（或单调减少）</p><p><strong>Th2:</strong> （取极值的必要条件）设函数$f(x)$在${{x}<em>{0}}$处可导，且在${{x}</em>{0}}$处取极值，则$f,&rsquo;({{x}_{0}})=0$。</p><p><strong>Th3:</strong> （取极值的第一充分条件）设函数$f(x)$在${{x}<em>{0}}$的某一邻域内可微，且$f,&rsquo;({{x}</em>{0}})=0$（或$f(x)$在${{x}<em>{0}}$处连续，但$f,&rsquo;({{x}</em>{0}})$不存在。）
(1)若当$x$经过${{x}<em>{0}}$时，$f,&rsquo;(x)$由“+”变“-”，则$f({{x}</em>{0}})$为极大值；
(2)若当$x​$经过${{x}<em>{0}}​$时，$f,&rsquo;(x)$由“-”变“+”，则$f({{x}</em>{0}})$为极小值；
(3)若$f,&rsquo;(x)$经过$x={{x}<em>{0}}$的两侧不变号，则$f({{x}</em>{0}})$不是极值。</p><p><strong>Th4:</strong> (取极值的第二充分条件)设$f(x)$在点${{x}<em>{0}}$处有$f&rsquo;&rsquo;(x)\ne 0$，且$f,&rsquo;({{x}</em>{0}})=0$，则 当$f&rsquo;,&rsquo;({{x}<em>{0}})&lt;0$时，$f({{x}</em>{0}})$为极大值；
当$f&rsquo;,&rsquo;({{x}<em>{0}})>0$时，$f({{x}</em>{0}})$为极小值。
注：如果$f&rsquo;,&rsquo;({{x}_{0}})&lt;0$，此方法失效。</p><p><strong>13.渐近线的求法</strong>
(1)水平渐近线 若$\underset{x\to +\infty }{\mathop{\lim }},f(x)=b$，或$\underset{x\to -\infty }{\mathop{\lim }},f(x)=b$，则</p><p>$y=b$称为函数$y=f(x)$的水平渐近线。</p><p>(2)铅直渐近线 若$\underset{x\to x_{0}^{-}}{\mathop{\lim }},f(x)=\infty $，或$\underset{x\to x_{0}^{+}}{\mathop{\lim }},f(x)=\infty $，则</p><p>$x={{x}_{0}}$称为$y=f(x)$的铅直渐近线。</p><p>(3)斜渐近线 若$a=\underset{x\to \infty }{\mathop{\lim }},\frac{f(x)}{x},\quad b=\underset{x\to \infty }{\mathop{\lim }},[f(x)-ax]$，则
$y=ax+b$称为$y=f(x)$的斜渐近线。</p><p><strong>14.函数凹凸性的判断</strong>
<strong>Th1:</strong> (凹凸性的判别定理）若在I上$f&rsquo;&rsquo;(x)&lt;0$（或$f&rsquo;&rsquo;(x)>0$），则$f(x)$在I上是凸的（或凹的）。</p><p><strong>Th2:</strong> (拐点的判别定理1)若在${{x}<em>{0}}$处$f&rsquo;&rsquo;(x)=0$，（或$f&rsquo;&rsquo;(x)$不存在），当$x$变动经过${{x}</em>{0}}$时，$f&rsquo;&rsquo;(x)$变号，则$({{x}<em>{0}},f({{x}</em>{0}}))$为拐点。</p><p><strong>Th3:</strong> (拐点的判别定理2)设$f(x)$在${{x}<em>{0}}$点的某邻域内有三阶导数，且$f&rsquo;&rsquo;(x)=0$，$f&rsquo;&rsquo;&rsquo;(x)\ne 0$，则$({{x}</em>{0}},f({{x}_{0}}))$为拐点。</p><p><strong>15.弧微分</strong></p><p>$dS=\sqrt{1+y{{&rsquo;}^{2}}}dx$</p><p><strong>16.曲率</strong></p><p>曲线$y=f(x)$在点$(x,y)$处的曲率$k=\frac{\left| y&rsquo;&rsquo; \right|}{{{(1+y{{&rsquo;}^{2}})}^{\tfrac{3}{2}}}}$。
对于参数方程$\left{ \begin{align} & x=\varphi (t) \ & y=\psi (t) \ \end{align} \right.,$$k=\frac{\left| \varphi &lsquo;(t)\psi &lsquo;&rsquo;(t)-\varphi &lsquo;&rsquo;(t)\psi &lsquo;(t) \right|}{{{[\varphi {{&rsquo;}^{2}}(t)+\psi {{&rsquo;}^{2}}(t)]}^{\tfrac{3}{2}}}}$。</p><p><strong>17.曲率半径</strong></p><p>曲线在点$M$处的曲率$k(k\ne 0)$与曲线在点$M$处的曲率半径$\rho $有如下关系：$\rho =\frac{1}{k}$。</p><h3 id=线性代数>线性代数</h3><h4 id=行列式>行列式</h4><p><strong>1.行列式按行（列）展开定理</strong></p><p>(1) 设$A = ( a_{{ij}} )<em>{n \times n}$，则：$a</em>{i1}A_{j1} +a_{i2}A_{j2} + \cdots + a_{{in}}A_{{jn}} = \begin{cases}|A|,i=j\ 0,i \neq j\end{cases}$</p><p>或$a_{1i}A_{1j} + a_{2i}A_{2j} + \cdots + a_{{ni}}A_{{nj}} = \begin{cases}|A|,i=j\ 0,i \neq j\end{cases}$即 $AA^{<em>} = A^{</em>}A = \left| A \right|E,$其中：$A^{*} = \begin{pmatrix} A_{11} & A_{12} & \ldots & A_{1n} \ A_{21} & A_{22} & \ldots & A_{2n} \ \ldots & \ldots & \ldots & \ldots \ A_{n1} & A_{n2} & \ldots & A_{{nn}} \ \end{pmatrix} = (A_{{ji}}) = {(A_{{ij}})}^{T}$</p><p>$D_{n} = \begin{vmatrix} 1 & 1 & \ldots & 1 \ x_{1} & x_{2} & \ldots & x_{n} \ \ldots & \ldots & \ldots & \ldots \ x_{1}^{n - 1} & x_{2}^{n - 1} & \ldots & x_{n}^{n - 1} \ \end{vmatrix} = \prod_{1 \leq j &lt; i \leq n}^{},(x_{i} - x_{j})$</p><p>(2) 设$A,B$为$n$阶方阵，则$\left| {AB} \right| = \left| A \right|\left| B \right| = \left| B \right|\left| A \right| = \left| {BA} \right|$，但$\left| A \pm B \right| = \left| A \right| \pm \left| B \right|$不一定成立。</p><p>(3) $\left| {kA} \right| = k^{n}\left| A \right|$,$A$为$n$阶方阵。</p><p>(4) 设$A$为$n$阶方阵，$|A^{T}| = |A|;|A^{- 1}| = |A|^{- 1}$（若$A$可逆），$|A^{*}| = |A|^{n - 1}$</p><p>$n \geq 2$</p><p>(5) $\left| \begin{matrix} & {A\quad O} \ & {O\quad B} \ \end{matrix} \right| = \left| \begin{matrix} & {A\quad C} \ & {O\quad B} \ \end{matrix} \right| = \left| \begin{matrix} & {A\quad O} \ & {C\quad B} \ \end{matrix} \right| =| A||B|$
，$A,B$为方阵，但$\left| \begin{matrix} {O} & A_{m \times m} \ B_{n \times n} & { O} \ \end{matrix} \right| = ({- 1)}^{{mn}}|A||B|$ 。</p><p>(6) 范德蒙行列式$D_{n} = \begin{vmatrix} 1 & 1 & \ldots & 1 \ x_{1} & x_{2} & \ldots & x_{n} \ \ldots & \ldots & \ldots & \ldots \ x_{1}^{n - 1} & x_{2}^{n 1} & \ldots & x_{n}^{n - 1} \ \end{vmatrix} = \prod_{1 \leq j &lt; i \leq n}^{},(x_{i} - x_{j})$</p><p>设$A$是$n$阶方阵，$\lambda_{i}(i = 1,2\cdots,n)$是$A$的$n$个特征值，则
$|A| = \prod_{i = 1}^{n}\lambda_{i}​$</p><h4 id=矩阵>矩阵</h4><p>矩阵：$m \times n$个数$a_{{ij}}$排成$m$行$n$列的表格$\begin{bmatrix} a_{11}\quad a_{12}\quad\cdots\quad a_{1n} \ a_{21}\quad a_{22}\quad\cdots\quad a_{2n} \ \quad\cdots\cdots\cdots\cdots\cdots \ a_{m1}\quad a_{m2}\quad\cdots\quad a_{{mn}} \ \end{bmatrix}$ 称为矩阵，简记为$A$，或者$\left( a_{{ij}} \right)_{m \times n}$ 。若$m = n$，则称$A$是$n$阶矩阵或$n$阶方阵。</p><p><strong>矩阵的线性运算</strong></p><p><strong>1.矩阵的加法</strong></p><p>设$A = (a_{{ij}}),B = (b_{{ij}})$是两个$m \times n$矩阵，则$m \times n$ 矩阵$C = c_{{ij}}) = a_{{ij}} + b_{{ij}}$称为矩阵$A$与$B$的和，记为$A + B = C$ 。</p><p><strong>2.矩阵的数乘</strong></p><p>设$A = (a_{{ij}})$是$m \times n$矩阵，$k$是一个常数，则$m \times n$矩阵$(ka_{{ij}})$称为数$k$与矩阵$A$的数乘，记为${kA}$。</p><p><strong>3.矩阵的乘法</strong></p><p>设$A = (a_{{ij}})$是$m \times n$矩阵，$B = (b_{{ij}})$是$n \times s$矩阵，那么$m \times s$矩阵$C = (c_{{ij}})$，其中$c_{{ij}} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{{in}}b_{{nj}} = \sum_{k =1}^{n}{a_{{ik}}b_{{kj}}}$称为${AB}$的乘积，记为$C = AB$ 。</p><p><strong>4.</strong> $\mathbf{A}^{\mathbf{T}}$<strong>、</strong>$\mathbf{A}^{\mathbf{-1}}$<strong>、</strong>$\mathbf{A}^{\mathbf{*}}$<strong>三者之间的关系</strong></p><p>(1) ${(A^{T})}^{T} = A,{(AB)}^{T} = B^{T}A^{T},{(kA)}^{T} = kA^{T},{(A \pm B)}^{T} = A^{T} \pm B^{T}$</p><p>(2) $\left( A^{- 1} \right)^{- 1} = A,\left( {AB} \right)^{- 1} = B^{- 1}A^{- 1},\left( {kA} \right)^{- 1} = \frac{1}{k}A^{- 1},$</p><p>但 ${(A \pm B)}^{- 1} = A^{- 1} \pm B^{- 1}$不一定成立。</p><p>(3) $\left( A^{<em>} \right)^{</em>} = |A|^{n - 2}\ A\ \ (n \geq 3)$，$\left({AB} \right)^{<em>} = B^{</em>}A^{<em>},$ $\left( {kA} \right)^{</em>} = k^{n -1}A^{*}{\ \ }\left( n \geq 2 \right)$</p><p>但$\left( A \pm B \right)^{<em>} = A^{</em>} \pm B^{*}$不一定成立。</p><p>(4) ${(A^{- 1})}^{T} = {(A^{T})}^{- 1},\ \left( A^{- 1} \right)^{<em>} ={(AA^{</em>})}^{- 1},{(A^{<em>})}^{T} = \left( A^{T} \right)^{</em>}$</p><p><strong>5.有关</strong>$\mathbf{A}^{\mathbf{*}}$<strong>的结论</strong></p><p>(1) $AA^{<em>} = A^{</em>}A = |A|E$</p><p>(2) $|A^{<em>}| = |A|^{n - 1}\ (n \geq 2),\ \ \ \ {(kA)}^{</em>} = k^{n -1}A^{<em>},{{\ \ }\left( A^{</em>} \right)}^{*} = |A|^{n - 2}A(n \geq 3)$</p><p>(3) 若$A$可逆，则$A^{<em>} = |A|A^{- 1},{(A^{</em>})}^{*} = \frac{1}{|A|}A$</p><p>(4) 若$A​$为$n​$阶方阵，则：</p><p>$r(A^*)=\begin{cases}n,\quad r(A)=n\ 1,\quad r(A)=n-1\ 0,\quad r(A)&lt;n-1\end{cases}$</p><p><strong>6.有关</strong>$\mathbf{A}^{\mathbf{- 1}}$<strong>的结论</strong></p><p>$A$可逆$\Leftrightarrow AB = E; \Leftrightarrow |A| \neq 0; \Leftrightarrow r(A) = n;$</p><p>$\Leftrightarrow A$可以表示为初等矩阵的乘积；$\Leftrightarrow A;\Leftrightarrow Ax = 0$。</p><p><strong>7.有关矩阵秩的结论</strong></p><p>(1) 秩$r(A)$=行秩=列秩；</p><p>(2) $r(A_{m \times n}) \leq \min(m,n);$</p><p>(3) $A \neq 0 \Rightarrow r(A) \geq 1$；</p><p>(4) $r(A \pm B) \leq r(A) + r(B);$</p><p>(5) 初等变换不改变矩阵的秩</p><p>(6) $r(A) + r(B) - n \leq r(AB) \leq \min(r(A),r(B)),$特别若$AB = O$
则：$r(A) + r(B) \leq n$</p><p>(7) 若$A^{- 1}$存在$\Rightarrow r(AB) = r(B);$ 若$B^{- 1}$存在
$\Rightarrow r(AB) = r(A);$</p><p>若$r(A_{m \times n}) = n \Rightarrow r(AB) = r(B);$ 若$r(A_{m \times s}) = n\Rightarrow r(AB) = r\left( A \right)$。</p><p>(8) $r(A_{m \times s}) = n \Leftrightarrow Ax = 0$只有零解</p><p><strong>8.分块求逆公式</strong></p><p>$\begin{pmatrix} A & O \ O & B \ \end{pmatrix}^{- 1} = \begin{pmatrix} A^{-1} & O \ O & B^{- 1} \ \end{pmatrix}$； $\begin{pmatrix} A & C \ O & B \\end{pmatrix}^{- 1} = \begin{pmatrix} A^{- 1}& - A^{- 1}CB^{- 1} \ O & B^{- 1} \ \end{pmatrix}$；</p><p>$\begin{pmatrix} A & O \ C & B \ \end{pmatrix}^{- 1} = \begin{pmatrix} A^{- 1}&{O} \ - B^{- 1}CA^{- 1} & B^{- 1} \\end{pmatrix}$； $\begin{pmatrix} O & A \ B & O \ \end{pmatrix}^{- 1} =\begin{pmatrix} O & B^{- 1} \ A^{- 1} & O \ \end{pmatrix}$</p><p>这里$A$，$B$均为可逆方阵。</p><h4 id=向量>向量</h4><p><strong>1.有关向量组的线性表示</strong></p><p>(1)$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性相关$\Leftrightarrow$至少有一个向量可以用其余向量线性表示。</p><p>(2)$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，$\beta$线性相关$\Leftrightarrow \beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$唯一线性表示。</p><p>(3) $\beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性表示
$\Leftrightarrow r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s}) =r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s},\beta)$ 。</p><p><strong>2.有关向量组的线性相关性</strong></p><p>(1)部分相关，整体相关；整体无关，部分无关.</p><p>(2) ① $n$个$n$维向量
$\alpha_{1},\alpha_{2}\cdots\alpha_{n}$线性无关$\Leftrightarrow \left|\left\lbrack \alpha_{1}\alpha_{2}\cdots\alpha_{n} \right\rbrack \right| \neq0$， $n$个$n$维向量$\alpha_{1},\alpha_{2}\cdots\alpha_{n}$线性相关
$\Leftrightarrow |\lbrack\alpha_{1},\alpha_{2},\cdots,\alpha_{n}\rbrack| = 0$
。</p><p>② $n + 1$个$n$维向量线性相关。</p><p>③ 若$\alpha_{1},\alpha_{2}\cdots\alpha_{S}$线性无关，则添加分量后仍线性无关；或一组向量线性相关，去掉某些分量后仍线性相关。</p><p><strong>3.有关向量组的线性表示</strong></p><p>(1) $\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性相关$\Leftrightarrow$至少有一个向量可以用其余向量线性表示。</p><p>(2) $\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，$\beta$线性相关$\Leftrightarrow\beta$ 可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$唯一线性表示。</p><p>(3) $\beta$可以由$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性表示
$\Leftrightarrow r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s}) =r(\alpha_{1},\alpha_{2},\cdots,\alpha_{s},\beta)$</p><p><strong>4.向量组的秩与矩阵的秩之间的关系</strong></p><p>设$r(A_{m \times n}) =r$，则$A$的秩$r(A)$与$A$的行列向量组的线性相关性关系为：</p><p>(1) 若$r(A_{m \times n}) = r = m$，则$A$的行向量组线性无关。</p><p>(2) 若$r(A_{m \times n}) = r &lt; m$，则$A$的行向量组线性相关。</p><p>(3) 若$r(A_{m \times n}) = r = n$，则$A$的列向量组线性无关。</p><p>(4) 若$r(A_{m \times n}) = r &lt; n$，则$A$的列向量组线性相关。</p><p><strong>5.</strong>$\mathbf{n}$<strong>维向量空间的基变换公式及过渡矩阵</strong></p><p>若$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$与$\beta_{1},\beta_{2},\cdots,\beta_{n}$是向量空间$V$的两组基，则基变换公式为：</p><p>$(\beta_{1},\beta_{2},\cdots,\beta_{n}) = (\alpha_{1},\alpha_{2},\cdots,\alpha_{n})\begin{bmatrix} c_{11}& c_{12}& \cdots & c_{1n} \ c_{21}& c_{22}&\cdots & c_{2n} \ \cdots & \cdots & \cdots & \cdots \ c_{n1}& c_{n2} & \cdots & c_{{nn}} \\end{bmatrix} = (\alpha_{1},\alpha_{2},\cdots,\alpha_{n})C$</p><p>其中$C$是可逆矩阵，称为由基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$到基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的过渡矩阵。</p><p><strong>6.坐标变换公式</strong></p><p>若向量$\gamma$在基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$与基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的坐标分别是
$X = {(x_{1},x_{2},\cdots,x_{n})}^{T}$，</p><p>$Y = \left( y_{1},y_{2},\cdots,y_{n} \right)^{T}$ 即： $\gamma =x_{1}\alpha_{1} + x_{2}\alpha_{2} + \cdots + x_{n}\alpha_{n} = y_{1}\beta_{1} +y_{2}\beta_{2} + \cdots + y_{n}\beta_{n}$，则向量坐标变换公式为$X = CY$ 或$Y = C^{- 1}X$，其中$C$是从基$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$到基$\beta_{1},\beta_{2},\cdots,\beta_{n}$的过渡矩阵。</p><p><strong>7.向量的内积</strong></p><p>$(\alpha,\beta) = a_{1}b_{1} + a_{2}b_{2} + \cdots + a_{n}b_{n} = \alpha^{T}\beta = \beta^{T}\alpha$</p><p><strong>8.Schmidt正交化</strong></p><p>若$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$线性无关，则可构造$\beta_{1},\beta_{2},\cdots,\beta_{s}$使其两两正交，且$\beta_{i}$仅是$\alpha_{1},\alpha_{2},\cdots,\alpha_{i}$的线性组合$(i= 1,2,\cdots,n)$，再把$\beta_{i}$单位化，记$\gamma_{i} =\frac{\beta_{i}}{\left| \beta_{i}\right|}$，则$\gamma_{1},\gamma_{2},\cdots,\gamma_{i}$是规范正交向量组。其中
$\beta_{1} = \alpha_{1}$， $\beta_{2} = \alpha_{2} -\frac{(\alpha_{2},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1}$ ， $\beta_{3} =\alpha_{3} - \frac{(\alpha_{3},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1} -\frac{(\alpha_{3},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2}$ ，</p><p>&mldr;&mldr;&mldr;&mldr;</p><p>$\beta_{s} = \alpha_{s} - \frac{(\alpha_{s},\beta_{1})}{(\beta_{1},\beta_{1})}\beta_{1} - \frac{(\alpha_{s},\beta_{2})}{(\beta_{2},\beta_{2})}\beta_{2} - \cdots - \frac{(\alpha_{s},\beta_{s - 1})}{(\beta_{s - 1},\beta_{s - 1})}\beta_{s - 1}$</p><p><strong>9.正交基及规范正交基</strong></p><p>向量空间一组基中的向量如果两两正交，就称为正交基；若正交基中每个向量都是单位向量，就称其为规范正交基。</p><h4 id=线性方程组>线性方程组</h4><p><strong>1．克莱姆法则</strong></p><p>线性方程组$\begin{cases} a_{11}x_{1} + a_{12}x_{2} + \cdots +a_{1n}x_{n} = b_{1} \ a_{21}x_{1} + a_{22}x_{2} + \cdots + a_{2n}x_{n} =b_{2} \ \quad\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots \ a_{n1}x_{1} + a_{n2}x_{2} + \cdots + a_{{nn}}x_{n} = b_{n} \ \end{cases}$，如果系数行列式$D = \left| A \right| \neq 0$，则方程组有唯一解，$x_{1} = \frac{D_{1}}{D},x_{2} = \frac{D_{2}}{D},\cdots,x_{n} =\frac{D_{n}}{D}$，其中$D_{j}$是把$D$中第$j$列元素换成方程组右端的常数列所得的行列式。</p><p><strong>2.</strong> $n$阶矩阵$A$可逆$\Leftrightarrow Ax = 0$只有零解。$\Leftrightarrow\forall b,Ax = b$总有唯一解，一般地，$r(A_{m \times n}) = n \Leftrightarrow Ax= 0$只有零解。</p><p><strong>3.非奇次线性方程组有解的充分必要条件，线性方程组解的性质和解的结构</strong></p><p>(1) 设$A$为$m \times n$矩阵，若$r(A_{m \times n}) = m$，则对$Ax =b$而言必有$r(A) = r(A \vdots b) = m$，从而$Ax = b$有解。</p><p>(2) 设$x_{1},x_{2},\cdots x_{s}$为$Ax = b$的解，则$k_{1}x_{1} + k_{2}x_{2}\cdots + k_{s}x_{s}$当$k_{1} + k_{2} + \cdots + k_{s} = 1$时仍为$Ax =b$的解；但当$k_{1} + k_{2} + \cdots + k_{s} = 0$时，则为$Ax =0$的解。特别$\frac{x_{1} + x_{2}}{2}$为$Ax = b$的解；$2x_{3} - (x_{1} +x_{2})$为$Ax = 0$的解。</p><p>(3) 非齐次线性方程组${Ax} = b$无解$\Leftrightarrow r(A) + 1 =r(\overline{A}) \Leftrightarrow b$不能由$A$的列向量$\alpha_{1},\alpha_{2},\cdots,\alpha_{n}$线性表示。</p><p><strong>4.奇次线性方程组的基础解系和通解，解空间，非奇次线性方程组的通解</strong></p><p>(1) 齐次方程组${Ax} = 0$恒有解(必有零解)。当有非零解时，由于解向量的任意线性组合仍是该齐次方程组的解向量，因此${Ax}= 0$的全体解向量构成一个向量空间，称为该方程组的解空间，解空间的维数是$n - r(A)$，解空间的一组基称为齐次方程组的基础解系。</p><p>(2) $\eta_{1},\eta_{2},\cdots,\eta_{t}$是${Ax} = 0$的基础解系，即：</p><ol><li><p>$\eta_{1},\eta_{2},\cdots,\eta_{t}$是${Ax} = 0$的解；</p></li><li><p>$\eta_{1},\eta_{2},\cdots,\eta_{t}$线性无关；</p></li><li><p>${Ax} = 0$的任一解都可以由$\eta_{1},\eta_{2},\cdots,\eta_{t}$线性表出.
$k_{1}\eta_{1} + k_{2}\eta_{2} + \cdots + k_{t}\eta_{t}$是${Ax} = 0$的通解，其中$k_{1},k_{2},\cdots,k_{t}$是任意常数。</p></li></ol><h4 id=矩阵的特征值和特征向量>矩阵的特征值和特征向量</h4><p><strong>1.矩阵的特征值和特征向量的概念及性质</strong></p><p>(1) 设$\lambda$是$A$的一个特征值，则 ${kA},{aA} + {bE},A^{2},A^{m},f(A),A^{T},A^{- 1},A^{*}$有一个特征值分别为
${kλ},{aλ} + b,\lambda^{2},\lambda^{m},f(\lambda),\lambda,\lambda^{- 1},\frac{|A|}{\lambda},$且对应特征向量相同（$A^{T}$ 例外）。</p><p>(2)若$\lambda_{1},\lambda_{2},\cdots,\lambda_{n}$为$A$的$n$个特征值，则$\sum_{i= 1}^{n}\lambda_{i} = \sum_{i = 1}^{n}a_{{ii}},\prod_{i = 1}^{n}\lambda_{i}= |A|$ ,从而$|A| \neq 0 \Leftrightarrow A$没有特征值。</p><p>(3)设$\lambda_{1},\lambda_{2},\cdots,\lambda_{s}$为$A$的$s$个特征值，对应特征向量为$\alpha_{1},\alpha_{2},\cdots,\alpha_{s}$，</p><p>若: $\alpha = k_{1}\alpha_{1} + k_{2}\alpha_{2} + \cdots + k_{s}\alpha_{s}$ ,</p><p>则: $A^{n}\alpha = k_{1}A^{n}\alpha_{1} + k_{2}A^{n}\alpha_{2} + \cdots +k_{s}A^{n}\alpha_{s} = k_{1}\lambda_{1}^{n}\alpha_{1} +k_{2}\lambda_{2}^{n}\alpha_{2} + \cdots k_{s}\lambda_{s}^{n}\alpha_{s}$ 。</p><p><strong>2.相似变换、相似矩阵的概念及性质</strong></p><p>(1) 若$A \sim B$，则</p><ol><li><p>$A^{T} \sim B^{T},A^{- 1} \sim B^{- 1},,A^{<em>} \sim B^{</em>}$</p></li><li><p>$|A| = |B|,\sum_{i = 1}^{n}A_{{ii}} = \sum_{i =1}^{n}b_{{ii}},r(A) = r(B)$</p></li><li><p>$|\lambda E - A| = |\lambda E - B|$，对$\forall\lambda$成立</p></li></ol><p><strong>3.矩阵可相似对角化的充分必要条件</strong></p><p>(1)设$A$为$n$阶方阵，则$A$可对角化$\Leftrightarrow$对每个$k_{i}$重根特征值$\lambda_{i}$，有$n-r(\lambda_{i}E - A) = k_{i}$</p><p>(2) 设$A$可对角化，则由$P^{- 1}{AP} = \Lambda,$有$A = {PΛ}P^{-1}$，从而$A^{n} = P\Lambda^{n}P^{- 1}$</p><p>(3) 重要结论</p><ol><li><p>若$A \sim B,C \sim D​$，则$\begin{bmatrix} A & O \ O & C \\end{bmatrix} \sim \begin{bmatrix} B & O \ O & D \\end{bmatrix}​$.</p></li><li><p>若$A \sim B$，则$f(A) \sim f(B),\left| f(A) \right| \sim \left| f(B)\right|$，其中$f(A)$为关于$n$阶方阵$A$的多项式。</p></li><li><p>若$A$为可对角化矩阵，则其非零特征值的个数(重根重复计算)＝秩($A$)</p></li></ol><p><strong>4.实对称矩阵的特征值、特征向量及相似对角阵</strong></p><p>(1)相似矩阵：设$A,B$为两个$n$阶方阵，如果存在一个可逆矩阵$P$，使得$B =P^{- 1}{AP}$成立，则称矩阵$A$与$B$相似，记为$A \sim B$。</p><p>(2)相似矩阵的性质：如果$A \sim B$则有：</p><ol><li><p>$A^{T} \sim B^{T}$</p></li><li><p>$A^{- 1} \sim B^{- 1}$ （若$A$，$B$均可逆）</p></li><li><p>$A^{k} \sim B^{k}$ （$k$为正整数）</p></li><li><p>$\left| {λE} - A \right| = \left| {λE} - B \right|$，从而$A,B$
有相同的特征值</p></li><li><p>$\left| A \right| = \left| B \right|$，从而$A,B$同时可逆或者不可逆</p></li><li><p>秩$\left( A \right) =$秩$\left( B \right),\left| {λE} - A \right| =\left| {λE} - B \right|$，$A,B$不一定相似</p></li></ol><h4 id=二次型>二次型</h4><p><strong>1.</strong>$\mathbf{n}$<strong>个变量</strong>$\mathbf{x}<em>{\mathbf{1}}\mathbf{,}\mathbf{x}</em>{\mathbf{2}}\mathbf{,\cdots,}\mathbf{x}_{\mathbf{n}}$<strong>的二次齐次函数</strong></p><p>$f(x_{1},x_{2},\cdots,x_{n}) = \sum_{i = 1}^{n}{\sum_{j =1}^{n}{a_{{ij}}x_{i}y_{j}}}$，其中$a_{{ij}} = a_{{ji}}(i,j =1,2,\cdots,n)$，称为$n$元二次型，简称二次型. 若令$x = \ \begin{bmatrix}x_{1} \ x_{1} \ \vdots \ x_{n} \ \end{bmatrix},A = \begin{bmatrix} a_{11}& a_{12}& \cdots & a_{1n} \ a_{21}& a_{22}& \cdots & a_{2n} \ \cdots &\cdots &\cdots &\cdots \ a_{n1}& a_{n2} & \cdots & a_{{nn}} \\end{bmatrix}$,这二次型$f$可改写成矩阵向量形式$f =x^{T}{Ax}$。其中$A$称为二次型矩阵，因为$a_{{ij}} =a_{{ji}}(i,j =1,2,\cdots,n)$，所以二次型矩阵均为对称矩阵，且二次型与对称矩阵一一对应，并把矩阵$A$的秩称为二次型的秩。</p><p><strong>2.惯性定理，二次型的标准形和规范形</strong></p><p>(1) 惯性定理</p><p>对于任一二次型，不论选取怎样的合同变换使它化为仅含平方项的标准型，其正负惯性指数与所选变换无关，这就是所谓的惯性定理。</p><p>(2) 标准形</p><p>二次型$f = \left( x_{1},x_{2},\cdots,x_{n} \right) =x^{T}{Ax}$经过合同变换$x = {Cy}$化为$f = x^{T}{Ax} =y^{T}C^{T}{AC}$</p><p>$y = \sum_{i = 1}^{r}{d_{i}y_{i}^{2}}$称为 $f(r \leq n)$的标准形。在一般的数域内，二次型的标准形不是唯一的，与所作的合同变换有关，但系数不为零的平方项的个数由$r(A)$唯一确定。</p><p>(3) 规范形</p><p>任一实二次型$f$都可经过合同变换化为规范形$f = z_{1}^{2} + z_{2}^{2} + \cdots z_{p}^{2} - z_{p + 1}^{2} - \cdots -z_{r}^{2}$，其中$r$为$A$的秩，$p$为正惯性指数，$r -p$为负惯性指数，且规范型唯一。</p><p><strong>3.用正交变换和配方法化二次型为标准形，二次型及其矩阵的正定性</strong></p><p>设$A$正定$\Rightarrow {kA}(k > 0),A^{T},A^{- 1},A^{*}$正定；$|A| >0$,$A$可逆；$a_{{ii}} > 0$，且$|A_{{ii}}| > 0$</p><p>$A$，$B$正定$\Rightarrow A +B$正定，但${AB}$，${BA}$不一定正定</p><p>$A$正定$\Leftrightarrow f(x) = x^{T}{Ax} > 0,\forall x \neq 0$</p><p>$\Leftrightarrow A$的各阶顺序主子式全大于零</p><p>$\Leftrightarrow A$的所有特征值大于零</p><p>$\Leftrightarrow A$的正惯性指数为$n$</p><p>$\Leftrightarrow$存在可逆阵$P$使$A = P^{T}P$</p><p>$\Leftrightarrow$存在正交矩阵$Q$，使$Q^{T}{AQ} = Q^{- 1}{AQ} =\begin{pmatrix} \lambda_{1} & & \ \begin{matrix} & \ & \ \end{matrix} &\ddots & \ & & \lambda_{n} \ \end{pmatrix},$</p><p>其中$\lambda_{i} > 0,i = 1,2,\cdots,n.$正定$\Rightarrow {kA}(k >0),A^{T},A^{- 1},A^{*}$正定； $|A| > 0,A$可逆；$a_{{ii}} >0$，且$|A_{{ii}}| > 0$ 。</p><h3 id=概率论和数理统计>概率论和数理统计</h3><h4 id=随机事件和概率>随机事件和概率</h4><p><strong>1.事件的关系与运算</strong></p><p>(1) 子事件：$A \subset B$，若$A$发生，则$B$发生。</p><p>(2) 相等事件：$A = B$，即$A \subset B$，且$B \subset A$ 。</p><p>(3) 和事件：$A\bigcup B$（或$A + B$），$A$与$B$中至少有一个发生。</p><p>(4) 差事件：$A - B$，$A$发生但$B$不发生。</p><p>(5) 积事件：$A\bigcap B$（或${AB}$），$A$与$B$同时发生。</p><p>(6) 互斥事件（互不相容）：$A\bigcap B$=$\varnothing$。</p><p>(7) 互逆事件（对立事件）：
$A\bigcap B=\varnothing ,A\bigcup B=\Omega ,A=\bar{B},B=\bar{A}$
<strong>2.运算律</strong>
(1) 交换律：$A\bigcup B=B\bigcup A,A\bigcap B=B\bigcap A$
(2) 结合律：$(A\bigcup B)\bigcup C=A\bigcup (B\bigcup C)$
(3) 分配律：$(A\bigcap B)\bigcap C=A\bigcap (B\bigcap C)$
<strong>3.德$\centerdot $摩根律</strong></p><p>$\overline{A\bigcup B}=\bar{A}\bigcap \bar{B}$ $\overline{A\bigcap B}=\bar{A}\bigcup \bar{B}$
<strong>4.完全事件组</strong></p><p>${{A}<em>{1}}{{A}</em>{2}}\cdots {{A}<em>{n}}$两两互斥，且和事件为必然事件，即${{A}</em>{i}}\bigcap {{A}_{j}}=\varnothing, i\ne j ,\underset{i=1}{\overset{n}{\mathop \bigcup }},=\Omega $</p><p><strong>5.概率的基本公式</strong>
(1)条件概率:
$P(B|A)=\frac{P(AB)}{P(A)}$,表示$A$发生的条件下，$B$发生的概率。
(2)全概率公式：
$P(A)=\sum\limits_{i=1}^{n}{P(A|{{B}<em>{i}})P({{B}</em>{i}}),{{B}<em>{i}}{{B}</em>{j}}}=\varnothing ,i\ne j,\underset{i=1}{\overset{n}{\mathop{\bigcup }}},{{B}_{i}}=\Omega $
(3) Bayes公式：</p><p>$P({{B}<em>{j}}|A)=\frac{P(A|{{B}</em>{j}})P({{B}<em>{j}})}{\sum\limits</em>{i=1}^{n}{P(A|{{B}<em>{i}})P({{B}</em>{i}})}},j=1,2,\cdots ,n$
注：上述公式中事件${{B}<em>{i}}$的个数可为可列个。
(4)乘法公式：
$P({{A}</em>{1}}{{A}<em>{2}})=P({{A}</em>{1}})P({{A}<em>{2}}|{{A}</em>{1}})=P({{A}<em>{2}})P({{A}</em>{1}}|{{A}<em>{2}})$
$P({{A}</em>{1}}{{A}<em>{2}}\cdots {{A}</em>{n}})=P({{A}<em>{1}})P({{A}</em>{2}}|{{A}<em>{1}})P({{A}</em>{3}}|{{A}<em>{1}}{{A}</em>{2}})\cdots P({{A}<em>{n}}|{{A}</em>{1}}{{A}<em>{2}}\cdots {{A}</em>{n-1}})$</p><p><strong>6.事件的独立性</strong>
(1)$A$与$B$相互独立$\Leftrightarrow P(AB)=P(A)P(B)$
(2)$A$，$B$，$C$两两独立
$\Leftrightarrow P(AB)=P(A)P(B)$;$P(BC)=P(B)P(C)$ ;$P(AC)=P(A)P(C)$;
(3)$A$，$B$，$C$相互独立
$\Leftrightarrow P(AB)=P(A)P(B)$; $P(BC)=P(B)P(C)$ ;
$P(AC)=P(A)P(C)$ ; $P(ABC)=P(A)P(B)P(C)$</p><p><strong>7.独立重复试验</strong></p><p>将某试验独立重复$n$次，若每次实验中事件A发生的概率为$p$，则$n$次试验中$A$发生$k$次的概率为：
$P(X=k)=C_{n}^{k}{{p}^{k}}{{(1-p)}^{n-k}}$
<strong>8.重要公式与结论</strong>
$(1)P(\bar{A})=1-P(A)$
$(2)P(A\bigcup B)=P(A)+P(B)-P(AB)$
$P(A\bigcup B\bigcup C)=P(A)+P(B)+P(C)-P(AB)-P(BC)-P(AC)+P(ABC)$
$(3)P(A-B)=P(A)-P(AB)$
$(4)P(A\bar{B})=P(A)-P(AB),P(A)=P(AB)+P(A\bar{B}),$
$P(A\bigcup B)=P(A)+P(\bar{A}B)=P(AB)+P(A\bar{B})+P(\bar{A}B)$
(5)条件概率$P(\centerdot |B)$满足概率的所有性质，
例如：. $P({{\bar{A}}<em>{1}}|B)=1-P({{A}</em>{1}}|B)$
$P({{A}<em>{1}}\bigcup {{A}</em>{2}}|B)=P({{A}<em>{1}}|B)+P({{A}</em>{2}}|B)-P({{A}<em>{1}}{{A}</em>{2}}|B)$
$P({{A}<em>{1}}{{A}</em>{2}}|B)=P({{A}<em>{1}}|B)P({{A}</em>{2}}|{{A}<em>{1}}B)$
(6)若${{A}</em>{1}},{{A}<em>{2}},\cdots ,{{A}</em>{n}}$相互独立，则$P(\bigcap\limits_{i=1}^{n}{{{A}<em>{i}}})=\prod\limits</em>{i=1}^{n}{P({{A}<em>{i}})},$
$P(\bigcup\limits</em>{i=1}^{n}{{{A}<em>{i}}})=\prod\limits</em>{i=1}^{n}{(1-P({{A}<em>{i}}))}$
(7)互斥、互逆与独立性之间的关系：
$A$与$B$互逆$\Rightarrow$ $A$与$B$互斥，但反之不成立，$A$与$B$互斥（或互逆）且均非零概率事件$\Rightarrow $$A$与$B$不独立.
(8)若${{A}</em>{1}},{{A}<em>{2}},\cdots ,{{A}</em>{m}},{{B}<em>{1}},{{B}</em>{2}},\cdots ,{{B}<em>{n}}$相互独立，则$f({{A}</em>{1}},{{A}<em>{2}},\cdots ,{{A}</em>{m}})$与$g({{B}<em>{1}},{{B}</em>{2}},\cdots ,{{B}_{n}})$也相互独立，其中$f(\centerdot ),g(\centerdot )$分别表示对相应事件做任意事件运算后所得的事件，另外，概率为1（或0）的事件与任何事件相互独立.</p><h4 id=随机变量及其概率分布>随机变量及其概率分布</h4><p><strong>1.随机变量及概率分布</strong></p><p>取值带有随机性的变量，严格地说是定义在样本空间上，取值于实数的函数称为随机变量，概率分布通常指分布函数或分布律</p><p><strong>2.分布函数的概念与性质</strong></p><p>定义： $F(x) = P(X \leq x), - \infty &lt; x &lt; + \infty$</p><p>性质：(1)$0 \leq F(x) \leq 1$</p><p>(2) $F(x)$单调不减</p><p>(3) 右连续$F(x + 0) = F(x)$</p><p>(4) $F( - \infty) = 0,F( + \infty) = 1$</p><p><strong>3.离散型随机变量的概率分布</strong></p><p>$P(X = x_{i}) = p_{i},i = 1,2,\cdots,n,\cdots\quad\quad p_{i} \geq 0,\sum_{i =1}^{\infty}p_{i} = 1$</p><p><strong>4.连续型随机变量的概率密度</strong></p><p>概率密度$f(x)$;非负可积，且:</p><p>(1)$f(x) \geq 0,$</p><p>(2)$\int_{- \infty}^{+\infty}{f(x){dx} = 1}$</p><p>(3)$x$为$f(x)$的连续点，则:</p><p>$f(x) = F&rsquo;(x)$分布函数$F(x) = \int_{- \infty}^{x}{f(t){dt}}$</p><p><strong>5.常见分布</strong></p><p>(1) 0-1分布:$P(X = k) = p^{k}{(1 - p)}^{1 - k},k = 0,1$</p><p>(2) 二项分布:$B(n,p)$： $P(X = k) = C_{n}^{k}p^{k}{(1 - p)}^{n - k},k =0,1,\cdots,n$</p><p>(3) <strong>Poisson</strong>分布:$p(\lambda)$： $P(X = k) = \frac{\lambda^{k}}{k!}e^{-\lambda},\lambda > 0,k = 0,1,2\cdots$</p><p>(4) 均匀分布$U(a,b)$：$f(x) = { \begin{matrix} & \frac{1}{b - a},a &lt; x&lt; b \ & 0, \ \end{matrix} $</p><p>(5) 正态分布:$N(\mu,\sigma^{2}):$ $\varphi(x) =\frac{1}{\sqrt{2\pi}\sigma}e^{- \frac{{(x - \mu)}^{2}}{2\sigma^{2}}},\sigma > 0,\infty &lt; x &lt; + \infty$</p><p>(6)指数分布:$E(\lambda):f(x) ={ \begin{matrix} & \lambda e^{-{λx}},x > 0,\lambda > 0 \ & 0, \ \end{matrix} $</p><p>(7)几何分布:$G(p):P(X = k) = {(1 - p)}^{k - 1}p,0 &lt; p &lt; 1,k = 1,2,\cdots.$</p><p>(8)超几何分布: $H(N,M,n):P(X = k) = \frac{C_{M}^{k}C_{N - M}^{n -k}}{C_{N}^{n}},k =0,1,\cdots,min(n,M)$</p><p><strong>6.随机变量函数的概率分布</strong></p><p>(1)离散型：$P(X = x_{1}) = p_{i},Y = g(X)$</p><p>则: $P(Y = y_{j}) = \sum_{g(x_{i}) = y_{i}}^{}{P(X = x_{i})}$</p><p>(2)连续型：$X\tilde{\ }f_{X}(x),Y = g(x)$</p><p>则:$F_{y}(y) = P(Y \leq y) = P(g(X) \leq y) = \int_{g(x) \leq y}^{}{f_{x}(x)dx}$， $f_{Y}(y) = F&rsquo;_{Y}(y)$</p><p><strong>7.重要公式与结论</strong></p><p>(1) $X\sim N(0,1) \Rightarrow \varphi(0) = \frac{1}{\sqrt{2\pi}},\Phi(0) =\frac{1}{2},$ $\Phi( - a) = P(X \leq - a) = 1 - \Phi(a)$</p><p>(2) $X\sim N\left( \mu,\sigma^{2} \right) \Rightarrow \frac{X -\mu}{\sigma}\sim N\left( 0,1 \right),P(X \leq a) = \Phi(\frac{a -\mu}{\sigma})$</p><p>(3) $X\sim E(\lambda) \Rightarrow P(X > s + t|X > s) = P(X > t)$</p><p>(4) $X\sim G(p) \Rightarrow P(X = m + k|X > m) = P(X = k)$</p><p>(5) 离散型随机变量的分布函数为阶梯间断函数；连续型随机变量的分布函数为连续函数，但不一定为处处可导函数。</p><p>(6) 存在既非离散也非连续型随机变量。</p><h4 id=多维随机变量及其分布>多维随机变量及其分布</h4><p><strong>1.二维随机变量及其联合分布</strong></p><p>由两个随机变量构成的随机向量$(X,Y)$， 联合分布为$F(x,y) = P(X \leq x,Y \leq y)$</p><p><strong>2.二维离散型随机变量的分布</strong></p><p>(1) 联合概率分布律 $P{ X = x_{i},Y = y_{j}} = p_{{ij}};i,j =1,2,\cdots$</p><p>(2) 边缘分布律 $p_{i \cdot} = \sum_{j = 1}^{\infty}p_{{ij}},i =1,2,\cdots$ $p_{\cdot j} = \sum_{i}^{\infty}p_{{ij}},j = 1,2,\cdots$</p><p>(3) 条件分布律 $P{ X = x_{i}|Y = y_{j}} = \frac{p_{{ij}}}{p_{\cdot j}}$
$P{ Y = y_{j}|X = x_{i}} = \frac{p_{{ij}}}{p_{i \cdot}}$</p><p><strong>3. 二维连续性随机变量的密度</strong></p><p>(1) 联合概率密度$f(x,y):$</p><ol><li><p>$f(x,y) \geq 0$</p></li><li><p>$\int_{- \infty}^{+ \infty}{\int_{- \infty}^{+ \infty}{f(x,y)dxdy}} = 1$</p></li></ol><p>(2) 分布函数：$F(x,y) = \int_{- \infty}^{x}{\int_{- \infty}^{y}{f(u,v)dudv}}$</p><p>(3) 边缘概率密度： $f_{X}\left( x \right) = \int_{- \infty}^{+ \infty}{f\left( x,y \right){dy}}$ $f_{Y}(y) = \int_{- \infty}^{+ \infty}{f(x,y)dx}$</p><p>(4) 条件概率密度：$f_{X|Y}\left( x \middle| y \right) = \frac{f\left( x,y \right)}{f_{Y}\left( y \right)}$ $f_{Y|X}(y|x) = \frac{f(x,y)}{f_{X}(x)}$</p><p><strong>4.常见二维随机变量的联合分布</strong></p><p>(1) 二维均匀分布：$(x,y) \sim U(D)$ ,$f(x,y) = \begin{cases} \frac{1}{S(D)},(x,y) \in D \ 0,其他 \end{cases}$</p><p>(2) 二维正态分布：$(X,Y)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$,$(X,Y)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$</p><p>$f(x,y) = \frac{1}{2\pi\sigma_{1}\sigma_{2}\sqrt{1 - \rho^{2}}}.\exp\left{ \frac{- 1}{2(1 - \rho^{2})}\lbrack\frac{{(x - \mu_{1})}^{2}}{\sigma_{1}^{2}} - 2\rho\frac{(x - \mu_{1})(y - \mu_{2})}{\sigma_{1}\sigma_{2}} + \frac{{(y - \mu_{2})}^{2}}{\sigma_{2}^{2}}\rbrack \right}$</p><p><strong>5.随机变量的独立性和相关性</strong></p><p>$X$和$Y$的相互独立:$\Leftrightarrow F\left( x,y \right) = F_{X}\left( x \right)F_{Y}\left( y \right)$:</p><p>$\Leftrightarrow p_{{ij}} = p_{i \cdot} \cdot p_{\cdot j}$（离散型）
$\Leftrightarrow f\left( x,y \right) = f_{X}\left( x \right)f_{Y}\left( y \right)$（连续型）</p><p>$X$和$Y$的相关性：</p><p>相关系数$\rho_{{XY}} = 0$时，称$X$和$Y$不相关，
否则称$X$和$Y$相关</p><p><strong>6.两个随机变量简单函数的概率分布</strong></p><p>离散型： $P\left( X = x_{i},Y = y_{i} \right) = p_{{ij}},Z = g\left( X,Y \right)$ 则：</p><p>$P(Z = z_{k}) = P\left{ g\left( X,Y \right) = z_{k} \right} = \sum_{g\left( x_{i},y_{i} \right) = z_{k}}^{}{P\left( X = x_{i},Y = y_{j} \right)}$</p><p>连续型： $\left( X,Y \right) \sim f\left( x,y \right),Z = g\left( X,Y \right)$
则：</p><p>$F_{z}\left( z \right) = P\left{ g\left( X,Y \right) \leq z \right} = \iint_{g(x,y) \leq z}^{}{f(x,y)dxdy}$，$f_{z}(z) = F&rsquo;_{z}(z)$</p><p><strong>7.重要公式与结论</strong></p><p>(1) 边缘密度公式： $f_{X}(x) = \int_{- \infty}^{+ \infty}{f(x,y)dy,}$
$f_{Y}(y) = \int_{- \infty}^{+ \infty}{f(x,y)dx}$</p><p>(2) $P\left{ \left( X,Y \right) \in D \right} = \iint_{D}^{}{f\left( x,y \right){dxdy}}$</p><p>(3) 若$(X,Y)$服从二维正态分布$N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},\rho)$
则有：</p><ol><li><p>$X\sim N\left( \mu_{1},\sigma_{1}^{2} \right),Y\sim N(\mu_{2},\sigma_{2}^{2}).$</p></li><li><p>$X$与$Y$相互独立$\Leftrightarrow \rho = 0$，即$X$与$Y$不相关。</p></li><li><p>$C_{1}X + C_{2}Y\sim N(C_{1}\mu_{1} + C_{2}\mu_{2},C_{1}^{2}\sigma_{1}^{2} + C_{2}^{2}\sigma_{2}^{2} + 2C_{1}C_{2}\sigma_{1}\sigma_{2}\rho)$</p></li><li><p>${\ X}$关于$Y=y$的条件分布为： $N(\mu_{1} + \rho\frac{\sigma_{1}}{\sigma_{2}}(y - \mu_{2}),\sigma_{1}^{2}(1 - \rho^{2}))$</p></li><li><p>$Y$关于$X = x$的条件分布为： $N(\mu_{2} + \rho\frac{\sigma_{2}}{\sigma_{1}}(x - \mu_{1}),\sigma_{2}^{2}(1 - \rho^{2}))$</p></li></ol><p>(4) 若$X$与$Y$独立，且分别服从$N(\mu_{1},\sigma_{1}^{2}),N(\mu_{1},\sigma_{2}^{2}),$
则：$\left( X,Y \right)\sim N(\mu_{1},\mu_{2},\sigma_{1}^{2},\sigma_{2}^{2},0),$</p><p>$C_{1}X + C_{2}Y\tilde{\ }N(C_{1}\mu_{1} + C_{2}\mu_{2},C_{1}^{2}\sigma_{1}^{2} C_{2}^{2}\sigma_{2}^{2}).$</p><p>(5) 若$X$与$Y$相互独立，$f\left( x \right)$和$g\left( x \right)$为连续函数， 则$f\left( X \right)$和$g(Y)$也相互独立。</p><h4 id=随机变量的数字特征>随机变量的数字特征</h4><p><strong>1.数学期望</strong></p><p>离散型：$P\left{ X = x_{i} \right} = p_{i},E(X) = \sum_{i}^{}{x_{i}p_{i}}$；</p><p>连续型： $X\sim f(x),E(X) = \int_{- \infty}^{+ \infty}{xf(x)dx}$</p><p>性质：</p><p>(1) $E(C) = C,E\lbrack E(X)\rbrack = E(X)$</p><p>(2) $E(C_{1}X + C_{2}Y) = C_{1}E(X) + C_{2}E(Y)$</p><p>(3) 若$X$和$Y$独立，则$E(XY) = E(X)E(Y)$</p><p>(4)$\left\lbrack E(XY) \right\rbrack^{2} \leq E(X^{2})E(Y^{2})$</p><p><strong>2.方差</strong>：$D(X) = E\left\lbrack X - E(X) \right\rbrack^{2} = E(X^{2}) - \left\lbrack E(X) \right\rbrack^{2}$</p><p><strong>3.标准差</strong>：$\sqrt{D(X)}$，</p><p><strong>4.离散型：</strong>$D(X) = \sum_{i}^{}{\left\lbrack x_{i} - E(X) \right\rbrack^{2}p_{i}}$</p><p><strong>5.连续型：</strong>$D(X) = {\int_{- \infty}^{+ \infty}\left\lbrack x - E(X) \right\rbrack}^{2}f(x)dx$</p><p>性质：</p><p>(1)$\ D(C) = 0,D\lbrack E(X)\rbrack = 0,D\lbrack D(X)\rbrack = 0$</p><p>(2) $X$与$Y$相互独立，则$D(X \pm Y) = D(X) + D(Y)$</p><p>(3)$\ D\left( C_{1}X + C_{2} \right) = C_{1}^{2}D\left( X \right)$</p><p>(4) 一般有 $D(X \pm Y) = D(X) + D(Y) \pm 2Cov(X,Y) = D(X) + D(Y) \pm 2\rho\sqrt{D(X)}\sqrt{D(Y)}$</p><p>(5)$\ D\left( X \right) &lt; E\left( X - C \right)^{2},C \neq E\left( X \right)$</p><p>(6)$\ D(X) = 0 \Leftrightarrow P\left{ X = C \right} = 1$</p><p><strong>6.随机变量函数的数学期望</strong></p><p>(1) 对于函数$Y = g(x)$</p><p>$X$为离散型：$P{ X = x_{i}} = p_{i},E(Y) = \sum_{i}^{}{g(x_{i})p_{i}}$；</p><p>$X$为连续型：$X\sim f(x),E(Y) = \int_{- \infty}^{+ \infty}{g(x)f(x)dx}$</p><p>(2) $Z = g(X,Y)$;$\left( X,Y \right)\sim P{ X = x_{i},Y = y_{j}} = p_{{ij}}$; $E(Z) = \sum_{i}^{}{\sum_{j}^{}{g(x_{i},y_{j})p_{{ij}}}}$ $\left( X,Y \right)\sim f(x,y)$;$E(Z) = \int_{- \infty}^{+ \infty}{\int_{- \infty}^{+ \infty}{g(x,y)f(x,y)dxdy}}$</p><p><strong>7.协方差</strong></p><p>$Cov(X,Y) = E\left\lbrack (X - E(X)(Y - E(Y)) \right\rbrack$</p><p><strong>8.相关系数</strong></p><p>$\rho_{{XY}} = \frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}$,$k$阶原点矩 $E(X^{k})$;
$k$阶中心矩 $E\left{ {\lbrack X - E(X)\rbrack}^{k} \right}$</p><p>性质：</p><p>(1)$\ Cov(X,Y) = Cov(Y,X)$</p><p>(2)$\ Cov(aX,bY) = abCov(Y,X)$</p><p>(3)$\ Cov(X_{1} + X_{2},Y) = Cov(X_{1},Y) + Cov(X_{2},Y)$</p><p>(4)$\ \left| \rho\left( X,Y \right) \right| \leq 1$</p><p>(5) $\ \rho\left( X,Y \right) = 1 \Leftrightarrow P\left( Y = aX + b \right) = 1$ ，其中$a > 0$</p><p>$\rho\left( X,Y \right) = - 1 \Leftrightarrow P\left( Y = aX + b \right) = 1$
，其中$a &lt; 0$</p><p><strong>9.重要公式与结论</strong></p><p>(1)$\ D(X) = E(X^{2}) - E^{2}(X)$</p><p>(2)$\ Cov(X,Y) = E(XY) - E(X)E(Y)$</p><p>(3) $\left| \rho\left( X,Y \right) \right| \leq 1,$且 $\rho\left( X,Y \right) = 1 \Leftrightarrow P\left( Y = aX + b \right) = 1$，其中$a > 0$</p><p>$\rho\left( X,Y \right) = - 1 \Leftrightarrow P\left( Y = aX + b \right) = 1$，其中$a &lt; 0$</p><p>(4) 下面5个条件互为充要条件：</p><p>$\rho(X,Y) = 0$ $\Leftrightarrow Cov(X,Y) = 0$ $\Leftrightarrow E(X,Y) = E(X)E(Y)$ $\Leftrightarrow D(X + Y) = D(X) + D(Y)$ $\Leftrightarrow D(X - Y) = D(X) + D(Y)$</p><p>注：$X$与$Y$独立为上述5个条件中任何一个成立的充分条件，但非必要条件。</p><h4 id=数理统计的基本概念>数理统计的基本概念</h4><p><strong>1.基本概念</strong></p><p>总体：研究对象的全体，它是一个随机变量，用$X$表示。</p><p>个体：组成总体的每个基本元素。</p><p>简单随机样本：来自总体$X$的$n$个相互独立且与总体同分布的随机变量$X_{1},X_{2}\cdots,X_{n}$，称为容量为$n$的简单随机样本，简称样本。</p><p>统计量：设$X_{1},X_{2}\cdots,X_{n},$是来自总体$X$的一个样本，$g(X_{1},X_{2}\cdots,X_{n})$）是样本的连续函数，且$g()$中不含任何未知参数，则称$g(X_{1},X_{2}\cdots,X_{n})$为统计量。</p><p>样本均值：$\overline{X} = \frac{1}{n}\sum_{i = 1}^{n}X_{i}$</p><p>样本方差：$S^{2} = \frac{1}{n - 1}\sum_{i = 1}^{n}{(X_{i} - \overline{X})}^{2}$</p><p>样本矩：样本$k$阶原点矩：$A_{k} = \frac{1}{n}\sum_{i = 1}^{n}X_{i}^{k},k = 1,2,\cdots$</p><p>样本$k$阶中心矩：$B_{k} = \frac{1}{n}\sum_{i = 1}^{n}{(X_{i} - \overline{X})}^{k},k = 1,2,\cdots$</p><p><strong>2.分布</strong></p><p>$\chi^{2}$分布：$\chi^{2} = X_{1}^{2} + X_{2}^{2} + \cdots + X_{n}^{2}\sim\chi^{2}(n)$，其中$X_{1},X_{2}\cdots,X_{n},$相互独立，且同服从$N(0,1)$</p><p>$t$分布：$T = \frac{X}{\sqrt{Y/n}}\sim t(n)$ ，其中$X\sim N\left( 0,1 \right),Y\sim\chi^{2}(n),$且$X$，$Y$ 相互独立。</p><p>$F$分布：$F = \frac{X/n_{1}}{Y/n_{2}}\sim F(n_{1},n_{2})$，其中$X\sim\chi^{2}\left( n_{1} \right),Y\sim\chi^{2}(n_{2}),$且$X$，$Y$相互独立。</p><p>分位数：若$P(X \leq x_{\alpha}) = \alpha,$则称$x_{\alpha}$为$X$的$\alpha$分位数</p><p><strong>3.正态总体的常用样本分布</strong></p><p>(1) 设$X_{1},X_{2}\cdots,X_{n}$为来自正态总体$N(\mu,\sigma^{2})$的样本，</p><p>$\overline{X} = \frac{1}{n}\sum_{i = 1}^{n}X_{i},S^{2} = \frac{1}{n - 1}\sum_{i = 1}^{n}{{(X_{i} - \overline{X})}^{2},}$则：</p><ol><li><p>$\overline{X}\sim N\left( \mu,\frac{\sigma^{2}}{n} \right){\ \ }$或者$\frac{\overline{X} - \mu}{\frac{\sigma}{\sqrt{n}}}\sim N(0,1)$</p></li><li><p>$\frac{(n - 1)S^{2}}{\sigma^{2}} = \frac{1}{\sigma^{2}}\sum_{i = 1}^{n}{{(X_{i} - \overline{X})}^{2}\sim\chi^{2}(n - 1)}$</p></li><li><p>$\frac{1}{\sigma^{2}}\sum_{i = 1}^{n}{{(X_{i} - \mu)}^{2}\sim\chi^{2}(n)}$</p></li></ol><p>4)${\ \ }\frac{\overline{X} - \mu}{S/\sqrt{n}}\sim t(n - 1)$</p><p><strong>4.重要公式与结论</strong></p><p>(1) 对于$\chi^{2}\sim\chi^{2}(n)$，有$E(\chi^{2}(n)) = n,D(\chi^{2}(n)) = 2n;$</p><p>(2) 对于$T\sim t(n)$，有$E(T) = 0,D(T) = \frac{n}{n - 2}(n > 2)$；</p><p>(3) 对于$F\tilde{\ }F(m,n)$，有 $\frac{1}{F}\sim F(n,m),F_{a/2}(m,n) = \frac{1}{F_{1 - a/2}(n,m)};$</p><p>(4) 对于任意总体$X$，有 $E(\overline{X}) = E(X),E(S^{2}) = D(X),D(\overline{X}) = \frac{D(X)}{n}$</p></div><div class=article-widget><div class="container-xl row post-nav"><div class="col-6 post-nav-item"><div class=meta-nav>上一页</div><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/lesson5-week3/ rel=next>lesson5-week3</a></div><div class="col-6 post-nav-item"><div class=meta-nav>下一页</div><a href=/books/deeplearning-notes/99.%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99/2019-andrew-ng-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/notation/ rel=prev>notation</a></div></div></div><div class=body-footer><p>最近更新于 0001-01-01</p><section id=comments class="mb-3 pt-0"><div id=disqus_thread></div><script>var disqus_config=function(){};(function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="https://ngte.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div></article><footer class=site-footer><div class="copyright py-4 bg-footer"><div class="row justify-content-center"><div class="text-center footer-color"><p class=mb-0>© 2017-2022 NGTE all rights reserved</p></div></div></div><script type=text/javascript id=clstr_globe async src="//clustrmaps.com/globe.js?d=kgpJG5sWZQpKujBmD-uW1B54-WBPol-DuDtrB2KFjKs"></script></footer></main></div></div><script src=//unpkg.com/heti/umd/heti-addon.min.js></script>
<script>const heti=new Heti(".article");heti.autoSpacing()</script><script type=text/javascript>window.$crisp=[],window.CRISP_WEBSITE_ID="12adcc35-9621-4313-8262-62dc654b29d8",function(){setTimeout(function(){d=document,s=d.createElement("script"),s.src="https://client.crisp.chat/l.js",s.async=1,d.getElementsByTagName("head")[0].appendChild(s)},2500)}()</script></div><div class=page-footer></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script id=search-hit-algolia-template type=text/html><div class=search-hit><div class=search-hit-content><div class=search-hit-name><a href={{relpermalink}}>{{#helpers.highlight}}{ "attribute": "title" }{{/helpers.highlight}}</a></div><div class="article-metadata search-hit-type">{{type}}</div><p class=search-hit-description>{{#helpers.highlight}}{ "attribute": "summary" }{{/helpers.highlight}}</p></div></div></script><script src=https://cdn.jsdelivr.net/npm/instantsearch.js@4/dist/instantsearch.production.min.js crossorigin=anonymous></script>
<script id=dsq-count-scr src=https://ngte.disqus.com/count.js async></script>
<script src=/zh/js/algolia-search-built.min.4387d694ca1258194aaf562b8cd1c400.js type=module></script>
<script id=page-data type=application/json>{"use_headroom":false}</script><script src=/zh/js/wowchemy.min.d1673c7a11d1238516cbe12a1e84257f.js></script>
<script>var mybutton=document.getElementById("backTopBtn");window.onscroll=function(){scrollFunction()};function scrollFunction(){document.body.scrollTop>20||document.documentElement.scrollTop>20?mybutton.style.display="block":mybutton.style.display="none"}function topFunction(){document.body.scrollTop=0,document.documentElement.scrollTop=0}</script><script src=https://cdn.jsdelivr.net/gh/bryanbraun/anchorjs@4.2.2/anchor.min.js integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin=anonymous></script>
<script>anchors.add()</script><script>(function(){"use strict";if(!document.queryCommandSupported("copy"))return;function e(e,t){e.className="highlight-copy-btn",e.textContent=t,setTimeout(function(){e.textContent="",e.className="highlight-copy-btn fa fa-copy"},1e3)}function t(e){var t=window.getSelection(),n=document.createRange();return n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n),t}function n(n){var o,s=document.createElement("button");s.className="highlight-copy-btn fa fa-copy",s.textContent="",o=n.firstElementChild,s.addEventListener("click",function(){try{var n=t(o);document.execCommand("copy"),n.removeAllRanges(),e(s,"已复制")}catch(t){console&&console.log(t),e(s,"Failed :'(")}}),n.appendChild(s)}var s=document.getElementsByClassName("highlight");Array.prototype.forEach.call(s,n)})()</script></body></html>