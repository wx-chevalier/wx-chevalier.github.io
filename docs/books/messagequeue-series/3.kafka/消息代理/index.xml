<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>消息代理 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/</link><atom:link href="https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><description>消息代理</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>消息代理</title><link>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/</link></image><item><title>ZooKeeper</title><link>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/zookeeper/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/zookeeper/</guid><description>&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://cubox.pro/c/LUhyY9" target="_blank" rel="noopener">https://cubox.pro/c/LUhyY9&lt;/a> 消息系统兴起二次革命：Kafka 不需要 ZooKeeper&lt;/li>
&lt;/ul></description></item><item><title>磁盘读写优化</title><link>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/%E7%A3%81%E7%9B%98%E8%AF%BB%E5%86%99%E4%BC%98%E5%8C%96/</guid><description>&lt;h1 id="kafka-性能优化之道">Kafka 性能优化之道&lt;/h1>
&lt;h1 id="顺序读写">顺序读写&lt;/h1>
&lt;p>Kafka 高度依赖于文件系统来存储和缓存消息。对于磁盘来说，它有一个特性，就是顺序读写的性能要远远好于随机读写。在 SSD（固态硬盘）上，顺序读写的性能要比随机读写快几倍，如果是机械硬盘，这个差距会达到几十倍。据 Kafka 官方网站介绍：6 块 7200r/min SATA RAID-5 阵列的磁盘线性写的速度为 600 MB/s，而随机写的速度为 100KB/s，线性写的速度约是随机写的 6000 多倍。由此看来磁盘的快慢取决于我们是如何去应用磁盘。加之现代的操作系统提供了预读(read-ahead)和延迟写(write-behind)技术，使得磁盘的写速度并不是大家想象的那么慢。操作系统每次从磁盘读写数据的时候，需要先寻址，也就是先要找到数据在磁盘上的物理位置，然后再进行数据读写。如果是机械硬盘，这个寻址需要比较长的时间，因为它要移动磁头，这是个机械运动，机械硬盘工作的时候会发出咔咔的声音，就是移动磁头发出的声音。顺序读写相比随机读写省去了大部分的寻址时间，它只要寻址一次，就可以连续地读写下去，所以说，性能要比随机读写要好很多。&lt;/p>
&lt;p>Kafka 就是充分利用了磁盘的这个特性。它的存储设计非常简单，对于每个分区，它把从 Producer 收到的消息，顺序地写入对应的 log 文件中，一个文件写满了，就开启一个新的文件这样顺序写下去。消费的时候，也是从某个全局的位置开始，也就是某一个 log 文件中的某个位置开始，顺序地把消息读出来。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6077a25c8322e6675cfc796a.jpg" alt="顺序写入示意" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>对于传统的 message queue 而言，一般会删除已经被消费的消息，而 Kafka 是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个 Topic 都有一个 offset 用来表示读取到了第几条数据。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6077a28e8322e6675cfcfb8d.jpg" alt="基于偏移的读取" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>即便是顺序写入硬盘，硬盘的访问速度还是不可能追上内存。所以 Kafka 的数据并不是实时的写入硬盘，它充分利用了现代操作系统分页存储来利用内存提高 I/O 效率。&lt;/p>
&lt;h1 id="利用-pagecache-加速消息读写">利用 PageCache 加速消息读写&lt;/h1>
&lt;p>在 Kafka 中，它会利用 PageCache 加速消息读写。PageCache 是现代操作系统都具有的一项基本特性。通俗地说，PageCache 就是操作系统在内存中给磁盘上的文件建立的缓存。无论我们使用什么语言编写的程序，在调用系统的 API 读写文件的时候，并不会直接去读写磁盘上的文件，应用程序实际操作的都是 PageCache，也就是文件在内存中缓存的副本。应用程序在写入文件的时候，操作系统会先把数据写入到内存中的 PageCache，然后再一批一批地写到磁盘上。读取文件的时候，也是从 PageCache 中来读取数据，这时候会出现两种可能情况。&lt;/p>
&lt;ul>
&lt;li>一种是 PageCache 中有数据，那就直接读取，这样就节省了从磁盘上读取数据的时间；&lt;/li>
&lt;li>另一种情况是，PageCache 中没有数据，这时候操作系统会引发一个缺页中断，应用程序的读取线程会被阻塞，操作系统把数据从文件中复制到 PageCache 中，然后应用程序再从 PageCache 中继续把数据读出来，这时会真正读一次磁盘上的文件，这个读的过程就会比较慢。&lt;/li>
&lt;/ul>
&lt;p>用户的应用程序在使用完某块 PageCache 后，操作系统并不会立刻就清除这个 PageCache，而是尽可能地利用空闲的物理内存保存这些 PageCache，除非系统内存不够用，操作系统才会清理掉一部分 PageCache。清理的策略一般是 LRU 或它的变种算法，这个算法我们不展开讲，它保留 PageCache 的逻辑是：优先保留最近一段时间最常使用的那些 PageCache。&lt;/p>
&lt;p>Kafka 在读写消息文件的时候，充分利用了 PageCache 的特性。一般来说，消息刚刚写入到服务端就会被消费，按照 LRU 的“优先清除最近最少使用的页”这种策略，读取的时候，对于这种刚刚写入的 PageCache，命中的几率会非常高。也就是说，大部分情况下，消费读消息都会命中 PageCache，带来的好处有两个：一个是读取的速度会非常快，另外一个是，给写入消息让出磁盘的 IO 资源，间接也提升了写入的性能。&lt;/p>
&lt;h1 id="zerocopy零拷贝技术">ZeroCopy：零拷贝技术&lt;/h1>
&lt;p>在 Linux Kernal 2.2 之后出现了一种叫做“零拷贝(zero-copy)”系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存空间的直接映射，数据不再复制到“用户态缓冲区”系统上下文切换减少 2 次，可以提升一倍性能。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6077a2f98322e6675cfdcd1a.jpg" alt="零拷贝示意" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;blockquote>
&lt;p>更多零拷贝相关介绍参阅《&lt;a href="https://github.com/wx-chevalier/Concurrent-Series?q=" target="_blank" rel="noopener">Concurrent-Series/零拷贝&lt;/a>》&lt;/p>
&lt;/blockquote>
&lt;p>我们知道，在服务端，处理消费的大致逻辑是这样的：&lt;/p>
&lt;ul>
&lt;li>首先，从文件中找到消息数据，读到内存中；&lt;/li>
&lt;li>然后，把消息通过网络发给客户端。&lt;/li>
&lt;/ul>
&lt;p>这个过程中，数据实际上做了 2 次或者 3 次复制：&lt;/p>
&lt;ul>
&lt;li>从文件复制数据到 PageCache 中，如果命中 PageCache，这一步可以省掉；&lt;/li>
&lt;li>从 PageCache 复制到应用程序的内存空间中，也就是我们可以操作的对象所在的内存；&lt;/li>
&lt;li>从应用程序的内存空间复制到 Socket 的缓冲区，这个过程就是我们调用网络应用框架的 API 发送数据的过程。&lt;/li>
&lt;/ul>
&lt;p>Kafka 使用零拷贝技术可以把这个复制次数减少一次，上面的 2、3 步骤两次复制合并成一次复制。直接从 PageCache 中把数据复制到 Socket 缓冲区中，这样不仅减少一次数据复制，更重要的是，由于不用把数据复制到用户内存空间，DMA 控制器可以直接完成数据复制，不需要 CPU 参与，速度更快。下面是这个零拷贝对应的系统调用：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-c" data-lang="c">&lt;span class="line">&lt;span class="cl">&lt;span class="cp">#include&lt;/span> &lt;span class="cpf">&amp;lt;sys/socket.h&amp;gt;&lt;/span>&lt;span class="cp">
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="cp">&lt;/span>&lt;span class="kt">ssize_t&lt;/span> &lt;span class="nf">sendfile&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="kt">int&lt;/span> &lt;span class="n">out_fd&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">int&lt;/span> &lt;span class="n">in_fd&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">off_t&lt;/span> &lt;span class="o">*&lt;/span>&lt;span class="n">offset&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="kt">size_t&lt;/span> &lt;span class="n">count&lt;/span>&lt;span class="p">);&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>它的前两个参数分别是目的端和源端的文件描述符，后面两个参数是源端的偏移量和复制数据的长度，返回值是实际复制数据的长度。如果你遇到这种从文件读出数据后再通过网络发送出去的场景，并且这个过程中你不需要对这些数据进行处理，那一定要使用这个零拷贝的方法，可以有效地提升性能。&lt;/p>
&lt;h2 id="消费者读取数据">消费者（读取数据）&lt;/h2>
&lt;p>传统模式下我们从硬盘读取一个文件是这样的：先复制到内核空间（read 是系统调用，放到了 DMA，所以用内核空间），然后复制到用户空间（1、2）；从用户空间重新复制到内核空间（你用的 socket 是系统调用，所以它也有自己的内核空间），最后发送给网卡（3、4）。Zero Copy 中直接从内核空间（DMA 的）到内核空间（Socket 的），然后发送网卡，Nginx 也是用的这种技术。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6077adfb8322e6675c13715d.jpg" alt="传统调用与零拷贝对比" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>实际上，Kafka 把所有的消息都存放在一个一个的文件中，当消费者需要数据的时候 Kafka 直接把“文件”发送给消费者。当不需要把整个文件发出去的时候，Kafka 通过调用 Zero Copy 的 sendfile 这个函数，这个函数包括：&lt;/p>
&lt;ul>
&lt;li>out_fd 作为输出（一般及时 socket 的句柄）&lt;/li>
&lt;li>in_fd 作为输入文件句柄&lt;/li>
&lt;li>off_t 表示 in_fd 的偏移（从哪里开始读取）&lt;/li>
&lt;li>size_t 表示读取多少个&lt;/li>
&lt;/ul></description></item><item><title>副本</title><link>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/%E5%89%AF%E6%9C%AC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/%E5%89%AF%E6%9C%AC/</guid><description>&lt;h1 id="副本replication策略">副本（replication）策略&lt;/h1>
&lt;p>Kafka 在 0.8 版本前没有提供 Partition 的 Replication 机制，一旦 Broker 宕机，其上的所有 Partition 就都无法提供服务，而 Partition 又没有备份数据，数据的可用性就大大降低了。所以 0.8 后提供了 Replication 机制来保证 Broker 的 failover。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/607709288322e6675c3740f0.jpg" alt="副本选择新的 Leader" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>Kafka 的高可靠性的保障来源于其健壮的副本（replication）策略，即多个服务端节点对其他节点的主题分区的日志进行复制。当集群中的某个节点出现故障，访问故障节点的请求会被转移到其他正常节点(这一过程通常叫 Reblance)。Kafka 的 replica 副本单元是 topic 的 partition，一个 partition 的 replica 数量不能超过 broker 的数量，因为一个 broker 最多只会存储这个 partition 的一个副本。所有消息生产、消费请求都是由 partition 的 leader replica 来处理，其他 follower replica 负责从 leader 复制数据进行备份，保持和 Leader 的数据同步。如果没有 Leader 副本，那就需要所有的副本都同时负责读/写请求处理，同时还得保证这些副本之间数据的一致性，假设有 n 个副本则需要有 n×n 条通路来同步数据，这样数据的一致性和有序性就很难保证。&lt;/p>
&lt;h2 id="isr">ISR&lt;/h2>
&lt;p>在 Kafka 中并不是所有的 Follower 都能被拿来替代 Leader，所以在 Kafka 的 Leader 节点中维护着一个 ISR(In sync Replicas)集合，翻译过来也叫正在同步中集合，在这个集合中的需要满足两个条件:&lt;/p>
&lt;ul>
&lt;li>一是它必须维护与 ZooKeeper 的 session（这个通过 ZooKeeper 的 Heartbeat 机制来实现）。&lt;/li>
&lt;li>二是 Follower 必须能够及时将 Leader 的消息复制过来，不能“落后太多”。&lt;/li>
&lt;/ul>
&lt;p>Leader 会跟踪与其保持同步的 Replica 列表，如果一个 Follower 宕机，或者落后太多，Leader 将把它从 ISR 中移除。这里所描述的“落后太多”指 Follower 复制的消息落后于 Leader 后的条数超过预定值或者 Follower 超过一定时间未向 Leader 发送 fetch 请求。&lt;/p>
&lt;h2 id="数据可靠性">数据可靠性&lt;/h2>
&lt;p>Producer 在发布消息到某个 Partition 时，先通过 ZooKeeper 找到该 Partition 的 Leader，然后无论该 Topic 的 Replication Factor 为多少，Producer 只将该消息发送到该 Partition 的 Leader。Leader 会将该消息写入其本地 Log。每个 Follower 都从 Leader 拉取数据：&lt;/p>
&lt;ul>
&lt;li>消息所在 partition 的 ISR replicas 会定时异步从 leader 上批量复制数据 log&lt;/li>
&lt;li>当所有 ISR replica 都返回 ack，告诉 leader 该消息已经写 log 成功后，leader 认为该消息 committed，并告诉 Producer 生产成功。这里和以上”alive”条件的第二点是不矛盾的，因为 leader 有超时机制，leader 等 ISR 的 follower 复制数据，如果一定时间不返回 ack（可能数据复制进度落后太多），则 leader 将该 follower replica 从 ISR 中剔除。&lt;/li>
&lt;li>一旦 Leader 收到了 ISR 中的所有 Replica 的 ACK，该消息就被认为已经 commit 了，Leader 将增加 HW 并且向 Producer 发送 ACK。HW(高水位)是 Consumer 能够看到的此 partition 的位置，LEO 是每个 partition 的 log 最后一条 Message 的位置。HW 能保证 leader 所在的 broker 失效，该消息仍然可以从新选举的 leader 中获取，不会造成消息丢失。&lt;/li>
&lt;/ul>
&lt;p>Kafka Replication 的数据流如下图所示：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/60779f188322e6675cf6ac9d.png" alt="同步策略" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>ISR 机制下的数据复制，既不是完全的同步复制，也不是单纯的异步复制，这是 Kafka 高吞吐很重要的机制。同步复制要求所有能工作的 follower 都复制完，这条消息才会被认为 committed，这种复制方式极大的影响了吞吐量。而异步复制方式下，follower 异步的从 leader 复制数据，数据只要被 leader 写入 log 就被认为已经 committed，这种情况下如果 follower 都复制完都落后于 leader，而如果 leader 突然宕机，则会丢失数据。而 Kafka 的这种使用 ISR 的方式则很好的均衡了确保数据不丢失以及吞吐量，follower 可以批量的从 leader 复制数据，数据复制到内存即返回 ack，这样极大的提高复制性能，当然数据仍然是有丢失风险的。&lt;/p>
&lt;p>对于 Producer 而言，它可以选择是否等待消息 commit。这种机制确保了只要 ISR 有一个或以上的 Follower，一条被 commit 的消息就不会丢失。当 Producer 向 Leader 发送数据时，可以通过 request.required.acks 参数来设置数据可靠性的级别：&lt;/p>
&lt;ul>
&lt;li>1（默认）：这意味着 producer 在 ISR 中的 leader 已成功收到的数据并得到确认后发送下一条 message。如果 leader 宕机了，则会丢失数据。&lt;/li>
&lt;li>0：这意味着 producer 无需等待来自 broker 的确认而继续发送下一批消息。这种情况下数据传输效率最高，但是数据可靠性确是最低的。&lt;/li>
&lt;li>-1：producer 需要等待 ISR 中的所有 follower 都确认接收到数据后才算一次发送完成，可靠性最高。但是这样也不能保证数据不丢失，比如当 ISR 中只有 leader 时(其他节点都和 zk 断开连接，或者都没追上)，这样就变成了 acks=1 的情况。&lt;/li>
&lt;/ul>
&lt;h2 id="副本放置策略">副本放置策略&lt;/h2>
&lt;p>为了更好的做负载均衡，Kafka 尽量将所有的 Partition 均匀分配到整个集群上。Kafka 分配 Replica 的算法如下：&lt;/p>
&lt;ul>
&lt;li>将所有存活的 N 个 Brokers 和待分配的 Partition 排序&lt;/li>
&lt;li>将第 i 个 Partition 分配到第(i mod n)个 Broker 上，这个 Partition 的第一个 Replica 存在于这个分配的 Broker 上，并且会作为 partition 的优先副本&lt;/li>
&lt;li>将第 i 个 Partition 的第 j 个 Replica 分配到第((i + j) mod n)个 Broker 上&lt;/li>
&lt;/ul>
&lt;p>假设集群一共有 4 个 brokers，一个 topic 有 4 个 partition，每个 Partition 有 3 个副本。下图是每个 Broker 上的副本分配情况。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/60779e818322e6675cf5b084.jpg" alt="Broker 副本分布情况" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h1 id="服务可用性">服务可用性&lt;/h1>
&lt;h2 id="leader-选举">Leader 选举&lt;/h2>
&lt;p>Kafka 所有收发消息请求都由 Leader 节点处理，由以上数据可靠性设计可知，当 ISR 的 follower replica 故障后，leader 会及时地从 ISR 列表中把它剔除掉，并不影响服务可用性。而如果 Leader 发生了故障，则 Kafka 会重新选举 Leader：&lt;/p>
&lt;ul>
&lt;li>Kafka 在 Zookeeper 存储 partition 的 ISR 信息，并且能动态调整 ISR 列表的成员，只有 ISR 里的成员 replica 才会被选为 leader，并且 ISR 所有的 replica 都有可能成为 leader；&lt;/li>
&lt;li>Leader 节点宕机后，Zookeeper 能监控发现，并由 broker 的 controller 节点从 ISR 中选举出新的 leader，并通知 ISR 内的所有 broker 节点。&lt;/li>
&lt;/ul>
&lt;p>Leader 选举本质上是一个分布式锁，有两种方式实现基于 ZooKeeper 的分布式锁：&lt;/p>
&lt;ul>
&lt;li>节点名称唯一性：多个客户端创建一个节点，只有成功创建节点的客户端才能获得锁&lt;/li>
&lt;li>临时顺序节点：所有客户端在某个目录下创建自己的临时顺序节点，只有序号最小的才获得锁&lt;/li>
&lt;/ul>
&lt;h2 id="容灾和数据一致性">容灾和数据一致性&lt;/h2>
&lt;p>分布式系统的容灾能力，跟其本身针对数据一致性考虑所选择的算法有关，例如，Zookeeper 的 Zab 算法，raft 算法等。Kafka 的 ISR 机制和这些 Majority Vote 算法对比如下：&lt;/p>
&lt;ul>
&lt;li>ISR 机制能容忍更多的节点失败。假如 replica 节点有 2f+1 个，每个 partition 最多能容忍 2f 个失败，且不丢失消息数据；但相对 Majority Vote 选举算法，只能最多容忍 f 个失败。&lt;/li>
&lt;li>在消息 committed 持久化上，ISR 需要等 2f 个节点返回 ack，但 Majority Vote 只需等 f+1 个节点返回 ack，且不依赖处理最慢的 follower 节点，因此 Majority Vote 有优势&lt;/li>
&lt;li>ISR 机制能节省更多 replica 节点数。例如，要保证 f 个节点可用，ISR 方式至少要 f 个节点，而 Majority Vote 至少需要 2f+1 个节点。&lt;/li>
&lt;/ul>
&lt;p>如果所有 replica 都宕机了，有两种方式恢复服务：&lt;/p>
&lt;ul>
&lt;li>等 ISR 任一节点恢复，并选举为 leader；&lt;/li>
&lt;li>选择第一个恢复的节点（不一定是 ISR 中的节点）为 leader&lt;/li>
&lt;/ul>
&lt;p>第一种方式消息不会丢失（只能说这种方式最有可能不丢而已），第二种方式可能会丢消息，但能尽快恢复服务可用。这是可用性和一致性场景的两种考虑，Kafka 默认选择第二种，用户也可以自主配置。大部分考虑 CP 的分布式系统（假设 2f+1 个节点），为了保证数据一致性，最多只能容忍 f 个节点的失败，而 Kafka 为了兼顾可用性，允许最多 2f 个节点失败，因此是无法保证数据强一致的。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6077f2fa8322e6675ca8d556.jpg" alt="ISR 容灾" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>如图所示，一开始 ISR 数量等于 3，正常同步数据，红色部分开始，leader 发现其他两个 follower 复制进度太慢或者其他原因（网络分区、节点故障等），将其从 ISR 剔除后，leader 单节点存储数据；然后，leader 宕机，触发重新选举第二节点为 leader，重新开始同步数据，但红色部分的数据在新 leader 上是没有的；最后原 leader 节点恢复服务后，重新从新 leader 上复制数据，而红色部分的数据已经消费不到了。&lt;/p>
&lt;p>因此，为了减少数据丢失的概率，可以设置 Kafka 的 ISR 最小 replica 数，低于该值后直接返回不可用，当然是以牺牲一定可用性和吞吐量为前提了。&lt;/p></description></item><item><title>日志文件</title><link>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6/</guid><description>&lt;h1 id="消息存储">消息存储&lt;/h1>
&lt;h1 id="日志文件格式">日志文件格式&lt;/h1>
&lt;p>一个 topic 可以认为一个一类消息，每个 topic 将被分成多个 partition，每个 partition 在存储层面是 append log 文件。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/607705e18322e6675c2f8ade.jpg" alt="Producer Topic 与 Partition" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>在 Kafka 文件存储中，同一个 topic 下有多个不同 partition，每个 partition 为一个目录，partiton 命名规则为 topic 名称+有序序号，第一个 partiton 序号从 0 开始，序号最大值为 partitions 数量减 1。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/607706098322e6675c2fe553.jpg" alt="文件分区与分割" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;ul>
&lt;li>每个 partion（目录）相当于一个巨型文件被平均分配到多个大小相等 segment（段）数据文件中。但每个段 segment file 消息数量不一定相等，这种特性方便 old segment file 快速被删除。&lt;/li>
&lt;li>每个 partiton 只需要支持顺序读写就行了，segment 文件生命周期由服务端配置参数决定。&lt;/li>
&lt;/ul>
&lt;p>这样做的好处就是能快速删除无用文件，有效提高磁盘利用率。&lt;/p>
&lt;ul>
&lt;li>segment file 组成：由 2 大部分组成，分别为 index file 和 data file，此 2 个文件一一对应，成对出现，后缀&amp;quot;.index&amp;quot;和“.log”分别表示为 segment 索引文件、数据文件.&lt;/li>
&lt;li>segment 文件命名规则：partion 全局的第一个 segment 从 0 开始，后续每个 segment 文件名为上一个 segment 文件最后一条消息的 offset 值。数值最大为 64 位 long 大小，20 位数字字符长度，没有数字用 0 填充。&lt;/li>
&lt;/ul>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/607706968322e6675c3140cd.jpg" alt="Segment 文件格式" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="segment-索引与-offset-查找">Segment 索引与 Offset 查找&lt;/h2>
&lt;p>由于 Kafka 消息数据太大，如果全部建立索引，即占了空间又增加了耗时，所以 Kafka 选择了稀疏索引的方式，这样的话索引可以直接进入内存，加快偏查询速度。，通过 mmap 可以直接内存操作，稀疏索引为数据文件的每个对应 message 设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。Segment 中 index 与 data file 对应关系物理结构如下：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/607706b98322e6675c3193c6.jpg" alt="Segment 中 index 与 data file 对应" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>上图中索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中 message 的物理偏移地址。其中以索引文件中元数据 3,497 为例，依次在数据文件中表示第 3 个 message（在全局 partiton 表示 offset 为 368772 的 message），以及该消息的物理偏移地址为 497。&lt;/p>
&lt;p>查找某个 offset 的消息，先二分法找出消息所在的 segment 文件（因为每个 segment 的命名都是以该文件中消息 offset 最小的值命名）；然后，加载对应的.index 索引文件到内存，同样二分法找出小于等于给定 offset 的最大的那个 offset 记录（相对 offset，position）；最后，根据 position 到.log 文件中，顺序查找出 offset 等于给定 offset 值的消息。例如读取 offset=368776 的 message，需要通过下面 2 个步骤查找：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>第一步查找 segment file，其中 00000000000000000000.index 表示最开始的文件，起始偏移量(offset)为 0。第二个文件 00000000000000368769.index 的消息量起始偏移量为 368770 = 368769 + 1.同样，第三个文件 00000000000000737337.index 的起始偏移量为 737338=737337 + 1，其他后续文件依次类推，以起始偏移量命名并排序这些文件，只要根据 offset &lt;strong>二分查找&lt;/strong>文件列表，就可以快速定位到具体文件。 当 offset=368776 时定位到 00000000000000368769.index|log&lt;/p>
&lt;/li>
&lt;li>
&lt;p>第二步通过 segment file 查找 message 通过第一步定位到 segment file，当 offset=368776 时，依次定位到 00000000000000368769.index 的元数据物理位置和 00000000000000368769.log 的物理偏移地址，然后再通过 00000000000000368769.log 顺序查找直到 offset=368776 为止。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>上面讲的是如果要找某个 offset 的流程，但是我们大多数时候并不需要查找某个 offset,只需要按照顺序读即可，而在顺序读中，操作系统会对内存和磁盘之间添加 page cahe，也就是我们平常见到的预读操作，所以我们的顺序读操作时速度很快。但是 kafka 有个问题，如果分区过多，那么日志分段也会很多，写的时候由于是批量写，其实就会变成随机写了，随机 I/O 这个时候对性能影响很大。所以一般来说 Kafka 不能有太多的 partition。针对这一点，RocketMQ 把所有的日志都写在一个文件里面，就能变成顺序写，通过一定优化，读也能接近于顺序读。&lt;/p>
&lt;h2 id="message-物理结构">Message 物理结构&lt;/h2>
&lt;p>了解到 segment data file 由许多 message 组成，下面详细说明 message 物理结构如下：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/607707668322e6675c333389.jpg" alt="message 物理结构" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>参数说明如下：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>关键字&lt;/th>
&lt;th>解释说明&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>8 byte offset&lt;/td>
&lt;td>在 parition(分区)内的每条消息都有一个有序的 id 号，这个 id 号被称为偏移(offset),它可以唯一确定每条消息在 parition(分区)内的位置。即 offset 表示 partiion 的第多少 message&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4 byte message size&lt;/td>
&lt;td>message 大小&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4 byte CRC32&lt;/td>
&lt;td>用 crc32 校验 message&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1 byte “magic&amp;quot;&lt;/td>
&lt;td>表示本次发布 Kafka 服务程序协议版本号&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>1 byte “attributes&amp;quot;&lt;/td>
&lt;td>表示为独立版本、或标识压缩类型、或编码类型。&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>4 byte key length&lt;/td>
&lt;td>表示 key 的长度,当 key 为-1 时，K byte key 字段不填&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>K byte key&lt;/td>
&lt;td>可选&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>value bytes payload&lt;/td>
&lt;td>表示实际消息数据。&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table></description></item><item><title>网络模型</title><link>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/messagequeue-series/3.kafka/%E6%B6%88%E6%81%AF%E4%BB%A3%E7%90%86/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</guid><description>&lt;h1 id="kafka-网络模型">Kafka 网络模型&lt;/h1>
&lt;h1 id="kafka-client单线程-selector">Kafka Client：单线程 Selector&lt;/h1>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6077b06b8322e6675c188f53.png" alt="单线程 Selector" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>单线程模式适用于并发链接数小，逻辑简单，数据量小。在 kafka 中，consumer 和 producer 都是使用的上面的单线程模式。这种模式不适合 kafka 的服务端，在服务端中请求处理过程比较复杂，会造成线程阻塞，一旦出现后续请求就会无法处理，会造成大量请求超时，引起雪崩。而在服务器中应该充分利用多线程来处理执行逻辑。&lt;/p>
&lt;h1 id="kafka-server多线程-selector">Kafka Server：多线程 Selector&lt;/h1>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://pic.imgdb.cn/item/6077dc738322e6675c6e637c.png" alt="Kafka Server" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>在 kafka 服务端采用的是多线程的 Selector 模型，Acceptor 运行在一个单独的线程中，对于读取操作的线程池中的线程都会在 selector 注册 read 事件，负责服务端读取请求的逻辑。成功读取后，将请求放入 message queue 共享队列中。然后在写线程池中，取出这个请求，对其进行逻辑处理，即使某个请求线程阻塞了，还有后续的县城从消息队列中获取请求并进行处理，在写线程中处理完逻辑处理，由于注册了 OP_WIRTE 事件，所以还需要对其发送响应。&lt;/p></description></item></channel></rss>