<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>主从节点 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/distributedsystem-series/%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80/2.%E8%8A%82%E7%82%B9%E4%B8%8E%E9%9B%86%E7%BE%A4/%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9/</link><atom:link href="https://ng-tech.icu/books/distributedsystem-series/%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80/2.%E8%8A%82%E7%82%B9%E4%B8%8E%E9%9B%86%E7%BE%A4/%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9/index.xml" rel="self" type="application/rss+xml"/><description>主从节点</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>主从节点</title><link>https://ng-tech.icu/books/distributedsystem-series/%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80/2.%E8%8A%82%E7%82%B9%E4%B8%8E%E9%9B%86%E7%BE%A4/%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9/</link></image><item><title>节点选举</title><link>https://ng-tech.icu/books/distributedsystem-series/%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80/2.%E8%8A%82%E7%82%B9%E4%B8%8E%E9%9B%86%E7%BE%A4/%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9/%E8%8A%82%E7%82%B9%E9%80%89%E4%B8%BE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/distributedsystem-series/%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80/2.%E8%8A%82%E7%82%B9%E4%B8%8E%E9%9B%86%E7%BE%A4/%E4%B8%BB%E4%BB%8E%E8%8A%82%E7%82%B9/%E8%8A%82%E7%82%B9%E9%80%89%E4%B8%BE/</guid><description>&lt;h1 id="节点选举">节点选举&lt;/h1>
&lt;p>在实际的系统演化过程中，最初的时候我们只有单节点，或者我们能够人为地去控制仅有的几个节点。但如果该领导者失效，或者如果网络中断导致领导者不可达，这样的系统就无法取得任何进展。应对这种情况可以有三种方法：&lt;/p>
&lt;ul>
&lt;li>等待领导者恢复，接受系统将在这段时间阻塞的事实。许多 XA/JTA 事务协调者选择这个选项。这种方法并不能完全达成共识，因为它不能满足终止属性的要求：如果领导者续命失败，系统可能会永久阻塞。&lt;/li>
&lt;li>人工故障切换，让人类选择一个新的领导者节点，并重新配置系统使之生效，许多关系型数据库都采用这种方方式。这是一种来自“天意”的共识，由计算机系统之外的运维人员做出决定。故障切换的速度受到人类行动速度的限制，通常要比计算机慢（得多）。&lt;/li>
&lt;li>使用算法自动选择一个新的领导者。这种方法需要一种共识算法，使用成熟的算法来正确处理恶劣的网络条件是明智之举。&lt;/li>
&lt;/ul>
&lt;p>尽管单领导者数据库可以提供线性一致性，且无需对每个写操作都执行共识算法，但共识对于保持及变更领导权仍然是必须的。因此从某种意义上说，使用单个领导者不过是“缓兵之计”：共识仍然是需要的，只是在另一个地方，而且没那么频繁。像 ZooKeeper 这样的工具为应用提供了“外包”的共识、故障检测和成员服务。它们扮演了重要的角色，虽说使用不易，但总比自己去开发一个能经受所有问题考验的算法要好得多。如果你发现自己想要解决的问题可以归结为共识，并且希望它能容错，使用一个类似 ZooKeeper 的东西是明智之举。&lt;/p>
&lt;h2 id="领导者与锁定">领导者与锁定&lt;/h2>
&lt;p>在&lt;a href="https://github.com/wx-chevalier/DistributedSystem-Series" target="_blank" rel="noopener">《不可靠的分布式系统&lt;/a>》中我们讨论过一种进程暂停的情形，想象一个经历了一个长时间 stop-the-world GC Pause 的节点，节点的所有线程被 GC 抢占并暂停一分钟，因此没有请求被处理，也没有响应被发送。其他节点等待，重试，不耐烦，并最终宣布节点死亡，并将其丢到灵车上。最后，GC 完成，节点的线程继续，好像什么也没有发生。其他节点感到惊讶，因为所谓的死亡节点突然从棺材中抬起头来，身体健康，开始和旁观者高兴地聊天。GC 后的节点最初甚至没有意识到已经经过了整整一分钟，而且自己已被宣告死亡。从它自己的角度来看，从最后一次与其他节点交谈以来，几乎没有经过任何时间。&lt;/p>
&lt;p>通常情况下，一些东西在一个系统中只能有一个。例如：&lt;/p>
&lt;ul>
&lt;li>数据库分区的领导者只能有一个节点，以避免脑裂（split brain）。&lt;/li>
&lt;li>特定资源的锁或对象只允许一个事务/客户端持有，以防同时写入和损坏。&lt;/li>
&lt;li>一个特定的用户名只能被一个用户所注册，因为用户名必须唯一标识一个用户。&lt;/li>
&lt;/ul>
&lt;p>在分布式系统中实现这一点需要注意：即使一个节点认为它是“天选者（the choosen one）”（分区的负责人，锁的持有者，成功获取用户名的用户的请求处理程序），但这并不一定意味着有法定人数的节点同意！一个节点可能以前是领导者，但是如果其他节点在此期间宣布它死亡（例如，由于网络中断或 GC 暂停），则它可能已被降级，且另一个领导者可能已经当选。如果一个节点继续表现为天选者，即使大多数节点已经声明它已经死了，则在考虑不周的系统中可能会导致问题。这样的节点能以自己赋予的权能向其他节点发送消息，如果其他节点相信，整个系统可能会做一些不正确的事情。&lt;/p>
&lt;p>下图显示了由于不正确的锁实现导致的数据损坏错误，该错误在 HBase 中就真实地存在过。假设你要确保一个存储服务中的文件一次只能被一个客户访问，因为如果多个客户试图写对此，该文件将被损坏。您尝试通过在访问文件之前要求客户端从锁定服务获取租约来实现此目的。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s2.ax1x.com/2020/02/12/17fYWt.md.png" alt="分布式锁的实现不正确：客户端1认为它仍然具有有效的租约，即使它已经过期，从而破坏了存储中的文件" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>如果持有租约的客户端暂停太久，它的租约将到期。另一个客户端可以获得同一文件的租约，并开始写入文件。当暂停的客户端回来时，它认为（不正确）它仍然有一个有效的租约，并继续写入文件。结果，客户的写入冲突和损坏的文件。&lt;/p>
&lt;h3 id="防护令牌">防护令牌&lt;/h3>
&lt;p>当使用锁或租约来保护对某些资源的访问时，需要确保一个被误认为自己是“天选者”的节点不能中断系统的其它部分。实现这一目标的一个相当简单的技术就是防护（fencing）。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s2.ax1x.com/2020/02/12/1746MV.png" alt="只允许以增加屏蔽令牌的顺序进行写操作，从而保证存储安全" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>我们假设每次锁定服务器授予锁或租约时，它还会返回一个防护令牌（fencing token），这个数字在每次授予锁定时都会增加（例如，由锁定服务增加）。然后，我们可以要求客户端每次向存储服务发送写入请求时，都必须包含当前的屏蔽令牌。客户端 1 以 33 的令牌获得租约，但随后进入一个长时间的停顿并且租约到期。客户端 2 以 34 的令牌（该数字总是增加）获取租约，然后将其写入请求发送到存储服务，包括 34 的令牌。稍后，客户端 1 恢复生机并将其写入存储服务，包括其令牌值 33.但是，存储服务器会记住它已经处理了一个具有更高令牌编号（34）的写入，因此它会拒绝带有令牌 33 的请求。&lt;/p>
&lt;p>如果将 ZooKeeper 用作锁定服务，则可将事务标识 zxid 或节点版本 cversion 用作屏蔽令牌。由于它们保证单调递增，因此它们具有所需的属性。请注意，这种机制要求资源本身在检查令牌方面发挥积极作用，通过拒绝使用旧的令牌，而不是已经被处理的令牌来进行写操作——仅仅依靠客户端检查自己的锁状态是不够的。对于不明确支持屏蔽令牌的资源，可能仍然可以解决此限制（例如，在文件存储服务的情况下，可以将防护令牌包含在文件名中）。但是，为了避免在锁的保护之外处理请求，需要进行某种检查。在服务器端检查一个令牌可能看起来像是一个缺点，但这可以说是一件好事：一个服务假定它的客户总是守规矩并不明智，因为使用客户端的人与运行服务的人优先级非常不一样。因此，任何服务保护自己免受意外客户的滥用是一个好主意。&lt;/p></description></item></channel></rss>