<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DataPipeline | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/</link><atom:link href="https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/index.xml" rel="self" type="application/rss+xml"/><description>DataPipeline</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>DataPipeline</title><link>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/</link></image><item><title>数据汇集层</title><link>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E6%95%B0%E6%8D%AE%E6%B1%87%E9%9B%86%E5%B1%82/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E6%95%B0%E6%8D%AE%E6%B1%87%E9%9B%86%E5%B1%82/</guid><description>&lt;h1 id="数据汇集层">数据汇集层&lt;/h1>
&lt;p>数据汇集层在部分场景下又称为变更分发平台。当各类数据从源端抽取后，首先应当被写入一个数据汇集层，然后再进行后继的转换处理，直至将最终结果写入目的地。数据汇集层的作用主要有两点：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>数据汇集层将异构的数据源数据存储为统一的格式，并且为后继的处理提供一致的访问接口。这就将处理逻辑和数据源解耦开来，同时屏蔽了数据抽取过程中可能发生的异常对后继作业的影响。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>数据汇集层独立于数据源，可被多次访问，亦可根据业务需要缓存全部或一定期限的原始数据，这为转换分析提供了更高的灵活度。当业务需求发生变化时，无需重复读取源端数据，直接基于数据汇集层就可以开发新的模型和应用。数据汇集层可基于任意支持海量 / 高可用的文件系统、数据仓库或者消息队列构建，常见的方案包括 HDFS、HBase、Kafka 等。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="数据汇集层的技术考量">数据汇集层的技术考量&lt;/h1>
&lt;p>变更分发平台可以有很多种形式，本质上它只是一个存储变更的中间件，那么如何进行选型呢？首先由于变更数据数据量级大，且操作时没有事务需求，所以先排除了关系型数据库，剩下的 NoSQL 如 Cassandra，mq 如 Kafka、RabbitMQ 都可以胜任。其区别在于，消费端到分发平台拉取变更时，假如是 NoSQL 的实现，那么就能很容易地实现条件过滤等操作(比如某个客户端只对特定字段为 true 的消息感兴趣); 但 NoSQL 的实现往往会在吞吐量和一致性上输给 mq。这里就是一个设计抉择的问题，最终我们选择了 mq，主要考虑的点是：消费端往往是无状态应用，很容易进行水平扩展，因此假如有条件过滤这样的需求，我们更希望把这样的计算压力放在消费端上。&lt;/p>
&lt;p>而在 mq 里，Kafka 则显得具有压倒性优势。Kafka 本身就有大数据的基因，通常被认为是目前吞吐量最大的消息队列，同时，使用 Kafka 有一项很适合该场景的特性：Log Compaction。Kafka 默认的过期清理策略(log.cleanup.policy)是 delete，也就是删除过期消息，配置为 compact 则可以启用 Log Compaction 特性，这时 Kafka 不再删除过期消息，而是对所有过期消息进行”折叠”：对于 key 相同的所有消息会，保留最新的一条。&lt;/p>
&lt;p>对应的在 mq 中的流总共会产生 4 条变更消息，而最下面两条分别是 id:1 id:2 下的最新记录，在它们之前的两条 INSERT 引起的变更就会被 Kafka 删除，最终我们在 Kafka 中看到的就是两行记录的最新状态，而一个持续订阅该流的消费者则能收到全部 4 条记录。这种行为有一个有趣的名字，流表二相性(Stream Table Durability)：Topic 中有无尽的变更消息不断被写入，这是流的特质；而 Topic 某一时刻的状态，恰恰是该时刻对应的数据表的一个快照(参见上面的例子)，每条新消息的到来相当于一次 Upsert，这又是表的特性。落到实践中来讲，Log Compaction 对于我们的场景有一个重要应用：全量数据迁移与数据补偿，我们可以直接编写针对每条变更数据的处理程序，就能兼顾全量迁移与之后的增量同步两个过程；而在数据异常时，我们可以重新回放整个 Kafka Topic：该 Topic 就是对应表的快照，针对上面的例子，我们回放时只会读到最新的两条消息，不需要读全部四条消息也能保证数据正确。&lt;/p>
&lt;p>关于 Kafka 作为变更分发平台，最后要说的就是消费顺序的问题。大家都知道 Kafka 只能保证单个 Partition 内消息有序，而对于整个 Topic，消息是无序的。一般的认知是，数据变更的消费为了逻辑的正确性，必须按序消费。按着这个逻辑，我们的 Topic 只能有单个 Partition，这就大大牺牲了 Kafka 的扩展性与吞吐量。其实这里有一个误区，对于数据库变更抓取，我们只要保证 同一行记录的变更有序 就足够了。还是上面的例子，我们只需要保证对 id:2 这行的 insert 消息先于 update 消息，该行数据最后就是正确的。而实现”同一行记录变更有序”就简单多了，Kafka Producer 对带 key 的消息默认使用 key 的 hash 决定分片，因此只要用数据行的主键作为消息的 key，所有该行的变更都会落到同一个 Parition 上，自然也就有序了。这有一个要求就是 CDC 模块必须解析出变更数据的主键：而这点 Debezium 已经帮助我们解决了。&lt;/p>
&lt;h1 id="统一数据格式">统一数据格式&lt;/h1>
&lt;p>数据格式的选择同样十分重要。首先想到的当然是 json, 目前最常见的消息格式，不仅易读，开发也都对它十分熟悉。但 json 本身有一个很大的不足，那就是契约性太弱，它的结构可以随意更改：试想假如有一个接口返回 String，注释上说这是个 json，那我们该怎么编写对应的调用代码呢？是不是需要翻接口文档，提前获知这段 json 的 schema，然后才能开始编写代码，并且这段代码随时可能会因为这段 json 的格式改变而 break。&lt;/p>
&lt;p>在规模不大的系统中，这个问题并不显著。但假如在一个拥有上千种数据格式的数据管道上工作，这个问题就会很麻烦，首先当你订阅一个变更 topic 时，你完全处于懵逼状态——不知道这个 topic 会给你什么，当你经过文档的洗礼与不断地调试终于写完了客户端代码，它又随时会因为 topic 中的消息格式变更而挂掉。参考 Yelp 和 Linkedin 的选择，我们决定使用 Apache Avro 作为统一的数据格式。Avro 依赖模式 Schema 来实现数据结构定义，而 Schema 通常使用 json 格式进行定义，一个典型的 Schema 如下：这里要介绍一点背景知识，Avro 的一个重要特性就是支持 Schema 演化，它定义了一系列的演化规则，只要符合该规则，使用不同的 Schema 也能够正常通信。也就是说，使用 Avro 作为数据格式进行通信的双方是有自由更迭 Schema 的空间的。&lt;/p>
&lt;p>在我们的场景中，数据库表的 Schema 变更会引起对应的变更数据 Schema 变更，而每次进行数据库表 Schema 变更就更新下游消费端显然是不可能的。所以这时候 Avro 的 Schema 演化机制就很重要了。我们做出约定，同一个 Topic 上传输的消息，其 Avro Schema 的变化必须符合演化规则，这么一来，消费者一旦开始正常消费之后就不会因为消息的 Schema 变化而挂掉。&lt;/p></description></item><item><title>数据源监听</title><link>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9B%91%E5%90%AC/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E6%95%B0%E6%8D%AE%E6%BA%90%E7%9B%91%E5%90%AC/</guid><description>&lt;h1 id="数据源监听">数据源监听&lt;/h1>
&lt;p>源数据变化捕获是数据集成的起点，获取数据源变化主要有三种方式：&lt;/p>
&lt;ul>
&lt;li>基于日志的解析模式；&lt;/li>
&lt;li>基于增量条件查询模式；&lt;/li>
&lt;li>数据源主动推送模式。&lt;/li>
&lt;/ul>
&lt;p>基于日志的解析模式常用于各种类型的数据库，例如 MySQL 的 Binlog、Oracle 的 Redo&amp;amp;Achieve Log、SQL Server Change Tracking &amp;amp; CDC 等。&lt;/p>
&lt;p>不同数据库日志解析的原理差别很大，以 MySQL Binlog 模式为例，解析程序本身是一个 Slave，能够实时收到 MySQL Master 的数据流推送，并解析还原成 DDL 和 DML 操作。而 SQL Server 的 CT 模式下，增量是通过定期查询 Change Tracking 表实现的。&lt;/p>
&lt;p>基于增量条件的查询模式不依赖于源端开启日志记录，但对于数据源通常有额外的格式要求。例如，数据库表或文档对象需要有标志更新时间的字段，这在一些业务系统中是无法满足的。&lt;/p>
&lt;p>数据源主动推送模式的常见形式为业务插码，即应用系统通过打点或者配置切面的方式，将数据变化封装为事件，额外发送一份给数据集成平台。这种方式一般需要对源端系统代码进行一定程度的修改。&lt;/p>
&lt;h1 id="数据库日志增量捕获">数据库日志增量捕获&lt;/h1>
&lt;p>通常而言，基于数据库的日志进行增量捕获应当被优先考虑。其具备以下几个显著优点：&lt;/p>
&lt;ul>
&lt;li>能够完整获取数据变化的操作类型，尤其是 Delete 操作，这是增量条件查询模式很难做到的；&lt;/li>
&lt;li>不依赖特别的数据字段语义，例如更新时间；&lt;/li>
&lt;li>多数情况下具备较强的实时性。&lt;/li>
&lt;/ul>
&lt;p>当然，事物都具有两面性。开启数据库日志通常会对源库性能产生一定的影响，需要额外的存储空间，甚至一些解析方法也会对源库资源造成额外消耗。因此，实施过程中需要在 DBA 的配合下，根据数据库特点和解析原理进行 DB 部署规划。&lt;/p>
&lt;p>推荐使用数据库的复制和灾备能力，在独立服务器对从库进行日志解析。此外，当数据库产生批量更新时，会在短时间内产生大量日志堆积，如果日志留存策略设置不当，容易出现数据丢失。这些都需要根据具体的业务数据增长特点，在前期做好规划，并在上线后根据业务变化定期进行评估和调整。&lt;/p>
&lt;h1 id="数据源推送">数据源推送&lt;/h1>
&lt;p>数据源主动 push 模式下，由于事件发送和业务处理很难做到事务一致性，所以当出现异常时，数据一致性就无从保证，比较适合对于数据一致性要求不高的场景，例如用户行为分析。&lt;/p>
&lt;h1 id="cdc-模块">CDC 模块&lt;/h1>
&lt;p>变更数据抓取通常需要针对不同数据源订制实现，而针对特定数据源，实现方式一般有两种：&lt;/p>
&lt;ul>
&lt;li>基于自增列或上次修改时间做增量查询；&lt;/li>
&lt;li>利用数据源本身的事务日志或 Slave 同步等机制实时订阅变更；&lt;/li>
&lt;/ul>
&lt;p>第一种方式实现简单，以 SQL 为例：相信大家都写过类似的 SQL, 每次查询时，查询 [last_query_time, now) 区间内的增量数据，lastmodified 列也可以用自增主键来替代。这种方式的缺点是实时性差，对数据库带来了额外压力，并且侵入了表设计：所有要实现变更抓取的表都必须有用于增量查询的列并且在该列上构建索引。另外，这种方式无法感知物理删除(Delete), 删除逻辑只能用一个 delete 列作为 flag 来实现。第二种方式实现起来相对困难，但它很好地解决了第一种方式的问题，因此前文提到的开源方案也都采用了这种方式。下面我们着重分析在 MySQL 中如何实现基于事务日志的实时变更抓取。&lt;/p>
&lt;p>MySQL 的事务日志称为 Binlog，常见的 MySQL 主从同步就是使用 Binlog 实现的：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s2.ax1x.com/2019/10/08/ufd4Bt.png" alt="MySQL Database Replication Internals" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>我们把 Slave 替换成 CDC 模块，CDC 模块模拟 MySQL Slave 的交互协议，便能收到 Master 的 Binlog 推送：&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s2.ax1x.com/2019/10/08/ufdxH0.png" alt="CDC 模块架构" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;p>CDC 模块解析 Binlog，产生特定格式的变更消息，也就完成了一次变更抓取。但这还不够，CDC 模块本身也可能挂掉，那么恢复之后如何保证不丢数据又是一个问题。这个问题的解决方案也是要针对不同数据源进行设计的，就 MySQL 而言，通常会持久化已经消费的 Binlog 位点或 Gtid(MySQL 5.6 之后引入)来标记上次消费位置。其中更好的选择是 Gtid，因为该位点对于一套 MySQL 体系（主从或多主）是全局的，而 Binlog 位点是单机的，无法支持主备或多主架构。&lt;/p>
&lt;p>MySQL CDC 模块的一个挑战是如何在 Binlog 变更事件中加入表的 Schema 信息(如标记哪些字段为主键，哪些字段可为 null)。Debezium 在这点上处理得很漂亮，它在内存中维护了数据库每张表的 Schema，并且全部写入一个 backup 的 Kafka Topic 中，每当 Binlog 中出现 DDL 语句，便应用这条 DDL 来更新 Schema。而在节点宕机，Debezium 实例被调度到另一个节点上后，又会通过 backup topic 恢复 Schema 信息，并从上次消费位点继续解析 Binlog。&lt;/p>
&lt;p>另一个挑战是，我们数据库已经有大量的现存数据，数据迁移时的现存数据要如何处理。这时，Debezium 独特的 Snapshot 功能就能帮上忙，它可以实现将现有数据作为一次”插入变更”捕捉到 Kafka 中，因此只要编写一次客户端就能一并处理全量数据与后续的增量数据。&lt;/p>
&lt;h1 id="开源方案对比">开源方案对比&lt;/h1>
&lt;ul>
&lt;li>databus: Linkedin 的分布式数据变更抓取系统；&lt;/li>
&lt;li>Yelp’s data pipeline: Yelp 的数据管道；&lt;/li>
&lt;li>Otter &amp;amp; Canal: 阿里开源的分布式数据库同步系统；&lt;/li>
&lt;li>Debezium: Redhat 开源的数据变更抓取组件；&lt;/li>
&lt;/ul>
&lt;p>这些解决方案关注的重点各有不同，但基本思想是一致的：使用变更抓取模块实时订阅数据库变更，并分发到一个中间存储供下游应用消费。下面是四个解决方案的对比矩阵：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>方案&lt;/th>
&lt;th>变更抓取&lt;/th>
&lt;th>分发平台&lt;/th>
&lt;th>消息格式&lt;/th>
&lt;th>额外特性&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>databus&lt;/td>
&lt;td>DatabusEventProducer, 支持 Oracle 和 MySQL 的变更抓取&lt;/td>
&lt;td>DatabusRelay, 基于 Netty 的中间件, 内部是一个 RingBuffer 存储变更消息&lt;/td>
&lt;td>Apache Avro&lt;/td>
&lt;td>有 BootstrapService 组件存储历史变更用以支持全量&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Yelp’s data pipeline&lt;/td>
&lt;td>MySQL Streamer, 基于 Binlog 抓取变更&lt;/td>
&lt;td>Apache Kafka&lt;/td>
&lt;td>Apache Avro&lt;/td>
&lt;td>Schematizer, 作为消息的 Avro Schema 注册中心的同时提供了 Schema 文档&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Otter&lt;/td>
&lt;td>&lt;a href="https://github.com/alibaba/canal" target="_blank" rel="noopener">Canal&lt;/a>, 阿里的另一个开源项目, 基于 Binlog&lt;/td>
&lt;td>work node 内存中的 ring buffer&lt;/td>
&lt;td>protobuf&lt;/td>
&lt;td>提供了一个完善的 admin ui&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Debezium&lt;/td>
&lt;td>提供 MySQL, MongoDB, PostgreSQL 三种 Connector&lt;/td>
&lt;td>Apache Kafka&lt;/td>
&lt;td>Apache Avro / json&lt;/td>
&lt;td>Snapshot mode 支持全量导入数据表&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="databus">databus&lt;/h2>
&lt;p>Linkedin databus 的论文有很强的指导性，但它的 MySQL 变更抓取模块很不成熟，官方支持的是 Oracle，MySQL 只是使用另一个开源组件 OpenReplicator 做了一个 demo。另一个不利因素 databus 使用了自己实现的一个 Relay 作为变更分发平台，相比于使用开源消息队列的方案，这对维护和外部集成都不友好。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s2.ax1x.com/2019/10/08/ufabO1.png" alt="databus 架构图" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="otter--canal">Otter &amp;amp; Canal&lt;/h2>
&lt;p>Otter 和 Canal 在国内相当知名，Canal 还支持了阿里云 DRDS 的二级索引构建和小表同步，工程稳定性上有保障。但 Otter 本身无法很好地支持多表聚合到新表，开源版本也不支持同步到分片表当中，能够采取的一个折衷方案是直接将 Canal 订阅的变更写入消息队列，自己写下游程序实现聚合同步等逻辑。该方案也是我们的候选方案。&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img src="https://s2.ax1x.com/2019/10/08/ufdATf.png" alt="Otter 架构图" loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>&lt;/p>
&lt;h2 id="data-pipeline">data pipeline&lt;/h2>
&lt;p>Yelp’s data pipeline 是一个大而全的解决方案。它使用 Mysql-Streamer（一个通过 Binlog 实现的 MySQL CDC 模块）将所有的数据库变更写入 Kafka，并提供了 Schematizer 这样的 Schema 注册中心和定制化的 Python 客户端库解决通信问题。遗憾的是该方案是 Python 构建的，与我们的 Java 技术栈相性不佳。&lt;/p>
&lt;h2 id="debezium">Debezium&lt;/h2>
&lt;p>Debezium 不同于上面的解决方案，它只专注于 CDC，它的亮点有:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>支持 MySQL、MongoDB、PostgreSQL 三种数据源的变更抓取，并且社区正在开发 Oracle 与 Cassandra 支持；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Snapshot Mode 可以将表中的现有数据全部导入 Kafka，并且全量数据与增量数据形式一致，可以统一处理；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>利用了 Kafka 的 Log Compaction 特性，变更数据可以实现”不过期”永久保存；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>利用了 Kafka Connect，自动拥有高可用与开箱即用的调度接口；&lt;/p>
&lt;/li>
&lt;li>
&lt;p>社区活跃：Debezium 很年轻，面世不到 1 年，但它的 Gitter 上每天都有百余条技术讨论，并且有两位 Redhat 全职工程师进行维护；&lt;/p>
&lt;/li>
&lt;/ul></description></item><item><title>数据转换与检索</title><link>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%E4%B8%8E%E6%A3%80%E7%B4%A2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E6%95%B0%E6%8D%AE%E8%BD%AC%E6%8D%A2%E4%B8%8E%E6%A3%80%E7%B4%A2/</guid><description>&lt;h1 id="数据转换与检索">数据转换与检索&lt;/h1>
&lt;p>数据转换是一个业务性很强的处理步骤。当数据进入汇集层后，一般会用于两个典型的后继处理场景：数仓构建和数据流服务。数仓构建包括模型定义和预计算两部分。数据工程师根据业务分析需要，使用星型或雪花模型设计数据仓库结构，利用数据仓库中间件完成模型构建和更新。&lt;/p>
&lt;p>如前文所述，源端采集的数据建议放入一个汇集层，优选是类似 Kafka 这样的消息队列。包括 Kylin 和 Druid 在内的数据仓库可以直接以流式的方式消费数据进行更新。一种常见的情形为：原始采集的数据格式、粒度不一定满足数据仓库中表结构的需要，而数仓提供的配置灵活度可能又不足够。这种情况下需要在进入数仓前对数据做额外的处理。&lt;/p>
&lt;p>常见的处理包括过滤、字段替换、嵌套结构一拆多、维度填充等，以上皆为无状态的转换。有状态的转换，例如 SUM、COUNT 等，在此过程中较少被使用，因为数仓本身就提供了这些聚合能力。数据流服务的构建则是基于流式计算引擎，对汇集层的数据进一步加工计算，并将结果实时输出给下游应用系统。&lt;/p></description></item><item><title>一致性语义</title><link>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E4%B8%80%E8%87%B4%E6%80%A7%E8%AF%AD%E4%B9%89/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E4%B8%80%E8%87%B4%E6%80%A7%E8%AF%AD%E4%B9%89/</guid><description>&lt;h1 id="一致性语义">一致性语义&lt;/h1>
&lt;p>批量同步需要以一种事务性的方式完成同步，无论是同步一整块的历史数据，还是同步某一天的增量，该部分数据到目的地，必须是以事务性的方式出现的。而不是在同步一半时，数据就已经在目的地出现了，这可能会影响下游的一些计算逻辑。并且作为一个数据融合产品，当用户在使用 DataPipeline 时，通常需要将存量数据同步完，后面紧接着去接增量。然后存量与增量之间需要进行一个无缝切换，中间的数据不要丢、也不要多。&lt;/p>
&lt;p>DataPipeline 作为一个产品，在客户的环境中，我们无法对客户数据本身的特性提出强制要求。我们不能要求客户数据一定要有主键或者有唯一性的索引。所以在不同场景下，对于一致性语义保证，用户的要求也不一样的：比如在有主键的场景下，一般我们做到至少有一次就够了，因为在下游如果对方也是一个类似于关系型数据库这样的目的地，其本身就有去重能力，不需要在过程中间做一个强一致的保证。但是，如果其本身没有主键，或者其下游是一个文件系统，如果不在过程中间做额外的一致性保证，就有可能在目的地产生多余的数据，这部分数据对于下游可能会造成非常严重的影响。&lt;/p>
&lt;h1 id="数据一致性的链路视角">数据一致性的链路视角&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>在源端做一个一致性抽取，即当数据从通过数据连接器写入到 MQ 时，和与其对应的 offset 必须是以事务方式进入 MQ 的。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一致性处理，譬如 Flink 提供了一个端到端一致性处理的能力，它是内部通过 checkpoint 机制，并结合 Sink 端的二阶段提交协议，实现从数据读取处理到写入的一个端到端事务一致性。其它框架，例如 Spark Streaming 和 Kafka Streams 也有各自的机制来实现一致性处理。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一致性写入，在 MQ 模式下，一致性写入，即 consumer offset 跟实际的数据写入目的时，必须是同时持久化的，要么全都成功，要么全部失败。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>一致性衔接，在 DataPipeline 的产品应用中，历史数据与实时数据的传输有时需要在一个任务中共同完成。所以产品本身需要有这种一致性衔接的能力，即历史数据和流式数据，必须能够在一个任务中，由程序自动完成它们之间的切换。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="links">Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://mp.weixin.qq.com/s/Ws0hy3XY6Bry4AmsMBjOiw" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Ws0hy3XY6Bry4AmsMBjOiw&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>运行环境与引擎</title><link>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%BC%95%E6%93%8E/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/dataengineering-series/3.%E6%95%B0%E6%8D%AE%E9%9B%86%E6%88%90/datapipeline/%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E4%B8%8E%E5%BC%95%E6%93%8E/</guid><description>&lt;h1 id="运行环境">运行环境&lt;/h1>
&lt;p>无论采用何种数据变化捕获技术，程序必须在一个可靠的平台运行。该平台需要解决分布式系统的一些共性问题，主要包括：水平扩展、容错、进度管理等。&lt;/p>
&lt;h2 id="水平扩展">水平扩展&lt;/h2>
&lt;p>程序必须能够以分布式 job 的形式在集群中运行，从而允许在业务增长时通过增加运行时节点的方式实现扩展。&lt;/p>
&lt;p>因为在一个规模化的企业中，通常要同时运行成百上千的 job。随着业务的增长，job 的数量以及 job 的负载还有可能持续增长。&lt;/p>
&lt;h2 id="容错">容错&lt;/h2>
&lt;p>分布式运行环境的执行节点可能因为过载、网络连通性等原因无法正常工作。&lt;/p>
&lt;p>当节点出现问题时，运行环境需要能够及时监测到，并将问题节点上的 job 分配给健康的节点继续运行。&lt;/p>
&lt;h2 id="进度管理">进度管理&lt;/h2>
&lt;p>job 需要记录自身处理的进度，避免重复处理数据。另外，job 会因为上下游系统的问题、网络连通性、程序 bug 等各种原因异常中止，当 job 重启后，必须能够从上次记录的正常进度位置开始处理后继的数据。&lt;/p>
&lt;p>有许多优秀的开源框架都可以满足上述要求，包括 Kafka Connect、Spark、Flink 等。&lt;/p>
&lt;p>Kafka Connect 是一个专注数据进出 Kafka 的数据集成框架。Spark 和 Flink 则更为通用，既可以用于数据集成，也适用于更加复杂的应用场景，例如机器学习的模型训练和流式计算。&lt;/p>
&lt;p>就数据集成这一应用场景而言，不同框架的概念是非常类似的。&lt;/p>
&lt;p>首先，框架提供 Source Connector 接口封装对数据源的访问。应用开发者基于这一接口开发适配特定数据源的 Connector，实现数据抽取逻辑和进度（offset）更新逻辑。&lt;/p>
&lt;p>其次，框架提供一个分布式的 Connector 运行环境，处理任务的分发、容错和进度更新等问题。&lt;/p>
&lt;p>不同之处在于，Kafka Connect 总是将数据抽取到 Kafka，而对于 Spark 和 Flink，Source Connector 是将数据抽取到内存中构建对象，写入目的地是由程序逻辑定义的，包括但不限于消息队列。&lt;/p>
&lt;p>但无论采用何种框架，都建议首先将数据写入一个汇集层，通常是 Kafka 这样的消息队列。单就数据源采集而言，Kafka Connect 这样专注于数据集成的框架是有一定优势的，这主要体现在两方面：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>首先是 Connector 的丰富程度，几乎所有较为流行的数据库、对象存储、文件系统都有开源的 Connector 实现。尤其在数据库的 CDC 方面，有 Debezium 这样优秀的开源项目存在，降低了应用的成本。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>其次是开发的便捷性，专有框架的设计相较于通用框架更为简洁，开发新的 Connector 门槛较低。Kafka Connect 的 runtime 实现也较为轻量，出现框架级别问题时 debug 也比较便捷。&lt;/p>
&lt;/li>
&lt;/ul>
&lt;h1 id="引擎对比">引擎对比&lt;/h1>
&lt;p>数据流服务的构建则是基于流式计算引擎，对汇集层的数据进一步加工计算，并将结果实时输出给下游应用系统。这涉及到流式计算引擎的选择：Spark Streaming、Flink、还是 Kafka Streams&lt;/p>
&lt;h2 id="延迟性">延迟性&lt;/h2>
&lt;p>Spark 对流的支持是 MicroBatch，提供的是亚秒级的延迟，相较于 Flink 和 Kafka Streams 在实时性上要差一些。&lt;/p>
&lt;h2 id="应用模式">应用模式&lt;/h2>
&lt;p>Spark 和 Flink 都是将作业提交到计算集群上运行，需要搭建专属的运行环境。Kafka Streams 的作业是以普通 Java 程序方式运行，本质上是一个调用 Kafka Streaming API 的 Kafka Consumer，可以方便地嵌入各种应用。&lt;/p>
&lt;p>但相应的，用户需要自己解决作业程序在不同服务器上的分发问题，例如通过 K8s 集群方案进行应用的容器化部署。如果使用 KSQL，还需要部署 KSQL 的集群。&lt;/p>
&lt;h2 id="sql-支持">SQL 支持&lt;/h2>
&lt;p>三者都提供 Streaming SQL，但 Flink 的 SQL 支持要更为强大些，可以运行更加复杂的分组聚合操作。&lt;/p>
&lt;h2 id="eos">EOS&lt;/h2>
&lt;p>Flink 对于数据进出计算集群提供了框架级别的支持，这是通过结合 CheckPoint 机制和 Sink Connector 接口封装的二阶段提交协议实现的。&lt;/p>
&lt;p>Kafka Streams 利用 Kafka 事务性消息，可以实现“消费 - 计算 - 写入 Kafka“的 EOS，但当结果需要输出到 Kafka 以外的目的地时，还需要利用 Kafka Connect 的 Sink Connector。遗憾的是，Kafka Connect 不提供 Kafka 到其它类型 Sink 的 EOS 保证，需要用户自己实现。&lt;/p>
&lt;p>Spark Streaming 与 Kafka Streams 类似，在读取和计算过程中可以保证 EOS，但将结果输出到外部时，依然需要额外做一些工作来确保数据一致性。常见的方式包括：利用数据库的事务写入机制将 Offset 持久化到外部、利用主键保证幂等写入、参考二阶段提交协议做分布式事务等。&lt;/p></description></item></channel></rss>