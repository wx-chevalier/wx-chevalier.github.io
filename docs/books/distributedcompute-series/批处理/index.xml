<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>批处理 | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/distributedcompute-series/%E6%89%B9%E5%A4%84%E7%90%86/</link><atom:link href="https://ng-tech.icu/books/distributedcompute-series/%E6%89%B9%E5%A4%84%E7%90%86/index.xml" rel="self" type="application/rss+xml"/><description>批处理</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>批处理</title><link>https://ng-tech.icu/books/distributedcompute-series/%E6%89%B9%E5%A4%84%E7%90%86/</link></image><item><title>使用 Unix 工具的批处理</title><link>https://ng-tech.icu/books/distributedcompute-series/%E6%89%B9%E5%A4%84%E7%90%86/%E4%BD%BF%E7%94%A8-unix-%E5%B7%A5%E5%85%B7%E7%9A%84%E6%89%B9%E5%A4%84%E7%90%86/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/distributedcompute-series/%E6%89%B9%E5%A4%84%E7%90%86/%E4%BD%BF%E7%94%A8-unix-%E5%B7%A5%E5%85%B7%E7%9A%84%E6%89%B9%E5%A4%84%E7%90%86/</guid><description>&lt;h1 id="使用-unix-工具的批处理">使用 Unix 工具的批处理&lt;/h1>
&lt;p>我们从一个简单的例子开始。假设您有一台 Web 服务器，每次处理请求时都会在日志文件中附加一行。例如，使用 nginx 默认访问日志格式，日志的一行可能如下所示：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">216.58.210.78 - - &lt;span class="o">[&lt;/span>27/Feb/2015:17:55:11 +0000&lt;span class="o">]&lt;/span> &lt;span class="s2">&amp;#34;GET /css/typography.css HTTP/1.1&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">200&lt;/span> &lt;span class="m">3377&lt;/span> &lt;span class="s2">&amp;#34;http://test.com/&amp;#34;&lt;/span> &lt;span class="s2">&amp;#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)
&lt;/span>&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="s2">AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>日志的格式定义如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">$remote_addr&lt;/span> - &lt;span class="nv">$remote_user&lt;/span> &lt;span class="o">[&lt;/span>&lt;span class="nv">$time_local&lt;/span>&lt;span class="o">]&lt;/span> &lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="nv">$request&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">$status&lt;/span> &lt;span class="nv">$body_bytes_sent&lt;/span> &lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="nv">$http_referer&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span> &lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="nv">$http_user_agent&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>日志的这一行表明在 2015 年 2 月 27 日 17:55:11 UTC，服务器从客户端 IP 地址 216.58.210.78 接收到对文件/css/typography.css 的请求。用户没有被认证，所以$remote_user 被设置为连字符（-）。响应状态是 200（即请求成功），响应的大小是 3377 字节。网页浏览器是 Chrome 40，URL &lt;a href="http://test.com/" target="_blank" rel="noopener">http://test.com/&lt;/a> 的页面中的引用导致该文件被加载。&lt;/p>
&lt;h2 id="分析简单日志">分析简单日志&lt;/h2>
&lt;p>很多工具可以从这些日志文件生成关于网站流量的漂亮的报告，但为了练手，让我们使用基本的 Unix 功能创建自己的工具。例如，假设你想在你的网站上找到五个最受欢迎的网页。则可以在 Unix shell 中这样做：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">cat /var/log/nginx/access.log &lt;span class="p">|&lt;/span> &lt;span class="c1">#1 读取日志文件&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> awk &lt;span class="s1">&amp;#39;{print $7}&amp;#39;&lt;/span> &lt;span class="p">|&lt;/span> &lt;span class="c1">#2 将每一行按空格分割成不同的字段，每行只输出第七个字段，恰好是请求的URL。在我们的例子中是/css/typography.css&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> sort &lt;span class="p">|&lt;/span> &lt;span class="c1">#3 按字母顺序排列请求的URL列表。如果某个URL被请求过n次，那么排序后，文件将包含连续重复出现n次的该URL&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> uniq -c &lt;span class="p">|&lt;/span> &lt;span class="c1">#4 uniq命令通过检查两个相邻的行是否相同来过滤掉输入中的重复行。-c则表示还要输出一个计数器：对于每个不同的URL，它会报告输入中出现该URL的次数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> sort -r -n &lt;span class="p">|&lt;/span> &lt;span class="c1">#5 第二种排序按每行起始处的数字（-n）排序，这是URL的请求次数。然后逆序（-r）返回结果，大的数字在前&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> head -n &lt;span class="m">5&lt;/span> &lt;span class="c1">#6 最后，只输出前五行（-n 5），并丢弃其余的&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>最后输出的结果如下：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">&lt;span class="m">4189&lt;/span> /favicon.ico
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">3631&lt;/span> /2013/05/24/improving-security-of-ssh-private-keys.html
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">2124&lt;/span> /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">1369&lt;/span> /
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="m">915&lt;/span> /css/typography.css
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Unix 工具非常强大，能在几秒钟内处理几 GB 的日志文件，并且您可以根据需要轻松修改命令。例如，如果要从报告中省略 CSS 文件，可以将 awk 参数更改为 &lt;code>'$7 !~ /\.css$/ {print \$7}'&lt;/code>,如果想统计最多的客户端 IP 地址,可以把 awk 参数改为 &lt;code>'{print $1}'&lt;/code> 等等。&lt;/p>
&lt;h2 id="命令链与自定义程序">命令链与自定义程序&lt;/h2>
&lt;p>除了 Unix 命令链，你还可以写一个简单的程序来做同样的事情。例如在 Ruby 中，它可能看起来像这样：&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-sh" data-lang="sh">&lt;span class="line">&lt;span class="cl">&lt;span class="nv">counts&lt;/span> &lt;span class="o">=&lt;/span> Hash.new&lt;span class="o">(&lt;/span>0&lt;span class="o">)&lt;/span> &lt;span class="c1"># 1 counts是一个存储计数器的哈希表，保存了每个URL被浏览的次数，默认为0。&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">File.open&lt;span class="o">(&lt;/span>&lt;span class="s1">&amp;#39;/var/log/nginx/access.log&amp;#39;&lt;/span>&lt;span class="o">)&lt;/span> &lt;span class="k">do&lt;/span> &lt;span class="p">|&lt;/span>file&lt;span class="p">|&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> file.each &lt;span class="k">do&lt;/span> &lt;span class="p">|&lt;/span>line&lt;span class="p">|&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nv">url&lt;/span> &lt;span class="o">=&lt;/span> line.split&lt;span class="o">[&lt;/span>6&lt;span class="o">]&lt;/span> &lt;span class="c1"># 2 逐行读取日志，抽取每行第七个被空格分隔的字段为URL（这里的数组索引是6，因为Ruby的数组索引从0开始计数）&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> counts&lt;span class="o">[&lt;/span>url&lt;span class="o">]&lt;/span> +&lt;span class="o">=&lt;/span> &lt;span class="m">1&lt;/span> &lt;span class="c1"># 3 将日志当前行中URL对应的计数器值加一。&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> end
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">end
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nv">top5&lt;/span> &lt;span class="o">=&lt;/span> counts.map&lt;span class="o">{&lt;/span>&lt;span class="p">|&lt;/span>url, count&lt;span class="p">|&lt;/span> &lt;span class="o">[&lt;/span>count, url&lt;span class="o">]&lt;/span> &lt;span class="o">}&lt;/span>.sort.reverse&lt;span class="o">[&lt;/span>0...5&lt;span class="o">]&lt;/span> &lt;span class="c1"># 4 按计数器值（降序）对哈希表内容进行排序，并取前五位。&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">top5.each&lt;span class="o">{&lt;/span>&lt;span class="p">|&lt;/span>count, url&lt;span class="p">|&lt;/span> puts &lt;span class="s2">&amp;#34;#{count} #{url}&amp;#34;&lt;/span> &lt;span class="o">}&lt;/span> &lt;span class="c1"># 5 打印出前五个条目。&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="排序-vs-内存中的聚合">排序 VS 内存中的聚合&lt;/h2>
&lt;p>Ruby 脚本在内存中保存了一个 URL 的哈希表，将每个 URL 映射到它出现的次数。Unix 管道没有这样的哈希表，而是依赖于对 URL 列表的排序，在这个 URL 列表中，同一个 URL 的只是简单地重复出现。&lt;/p>
&lt;p>哪种方法更好？这取决于你有多少个不同的 URL。对于大多数中小型网站，你可能可以为所有不同网址提供一个计数器（假设我们使用 1GB 内存）。在此例中，作业的工作集（working set）（作业需要随机访问的内存大小）仅取决于不同 URL 的数量：如果日志中只有单个 URL，重复出现一百万次，则哈希表所需的空间表就只有一个 URL 加上一个计数器的大小。当工作集足够小时，内存哈希表表现良好，甚至在性能较差的笔记本电脑上也可以正常工作。&lt;/p>
&lt;p>另一方面，如果作业的工作集大于可用内存，则排序方法的优点是可以高效地使用磁盘。这与我们在“SSTables 和 LSM 树”中讨论过的原理是一样的：数据块可以在内存中排序并作为段文件写入磁盘，然后多个排序好的段可以合并为一个更大的排序文件。归并排序具有在磁盘上运行良好的顺序访问模式。（请记住，针对顺序 I/O 进行优化是第 3 章中反复出现的主题，相同的模式在此重现）&lt;/p>
&lt;p>GNU Coreutils（Linux）中的 sort 程序通过溢出至磁盘的方式来自动应对大于内存的数据集，并能同时使用多个 CPU 核进行并行排序。这意味着我们之前看到的简单的 Unix 命令链很容易扩展到大数据集，且不会耗尽内存。瓶颈可能是从磁盘读取输入文件的速度。&lt;/p></description></item></channel></rss>