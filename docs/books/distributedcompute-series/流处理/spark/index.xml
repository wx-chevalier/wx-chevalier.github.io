<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Spark | Next-gen Tech Edu</title><link>https://ng-tech.icu/books/distributedcompute-series/%E6%B5%81%E5%A4%84%E7%90%86/spark/</link><atom:link href="https://ng-tech.icu/books/distributedcompute-series/%E6%B5%81%E5%A4%84%E7%90%86/spark/index.xml" rel="self" type="application/rss+xml"/><description>Spark</description><generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><image><url>https://ng-tech.icu/media/sharing.png</url><title>Spark</title><link>https://ng-tech.icu/books/distributedcompute-series/%E6%B5%81%E5%A4%84%E7%90%86/spark/</link></image><item><title>代码开发</title><link>https://ng-tech.icu/books/distributedcompute-series/%E6%B5%81%E5%A4%84%E7%90%86/spark/%E4%BB%A3%E7%A0%81%E5%BC%80%E5%8F%91/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/distributedcompute-series/%E6%B5%81%E5%A4%84%E7%90%86/spark/%E4%BB%A3%E7%A0%81%E5%BC%80%E5%8F%91/</guid><description>&lt;h1 id="debugtest">DebugTest&lt;/h1>
&lt;h2 id="unittest">UnitTest&lt;/h2>
&lt;p>Spark 的任务需要允许在 Spark 的上下文中，可以用一个叫做 FindSpark 的包来辅助：&lt;/p>
&lt;pre tabindex="0">&lt;code>pip install findspark
&lt;/code>&lt;/pre>&lt;p>在初始化上下文之前，需要调用&lt;code>findspark.init()&lt;/code>方法，它会自动地根据本地的&lt;code>SPARK_HOME&lt;/code>环境变量来定位到 Spark 的 Python 依赖包。&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">unittest2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">logging&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">findspark&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">findspark&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">init&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">pyspark.context&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">SparkContext&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">ExampleTest&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">unittest2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">TestCase&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">setUp&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">SparkContext&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;local[4]&amp;#39;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">quiet_logs&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">tearDown&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stop&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">test_something&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># start by creating a mockup dataset&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">l&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[(&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;hello&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;world&amp;#39;&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s1">&amp;#39;world&amp;#39;&lt;/span>&lt;span class="p">)]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># and create a RDD out of it&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">rdd&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sc&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">parallelize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">l&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># pass it to the transformation you&amp;#39;re unit testing&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">non_trivial_transform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rdd&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># collect the results&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">output&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">result&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">collect&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># since it&amp;#39;s unit test let&amp;#39;s make an assertion&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">assertEqual&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">output&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">][&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">non_trivial_transform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">rdd&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="s2">&amp;#34;&amp;#34;&amp;#34; a transformation to unit test (word count) - defined here for convenience only&amp;#34;&amp;#34;&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">rdd&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">map&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">reduceByKey&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="k">lambda&lt;/span> &lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="n">a&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">if&lt;/span> &lt;span class="vm">__name__&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;__main__&amp;#34;&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">unittest2&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">main&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div></description></item><item><title>环境配置</title><link>https://ng-tech.icu/books/distributedcompute-series/%E6%B5%81%E5%A4%84%E7%90%86/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ng-tech.icu/books/distributedcompute-series/%E6%B5%81%E5%A4%84%E7%90%86/spark/%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/</guid><description>&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;h2 id="tutorial--docs">Tutorial &amp;amp; Docs&lt;/h2>
&lt;h1 id="quick-start">Quick Start&lt;/h1>
&lt;h2 id="deploy">Deploy&lt;/h2>
&lt;p>Spark 的部署方式可以独立部署，也可以基于 Yarn 或者 Mesos 这样的资源调度框架进行部署。&lt;/p>
&lt;ul>
&lt;li>[Spark 下载地址][1]&lt;/li>
&lt;/ul>
&lt;h3 id="standalone">StandAlone&lt;/h3>
&lt;p>将 Spark 的程序文件解压之后可以通过如下命令直接启动一个独立的主机：&lt;/p>
&lt;pre tabindex="0">&lt;code>./sbin/start-master.sh
&lt;/code>&lt;/pre>&lt;p>执行该命令之后 Spark 会自动执行 jetty 命令启动服务器，同时在命令行或者 log 日志文件中打印出系统地址，譬如：&lt;/p>
&lt;pre tabindex="0">&lt;code>15/05/28 13:20:57 INFO Master: Starting Spark master at spark://localhost.localdomain:7077
15/05/28 13:21:07 INFO MasterWebUI: Started MasterWebUI at http://192.168.199.166:8080
&lt;/code>&lt;/pre>&lt;p>给出的这个 spark://HOST:PORT 地址可以供 Spark work 节点连接或者作为 master 的参数传入到 SparkContext 中。&lt;/p>
&lt;p>需要启动 work 进程并在 master 中完成注册，可以使用如下命令：&lt;/p>
&lt;pre tabindex="0">&lt;code>./bin/spark-class org.apache.spark.deploy.worker.Worker spark://IP:PORT
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>注意，这边的 IP 地址实际上指的是启动 master 时候他监听的域名而来的 IP 地址。&lt;/p>
&lt;/blockquote>
&lt;p>![enter description here][2]&lt;/p>
&lt;p>执行上述命令时可以传入的参数为：&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Argument&lt;/th>
&lt;th>Meaning&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>-h HOST, &amp;ndash;host HOST&lt;/td>
&lt;td>Hostname to listen on&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>-i HOST, &amp;ndash;ip HOST&lt;/td>
&lt;td>Hostname to listen on (deprecated, use -h or &amp;ndash;host)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>-p PORT, &amp;ndash;port PORT&lt;/td>
&lt;td>Port for service to listen on (default: 7077 for master, random for worker)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&amp;ndash;webui-port PORT&lt;/td>
&lt;td>Port for web UI (default: 8080 for master, 8081 for worker)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>-c CORES, &amp;ndash;cores CORES&lt;/td>
&lt;td>Total CPU cores to allow Spark applications to use on the machine (default: all available); only on worker&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>-m MEM, &amp;ndash;memory MEM&lt;/td>
&lt;td>Total amount of memory to allow Spark applications to use on the machine, in a format like 1000M or 2G (default: your machine&amp;rsquo;s total RAM minus 1 GB); only on worker&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>-d DIR, &amp;ndash;work-dir DIR&lt;/td>
&lt;td>Directory to use for scratch space and job output logs (default: SPARK_HOME/work); only on worker&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&amp;ndash;properties-file FILE&lt;/td>
&lt;td>Path to a custom Spark properties file to load (default: conf/spark-defaults.conf)&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h4 id="cluster-launch">Cluster Launch&lt;/h4>
&lt;ul>
&lt;li>sbin/start-master.sh - Starts a master instance on the machine the script is executed on.&lt;/li>
&lt;li>sbin/start-slaves.sh - Starts a slave instance on each machine specified in the conf/slaves file.&lt;/li>
&lt;li>sbin/start-all.sh - Starts both a master and a number of slaves as described above.&lt;/li>
&lt;li>sbin/stop-master.sh - Stops the master that was started via the bin/start-master.sh script.&lt;/li>
&lt;li>sbin/stop-slaves.sh - Stops all slave instances on the machines specified in the conf/slaves file.&lt;/li>
&lt;li>sbin/stop-all.sh - Stops both the master and the slaves as described above.&lt;/li>
&lt;/ul>
&lt;h2 id="docker">Docker&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="http://blog.csdn.net/yeasy/article/details/48654965" target="_blank" rel="noopener">基于 Docker 的 Spark 集群搭建&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="application-submit">Application Submit&lt;/h2>
&lt;pre tabindex="0">&lt;code>./bin/spark-submit \
--class &amp;lt;main-class&amp;gt;
--master &amp;lt;master-url&amp;gt; \
--deploy-mode &amp;lt;deploy-mode&amp;gt; \
--conf &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; \
... # other options
&amp;lt;application-jar&amp;gt; \
[application-arguments]
&lt;/code>&lt;/pre>&lt;blockquote>
&lt;p>参数如下：&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&amp;ndash;class: The entry point for your application (e.g. org.apache.spark.examples.SparkPi)&lt;/li>
&lt;li>&amp;ndash;master: The master URL for the cluster (e.g. spark://23.195.26.187:7077)&lt;/li>
&lt;li>&amp;ndash;deploy-mode: Whether to deploy your driver on the worker nodes (cluster) or locally as an external client (client) (default: client)&lt;/li>
&lt;li>&amp;ndash;conf: Arbitrary Spark configuration property in key=value format. For values that contain spaces wrap “key=value” in quotes (as shown).&lt;/li>
&lt;li>application-jar: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an hdfs:// path or a file:// path that is present on all nodes.&lt;/li>
&lt;li>application-arguments: Arguments passed to the main method of your main class, if any&lt;/li>
&lt;/ul>
&lt;h3 id="standalone-1">StandAlone&lt;/h3>
&lt;pre tabindex="0">&lt;code># Run application locally on 8 cores
./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local[8] \
/path/to/examples.jar \
100
# Run on a Spark Standalone cluster in client deploy mode
./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://207.184.161.138:7077 \
--executor-memory 20G \
--total-executor-cores 100 \
/path/to/examples.jar \
1000
# Run on a Spark Standalone cluster in cluster deploy mode with supervise
./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master spark://207.184.161.138:7077 \
--deploy-mode cluster
--supervise
--executor-memory 20G \
--total-executor-cores 100 \
/path/to/examples.jar \
1000
&lt;/code>&lt;/pre>&lt;p>在任务提交之后，在管理界面会显示：&lt;/p>
&lt;p>![enter description here][3]&lt;/p>
&lt;h1 id="program">Program&lt;/h1>
&lt;h2 id="initializing-spark">Initializing Spark&lt;/h2>
&lt;p>编写 Spark 程序的首先是需要创建一个 JavaSparkContext 对象，用于确定如何去连接到一个集群中。而如果需要创建一个 SparkContext 对象则需要新创建一个包含应用的基本信息 SparkConf 对象。&lt;/p>
&lt;pre tabindex="0">&lt;code>SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
JavaSparkContext sc = new JavaSparkContext(conf);
&lt;/code>&lt;/pre>&lt;p>上述代码中的 appName 是该应用的名称，而 master 指 Spark 进程的 URL，如果对于 StandAlone 进程既是类似于 spark://Host:Port&lt;/p>
&lt;h2 id="resilient-distributed-datasets-rdds">Resilient Distributed Datasets (RDDs)&lt;/h2>
&lt;h3 id="parallelized-collections">Parallelized Collections&lt;/h3>
&lt;pre tabindex="0">&lt;code>List&amp;lt;Integer&amp;gt; data = Arrays.asList(1, 2, 3, 4, 5);
JavaRDD&amp;lt;Integer&amp;gt; distData = sc.parallelize(data);
&lt;/code>&lt;/pre>&lt;h3 id="external-datasets">External Datasets&lt;/h3>
&lt;h3 id="rdd-operations">RDD Operations&lt;/h3>
&lt;p>注意，由于 Spark 中采用了大量的异步操作，并不能像普通的 Java 程序中一样去同步进行遍历，大量的遍历等操作是利用类似于回调的方式构造的。&lt;/p></description></item></channel></rss>