
---
title: 35-分布式训练
linktitle: 35-分布式训练
type: book
commentable: true
---

## 35-分布式训练

### 本节目录

- [1.分布式计算](#1分布式计算)
- [2. GPU 机器架构](#2-gpu机器架构)
  - [2.1 样例：计算一个小批量](#21-样例计算一个小批量)
  - [2.2 总结](#22-总结)
- [3. 关于性能](#3--关于性能)
  - [3.1 对于**同步 SGD**：](#31-对于同步sgd)
  - [3.2 **性能**：](#32-性能)
  - [3.3 性能的权衡](#33-性能的权衡)
- [4. 实践时的建议](#4-实践时的建议)
- [5. 总结](#5-总结)

### 1.分布式计算

- 本质上来说和之前讲的单机多卡并行没有区别。二者之间的区别是分布式计算是通过网络把数据从一台机器搬到另一台机器

<img src="https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/35/35-01.png" alt="image" width="300" />

### 2. GPU 机器架构

- 总的来说，gpu 到 gpu 的通讯是很快的，gpu 到 cpu 慢一点。机器到机器更慢。因而总体性能的关键就是尽量在本地做通讯而少在机器之间做通讯

##### 2.1 样例：计算一个小批量

- 每个 worker 从参数服务器那里获取模型参数：首先把样本复制到机器的内存，然后把样本分到每个 gpu 上
- 复制参数到每个 gpu 上：同样，先把每一次的参数放到内存里，然后再复制到每个 gpu 上
- 每个 gpu 计算梯度
- 再主内存上把所有 gpu 上的梯度加起来
- 梯度从主内存传回服务器
- 每个服务器对梯度求和，并更新参数

##### 2.2 总结

- 由于 gpu 到 gpu 和 gpu 到内存的通讯速度还不错，因此我们尽量再本地做聚合（如梯度相加），并减少再网络上的多次通讯

### 3. 关于性能

##### 3.1 对于**同步 SGD**：

- 这里每个 worker 都是同步计算一个批量，称为同步 SGD
- 假设有 n 个 ggpu，每个 gpu 每次处理 b 个样本，那么同步 SGD 等价于再单 gpu 运行批量大小为 nb 的 SGD
- 再理想情况下，n 个 gpu 可以得到相对单 gpu 的 n 倍加速

##### 3.2 **性能**：

- t1 = 在单 gpu 上计算 b 个样本梯度时间
- 假设有 m 个参数，一个 worker 每次发送和接受 m 个参数、梯度
  - t2 = 发送和接受所用时间
- 每个批量的计算时间为 max（t1，t2）
  - 选取足够大的 b 使 t1>t2
  - 增加 b 或 n 导致更大的批量 大小，当值需要更多计算来得到给定的模型精度

##### 3.3 性能的权衡

<img src="https://assets.ng-tech.icu/book/DeepLearning-MuLi-Notes/imgs/35/35-02.png" alt="image" width="300" />

### 4. 实践时的建议

- 使用一个大数据集
- 需要好的 gpu-gpu 和机器-机器带宽
- 高效的数据读取和预处理
- 模型需要有好的计算和通讯比
  - Inception>ResNet>AlexNet
- 使用足够大的批量大小来得到更好的系统性能
- 使用高效的优化算法对应大批量大小

### 5. 总结

- 分布式同步数据并行是多 gpu 数据并行在多机器上的拓展
- 网络通讯通常是瓶颈
- 需要注意使用特别大的批量大小时的收敛效率
- 更复杂的分布式有异步、模型并行（这里没有介绍）

    