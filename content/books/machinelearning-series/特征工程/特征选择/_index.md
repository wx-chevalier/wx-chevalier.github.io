
---
title: 特征选择
linktitle: 特征选择
type: book
commentable: true
---

# 数据规约

数据归约技术可以用来得到数据集的归约表示，它小得多，但仍接近地保持原数据的完整性。这样，在归约后的数据集上挖掘将更有效，并产生相同(或几乎相同)的分析结果。用于数据分析的数据可能包含数以百计的属性，其中大部分属性与挖掘任务不相关，是冗余的。维度归约通过删除不相关的属性，来减少数据量，并保证信息的损失最小。

# 属性子集选择

目标是找出最小属性集，使得数据类的概率分布尽可能地接近使用所有属性的原分布。在压缩 的属性集上挖掘还有其它的优点。它减少了出现在发现模式上的属性的数目，使得模式更易于理解。

- 逐步向前选择：该过程由空属性集开始，选择原属性集中最好的属性，并将它添加到该集合中。在其后的每一次迭代，将原属性集剩下的属性中的最好的属性添加到该集合中。

- 逐步向后删除：该过程由整个属性集开始。在每一步，删除掉尚在属性集中的最坏属性。

- 向前选择和向后删除的结合：向前选择和向后删除方法可以结合在一起，每一步选择一个最 好的属性，并在剩余属性中删除一个最坏的属性。

Python scikit-learn 中的递归特征消除算法 Recursive feature elimination (RFE)，就是利用这样的思想进行特征子集筛选的，一般考虑建立 SVM 或回归模型。

# 单变量重要性

分析单变量和目标变量的相关性，删除预测能力较低的变量。这种方法不同于属性子集选择，通常从统计学和信息的角度去分析。

- pearson 相关系数和卡方检验，分析目标变量和单变量的相关性。

- 回归系数：训练线性回归或逻辑回归，提取每个变量的表决系数，进行重要性排序。

- 树模型的 Gini 指数：训练决策树模型，提取每个变量的重要度，即 Gini 指数进行排序。

- Lasso 正则化：训练回归模型时，加入 L1 正则化参数，将特征向量稀疏化。

- IV 指标：风控模型中，通常求解每个变量的 IV 值，来定义变量的重要度，一般将阀值设定在 0.02 以上。

通常的做法是根据业务需求来定，如果基于业务的用户或商品特征，需要较多的解释性，考虑采用统计上的一些方法，如变量的分布曲线，直方图等，再计算相关性指标，最后去考虑一些模型方法。如果建模需要，则通常采用模型方法去筛选特征，如果用一些更为复杂的 GBDT，DNN 等模型，建议不做特征选择，而做特征交叉。

    