
---
title: 模型层
linktitle: 模型层
type: book
commentable: true
---

# 模型层

深度学习模型一般由各种模型层组合而成。tf.keras.layers 内置了非常丰富的各种功能的模型层。例如，layers.Dense,layers.Flatten,layers.Input,layers.DenseFeature,layers.Dropout,layers.Conv2D,layers.MaxPooling2D,layers.Conv1D,layers.Embedding,layers.GRU,layers.LSTM,layers.Bidirectional 等等。

如果这些内置模型层不能够满足需求，我们也可以通过编写 tf.keras.Lambda 匿名模型层或继承 tf.keras.layers.Layer 基类构建自定义的模型层。其中 tf.keras.Lambda 匿名模型层只适用于构造没有学习参数的模型层。

# 内置模型层

## 基础层

- Dense：密集连接层。参数个数 = 输入层特征数 × 输出层特征数(weight)＋ 输出层特征数(bias)
- Activation：激活函数层。一般放在 Dense 层后面，等价于在 Dense 层中指定 activation。
- Dropout：随机置零层。训练期间以一定几率将输入置 0，一种正则化手段。
- BatchNormalization：批标准化层。通过线性变换将输入批次缩放平移到稳定的均值和标准差。可以增强模型对输入不同分布的适应性，加快模型训练速度，有轻微正则化效果。一般在激活函数之前使用。
- SpatialDropout2D：空间随机置零层。训练期间以一定几率将整个特征图置 0，一种正则化手段，有利于避免特征图之间过高的相关性。
- Input：输入层。通常使用 Functional API 方式构建模型时作为第一层。
- DenseFeature：特征列接入层，用于接收一个特征列列表并产生一个密集连接层。
- Flatten：压平层，用于将多维张量压成一维。
- Reshape：形状重塑层，改变输入张量的形状。
- Concatenate：拼接层，将多个张量在某个维度上拼接。
- Add：加法层。
- Subtract：减法层。
- Maximum：取最大值层。
- Minimum：取最小值层。

## 卷积网络相关层

- Conv1D：普通一维卷积，常用于文本。参数个数 = 输入通道数 × 卷积核尺寸(如 3)× 卷积核个数
- Conv2D：普通二维卷积，常用于图像。参数个数 = 输入通道数 × 卷积核尺寸(如 3 乘 3)× 卷积核个数
- Conv3D：普通三维卷积，常用于视频。参数个数 = 输入通道数 × 卷积核尺寸(如 3 乘 3 乘 3)× 卷积核个数
- SeparableConv2D：二维深度可分离卷积层。不同于普通卷积同时对区域和通道操作，深度可分离卷积先操作区域，再操作通道。即先对每个通道做独立卷积操作区域，再用 1 乘 1 卷积跨通道组合操作通道。参数个数 = 输入通道数 × 卷积核尺寸 + 输入通道数 ×1×1× 输出通道数。深度可分离卷积的参数数量一般远小于普通卷积，效果一般也更好。
- DepthwiseConv2D：二维深度卷积层。仅有 SeparableConv2D 前半部分操作，即只操作区域，不操作通道，一般输出通道数和输入通道数相同，但也可以通过设置 depth_multiplier 让输出通道为输入通道的若干倍数。输出通道数 = 输入通道数 × depth_multiplier。参数个数 = 输入通道数 × 卷积核尺寸 × depth_multiplier。
- Conv2DTranspose：二维卷积转置层，俗称反卷积层。并非卷积的逆操作，但在卷积核相同的情况下，当其输入尺寸是卷积操作输出尺寸的情况下，卷积转置的输出尺寸恰好是卷积操作的输入尺寸。
- LocallyConnected2D: 二维局部连接层。类似 Conv2D，唯一的差别是没有空间上的权值共享，所以其参数个数远高于二维卷积。
- MaxPool2D: 二维最大池化层。也称作下采样层。池化层无可训练参数，主要作用是降维。
- AveragePooling2D: 二维平均池化层。
- GlobalMaxPool2D: 全局最大池化层。每个通道仅保留一个值。一般从卷积层过渡到全连接层时使用，是 Flatten 的替代方案。
- GlobalAvgPool2D: 全局平均池化层。每个通道仅保留一个值。

## 循环网络相关层

- Embedding：嵌入层。一种比 Onehot 更加有效的对离散特征进行编码的方法。一般用于将输入中的单词映射为稠密向量。嵌入层的参数需要学习。
- LSTM：长短记忆循环网络层。最普遍使用的循环网络层。具有携带轨道，遗忘门，更新门，输出门。可以较为有效地缓解梯度消失问题，从而能够适用长期依赖问题。设置 return_sequences = True 时可以返回各个中间步骤输出，否则只返回最终输出。
- GRU：门控循环网络层。LSTM 的低配版，不具有携带轨道，参数数量少于 LSTM，训练速度更快。
- SimpleRNN：简单循环网络层。容易存在梯度消失，不能够适用长期依赖问题。一般较少使用。
- ConvLSTM2D：卷积长短记忆循环网络层。结构上类似 LSTM，但对输入的转换操作和对状态的转换操作都是卷积运算。
- Bidirectional：双向循环网络包装器。可以将 LSTM，GRU 等层包装成双向循环网络。从而增强特征提取能力。
- RNN：RNN 基本层。接受一个循环网络单元或一个循环单元列表，通过调用 tf.keras.backend.rnn 函数在序列上进行迭代从而转换成循环网络层。
- LSTMCell：LSTM 单元。和 LSTM 在整个序列上迭代相比，它仅在序列上迭代一步。可以简单理解 LSTM 即 RNN 基本层包裹 LSTMCell。
- GRUCell：GRU 单元。和 GRU 在整个序列上迭代相比，它仅在序列上迭代一步。
- SimpleRNNCell：SimpleRNN 单元。和 SimpleRNN 在整个序列上迭代相比，它仅在序列上迭代一步。
- AbstractRNNCell：抽象 RNN 单元。通过对它的子类化用户可以自定义 RNN 单元，再通过 RNN 基本层的包裹实现用户自定义循环网络层。
- Attention：Dot-product 类型注意力机制层。可以用于构建注意力模型。
- AdditiveAttention：Additive 类型注意力机制层。可以用于构建注意力模型。
- TimeDistributed：时间分布包装器。包装后可以将 Dense、Conv2D 等作用到每一个时间片段上。

# 自定义模型层

如果自定义模型层没有需要被训练的参数，一般推荐使用 Lamda 层实现，如果自定义模型层有需要被训练的参数，则可以通过对 Layer 基类子类化实现。

## Lambda

Lambda 层由于没有需要被训练的参数，只需要定义正向传播逻辑即可，使用比 Layer 基类子类化更加简单；Lambda 层的正向逻辑可以使用 Python 的 lambda 函数来表达，也可以用 def 关键字定义函数来表达。

```py
import tensorflow as tf
from tensorflow.keras import layers,models,regularizers

mypower = layers.Lambda(lambda x:tf.math.pow(x,2))
mypower(tf.range(5))

<tf.Tensor: shape=(5,), dtype=int32, numpy=array([ 0,  1,  4,  9, 16], dtype=int32)>
```

## Layer

Layer 的子类化一般需要重新实现初始化方法，Build 方法和 Call 方法。下面是一个简化的线性层的范例，类似 Dense.

```py
class Linear(layers.Layer):
    def __init__(self, units=32, **kwargs):
        super(Linear, self).__init__(**kwargs)
        self.units = units

    #build方法一般定义Layer需要被训练的参数。
    def build(self, input_shape):
        self.w = self.add_weight("w",shape=(input_shape[-1], self.units),
                                 initializer='random_normal',
                                 trainable=True) #注意必须要有参数名称"w",否则会报错
        self.b = self.add_weight("b",shape=(self.units,),
                                 initializer='random_normal',
                                 trainable=True)
        super(Linear,self).build(input_shape) # 相当于设置self.built = True

    #call方法一般定义正向传播运算逻辑，__call__方法调用了它。
    @tf.function
    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

    #如果要让自定义的Layer通过Functional API 组合成模型时可以被保存成h5模型，需要自定义get_config方法。
    def get_config(self):
        config = super(Linear, self).get_config()
        config.update({'units': self.units})
        return config
```

```py
linear = Linear(units = 8)
print(linear.built)
#指定input_shape，显式调用build方法，第0维代表样本数量，用None填充
linear.build(input_shape = (None,16))
print(linear.built)

False
True

linear = Linear(units = 8)
print(linear.built)
linear.build(input_shape = (None,16))
print(linear.compute_output_shape(input_shape = (None,16)))

False
(None, 8)

linear = Linear(units = 16)
print(linear.built)
#如果built = False，调用__call__时会先调用build方法, 再调用call方法。
linear(tf.random.uniform((100,64)))
print(linear.built)
config = linear.get_config()
print(config)

False
True
{'name': 'linear_3', 'trainable': True, 'dtype': 'float32', 'units': 16}

tf.keras.backend.clear_session()

model = models.Sequential()
#注意该处的input_shape会被模型加工，无需使用None代表样本数量维
model.add(Linear(units = 1,input_shape = (2,)))
print("model.input_shape: ",model.input_shape)
print("model.output_shape: ",model.output_shape)
model.summary()
model.input_shape:  (None, 2)
model.output_shape:  (None, 1)

Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
linear (Linear)              (None, 1)                 3
=================================================================
Total params: 3
Trainable params: 3
Non-trainable params: 0
_________________________________________________________________

model.compile(optimizer = "sgd",loss = "mse",metrics=["mae"])
print(model.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))


# 保存成 h5模型
model.save("./data/linear_model.h5",save_format = "h5")
model_loaded_keras = tf.keras.models.load_model(
    "./data/linear_model.h5",custom_objects={"Linear":Linear})
print(model_loaded_keras.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))


# 保存成 tf 模型
model.save("./data/linear_model",save_format = "tf")
model_loaded_tf = tf.keras.models.load_model("./data/linear_model")
print(model_loaded_tf.predict(tf.constant([[3.0,2.0],[4.0,5.0]])))

[[-0.04092304]
 [-0.06150477]]
[[-0.04092304]
 [-0.06150477]]
INFO:tensorflow:Assets written to: ./data/linear_model/assets
[[-0.04092304]
 [-0.06150477]]
```

    